<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Session 3: Introduction to Bayesian modeling | Open Science Synthesis for the Delta Science Program: Week 2</title>
  <meta name="description" content="4 Session 3: Introduction to Bayesian modeling | Open Science Synthesis for the Delta Science Program: Week 2" />
  <meta name="generator" content="bookdown 0.24.2 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Session 3: Introduction to Bayesian modeling | Open Science Synthesis for the Delta Science Program: Week 2" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Session 3: Introduction to Bayesian modeling | Open Science Synthesis for the Delta Science Program: Week 2" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="session-2-collaborative-synthesis.html"/>
<link rel="next" href="session-4-collaborative-synthesis.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.6.1/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Open Science Synthesis for the Delta Science Program: Week 2</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#schedule"><i class="fa fa-check"></i><b>0.1</b> Schedule</a>
<ul>
<li class="chapter" data-level="0.1.1" data-path="index.html"><a href="index.html#code-of-conduct"><i class="fa fa-check"></i><b>0.1.1</b> Code of Conduct</a></li>
<li class="chapter" data-level="0.1.2" data-path="index.html"><a href="index.html#logistics"><i class="fa fa-check"></i><b>0.1.2</b> Logistics</a></li>
<li class="chapter" data-level="0.1.3" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i><b>0.1.3</b> About this book</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="session-1-re-introductions-and-setup.html"><a href="session-1-re-introductions-and-setup.html"><i class="fa fa-check"></i><b>1</b> Session 1: Re-Introductions and Setup</a>
<ul>
<li class="chapter" data-level="1.1" data-path="session-1-re-introductions-and-setup.html"><a href="session-1-re-introductions-and-setup.html#datasets-of-interest"><i class="fa fa-check"></i><b>1.1</b> Datasets of Interest</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html"><i class="fa fa-check"></i><b>2</b> Session 2: git conflicts</a>
<ul>
<li class="chapter" data-level="2.1" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#learning-objectives"><i class="fa fa-check"></i><b>2.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#introduction"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#collaborating-with-a-trusted-colleague-without-conflicts"><i class="fa fa-check"></i><b>2.3</b> Collaborating with a trusted colleague <strong>without conflicts</strong></a>
<ul>
<li class="chapter" data-level="" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#setup"><i class="fa fa-check"></i>Setup</a></li>
<li><a href="session-2-git-conflicts.html#section"></a></li>
<li class="chapter" data-level="2.3.1" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#step-1-collaborator-clone"><i class="fa fa-check"></i><b>2.3.1</b> Step 1: Collaborator clone</a></li>
<li class="chapter" data-level="2.3.2" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#step-2-collaborator-edits"><i class="fa fa-check"></i><b>2.3.2</b> Step 2: Collaborator Edits</a></li>
<li class="chapter" data-level="2.3.3" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#step-3-collaborator-commit-and-push"><i class="fa fa-check"></i><b>2.3.3</b> Step 3: Collaborator commit and push</a></li>
<li class="chapter" data-level="2.3.4" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#step-4-owner-pull"><i class="fa fa-check"></i><b>2.3.4</b> Step 4: Owner pull</a></li>
<li class="chapter" data-level="2.3.5" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#step-5-owner-edits-commit-and-push"><i class="fa fa-check"></i><b>2.3.5</b> Step 5: Owner edits, commit, and push</a></li>
<li class="chapter" data-level="2.3.6" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#step-6-collaborator-pull"><i class="fa fa-check"></i><b>2.3.6</b> Step 6: Collaborator pull</a></li>
<li class="chapter" data-level="" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#challenge"><i class="fa fa-check"></i>Challenge</a></li>
<li><a href="session-2-git-conflicts.html#section-1"></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#merge-conflicts"><i class="fa fa-check"></i><b>2.4</b> Merge conflicts</a></li>
<li class="chapter" data-level="2.5" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#how-to-resolve-a-conflict"><i class="fa fa-check"></i><b>2.5</b> How to resolve a conflict</a>
<ul>
<li class="chapter" data-level="" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#abort-abort-abort"><i class="fa fa-check"></i>Abort, abort, abort…</a></li>
<li class="chapter" data-level="" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#checkout"><i class="fa fa-check"></i>Checkout</a></li>
<li class="chapter" data-level="" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#pull-and-edit-the-file"><i class="fa fa-check"></i>Pull and edit the file</a></li>
<li class="chapter" data-level="2.5.1" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#producing-and-resolving-merge-conflicts"><i class="fa fa-check"></i><b>2.5.1</b> Producing and resolving merge conflicts</a></li>
<li class="chapter" data-level="" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#merge-conflict-challenge"><i class="fa fa-check"></i>Merge Conflict Challenge</a></li>
<li><a href="session-2-git-conflicts.html#section-2"></a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#workflows-to-avoid-merge-conflicts"><i class="fa fa-check"></i><b>2.6</b> Workflows to avoid merge conflicts</a></li>
<li class="chapter" data-level="2.7" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#collaborating-using-git"><i class="fa fa-check"></i><b>2.7</b> Collaborating using Git</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#learning-objectives-1"><i class="fa fa-check"></i><b>2.7.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="2.7.2" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#pull-requests"><i class="fa fa-check"></i><b>2.7.2</b> Pull requests</a></li>
<li class="chapter" data-level="2.7.3" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#exercise-create-and-merge-pull-requests"><i class="fa fa-check"></i><b>2.7.3</b> Exercise: Create and merge pull requests</a></li>
<li class="chapter" data-level="2.7.4" data-path="session-2-git-conflicts.html"><a href="session-2-git-conflicts.html#branches"><i class="fa fa-check"></i><b>2.7.4</b> Branches</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="session-2-collaborative-synthesis.html"><a href="session-2-collaborative-synthesis.html"><i class="fa fa-check"></i><b>3</b> Session 2: Collaborative Synthesis</a></li>
<li class="chapter" data-level="4" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html"><i class="fa fa-check"></i><b>4</b> Session 3: Introduction to Bayesian modeling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#learning-objectives-2"><i class="fa fa-check"></i><b>4.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#why-choose-bayesian"><i class="fa fa-check"></i><b>4.2</b> Why choose Bayesian?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#philosophically-sound-and-consistent"><i class="fa fa-check"></i><b>4.2.1</b> Philosophically sound and consistent</a></li>
<li class="chapter" data-level="4.2.2" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#flexible"><i class="fa fa-check"></i><b>4.2.2</b> Flexible</a></li>
<li class="chapter" data-level="4.2.3" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#clear-inference"><i class="fa fa-check"></i><b>4.2.3</b> Clear inference</a></li>
<li class="chapter" data-level="4.2.4" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#uses-all-available-information"><i class="fa fa-check"></i><b>4.2.4</b> Uses all available information</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#review-of-probability-bayes-rule-and-distributions"><i class="fa fa-check"></i><b>4.3</b> Review of probability, Bayes’ rule, and distributions</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#probabability"><i class="fa fa-check"></i><b>4.3.1</b> Probabability</a></li>
<li class="chapter" data-level="4.3.2" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#bayes-rule"><i class="fa fa-check"></i><b>4.3.2</b> Bayes’ rule</a></li>
<li class="chapter" data-level="4.3.3" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#bayesian-inference"><i class="fa fa-check"></i><b>4.3.3</b> Bayesian inference</a></li>
<li class="chapter" data-level="4.3.4" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#quick-review-of-distributions"><i class="fa fa-check"></i><b>4.3.4</b> Quick review of distributions</a></li>
<li class="chapter" data-level="4.3.5" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#selecting-priors"><i class="fa fa-check"></i><b>4.3.5</b> Selecting priors</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#drafting-bayesian-models"><i class="fa fa-check"></i><b>4.4</b> Drafting Bayesian models</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#graphical-representations-of-hierarchical-models"><i class="fa fa-check"></i><b>4.4.1</b> Graphical representations of hierarchical models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#simple-hierachical-example-snow-fences"><i class="fa fa-check"></i><b>4.5</b> Simple hierachical example: snow fences</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="session-3-introduction-to-bayesian-modeling.html"><a href="session-3-introduction-to-bayesian-modeling.html#acknowledgements"><i class="fa fa-check"></i><b>4.5.1</b> Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="session-4-collaborative-synthesis.html"><a href="session-4-collaborative-synthesis.html"><i class="fa fa-check"></i><b>5</b> Session 4: Collaborative Synthesis</a></li>
<li class="chapter" data-level="6" data-path="session-5-implementing-bayesian-models.html"><a href="session-5-implementing-bayesian-models.html"><i class="fa fa-check"></i><b>6</b> Session 5: Implementing Bayesian models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="session-5-implementing-bayesian-models.html"><a href="session-5-implementing-bayesian-models.html#learning-objectives-3"><i class="fa fa-check"></i><b>6.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="6.2" data-path="session-5-implementing-bayesian-models.html"><a href="session-5-implementing-bayesian-models.html#overview-of-the-bayesian-modeling-process"><i class="fa fa-check"></i><b>6.2</b> Overview of the Bayesian modeling process</a></li>
<li class="chapter" data-level="6.3" data-path="session-5-implementing-bayesian-models.html"><a href="session-5-implementing-bayesian-models.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>6.3</b> Markov chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="session-5-implementing-bayesian-models.html"><a href="session-5-implementing-bayesian-models.html#inference-from-iterative-simulation"><i class="fa fa-check"></i><b>6.3.1</b> Inference from iterative simulation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="session-5-implementing-bayesian-models.html"><a href="session-5-implementing-bayesian-models.html#programing-statistical-models"><i class="fa fa-check"></i><b>6.4</b> Programing statistical models</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="session-5-implementing-bayesian-models.html"><a href="session-5-implementing-bayesian-models.html#compiling-a-jags-model"><i class="fa fa-check"></i><b>6.4.1</b> Compiling a JAGS model</a></li>
<li class="chapter" data-level="6.4.2" data-path="session-5-implementing-bayesian-models.html"><a href="session-5-implementing-bayesian-models.html#sampling-the-posterior-distribution"><i class="fa fa-check"></i><b>6.4.2</b> Sampling the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="session-5-implementing-bayesian-models.html"><a href="session-5-implementing-bayesian-models.html#evaluating-model-diagnostics"><i class="fa fa-check"></i><b>6.5</b> Evaluating model diagnostics</a>
<ul>
<li class="chapter" data-level="" data-path="session-5-implementing-bayesian-models.html"><a href="session-5-implementing-bayesian-models.html#interactive-problem-logistic-regression-model"><i class="fa fa-check"></i>Interactive problem: Logistic regression model</a></li>
<li class="chapter" data-level="6.5.1" data-path="session-5-implementing-bayesian-models.html"><a href="session-5-implementing-bayesian-models.html#acknowledgements-1"><i class="fa fa-check"></i><b>6.5.1</b> Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="session-6-collaborative-synthesis.html"><a href="session-6-collaborative-synthesis.html"><i class="fa fa-check"></i><b>7</b> Session 6: Collaborative Synthesis</a></li>
<li class="chapter" data-level="8" data-path="session-7-collaborative-synthesis.html"><a href="session-7-collaborative-synthesis.html"><i class="fa fa-check"></i><b>8</b> Session 7: Collaborative Synthesis</a></li>
<li class="chapter" data-level="9" data-path="session-8-informative-priors.html"><a href="session-8-informative-priors.html"><i class="fa fa-check"></i><b>9</b> Session 8: Informative Priors</a></li>
<li class="chapter" data-level="10" data-path="session-9-missing-data.html"><a href="session-9-missing-data.html"><i class="fa fa-check"></i><b>10</b> Session 9: Missing Data</a></li>
<li class="chapter" data-level="11" data-path="session-10-collaborative-synthesis.html"><a href="session-10-collaborative-synthesis.html"><i class="fa fa-check"></i><b>11</b> Session 10: Collaborative Synthesis</a></li>
<li class="chapter" data-level="12" data-path="session-11-timeseries.html"><a href="session-11-timeseries.html"><i class="fa fa-check"></i><b>12</b> Session 11: Timeseries</a></li>
<li class="chapter" data-level="13" data-path="session-12-forecasting.html"><a href="session-12-forecasting.html"><i class="fa fa-check"></i><b>13</b> Session 12: Forecasting</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Open Science Synthesis for the Delta Science Program: Week 2</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="session-3-introduction-to-bayesian-modeling" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Session 3: Introduction to Bayesian modeling</h1>
<div id="learning-objectives-2" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Learning Objectives</h2>
<p>In this lesson, you will learn:</p>
<ul>
<li><p>Why Bayesian approaches are useful</p></li>
<li><p>Refresher on probability, distributions, and Bayes’ rule</p></li>
<li><p>Drafting models with directed acyclic graphs</p></li>
</ul>
</div>
<div id="why-choose-bayesian" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Why choose Bayesian?</h2>
<div id="philosophically-sound-and-consistent" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Philosophically sound and consistent</h3>
<p>While the methods section of a Bayesian paper can seem complex and opaque, the underlying principles of Bayesian thinking are more intuitive than for frequentist tests. Kruschke (2015) breaks Bayesian data analysis down into two foundational principles:</p>
<ol style="list-style-type: decimal">
<li><p>Using data to reallocate credibility among possibilities</p></li>
<li><p>The possibilities are parameter values in meaningful mathematical models</p></li>
</ol>
<p>Suppose we step outside and notice that a cultivated plant is yellowing and losing its leaves. We can consider the many possible causes, such as under watering or over watering, among others. Each possibility has some prior credibility based on previous knowledge. For example, where I live in the Sonoran desert, drought or under watering has a greater probability of causing mortality than over watering. As we continue to walk around the garden, we collect new observations. If the other individuals of the same species are green and thriving, we might decrease the probability of under watering, which would probably affect all individuals similarly, and increase the probability of over watering (e.g., leaking pipe). Therefore, Bayesian inference closely mimics deductive reasoning in its reallocation of credibility across possibilities.</p>
<p>In real life, data are noisy and inferences are probabilistic. For example, consider testing for COVID in a population where the test is not perfect and can produce both false positive and false negatives. But, we must take into account the prevalence of COVID in population. In areas with high disease prevalence, the false positive rate is lower than in areas with low prevalence. Therefore, the true outcome (positive or negative for COVID) depends on previous knowledge of COVID prevalence and the noisy data (imperfect COVID test). We use Bayesian inference to reallocate credibility across the possibilities.</p>
<p>The second foundational principle calls us to define and therefore constrain the set of possibilities. We begin by describing the data from a family of candidate distributions, which are mathematical formulas that can characterize the trends and spreads in data. Each of these distributions is defined by one or more parameter values, which determine the exact shape of the distribution.</p>
<p><img src="reproducible-research-short-course_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>In Bayesian inference, model parameters are the possibilities over which credibility is allocated. For example, the above histograms show a roughly unimodal and symmetric distribution. The red and blue lines represent candidate descriptions of the data, normal distributions with different sets of parameter values (<span class="math inline">\(\mu, \sigma\)</span>). Both choices are plausible, but given the data, the red line has greater credence. Bayesian inference uses the prior probabilities and the data to compute the exact credibility of parameter values.</p>
</div>
<div id="flexible" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Flexible</h3>
<p>Bayesian modeling allows practitioners to design models to fit the data they have, rather than transforming data in an attempt to satisfy model assumptions. This flexibility extends to accounting for hierarchy, treating data as drawn from any kind of distribution, and defining meaningful parameters.</p>
<p>For an example of hierarchy, consider a trait collected across multiple individuals and nested within taxonomic levels. We might be interested in comparisons across species, genera, and families. A hierarchical means model might specify:</p>
<p><span class="math inline">\(trait_i \sim Normal(\mu_{sp(i)}, \sigma^2)\)</span></p>
<p><span class="math inline">\(\mu_{sp} \sim Normal(\mu_{g(sp)}, \sigma^2_{g})\)</span></p>
<p><span class="math inline">\(\mu_{g} \sim Normal(\mu_{f(g)}, \sigma^2_{f})\)</span></p>
<p>where <em>sp</em> represents species, <em>g</em> represents genus, and <em>f</em> represents families. The notation indicates that each observation <em>i</em> belongs to a species, each species belongs to a genus, and each genus belongs to a family. Thus, in a single model, the hierarchical relationship between species, genus, and family can be represented.</p>
<p>Data can also be treated as arising from any distribution. For example, a inventory survey might yield counts of a particular species, but those counts might be clumped on the landscape, yielding large number of zero observations. A Poisson distribution describes the probability of a given number of events occurring in an interval of time or space, but doesn’t accommodate the extra zeros. In Bayesian modeling, it is straightforward to specify the surveyed counts as a mixture between Bernoulli and Poisson distributions, which accounts for the separate processes of dispersal (species arrives or not in plot) and frequency (if species arrives, the rate or density of arrival).</p>
<p>Finally, Bayesian inference can accommodate a wide range of model and data possibilities, rather than having separate tools or approaches for different types of data or tests. T-test, ANOVA, linear model, non-linear models, and more can be specified in the same framework, using the same set of tools, and can even be combined. This allows the implementation of mathematical models with scientifically meaningful parameters, possibly in conjunction with an ANOVA or regression framework. For example, we might have leaf-level gas exchange data and want to fit a biochemical model of photosynthesis, but the scientific question is whether photosynthetic parameters differed between species and treatments. The flexible nature of a hierarchical approach means that the meaningful parameters (e.g., <span class="math inline">\(V_{cmax}\)</span>) can be represented by a linear model. We can use the data to make inference about the meaningful parameters in one step.</p>
</div>
<div id="clear-inference" class="section level3" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Clear inference</h3>
<p>In frequentist paradigms, confidence intervals and p-values have very specific, non-intuitive definitions. A 95% confidence interval indicates that out of 100 replications of the experiment, 95% of the resulting confidence intervals will include the true parameter value. In contrast, Bayesian inference results in parameters themselves having distributions, and conditioned on a particular dataset, the 95% credible interval includes 95% of the probability of the parameter value. Bayesian credible intervals and p-values are simple to define, calculate, and interpret.</p>
<p>A corollary to the direct quantification of uncertainty is that the purpose of the analysis can be on estimation of parameters, rather than strict hypothesis testing. Bayesian inference provides a way to quantitatively describe relationships in complex datasets, which allows for inquiry to be driven by questions and models of understanding, rather than falsifiable hypotheses.</p>
</div>
<div id="uses-all-available-information" class="section level3" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Uses all available information</h3>
<p>Bayesian modeling allows for simultaneous analysis of multiple-related datasets. For example, a response variable and its measurement error can be incorporated into a single analysis. Partially missing data do not have to be excluded, and in fact the missing values can be imputed by the model in the same step as the analysis. Finally, prior knowledge can be included in the form of informative priors. In practice, many Bayesian practitioners use relatively non-informative priors, but knowledge of the acceptable values a parameter can take can be incorporated as informative priors, which can improve model speed and convergence.</p>
<div id="brainstorming-possible-applications-of-bayesian-modeling" class="section level4 unnumbered aside">
<h4>Brainstorming: Possible applications of Bayesian modeling</h4>
<p>Have you encountered any research roadblocks, past or present, that could potentially be addressed with the techniques mentioned above?</p>
</div>
</div>
</div>
<div id="review-of-probability-bayes-rule-and-distributions" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Review of probability, Bayes’ rule, and distributions</h2>
<div id="probabability" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Probabability</h3>
<p><img src="reproducible-research-short-course_files/figure-html/unnamed-chunk-9-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>We define the marginal probability as the total area of circles A and B.
<span class="math inline">\(P(A) = {A_A}\)</span></p>
<p><span class="math inline">\(P(B) = {A_B}\)</span></p>
<p>The joint probability is the shared area of circles A and B.</p>
<p><span class="math inline">\(P(A,B) = A_{AB}\)</span></p>
<p>The conditional probability describes the the shared area scaled by the whole area of one circle.</p>
<p><span class="math inline">\(P(B|A) = \frac{A_{AB}}{A_A}\)</span></p>
<p><span class="math inline">\(P(B|A) = \frac{P(A,B)}{P(A)}\)</span></p>
<p>The joint probability can be rearranged algebraically to produce:</p>
<p><span class="math inline">\(P(A,B) = P(B|A)P(A)\)</span></p>
<p>Thus, we can describe the joint probability as a product of a conditional and marginal probabilities.</p>
<div id="interactive-problem-conditional-probability" class="section level4 unnumbered aside">
<h4>Interactive problem: Conditional probability</h4>
<p>Conditional probability rules are very useful for breaking down complex problems. What are some possible ways to break down <span class="math inline">\(P(A, B, C)\)</span>?</p>
<p><img src="reproducible-research-short-course_files/figure-html/unnamed-chunk-10-1.png" width="384" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="bayes-rule" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Bayes’ rule</h3>
<p>Bayes’ rule can be derived by describing the joint probability in two ways.</p>
<p><span class="math inline">\(P(A,B) = P(B|A)P(A)\)</span></p>
<p><span class="math inline">\(P(A,B) = P(A|B)P(B)\)</span></p>
<p>By setting these two descriptions equal to each other and rearranging algebraically, we obtain:</p>
<p><span class="math inline">\(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\)</span></p>
<p>Thus, we can obtain the conditional probability of <span class="math inline">\(A|B\)</span> from the conditional probability of <span class="math inline">\(B|A\)</span>, plus the marginal probabilities of A and B.</p>
<div id="interactive-problem-applying-bayes-rule" class="section level4 unnumbered aside">
<h4>Interactive problem: Applying Bayes’ rule</h4>
<p>On a hike, we observe a dead tree and wonder, what is the probability the tree was attacked by beetles, given that it is dead? There are two random variables of interest, beetle and death, each of which can take on the values 1 or 0. We are interested in <span class="math inline">\(P(beetle = 1 | death = 1)\)</span>. Recent data show the following probabilities:</p>
<table>
<thead>
<tr class="header">
<th>Condition</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P(beetle = 0)\)</span></td>
<td>0.7</td>
</tr>
<tr class="even">
<td><span class="math inline">\(P(beetle = 1)\)</span></td>
<td>0.3</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(P(death = 0|beetle = 0)\)</span></td>
<td>0.8</td>
</tr>
<tr class="even">
<td><span class="math inline">\(P(death = 0|beetle = 1)\)</span></td>
<td>0.1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(P(death = 1|beetle = 0)\)</span></td>
<td>0.2</td>
</tr>
<tr class="even">
<td><span class="math inline">\(P(death = 1|beetle = 1)\)</span></td>
<td>0.9</td>
</tr>
</tbody>
</table>
<p>What is <span class="math inline">\(P(beetle = 1 | death = 1)\)</span>?</p>
</div>
</div>
<div id="bayesian-inference" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Bayesian inference</h3>
<p>In Bayesian inference, we can use the inversion of probability from Bayes’ rule to understand the conditional probability of the parameters given the data from the conditional probability of the data. Here, we use <span class="math inline">\(y\)</span> to represent data and <span class="math inline">\(\theta\)</span> to represent parameters:</p>
<p><span class="math inline">\(P(\theta|y) = \frac{P(y|\theta)P(\theta)}{P(y)}\)</span></p>
<p>To simplify further, we can remove the marginal probability of <span class="math inline">\(y\)</span>, which is the normalizing constant in the denominator. The unnormalized posterior distribution can be described as:</p>
<p><span class="math inline">\(P(\theta|y) \propto {P(y|\theta)P(\theta)}\)</span></p>
<p>We read this as ‘the posterior is proportional to the likelihood times the prior’.</p>
<p>In Bayesian inference, we can learn about the posterior parameter distribution given the observed data. In so doing, we specify the data as stochastically drawn from a probability distribution conditioned on the ‘true’ parameter values and define the prior probability of those parameters.</p>
</div>
<div id="quick-review-of-distributions" class="section level3" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Quick review of distributions</h3>
<p>We identified above that to derive the posterior parameter probabilities, we need to specify both the likelihood and the prior. To do so, we should be aware of a handful of probability distributions that mathematically describe a sample space, or possible outcomes of a random variable.</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Continuous</th>
<th>Description</th>
<th>Discrete</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Normal <img src="images/distributions/normal_pdf.png" /></td>
<td>Domain: <span class="math inline">\((-\infty, \infty)\)</span><br/>Parameters: <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span><br/>Example: Net ecosystem exchange</td>
<td>Poisson <img src="images/distributions/Poisson_pmf.svg" /></td>
<td>Domain: 0 to <span class="math inline">\(\infty\)</span><br/>Parameter: <span class="math inline">\(\lambda\)</span><br/>Example: Count data</td>
</tr>
<tr class="even">
<td>Gamma <img src="images/distributions/gamma_pdf.svg" /></td>
<td>Domain: <span class="math inline">\((0, \infty)\)</span><br/>Parameters: <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span><br/>Example: Rates</td>
<td>Bernoulli <img src="images/distributions/Bernoulli_pmf.png" /></td>
<td>Domain: 0 or 1<br/>Parameter: <span class="math inline">\(p\)</span><br/>Example: Presence or absence</td>
</tr>
<tr class="odd">
<td>Beta <img src="images/distributions/beta_pdf.svg" /></td>
<td>Domain: <span class="math inline">\((0, 1)\)</span><br/>Parameters: <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span><br/>Example: Survival probability</td>
<td>Binomial <img src="images/distributions/binomial_pmf.svg" /></td>
<td>Domain: 0 to <span class="math inline">\(\infty\)</span><br/>Parameters: <span class="math inline">\(n, p\)</span><br/>Example: Deaths in a population</td>
</tr>
</tbody>
</table>
<p>Once we are aware of the general features of distributions, we can select an appropriate likelihood for the data. <em>The likelihood should match the data and the data-generating process</em>.</p>
<p>A few questions to consider are:</p>
<ul>
<li><p>Are the data (response variables) continuous or discrete?</p></li>
<li><p>What are the range of possible values that the data can take?</p></li>
<li><p>How does the variance of the data change as a function of the mean?</p></li>
</ul>
<div id="interactive-problem-identifying-potential-likelihoods" class="section level4 unnumbered aside">
<h4>Interactive problem: Identifying potential likelihoods</h4>
<p>For each example, identify the potential probability distribution(s) that would be appropriate. Use the questions above to guide your choices.</p>
<ol style="list-style-type: decimal">
<li>Number of trees classified as dead in a forest plot</li>
<li>Total leaf area of a tree</li>
<li>Reproductive status of an individual (reproductive vs. non-reproductive)</li>
<li>Proportion of trees classified as dead in a forest plot</li>
<li>Seed mass</li>
<li>Number of sea stars in a tide pool</li>
<li>Concentration of oxygen in seawater</li>
<li>Time between flood events</li>
</ol>
</div>
</div>
<div id="selecting-priors" class="section level3" number="4.3.5">
<h3><span class="header-section-number">4.3.5</span> Selecting priors</h3>
<p>Selecting a likelihood to describe the data distribution is necessary but not sufficient for specifying a Bayesian model. Careful thought must be put in to picking priors as well, which represent existing knowledge about a parameter.</p>
<p>A few questions to consider are:</p>
<ul>
<li><p>What range can the parameter take mathematically?</p></li>
<li><p>What values are ecologically realistic?</p></li>
<li><p>Is there an appropriate conjugate prior for this likelihood?</p></li>
</ul>
<p>Conjugacy has a formal definition, but for our purposes, natural conjugate prior families have practical advantages for computation and interpretation.</p>
<p>Some examples include the binomial-beta, Poisson-gamma, multinomial-Dirichlet, and exponential-gamma. For a normal likelihood, the conjugate prior for <span class="math inline">\(\mu\)</span> is normal and the conjugate prior for <span class="math inline">\(\sigma^2\)</span> is the inverse-gamma. In common Bayesian simulation software such as JAGS, OpenBUGS, and Stan, the scale parameter of a normal likelihood is defined as the precision (<span class="math inline">\(\tau = \frac{1}{\sigma^2}\)</span>); the conjugate prior for <span class="math inline">\(\tau\)</span> is a gamma distribution.</p>
<p>Selecting priors is an important consideration of Bayesian modeling that will have its own lecture later in the week. For now, we can say that many parameters do not need informative priors in order to be reliably estimated by the data (e.g., regression parameters). Other parameters, such as observation error, will rarely be identifiable without strong prior information (or additional data).</p>
<p>In practice, selecting priors can be part of the model development process. Different priors can be tested for their contribution to the posterior, which allows for evaluation of the sensitivity of the posterior to the prior specification. Particularly for non-identifiable parameters, convergence can be achieved more quickly with a more informative prior. One method can be to specify a uniform prior with realistic but conservative upper and lower bounds:</p>
<p><span class="math inline">\(\theta \sim Uniform(A, B)\)</span></p>
<p>As such, the posterior distribution for <span class="math inline">\(\theta\)</span> will also be restricted to the interval [A,B], and the data likelihood will contribute to its shape.</p>
</div>
</div>
<div id="drafting-bayesian-models" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Drafting Bayesian models</h2>
<p>Given the flexibility of Bayesian modeling and the considerations of selecting likelihoods and priors, a key step in model development is to write out the model components after considering the features of the scientific question and dataset at hand. The model can be be represented a series of equations, as a diagram, and in code. Ultimately, the manuscript will require the equations in the Methods section and the code as part of the publishing requirement. Diagrams are useful tool to visually represent the model and consider alternative model formulations.</p>
<div id="graphical-representations-of-hierarchical-models" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Graphical representations of hierarchical models</h3>
<p>Complex or hierarchical models can be easily represented as a graphical model, particularly a Directed Acyclic Graph (DAG). General convention for DAGs in Bayesian modeling are:</p>
<ul>
<li><em>Nodes</em> can be stochastic (circles) or deterministic/fixed (squares)
<ul>
<li>Child nodes depend on parent nodes</li>
<li>A root node does not have any parents</li>
<li>A terminal node does not have any children</li>
<li>An internal node gives rise to children nodes and has parent node(s)</li>
</ul></li>
<li><em>Edges</em> are directional (arrows) that indicate the direction of the conditional relationship between two nodes
<ul>
<li>Edges must be unidirectional (node A cannot be parent to and child of node B)</li>
<li>No sequence of edges returns to the parent node</li>
</ul></li>
</ul>
<div id="htmlwidget-c63ed51b72f8dedd1f6c" style="width:672px;height:300px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-c63ed51b72f8dedd1f6c">{"x":{"diagram":"digraph {\n  graph [layout = dot, rankdir = TB, dir = back]\n  \n  node [shape = oval]        \n  rec1 [label = \"A\"]\n  rec2 [label = \"B\"]\n  rec3 [label = \"C\"]\n  \n  # edge definitions with the node IDs\n  rec1 -> rec2 -> rec3\n  }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p>In this DAG, A is a root node, C is a terminal node, and B is an internal node. The conditional probability model can be defined as:</p>
<p><span class="math inline">\(P(A, B, C) = P(C|B)P(B|A)P(A)\)</span></p>
<p>We will see in the example below that DAGs can be especially useful for visualizing the mathematical representation of models.</p>
</div>
</div>
<div id="simple-hierachical-example-snow-fences" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Simple hierachical example: snow fences</h2>
<p>Consider a study to quantify the effects of snow fences on invasive plant establishment. 10 snow fences are monitored and their invasive plants counted within the “footprint” of each snow fence.</p>
<ul>
<li>Let <span class="math inline">\(y_i\)</span> represent the number of invasive plants associated with snow fence <span class="math inline">\(i\)</span>.</li>
<li>Let <span class="math inline">\(x_i\)</span> represent the footprint (in square meters) of snow fence <span class="math inline">\(i\)</span></li>
</ul>
<p>The data are:</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(y_i\)</span></th>
<th><span class="math inline">\(x_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>138</td>
<td>72</td>
</tr>
<tr class="even">
<td>2</td>
<td>91</td>
<td>50</td>
</tr>
<tr class="odd">
<td>3</td>
<td>132</td>
<td>55</td>
</tr>
<tr class="even">
<td>4</td>
<td>123</td>
<td>60</td>
</tr>
<tr class="odd">
<td>5</td>
<td>173</td>
<td>78</td>
</tr>
<tr class="even">
<td>6</td>
<td>124</td>
<td>63</td>
</tr>
<tr class="odd">
<td>7</td>
<td>109</td>
<td>54</td>
</tr>
<tr class="even">
<td>8</td>
<td>154</td>
<td>70</td>
</tr>
<tr class="odd">
<td>9</td>
<td>138</td>
<td>80</td>
</tr>
<tr class="even">
<td>10</td>
<td>134</td>
<td>68</td>
</tr>
</tbody>
</table>
<p>Given these count data, let’s define the likelihood with a Poisson distribution. However, we have reason to believe that each snow fence has its own rate of invasive plants due to local topographical factors. Therefore, we specify the likelihood as:</p>
<p><span class="math inline">\(y_i \sim Poisson(\theta_i \cdot x_i)\)</span></p>
<p>such that each snow fence <span class="math inline">\(i\)</span> has its own rate parameter, <span class="math inline">\(\theta_i\)</span>. However, given that the snow fences are located at the same field site, we believe that the individual rates of invasive plants are drawn from an overall regional rate:</p>
<p><span class="math inline">\(\theta_i \sim Gamma(\alpha, \beta)\)</span></p>
<p>In this hierarchical model, the prior for the parameter of interest is a gamma distribution described by <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, which are unknown/stochastic. So, we must define these hyperparameters with their own priors.</p>
<p><span class="math inline">\(\alpha \sim Gamma(2, 1)\)</span></p>
<p><span class="math inline">\(\beta \sim Exponential(1)\)</span></p>
<p>The hyperparameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are root nodes, whose distributions are fixed rather than stochastic.</p>
<p>The levels of this hierarchical model can be visualized by the following DAG:</p>
<div id="htmlwidget-91b68a2ce31c9ef743a7" style="width:672px;height:300px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-91b68a2ce31c9ef743a7">{"x":{"diagram":"digraph {\n  graph [layout = dot, rankdir = TB]\n  \n  node [shape = oval]        \n  rec1 [label = \"&#945;\"]\n  rec2 [label = \"&#946;\"]\n  rec3 [label = <<B>&#952;<\/B>>]\n  rec4 [label = <<B>y<\/B>>]\n  node [shape = rectangle] \n  rec5 [label = <<B>x<\/B>>]\n  \n  # edge definitions with the node IDs\n  rec1 -> rec3 -> rec4 \n  rec2 -> rec3\n  rec4 -> rec5 [dir = back]\n  }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p>The bolded variables indicate vectors, which are indexed by <span class="math inline">\(i\)</span> in statistical notation. The rectangle around <span class="math inline">\(\bf{x}\)</span> indicates that it is a deterministic node, or fixed as data. Notably,the oval around the response variable <span class="math inline">\(\bf{y}\)</span> indicates that it is a stochastic node; that is, drawn from a distribution and given the Poisson likelihood. <span class="math inline">\(\bf{\theta}\)</span> is the parameter in the Poisson likelihood, and it is drawn from a conjugate gamma prior with two parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<p>All of the stochastic nodes (ovals) are drawn from distributions. <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are root nodes, meaning that the parameter values for their distributions are fixed and must be defined numerically.</p>
<div id="interactive-problem-logistic-regression-example" class="section level4 unnumbered aside">
<h4>Interactive problem: Logistic regression example</h4>
<p>Consider the case of selection pressure from discoloring of tree trunks from air pollution. One species of moth rests on tree trunks during the day, and their coloration acts as camouflage against bird predation. Researchers established a gradient of tree trunk color from Liverpool the countryside and selected 7 sites. At each site, they glued an equal number of dead light and dead dark moths to tree trunks and counted the number of removed moths after 24 hours.</p>
<p>The dataset includes <em>Site</em> (1,2, … 7), <em>Morph</em> (1 = light, 2 = dark), <em>Distance</em> (from Liverpool, km) <em>Placed</em> (number of moths glued to trunks), and <em>Removed</em> (number of moths removed from trunks after 24 hours).</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(Site\)</span></th>
<th><span class="math inline">\(Morph\)</span></th>
<th><span class="math inline">\(Distance\)</span></th>
<th><span class="math inline">\(Placed\)</span></th>
<th><span class="math inline">\(Removed\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>0</td>
<td>56</td>
<td>17</td>
</tr>
<tr class="even">
<td>1</td>
<td>2</td>
<td>0</td>
<td>56</td>
<td>14</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1</td>
<td>7.2</td>
<td>80</td>
<td>28</td>
</tr>
<tr class="even">
<td>2</td>
<td>2</td>
<td>7.2</td>
<td>80</td>
<td>20</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1</td>
<td>24.1</td>
<td>52</td>
<td>18</td>
</tr>
<tr class="even">
<td>3</td>
<td>2</td>
<td>24.1</td>
<td>52</td>
<td>22</td>
</tr>
<tr class="odd">
<td>4</td>
<td>1</td>
<td>30.2</td>
<td>60</td>
<td>9</td>
</tr>
<tr class="even">
<td>4</td>
<td>2</td>
<td>30.2</td>
<td>60</td>
<td>16</td>
</tr>
<tr class="odd">
<td>5</td>
<td>1</td>
<td>36.4</td>
<td>60</td>
<td>16</td>
</tr>
<tr class="even">
<td>5</td>
<td>2</td>
<td>36.4</td>
<td>60</td>
<td>23</td>
</tr>
<tr class="odd">
<td>6</td>
<td>1</td>
<td>41.5</td>
<td>84</td>
<td>20</td>
</tr>
<tr class="even">
<td>6</td>
<td>2</td>
<td>41.5</td>
<td>84</td>
<td>40</td>
</tr>
<tr class="odd">
<td>7</td>
<td>1</td>
<td>51.2</td>
<td>92</td>
<td>24</td>
</tr>
<tr class="even">
<td>7</td>
<td>2</td>
<td>51.2</td>
<td>92</td>
<td>39</td>
</tr>
</tbody>
</table>
<p>Here, our goal is to evaluate whether the probability of removal differed between the light and dark morphs, and whether this difference depends on distance from Liverpool.</p>
<p>Consider the following:</p>
<ul>
<li><p>Let <span class="math inline">\(y\)</span> represent the number of removed moths and <span class="math inline">\(n\)</span> represent the number of moths placed. Write out the likelihood for this response variable.</p></li>
<li><p>The probability <span class="math inline">\(p\)</span> of removal differs depending on distance from Liverpool, <span class="math inline">\(x\)</span>. Thus, we should index the variables as <span class="math inline">\(p_i\)</span> and <span class="math inline">\(x_i\)</span>, to indicate that each observation will be associated with a unique probability. Let <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> as the intercept and slope of a linear regression. Write out the expression relating <span class="math inline">\(p_i\)</span> as linear function of <span class="math inline">\(x_i\)</span>.</p></li>
<li><p>Furthermore, we hypothesize that color morph influences the relationship with distance. Let <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> each be vectors. How can the linear function be updated to account for the morph of observation <span class="math inline">\(i\)</span>?</p></li>
<li><p>Regressions work best on the whole real line, but probability <span class="math inline">\(p\)</span> can only take on values between <span class="math inline">\([0,1]\)</span>. What kind of link function can be used to relate <span class="math inline">\([0,1]\)</span> to the whole real line?</p></li>
<li><p>Priors are needed for the regression parameters. In the absence of prior information, what might be a relatively non-informative prior to give <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>?</p></li>
<li><p>Draw a DAG for this model.</p></li>
</ul>
</div>
<div id="acknowledgements" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Acknowledgements</h3>
<p>These materials are derived primarily from Bayesian course materials developed and taught by Kiona Ogle. Additional ideas and code have been adapted from materials authored by Kelly Heilman, Robert Shriver, and Drew Peltier. The texts ‘Doing Bayesian Data Analyis’ (Kruschke 2015), ‘Statistical Rethinking’ (McElreath 2016), and ‘Bayesian Data Analysis’ (Gelman et al. 2014) were strongly influential and recommended references.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="session-2-collaborative-synthesis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="session-4-collaborative-synthesis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
