[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentals in Data Management for Qualitative and Quantitative Arctic Research",
    "section": "",
    "text": "About the course\nThis 5-day in-person workshop will provide researchers with an overview of reproducible and ethical research practices, steps and methods for more easily documenting and preserving their data at the Arctic Data Center, and an introduction to programming in R. Special attention will be paid to qualitative data management, including practices working with sensitive data. Example datasets will draw from natural and social sciences, and methods for conducting reproducible research will be discussed in the context of both qualitative and quantitative data. Responsible and reproducible data management practices will be discussed as they apply to all aspects of the data life cycle. This includes ethical data collection and data sharing, data sovereignty, and the CARE principles. The CARE principles are guidelines that help ensure open data practices (like the FAIR principles) appropriately engage with Indigenous Peoples’ rights and interests.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#weeks-schedule",
    "href": "index.html#weeks-schedule",
    "title": "Fundamentals in Data Management for Qualitative and Quantitative Arctic Research",
    "section": "Week’s Schedule",
    "text": "Week’s Schedule",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Fundamentals in Data Management for Qualitative and Quantitative Arctic Research",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Fundamentals in Data Management for Qualitative and Quantitative Arctic Research",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Matthew B. Jones, Angie Garcia, Nicole Greco, Justin Kadi, Jim Regetz (2025), Fundamentals in Data Management for Qualitative and Quantitative Arctic Research. Arctic Data Center & NCEAS Learning Hub. URL https://learning.nceas.ucsb.edu/2025-01-arctic.\nAdditional contributors: Ben Bolker, Amber E. Budden, Julien Brun, Samantha Csik, Halina Do-Linh, Natasha Haycock-Chavez, S. Jeanette Clark, Julie Lowndes, Stephanie Hampton, Samanta Katz, Erin McLean, Bryce Mecum, Casey O’Hara, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Camila Vargas-Poulsen, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "session_01.html",
    "href": "session_01.html",
    "title": "1  Introduction to the Arctic Data Center",
    "section": "",
    "text": "1.1 Introduction to the Arctic Data Center and NSF Standards and Policies",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Arctic Data Center</span>"
    ]
  },
  {
    "objectID": "session_01.html#introduction-to-the-arctic-data-center-and-nsf-standards-and-policies",
    "href": "session_01.html#introduction-to-the-arctic-data-center-and-nsf-standards-and-policies",
    "title": "1  Introduction to the Arctic Data Center",
    "section": "",
    "text": "1.1.1 Learning Objectives\nIn this lesson, we will discuss:\n\nThe mission and structure of the Arctic Data Center\nHow the Arctic Data Center supports the research community\nAbout data policies from the NSF Arctic program\n\n\n\n1.1.2 Arctic Data Center - History and Introduction\nThe Arctic Data Center is the primary data and software repository for the Arctic section of National Science Foundation’s Office of Polar Programs (NSF OPP).\nWe’re best known in the research community as a data archive – researchers upload their data to preserve it for the future and make it available for re-use. This isn’t the end of that data’s life, though. These data can then be downloaded for different analyses or synthesis projects. In addition to being a data discovery portal, we also offer top-notch tools, support services, and training opportunities. We also provide data rescue services.\n\nNSF has long had a commitment to data reuse and sharing. Since our start in 2016, we’ve grown substantially – from that original 4 TB of data from ACADIS to now over 140 TB towards the end of 2024. In 2021 alone, we saw 16% growth in dataset count, and about 30% growth in data volume. This increase has come from advances in tools – both ours and of the scientific community, plus active community outreach and a strong culture of data preservation from NSF and from researchers. We plan to add more storage capacity in the coming months, as researchers are coming to us with datasets in the terabytes, and we’re excited to preserve these research products in our archive. We’re projecting our growth to be around several hundred TB this year, which has a big impact on processing time. Give us a heads up if you’re planning on having larger submissions so that we can work with you and be prepared for a large influx of data.\n\nThe data that we have in the Arctic Data Center comes from a wide variety of disciplines. These different programs within NSF all have different focuses – the Arctic Observing Network supports scientific and community-based observations of biodiversity, ecosystems, human societies, land, ice, marine and freshwater systems, and the atmosphere as well as their social, natural, and/or physical environments, so that encompasses a lot right there in just that one program. We’re also working on a way right now to classify the datasets by discipline, so keep an eye out for that coming soon.\n\nAlong with that diversity of disciplines comes a diversity of file types. The most common file type we have are image files in four different file types. Probably less than 200-300 of the datasets have the majority of those images – we have some large datasets that have image and/or audio files from drones. Most of those 6600+ datasets are tabular datasets. There’s a large diversity of data files, though, whether you want to look at remote sensing images, listen to passive acoustic audio files, or run applications – or something else entirely. We also cover a long period of time, at least by human standards. The data represented in our repository spans across centuries.\n\nWe also have data that spans the entire Arctic, as well as the sub-Arctic, regions.\n\n\n\n1.1.3 Data Discovery Portal\nTo browse the data catalog, navigate to arcticdata.io. Go to the top of the page and under data, go to search. Right now, you’re looking at the whole catalog. You can narrow your search down by the map area, a general search, or searching by an attribute.\n Clicking on a dataset brings you to this page. You have the option to download all the files by clicking the green “Download All” button, which will zip together all the files in the dataset to your Downloads folder. You can also pick and choose to download just specific files.\n\nAll the raw data is in open formats to make it easily accessible and compliant with FAIR principles – for example, tabular documents are in .csv (comma separated values) rather than Excel documents.\nThe metrics at the top give info about the number of citations with this data, the number of downloads, and the number of views. This is what it looks like when you click on the Downloads tab for more information.\n\nScroll down for more info about the dataset – abstract, keywords. Then you’ll see more info about the data itself. This shows the data with a description, as well as info about the attributes (or variables or parameters) that were measured. The green check mark indicates that those attributes have been annotated, which means the measurements have a precise definition to them. Scrolling further, we also see who collected the data, where they collected it, and when they collected it, as well as any funding information like a grant number. For biological data, there is the option to add taxa.\n\n\n1.1.4 Tools and Infrastructure\nAcross all our services and partnership, we are strongly aligned with the community principles of making data FAIR (Findable, Accesible, Interoperable and Reusable).\n\nWe have a number of tools available to submitters and researchers who are there to download data. We also partner with other organizations, like Make Data Count and DataONE, and leverage those partnerships to create a better data experience.\n\nOne of those tools is provenance tracking. With provenance tracking, users of the Arctic Data Center can see exactly what datasets led to what product, using the particular script that the researcher ran.\n\nAnother tool are our Metadata Quality Checks. We know that data quality is important for researchers to find datasets and to have trust in them to use them for another analysis. For every submitted dataset, the metadata is run through a quality check to increase the completeness of submitted metadata records. These checks are seen by the submitter as well as are available to those that view the data, which helps to increase knowledge of how complete their metadata is before submission. That way, the metadata that is uploaded to the Arctic Data Center is as complete as possible, and close to following the guideline of being understandable to any reasonable scientist.\n\n\n\n1.1.5 Support Services\nMetadata quality checks are the automatic way that we ensure quality of data in the repository, but the real quality and curation support is done by our curation team. The process by which data gets into the Arctic Data Center is iterative, meaning that our team works with the submitter to ensure good quality and completeness of data. When a submitter submits data, our team gets a notification and beings to evaluate the data for upload. They then go in and format it for input into the catalog, communicating back and forth with the researcher if anything is incomplete or not communicated well. This process can take anywhere from a few days or a few weeks, depending on the size of the dataset and how quickly the researcher gets back to us. Once that process has been completed, the dataset is published with a DOI (digital object identifier).\n\n\n\n1.1.6 Training and Outreach\nIn addition to the tools and support services, we also interact with the community via trainings like this one and outreach events. We run workshops at conferences like the American Geophysical Union, Arctic Science Summit Week and others. We also run an intern and fellows program, and webinars with different organizations. We’re invested in helping the Arctic science community learn reproducible techniques, since it facilitates a more open culture of data sharing and reuse.\n\nWe strive to keep our fingers on the pulse of what researchers like yourselves are looking for in terms of support. We’re active on Twitter and BlueSky to share Arctic updates, data science updates, and specifically Arctic Data Center updates, but we’re also happy to feature new papers or successes that you all have had with working with the data. We can also take data science questions if you’re running into those in the course of your research, or how to make a quality data management plan. Follow us on our media platforms and interact with us – we love to be involved in your research as it’s happening as well as after it’s completed.\n\n\n\n1.1.7 Data Rescue\nWe also run data rescue operations. We digitized Autin Post’s collection of glacier photos that were taken from 1964 to 1997. There were 100,000+ files and almost 5 TB of data to ingest, and we reconstructed flight paths, digitized the images of his notes, and documented image metadata, including the camera specifications.\n\n\n\n1.1.8 Who Must Submit\nProjects that have to submit their data include all Arctic Research Opportunities through the NSF Office of Polar Programs. That data has to be uploaded within two years of collection. The Arctic Observing Network has a shorter timeline – their data products must be uploaded within 6 months of collection. Additionally, we have social science data, though that data often has special exceptions due to sensitive human subjects data. At the very least, the metadata has to be deposited with us.\nArctic Research Opportunities (ARC)\n\nComplete metadata and all appropriate data and derived products\nWithin 2 years of collection or before the end of the award, whichever comes first\n\nARC Arctic Observation Network (AON)\n\nComplete metadata and all data\nReal-time data made public immediately\nWithin 6 months of collection\n\nArctic Social Sciences Program (ASSP)\n\nNSF policies include special exceptions for ASSP and other awards that contain sensitive data\nHuman subjects, governed by an Institutional Review Board, ethically or legally sensitive, at risk of decontextualization\nMetadata record that documents non-sensitive aspects of the project and data\n\nTitle, Contact information, Abstract, Methods\n\n\nFor more complete information see our “Who Must Submit” webpage\nRecognizing the importance of sensitive data handling and of ethical treatment of all data, the Arctic Data Center submission system provides the opportunity for researchers to document the ethical treatment of data and how collection is aligned with community principles (such as the CARE principles). Submitters may also tag the metadata according to community develop data sensitivity tags. We will go over these features in more detail shortly.\n\n\n\n1.1.9 Summary\nAll the above informtion can be found on our website or if you need help, ask our support team at support@arcticdata.io or tweet us @arcticdatactr!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Arctic Data Center</span>"
    ]
  },
  {
    "objectID": "session_02.html",
    "href": "session_02.html",
    "title": "2  RStudio Server Setup",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_02.html#learning-objectives",
    "href": "session_02.html#learning-objectives",
    "title": "2  RStudio Server Setup",
    "section": "",
    "text": "Practice creating an R Project\nOrganize an R Project for effective project management\nUnderstand how to move in an R Project using paths and working directories",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_02.html#logon-to-the-rstudio-server",
    "href": "session_02.html#logon-to-the-rstudio-server",
    "title": "2  RStudio Server Setup",
    "section": "2.1 Logon to the RStudio Server",
    "text": "2.1 Logon to the RStudio Server\nTo prevent us from spending most of this lesson troubleshooting the myriad of issues that can arise when setting up the R, RStudio, and git environments, we have chosen to have everyone work on a remote server with all of the software you need installed. We will be using a special kind of RStudio just for servers called RStudio Server. If you have never worked on a remote server before, you can think of it like working on a different computer via the internet. Note that the server has no knowledge of the files on your local filesystem, but it is easy to transfer files from the server to your local computer, and vice-versa, using the RStudio server interface.\n\n\n\n\n\n\nServer Setup\n\n\n\nYou should have received an email prompting you to change your password for your server account. If you did not, please put up a post-it and someone will help you.\nAfter you have successfully changed your password log in at: https://included-crab.nceas.ucsb.edu/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_02.html#create-an-r-project",
    "href": "session_02.html#create-an-r-project",
    "title": "2  RStudio Server Setup",
    "section": "2.2 Create an R Project",
    "text": "2.2 Create an R Project\nIn this course, we are going to be using an R project to organize our work. An R project is tied to a directory on your local computer, and makes organizing your work and collaborating with others easier.\nThe Big Idea: using an R project is a reproducible research best practice because it bundles all your work within a working directory. Consider your current data analysis workflow. Where do you import you data? Where do you clean and wrangle it? Where do you create graphs, and ultimately, a final report? Are you going back and forth between multiple software tools like Microsoft Excel, JMP, and Google Docs? An R project and the tools in R that we will talk about today will consolidate this process because it can all be done (and updated) in using one software tool, RStudio, and within one R project.\n\n\n\n\n\n\nR Project Setup\n\n\n\n\nIn the “File” menu, select “New Project”\nClick “New Directory”\nClick “New Project”\nUnder “Directory name” type: training_{USERNAME} (i.e. training_vargas)\nLeave “Create Project as subdirectory of:” set to ~\nClick “Create Project”\n\nRStudio should open your new project automatically after creating it. One way to check this is by looking at the top right corner and checking for the project name.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_02.html#organizing-an-r-project",
    "href": "session_02.html#organizing-an-r-project",
    "title": "2  RStudio Server Setup",
    "section": "2.3 Organizing an R Project",
    "text": "2.3 Organizing an R Project\nWhen starting a new research project, one of the first things I do is create an R Project for it (just like we have here!). The next step is to then populate that project with relevant directories. There are many tools out there that can do this automatically. Some examples are rrtools or usethis::create_package(). The goal is to organize your project so that it is a compendium of your research. This means that the project has all of the digital parts needed to replicate your analysis, like code, figures, the manuscript, and data access.\nSome common directories are:\n\n\n\ndata: where we store our data (often contains subdirectories for raw, processed, and metadata data)\nR: contains scripts with your custom R functions, etc. (some find this name misleading if their work has other scripts beyond the R programming language, in which case they call this directory scripts)\nplots or figs: generated plots, graphs, and figures\ndocs: summaries or reports of analysis or other relevant project information\nscripts: has all scripts where you clean and wrangle data and run your analysis.\n\nDirectory organization will vary from project to project, but the ultimate goal is to create a well organized project for both reproducibility and collaboration.\n\n\n\n\n\n\n\n\n\n\nProject Sub-directories\n\n\n\nFor this week we are going to create 3 folder (directories) in our training_{USERNAME} Rproject.\n\nIn the files pane in RStudio (bottom right), click on Folder button (with a green circle and plus sign) and create 3 new folders: data, plots, scripts.\n\nThe idea here is treat this RProject as an example of how to organize our work.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_02.html#moving-in-an-r-project-using-paths-working-directories",
    "href": "session_02.html#moving-in-an-r-project-using-paths-working-directories",
    "title": "2  RStudio Server Setup",
    "section": "2.4 Moving in an R Project using Paths & Working Directories",
    "text": "2.4 Moving in an R Project using Paths & Working Directories\n\nNow that we have your project created (and notice we know it’s an R Project because we see a .Rproj file in our Files pane), let’s learn how to move in a project. We do this using paths.\nThere are two types of paths in computing: absolute paths and relative paths.\n\nAn absolute path always starts with the root of your file system and locates files from there. The absolute path to my project directory is: /home/vargas-poulsen/training_vargas\nRelative paths start from some location in your file system that is below the root. Relative paths are combined with the path of that location to locate files on your system. R (and some other languages like MATLAB) refer to the location where the relative path starts as our working directory.\n\nRStudio projects automatically set the working directory to the directory of the project. This means that you can reference files from within the project without worrying about where the project directory itself is. If I want to read in a file from the data directory within my project, I can simply type read.csv(\"data/samples.csv\") as opposed to read.csv(\"/home/vargas-poulsen/training_vargas/data/samples.csv\").\nThis is not only convenient for you, but also when working collaboratively. We will talk more about this later, but if Matt makes a copy of my R project that I have published on GitHub, and I am using relative paths, he can run my code exactly as I have written it, without going back and changing /home/vargas-poulsen/training_vargas/data/samples.csv to /home/jones/training_jones/data/samples.csv.\nNote that once you start working in projects you should basically never need to run the setwd() command. If you are in the habit of doing this, stop and take a look at where and why you do it. Could leveraging the working directory concept of R projects eliminate this need? Almost definitely!\nSimilarly, think about how you work with absolute paths. Could you leverage the working directory of your R project to replace these with relative paths and make your code more portable? Probably!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_03.html",
    "href": "session_03.html",
    "title": "3  Introduction to R Programming",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#learning-objectives",
    "href": "session_03.html#learning-objectives",
    "title": "3  Introduction to R Programming",
    "section": "",
    "text": "Get oriented with the RStudio interface\nRun code and basic arithmetic in the Console\nPractice writing code in an R Script\nBe introduced to built-in R functions\nUse the Help pages to look up function documentation\n\n\nThis lesson is a combination of excellent lessons by others. Huge thanks to Julie Lowndes for writing most of this content and letting us build on her material, which in turn was built on Jenny Bryan’s materials. We highly recommend reading through the original lessons and using them as reference (see in the resources section below).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#welcome-to-r-programming",
    "href": "session_03.html#welcome-to-r-programming",
    "title": "3  Introduction to R Programming",
    "section": "3.1 Welcome to R Programming",
    "text": "3.1 Welcome to R Programming\n\n\n\nArtwork by Allison Horst\n\n\nThere is a vibrant community out there that is collectively developing increasingly easy to use and powerful open source programming tools. The changing landscape of programming is making learning how to code easier than it ever has been. Incorporating programming into analysis workflows not only makes science more efficient, but also more computationally reproducible. In this course, we will use the programming language R, and the accompanying integrated development environment (IDE) RStudio. R is a great language to learn for data-oriented programming because it is widely adopted, user-friendly, and (most importantly) open source!\nSo what is the difference between R and RStudio? Here is an analogy to start us off. If you were a chef, R is a knife. You have food to prepare, and the knife is one of the tools that you’ll use to accomplish your task.\nAnd if R were a knife, RStudio is the kitchen. RStudio provides a place to do your work! RStudio makes your life as a researcher easier by bringing together other tools you need to do your work efficiently - like a file browser, data viewer, help pages, terminal, community, support, the list goes on. So it’s not just the infrastructure (the user interface or IDE), although it is a great way to learn and interact with your variables, files, and interact directly with git. It’s also data science philosophy, R packages, community, and more. Although you can prepare food without a kitchen and we could learn R without RStudio, that’s not what we’re going to do. We are going to take advantage of the great RStudio support, and learn R and RStudio together.\nSomething else to start us off is to mention that you are learning a new language here. It’s an ongoing process, it takes time, you’ll make mistakes, it can be frustrating, but it will be overwhelmingly awesome in the long run. We all speak at least one language; it’s a similar process, really. And no matter how fluent you are, you’ll always be learning, you’ll be trying things in new contexts, learning words that mean the same as others, etc, just like everybody else. And just like any form of communication, there will be miscommunication that can be frustrating, but hands down we are all better off because of it.\nWhile language is a familiar concept, programming languages are in a different context from spoken languages and you will understand this context with time. For example: you have a concept that there is a first meal of the day, and there is a name for that: in English it’s “breakfast.” So if you’re learning Spanish, you could expect there is a word for this concept of a first meal. (And you’d be right: “desayuno”). We will get you to expect that programming languages also have words (called functions in R) for concepts as well. You’ll soon expect that there is a way to order values numerically. Or alphabetically. Or search for patterns in text. Or calculate the median. Or reorganize columns to rows. Or subset exactly what you want. We will get you to increase your expectations and learn to ask and find what you’re looking for.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#rstudio-ide",
    "href": "session_03.html#rstudio-ide",
    "title": "3  Introduction to R Programming",
    "section": "3.2 RStudio IDE",
    "text": "3.2 RStudio IDE\nLet’s take a tour of the RStudio interface.\n\nNotice the default panes:\n\nConsole (entire left)\nEnvironment/History (tabbed in upper right)\nFiles/Plots/Packages/Help (tabbed in lower right)\n\n\n\n\n\n\n\nQuick Tip\n\n\n\nYou can change the default location of the panes, among many other things, see Customizing RStudio.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#coding-in-the-console",
    "href": "session_03.html#coding-in-the-console",
    "title": "3  Introduction to R Programming",
    "section": "3.3 Coding in the Console",
    "text": "3.3 Coding in the Console\n\n\n\n\n\n\nBut first, an important first question: where are we?\n\n\n\nIf you’ve just opened RStudio for the first time, you’ll be in your Home directory. This is noted by the ~/ at the top of the console. You can see too that the Files pane in the lower right shows what is in the Home directory where you are. You can navigate around within that Files pane and explore, but note that you won’t change where you are: even as you click through you’ll still be Home: ~/.\n\n\n\n\n\n\n\nWe can run code in a couple of places in RStudio, including the Console, let’s start there.\nAt it’s most basic, we can use R as a calculator, let’s try a couple of examples in the console.\n\n# run in the console\n# really basic examples\n3*4\n3+4\n3-4\n3/4\n\nWhile there are many cases where it makes sense to type code directly in to the the console, it is not a great place to write most of your code since you can’t save what you ran. A better way is to create an R Script, and write your code there. Then when you run your code from the script, you can save it when you are done. We’re going to continue writing code in the Console for now, but we’ll code in an R Script later in this lesson\n\n\n\n\n\n\nQuick Tip\n\n\n\nWhen you’re in the console you’ll see a greater than sign (&gt;) at the start of a line. This is called the “prompt” and when we see it, it means R is ready to accept commands. If you see a plus sign (+) in the Console, it means R is waiting on additional information before running. You can always press escape (esc) to return to the prompt. Try practicing this by running 3* (or any incomplete expression) in the console.\n\n\n\n3.3.1 Objects in R\nLet’s say the value of 12 that we got from running 3*4 is a really important value we need to keep. To keep information in R, we need to create an object. The way information is stored in R is through objects.\nWe can assign a value of a mathematical operation (and more!) to an object in R using the assignment operator, &lt;- (greater than sign and minus sign). All objects in R are created using the assignment operator, following this form: object_name &lt;- value.\n\n\n\n\n\n\nExercise: Create an object\n\n\n\nAssign 3*4 to an object called important_value and then inspect the object you just created.\n\n\n\n# think of this code as someone saying \"important_value gets 12\".\nimportant_value &lt;- 3*4\n\nNotice how after creating the object, R doesn’t print anything. However, we know our code worked because we see the object, and the value we wanted to store is now visible in our Global Environment. We can force R to print the value of the object by calling the object name (aka typing it out) or by using parentheses.\n\n\n\n\n\n\nQuick Tip\n\n\n\nWhen you begin typing an object name RStudio will automatically show suggested completions for you that you can select by hitting tab, then press return.\n\n\n\n# printing the object by calling the object name\nimportant_value\n\n[1] 12\n\n# printing the object by wrapping the assignment syntax in parentheses\n(important_value &lt;- 3*4)\n\n[1] 12\n\n\n\n\n\n\n\n\nQuick Tip\n\n\n\nWhen you’re in the Console use the up and down arrow keys to call your command history, with the most recent commands being shown first.\n\n\n\n\n3.3.2 Naming Conventions\nBefore we run more calculations, let’s talk about naming objects. For the object, important_value we used an underscore to separate the object name. This naming convention is called snake case. There are other naming conventions including, but not limited to:\n\nwe_used_snake_case\nsomeUseCamelCase\nSomeUseUpperCamelCaseAlsoCalledPascalCase\n\nChoosing a naming convention is a personal preference, but once you choose one - be consistent! A consistent naming convention will increase the readability of your code for others and your future self.\n\n\n\n\n\n\nQuick Tip\n\n\n\nObject names cannot start with a digit and cannot contain certain characters such as a comma or a space.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#running-code-in-an-r-script",
    "href": "session_03.html#running-code-in-an-r-script",
    "title": "3  Introduction to R Programming",
    "section": "3.4 Running code in an R Script",
    "text": "3.4 Running code in an R Script\nSo far we’ve been running code in the Console, let’s try running code in an R Script. An R Script is a simple text file. RStudio uses an R Script by copying R commands from text in the file and pastes them into the Console as if you were manually entering commands yourself.\n\n\n\n\n\n\nCreating an R Script\n\n\n\n\nFrom the “File” menu, select “New File”\nClick “R Script” from the list of options\n\nRStudio should open your R Script automatically after creating it. Notice a new pane appears above the Console. This is called the Source pane and is where we write and edit R code and documents. This pane is only present if there are files open in the editor.\n\nSave the R Script in your script folder, name the file intro-to-programming.R\n\n\n\n\n3.4.1 How to run code in an R Script\nRunning code in an R Script is different than running code in the Console (aka you can’t just press return / enter). To interpret and run the code you’ve written, R needs you to send the code from the script (or editor) to the Console. Some common ways to run code in an R Script include:\n\nPlace your cursor on the line of code you want to run and use the shortcut command + return or click the Run button in the top right of the Source pane.\nHighlight the code you want to run, then use the shortcut command + return or click the Run button.\n\n\n\n3.4.2 R calculations with objects\nSo we know that objects are how R stores information, and we know we create objects using the assignment operator &lt;-. Let’s build upon that and learn how to use an object in calculations.\nImagine we have the weight of a dog in kilograms. Create the object weight_kg and assign it a value of 25.\n\n# weight of a dog in kilograms\nweight_kg &lt;- 25\n\nNow that R has weight_kg saved in the Global Environment, we can run calculations with it.\n\n\n\n\n\n\nExercise: Using weight_kg run a simple calculation\n\n\n\nLet’s convert the weight into pounds. Weight in pounds is 2.2 times the weight in kg.\n\n\n\n# converting weight from kilograms to pounds\n2.2 * weight_kg\n\n[1] 55\n\n\nYou can also store more than one value in a single object. Storing a series of weights in a single object is a convenient way to perform the same operation on multiple values at the same time. One way to create such an object is with the function c(), which stands for combine or concatenate.\nFirst let’s create a vector of weights in kilograms using c() (a vector is just an ordered collection of vales, and we’ll talk more about vectors in the next section, Data structures in R).\n\n# create a vector of weights in kilograms\nweight_kg &lt;- c(25, 33, 12)\n# call the object to inspect\nweight_kg\n\n[1] 25 33 12\n\n\nNow convert the vector weight_kg to pounds. Note that the conversion operates on all of the values in the vector.\n\n# covert `weight_kg` to pounds \nweight_kg * 2.2\n\n[1] 55.0 72.6 26.4\n\n\nWouldn’t it be helpful if we could save these new weight values we just converted? This might be important information we may need for a future calculation. How would you save these new weights in pounds?\n\n# create a new object \nweight_lb &lt;- weight_kg * 2.2\n# call `weight_lb` to check if the information you expect is there\nweight_lb\n\n[1] 55.0 72.6 26.4\n\n\n\n\n\n\n\n\nQuick Tip\n\n\n\nYou will make many objects and the assignment operator &lt;- can be tedious to type over and over. Instead, use RStudio’s keyboard shortcut: option + - (the minus sign).\nNotice that RStudio automatically surrounds &lt;- with spaces, which demonstrates a useful code formatting practice. Code is miserable to read on a good day. Give your eyes a break and use spaces.\nRStudio offers many handy keyboard shortcuts. Also, option+Shift+K brings up a keyboard shortcut reference card.\nFor more RStudio tips, check out Master of Environmental Data Science (MEDS) workshop: IDE Tips & Tricks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#data-types-class-in-r",
    "href": "session_03.html#data-types-class-in-r",
    "title": "3  Introduction to R Programming",
    "section": "3.5 Data types (class) in R",
    "text": "3.5 Data types (class) in R\n\nCommon data types in R\n\n\n\n\n\n\nData Type\nDefinition\n\n\n\n\nboolean (also called logical)\nData take on the value of either TRUE, FALSE, or NA. NA is used to represent missing values.\n\n\ncharacter\nData are string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order).\n\n\ninteger\nData are whole numbers (those numbers without a decimal point). To explicitly create an integer data type, use the suffix L (e.g. 2L).\n\n\nnumeric (also called double)\nData are numbers that contain a decimal.\n\n\n\n\nLess common data types (we won’t be going into these data types this course)\n\n\n\n\n\n\nData Type\nDefinition\n\n\n\n\ncomplex\nData are complex numbers with real and imaginary parts.\n\n\nraw\nData are raw bytes.\n\n\n\nWe’ve been using primarily integer or numeric data types so far. Let’s create an object that has a string value or a character data type.\n\nscience_rocks &lt;- \"yes it does!\"\n\n“yes it does!” is a character string, and R knows it is not a number because it has quotes \" \". You can work with character strings in your data in R easily thanks to the stringr and tidytext packages.\nThis lead us to an important concept in programming: As we now know, there are different “classes” or types of objects in R. The operations you can do with an object will depend on what type of object it is because each object has their own specialized format, designed for a specific purpose. This makes sense! Just like you wouldn’t do certain things with your car (like use it to eat soup), you won’t do certain operations with character objects (strings), such as multiply them.\nAlso, everything in R is an object. An object can be any variable, function, data structure, or method that you have written to your environment.\nTry running the following line in your script:\n\n\"Hello world!\" * 3\n\nError in \"Hello world!\" * 3 : non-numeric argument to binary operator\nWhat happened? What do you see in the Console? Why?\nLet’s break down that error message. Everything before the colon indicates that we encountered an Error, which is R’s way of saying that it could not execute the command we gave it. Despite being a bit cryptic, everything after the colon (non-numeric argument to binary operator) explains what went wrong. In this case, non-numeric argument refers to the fact that one of our arguments is not a number, while to binary operator refers to the multiplication operator, which takes two numeric objects and multiplies them together. In our case, we passed a character string (\"Hello world!\") as one of the arguments, and R does not know how to use that in the multiplication operation. Makes sense, right?\n\n\n\n\n\n\nQuick Tip\n\n\n\nYou can see what data type or class an object is using the class() function, or you can use a logical test such as: is.numeric(), is.character(), is.logical(), and so on.\n\nclass(science_rocks) # returns character\n\n[1] \"character\"\n\nis.numeric(science_rocks) # returns FALSE\n\n[1] FALSE\n\nis.character(science_rocks) # returns TRUE\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#data_structures",
    "href": "session_03.html#data_structures",
    "title": "3  Introduction to R Programming",
    "section": "3.6 Vector Data Structures in R",
    "text": "3.6 Vector Data Structures in R\nOkay, now let’s talk about vectors.\n\n\n\n\n\n\nVectors\n\n\n\nIn R, a vector is an ordered collection of values.\nFor example, [1, 7, 9] or [TRUE, FALSE, FALSE].\n\n\nInterestingly, every object in R is a vector, making it the most common and most basic data structure in R. Vectors can be thought of as a way R stores a collection of values or elements. Think back to our weight_lb vector. That was a vector of three elements each with a data type or class of numeric. Even a single value like the number 5.5 is stored as a vector with a single element (and thus has length of 1).\nWhat we’re describing is a specific type of vector called atomic vectors. To put it simply, atomic vectors only contain elements of the same data type. Atomic vectors are very common. The other type of vectors in R are lists, which are similar but may contain values of different types.\nVectors are foundational for other data structures in R, including data frames, and while we won’t go into detail about other data structures there are great resources online that do. We recommend the chapter Vectors from the online book Advanced R by Hadley Wickham.\nLet’s create some example vectors using the c() function.\n\n# atomic vector examples #\n# character vector\n(chr_vector &lt;- c(\"hello\", \"good bye\", \"see you later\"))\n\n[1] \"hello\"         \"good bye\"      \"see you later\"\n\n# numeric vector\n(numeric_vector &lt;- c(5, 1.3, 10))\n\n[1]  5.0  1.3 10.0\n\n# logical vector\n(boolean_vector &lt;- c(TRUE, FALSE, TRUE))\n\n[1]  TRUE FALSE  TRUE\n\n\nSubsetting and slicing vectors\nGiven a vector, sometimes you might want to work with all of the values (like when we converted the whole weight_kg vector above), but at other times you want to work with only one of the values or a subset. To do that, you can use square brackets [] to provide the position index of the value that you want to access. This works because vectors are ordered lists, so the values can be accessed via their index position. For example, if we create a numeric vector nv with a sequence of 9 values, we can print the whole set to screen:\n\nnv &lt;- c(1:9)*2\nnv\n\n[1]  2  4  6  8 10 12 14 16 18\n\n\nWe can also access the first value from the set using the index position 1:\n\nnv[1]              # select the first value from the vector\n\n[1] 2\n\n\nAnd the third value via the index position 3:\n\nnv[3]              # select the third value from the vector\n\n[1] 6\n\n\nVectors also have a length (the number of values they contain). Consequently, the last value in a vector is at the position determined by that length.\n\nnv[length(nv)]     # select the value at the last position in the vector\n\n[1] 18\n\n\nWe can even access multiple values from a vector, which produces a new, shorter vector that is a subset of the original:\n\nnv[3:4]            # select values from the index sequence 3 to 4\n\n[1] 6 8\n\nnv[3:6]            # select values from the index sequence 3 to 6\n\n[1]  6  8 10 12\n\n\nMultidimensional vectors\nSo far, we’ve been only dealing with one dimensional vectors (i.e., the values are indexed in a single, long collection). It’s also possible to model two dimensional vectors (matrices) and multi-dimensional arrays. These are commonly used for statistical analyses and other computations, and work similarly to one-dimensional vectors. The main difference is that, for a two dimensional array, you use two dimensions to access the values (a column index and a row index). And for multidimensional arrays, you use as many indices as you have dimensions.\nTo illustrate, let’s convert our nv vector above into a two dimensional matric by assigning it a a dim attribute listing how many rows and columns it will have. We can then access specific elements in the matrix (using two index subscripts), or a whole row, or a whole column.\n\ndim(nv) &lt;- c(3,3)  # A matrix with 3 rows and 3 columns\nnv\n\n     [,1] [,2] [,3]\n[1,]    2    8   14\n[2,]    4   10   16\n[3,]    6   12   18\n\n\nNote how the first three values make up the first column, the second three values constitute the second column, and the last three values constitute the third column. Not let’s pull out some subsets:\n\nnv[2,3]            # select the value from row2, column3\n\n[1] 16\n\nnv[,2]             # select values from the entire column2\n\n[1]  8 10 12\n\nnv[3,]             # select values from the entire row3\n\n[1]  6 12 18\n\n\nYou’ll frequently see the use of these subsetting operations throughout R code, but it all follows this same basic pattern.\n\n\n\n\n\n\nExercise: Your turn with vectors and functions\n\n\n\nImagine we have a study with 3 subjects with response times in seconds on task 1 (3.3, 5.1, 6.2) and task 2 (9.2, 7.2, 6.5). You can use the sum() function to add these response times. For example, using sum(3.3, 9.2) one gets 12.5. How coud you do the following?\n\nEncode this data in a two dimensional matrix\nCalculate the sum of the response times for subject 2 and 3 using the sum() function and a subset of the values from the matrix for each subject\n\n\n\n\n\n\n\n\n\nCode solution\n\n\n\n\n\n\nresponse_times &lt;- c(3.3, 5.1, 6.2, 9.2, 7.2, 6.5)\ndim(response_times) &lt;- c(3,2)\nresponse_times\n\n     [,1] [,2]\n[1,]  3.3  9.2\n[2,]  5.1  7.2\n[3,]  6.2  6.5\n\n(subject_2 &lt;- sum(response_times[2,]))\n\n[1] 12.3\n\n(subject_3 &lt;- sum(response_times[3,]))\n\n[1] 12.7",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#r-functions",
    "href": "session_03.html#r-functions",
    "title": "3  Introduction to R Programming",
    "section": "3.7 R Functions",
    "text": "3.7 R Functions\nSo far we’ve learned some of the basic syntax and concepts of R programming, and how to navigate RStudio, but we haven’t done any complicated or interesting programming processes yet. This is where functions come in!\nA function is a way to group a set of commands together to undertake a task in a reusable way. When a function is executed, it produces a return value. We often say that we are “calling” a function when it is executed. Functions can be user defined and saved to an object using the assignment operator, so you can write whatever functions you need, but R also has a mind-blowing collection of built-in functions ready to use. To start, we will be using some built in R functions.\nAll functions are called using the same syntax: function name with parentheses around what the function needs in order to do what it was built to do. These “needs” are pieces of information called arguments, and are required to return an expected value.\n\n\n\n\n\n\nSyntax of a function will look something like:\n\n\n\nresult_value &lt;- function_name(argument1 = value1, argument2 = value2, ...)\n\n\nBefore we use a function, let’s talk about Help pages.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#getting-help-using-help-pages",
    "href": "session_03.html#getting-help-using-help-pages",
    "title": "3  Introduction to R Programming",
    "section": "3.8 Getting help using help pages",
    "text": "3.8 Getting help using help pages\nWhat if you know the name of the function that you want to use, but don’t know exactly how to use it? Thankfully RStudio provides an easy way to access the help documentation for functions.\nThe next function we’re about to use is the mean() function.\nTo access the help page for mean(), enter the following into your console:\n\n?mean\n\nThe Help pane will show up in the lower right hand corner of your RStudio.\nThe Help page is broken down into sections:\n\nDescription: An extended description of what the function does.\nUsage: The arguments of the function(s) and their default values.\nArguments: An explanation of the data each argument is expecting.\nDetails: Any important details to be aware of.\nValue: The data the function returns.\nSee Also: Any related functions you might find useful.\nExamples: Some examples for how to use the function.\n\nAnd there’s also help for when you only sort of remember the function name: double-question mark:\n\n??install \n\n\n\n\n\n\n\nNot all functions have (or require) arguments\n\n\n\nCheck out the documentation or Help page for date().\n\n?date()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#examples-using-built-in-r-functions-mean-and-read.csv",
    "href": "session_03.html#examples-using-built-in-r-functions-mean-and-read.csv",
    "title": "3  Introduction to R Programming",
    "section": "3.9 Examples using built-in R functions mean() and read.csv()",
    "text": "3.9 Examples using built-in R functions mean() and read.csv()\n\n3.9.1 Use the mean() function to run a more complex calculation\nLet’s override our weight object with some new values, and this time we’ll assign it three dog weights in pounds:\n\nweight_lb &lt;- c(55, 25, 12)\n\n\n\n\n\n\n\nExercise: Use the mean() function to calculate the mean weight.\n\n\n\nFrom the its Help page, we learned this function will take the mean of a set of numbers. Very convenient!\nWe also learned that mean() only has one argument we need to supply a value to (x). The rest of the arguments have default values.\n\n\n\n\nCode\nmean(x = weight_lb)\n\n\n[1] 30.66667\n\n\n\n\n\n\n\n\nExercise: Save the mean to an object called mean_weight_lb\n\n\n\nHint: What operator do we use to save values to an object?\n\n\n\n\nCode\n# saving the mean using the assignment operator `&lt;-`\nmean_weight_lb &lt;- mean(x = weight_lb)\n\n\n\n\n\n\n\n\nExercise: Update weight_lb\n\n\n\nLet’s say each of the dogs gained 5 pounds and we need to update our vector, so let’s change our object’s value by assigning it new values.\n\n\n\n\nCode\nweight_lb &lt;- c(60, 30, 17)\n\n\nCall mean_weight_lb in the console or take a look at your Global Environment. Is that the value you expected? Why or why not?\nIt wasn’t the value we expected because mean_weight_lb did not change. This demonstrates an important R programming concept: Assigning a value to one object does not change the values of other objects in R.\nNow that we understand why the object’s value hasn’t changed - how do we update the value of mean_weight_lb? How is an R Script useful for this?\nThis lead us to another important programming concept, specifically for R Scripts: An R Script runs top to bottom.\nThis order of operations is important because if you are running code line by line, the values in object may be unexpected. When you are done writing your code in an R Script, it’s good practice to clear your Global Environment and use the Run button and select “Run all” to test that your R Script successfully runs top to bottom.\n\n\n3.9.2 Use the read.csv() function to read a file into R\nSo far we have learned how to assign values to objects in R, and what a function is, but we haven’t quite put it all together yet with real data yet. To do this, we will introduce the function read.csv(), which will be in the first lines of many of your future scripts. It does exactly what it says, it reads in a csv file to R.\nSince this is our first time using this function, first access the help page for read.csv(). This has a lot of information in it, as this function has a lot of arguments, and the first one is especially important - we have to tell it what file to look for. Let’s get a file!\n\n\n\n\n\n\nDownload a file from the Arctic Data Center\n\n\n\n\nNavigate to this dataset by Craig Tweedie that is published on the Arctic Data Center. Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X.\nDownload the first csv file called BGchem2008data.csv by clicking the “download” button next to the file.\nClick the “Upload” button in your RStudio server file browser.\nIn the dialog box, make sure the destination directory is the data directory in your R project, click “Choose File,” and locate the BGchem2008data.csv file. Press “OK” to upload the file.\nCheck your file was successfully uploaded by navigating into your data folder in the Files pane.\n\n\n\nNow we have to tell read.csv() how to find the file. We do this using the file argument which you can see in the usage section in the help page. In R, you can either use absolute paths (which will start with your home directory ~/) or paths relative to your current working directory. RStudio has some great auto-complete capabilities when using relative paths, so we will go that route.\nAssuming you have moved your file to a folder within training_{USERNAME} called data, and your working directory is your project directory (training_{USERNAME}) your read.csv() call will look like this:\n\n# reading in data using relative paths\nbg_chem_dat &lt;- read.csv(file = \"data/BGchem2008data.csv\")\n\nYou should now have an object of the class data.frame in your environment called bg_chem_dat. Check your environment pane to ensure this is true. Or you can check the class using the function class() in the console.\n\n\n\n\n\n\nOptional Arguments\n\n\n\nNotice that in the Help page there are many arguments that we didn’t use in the call above. Some of the arguments in function calls are optional, and some are required.\nOptional arguments will be shown in the usage section with a name = value pair, with the default value shown. If you do not specify a name = value pair for that argument in your function call, the function will assume the default value (example: header = TRUE for read.csv()).\nRequired arguments will only show the name of the argument, without a value. Note that the only required argument for read.csv() is file.\n\n\nYou can always specify arguments in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want file = \"data/BGchem2008data.csv\", since file is the first argument.\nIf we explicitly called the file argument our code would like this:\n\nbg_chem_dat &lt;- read.csv(file = \"data/BGchem2008data.csv\")\n\nIf we wanted to add another argument, say stringsAsFactors, we need to specify it explicitly using the name = value pair, since the second argument is header.\nMany R users (including myself) will set the stringsAsFactors argument using the following call:\n\n# relative file path\nbg_chem_dat &lt;- read.csv(\"data/BGchem2008data.csv\", stringsAsFactors = FALSE)\n\n\n\n\n\n\n\nQuick Tip\n\n\n\nFor functions that are used often, you’ll see many programmers will write code that does not explicitly name the first or second argument of a function, depending on the order of arguments instead of the names of those arguments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#working-with-data-frames-in-r-using-the-subset-operator",
    "href": "session_03.html#working-with-data-frames-in-r-using-the-subset-operator",
    "title": "3  Introduction to R Programming",
    "section": "3.10 Working with data frames in R using the Subset Operator $",
    "text": "3.10 Working with data frames in R using the Subset Operator $\nA data.frame is a list data structure in R that can represent tables and spreadsheets – we can think of it as a table. It is a collection of rows and columns of data, where each column has a name and represents a variable, and each row represents an observation containing a measurement of that variable. When we ran read.csv(), the object bg_chem_dat that we created was a data.frame. The columns in a data.frame might represent measured numeric response values (e.g., weight_kg), classifier variables (e.g., site_name), or categorical response variables (e.g., course_satisfaction). There are many ways R and RStudio help you explore data frames. Here are a few, give them each a try:\n\nClick on the word bg_chem_dat in the environment pane\nClick on the arrow next to bg_chem_dat in the environment pane\nExecute head(bg_chem_dat) in the Console\nExecute View(bg_chem_dat) in the Console\n\nUsually we will want to run functions on individual columns in a data.frame. To call a specific column, we use the list subset operator $.\nSay you want to look at the first few rows of the Date column only:\n\nhead(bg_chem_dat$Date)\n\nYou can also use the subset operator $ calculations. For example, let’s calculated the mean temperature of all the CTD samples.\n\nmean(bg_chem_dat$CTD_Temperature)\n\nYou can also save this calculation to an object that was created using the subset operator $.\n\nmean_temp &lt;- mean(bg_chem_dat$CTD_Temperature)\n\n\n\n\n\n\n\nOther ways to load tablular data\n\n\n\nWhile the base R package provides read.csv as a common way to load tabular data from text files, there are many other ways that can be convenient and will also produce a data.frame as output. Here are a few:\n\nUse the readr::read_csv() function from the Tidyverse to load the data file. The readr package has a bunch of convenient helpers and handles CSV files in typically expected ways, like properly typing dates and time columns. bg_chem_dat &lt;- readr::read_csv(\"data/BGchem2008data.csv\")\nLoad tabular data from Excel spreadsheets using the readxl::read_excel() function.\nLoad tabular data from Google Sheets using the googlesheets4::read_sheet() function.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#error-messages-are-your-friends",
    "href": "session_03.html#error-messages-are-your-friends",
    "title": "3  Introduction to R Programming",
    "section": "3.11 Error messages are your friends",
    "text": "3.11 Error messages are your friends\nThere is an implicit contract with the computer/scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Pay attention to how you type.\nRemember that this is a language, not dissimilar to English! There are times you aren’t understood – it’s going to happen. There are different ways this can happen. Sometimes you’ll get an error. This is like someone saying ‘What?’ or ‘Pardon’? Error messages can also be more useful, like when they say ‘I didn’t understand this specific part of what you said, I was expecting something else’. That is a great type of error message. Error messages are your friend. Google them (copy-and-paste!) to figure out what they mean. Note that knowing how to Google is a skill and takes practice - use our Masters of Environmental Data Science (MEDS) program workshop Teach Me How to Google as a guide.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd also know that there are errors that can creep in more subtly, without an error message right away, when you are giving information that is understood, but not in the way you meant. Like if I’m telling a story about tables and you’re picturing where you eat breakfast and I’m talking about data. This can leave me thinking I’ve gotten something across that the listener (or R) interpreted very differently. And as I continue telling my story you get more and more confused… So write clean code and check your work as you go to minimize these circumstances!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#r-packages",
    "href": "session_03.html#r-packages",
    "title": "3  Introduction to R Programming",
    "section": "3.12 R Packages",
    "text": "3.12 R Packages\n\n\n\nArtwork by Allison Horst\n\n\nR packages are the building blocks of computational reproducibility in R. Each package contains a set of related functions that enable you to more easily do a task or set of tasks in R. There are thousands of community-maintained packages out there for just about every imaginable use of R - including many that you have probably never thought of!\nTo install a package, we use the syntax install.packages(\"packge_name\"). A package only needs to be installed once, so this code can be run directly in the console if needed. Generally, you don’t want to save your install package calls in a script, because when you run the script it will re-install the package, which you only need to do once, or if you need to update the package.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#r-resources",
    "href": "session_03.html#r-resources",
    "title": "3  Introduction to R Programming",
    "section": "3.13 R Resources",
    "text": "3.13 R Resources\n\nAwesome R Resources to Check out\n\n\n\n\n\n\nLearning R Resources\n\nIntroduction to R lesson in Data Carpentry’s R for data analysis course\nJenny Bryan’s Stat 545 course materials\nJulie Lowndes’ Data Science Training for the Ocean Health Index\nLearn R in the console with swirl\nProgramming in R\nR, RStudio, RMarkdown\n\n\n\nCommunity Resources\n\nNCEAS’ EcoDataScience\nR-Ladies\nrOpenSci\nMinorities in R (MiR)\nTwitter - there is a lot here but some hashtags to start with are:\n\n#rstats\n#TidyTuesday\n#dataviz\n\n\n\n\nCheatsheets\n\nBase R Cheatsheet\nLaTeX Equation Formatting\nMATLAB/R Translation Cheatsheet",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#clearing-the-environment",
    "href": "session_03.html#clearing-the-environment",
    "title": "3  Introduction to R Programming",
    "section": "3.14 Clearing the environment",
    "text": "3.14 Clearing the environment\nTake a look at the objects in your Environment (Workspace) in the upper right pane. The Workspace is where user-defined objects accumulate. There are a few useful commands for getting information about your Environment, which make it easier for you to reference your objects when your Environment gets filled with many, many objects.\n\nYou can get a listing of these objects with a couple of different R functions:\n\nobjects()\n\n [1] \"boolean_vector\"  \"chr_vector\"      \"important_value\" \"mean_weight_lb\" \n [5] \"numeric_vector\"  \"nv\"              \"response_times\"  \"science_rocks\"  \n [9] \"subject_2\"       \"subject_3\"       \"weight_kg\"       \"weight_lb\"      \n\nls()\n\n [1] \"boolean_vector\"  \"chr_vector\"      \"important_value\" \"mean_weight_lb\" \n [5] \"numeric_vector\"  \"nv\"              \"response_times\"  \"science_rocks\"  \n [9] \"subject_2\"       \"subject_3\"       \"weight_kg\"       \"weight_lb\"      \n\n\nIf you want to remove the object named weight_kg, you can do this:\n\nrm(weight_kg)\n\nTo remove everything (or click the Broom icon in the Environment pane):\n\nrm(list = ls())\n\n\n\n\n\n\n\nQuick Tip\n\n\n\nIt’s good practice to clear your environment. Over time your Global Environmental will fill up with many objects, and this can result in unexpected errors or objects being overridden with unexpected values. Also it’s difficult to read / reference your environment when it’s cluttered!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#save-workspace-image-to-.rdata",
    "href": "session_03.html#save-workspace-image-to-.rdata",
    "title": "3  Introduction to R Programming",
    "section": "3.15 Save Workspace Image to .RData?",
    "text": "3.15 Save Workspace Image to .RData?\n\nDON’T SAVE\nWhen ever you close or switch projects you will be promped with the question: Do you want to save your workspace image to /“currente-project”/ .RData?\nRStudio by default wants to save the state of your environment (the objects you have in your environment pane) into the RData file so that when you open the project again you have the same environment. However, as we discussed above, it is good practice to constantly clear and clean your environment. It is generally NOT a good practice to rely on the state of your environment for your script to run and work. If you are coding reproducibly, your code should be able to reproduce the state of your environment (all the necessary objects) every time you run it. It is much better to rely on your code recreating the environment than the saving the workspace status.\nYou can change the Global Options configuration for the default to be NEVER SAVE MY WORKSPACE. Go to Tools &gt; Global Options. Under the General menu, select Never next to “Save workspace to .RData on exit”. This way you won’t get asked every time you close a project, instead RStudio knows not to save.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_03.html#logical-operators-and-expressions",
    "href": "session_03.html#logical-operators-and-expressions",
    "title": "3  Introduction to R Programming",
    "section": "3.16 Logical operators and expressions",
    "text": "3.16 Logical operators and expressions\nWe can ask questions about an object using logical operators and expressions. Let’s ask some “questions” about the weight_lb object we made.\n\n== means ‘is equal to’\n!= means ‘is not equal to’\n&lt; means ‘is less than’\n&gt; means ‘is greater than’\n&lt;= means ‘is less than or equal to’\n&gt;= means ‘is greater than or equal to’\n\n\n# examples using logical operators and expressions\nweight_lb == 2\n\n[1] FALSE FALSE FALSE\n\nweight_lb &gt;= 30\n\n[1]  TRUE  TRUE FALSE\n\nweight_lb != 5\n\n[1] TRUE TRUE TRUE",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R Programming</span>"
    ]
  },
  {
    "objectID": "session_04.html",
    "href": "session_04.html",
    "title": "4  Thinking Preferences & Social Aspects of Collaboration",
    "section": "",
    "text": "Learning Objectives\nAn activity and discussion that will provide:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Preferences & Social Aspects of Collaboration</span>"
    ]
  },
  {
    "objectID": "session_04.html#learning-objectives",
    "href": "session_04.html#learning-objectives",
    "title": "4  Thinking Preferences & Social Aspects of Collaboration",
    "section": "",
    "text": "Opportunity to get to know fellow participants and trainers\nAn introduction to variation in thinking preferences\nDiscuss about important norms and polices when working in research teams",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Preferences & Social Aspects of Collaboration</span>"
    ]
  },
  {
    "objectID": "session_04.html#thinking-preferences-activity",
    "href": "session_04.html#thinking-preferences-activity",
    "title": "4  Thinking Preferences & Social Aspects of Collaboration",
    "section": "4.1 Thinking Preferences Activity",
    "text": "4.1 Thinking Preferences Activity\nStep 1:\n\nDon’t read ahead!! We’re headed to the patio.\n\n\n4.1.1 About the Whole Brain Thinking System\nEveryone thinks differently. The way individuals think guides the way they work, and the way groups of individuals think guides how teams work. Understanding thinking preferences facilitates effective collaboration and team work.\nThe Whole Brain Model, developed by Ned Herrmann, builds upon early conceptualizations of brain functioning. For example, the left and right hemispheres were thought to be associated with different types of information processing while our neocortex and limbic system would regulate different functions and behaviours.\n\nThe Herrmann Brain Dominance Instrument (HBDI) provides insight into dominant characteristics based on thinking preferences. There are four major thinking styles that reflect the left cerebral, left limbic, right cerebral and right limbic.\n\nAnalytical (Blue)\nPractical (Green)\nRelational (Red)\nExperimental (Yellow)\n\n\nThese four thinking styles are characterized by different traits. Those in the BLUE quadrant have a strong logical and rational side. They analyze information and may be technical in their approach to problems. They are interested in the ‘what’ of a situation. Those in the GREEN quadrant have a strong organizational and sequential side. They like to plan details and are methodical in their approach. They are interested in the ‘when’ of a situation. The RED quadrant includes those that are feelings-based in their apporach. They have strong interpersonal skills and are good communicators. They are interested in the ‘who’ of a situation. Those in the YELLOW quadrant are ideas people. They are imaginative, conceptual thinkers that explore outside the box. Yellows are interested in the ‘why’ of a situation.\n\nMost of us identify with thinking styles in more than one quadrant and these different thinking preferences reflect a complex self made up of our rational, theoretical self; our ordered, safekeeping self; our emotional, interpersonal self; and our imaginitive, experimental self.\n\nUnderstanding the complexity of how people think and process information helps us understand not only our own approach to problem solving, but also how individuals within a team can contribute. There is great value in diversity of thinking styles within collaborative teams, each type bringing strengths to different aspects of project development.\n\n\n\n4.1.2 Bonus Activity\nStep 1:\n\nRead through the statements contained within this document and determine which descriptors are most like you. Make a note of them.\nReview the descriptors again and determine which are quite like you.\nYou are working towards identifying your top 20. If you have more than 20, discard the descriptors that resonate the least.\nUsing the letter codes in the right hand column, count the number of descriptors that fall into the categories A B C and D.\n\nStep 2:\n\nScroll to the second page and copy the graphic onto a piece of paper, completing the quadrant with your scores for A, B, C and D.\n\nStep 3:\n\nReflect and share out: Do you have a dominant letter? Were some of the statements you included in your top 20 easier to resonate with than others? Were you answering based on how you are or how you wish to be?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Preferences & Social Aspects of Collaboration</span>"
    ]
  },
  {
    "objectID": "session_04.html#developing-a-code-of-conduct",
    "href": "session_04.html#developing-a-code-of-conduct",
    "title": "4  Thinking Preferences & Social Aspects of Collaboration",
    "section": "4.2 Developing a Code of Conduct",
    "text": "4.2 Developing a Code of Conduct\nWhether you are joining a lab group or establishing a new collaboration, articulating a set of shared agreements about how people in the group will treat each other will help create the conditions for successful collaboration. If agreements or a code of conduct do not yet exist, invite a conversation among all members to create them. Co-creation of a code of conduct will foster collaboration and engagement as a process in and of itself, and is important to ensure all voices heard such that your code of conduct represents the perspectives of your community. If a code of conduct already exists, and your community will be a long-acting collaboration, you might consider revising the code of conduct. Having your group ‘sign off’ on the code of conduct, whether revised or not, supports adoption of the principles.\n\n\n\n\n\n\nWhen developing a code of conduct, keep in mind:\n\n\n\n\nInvite a conversation among all members to create grounding guidelines\nCo-creation of a code of conduct will foster collaboration and engagement as a process in it self.\nIt is important to ensure all voices are heard\nThings to consider\n\nBehaviors you want to encourage\nBehaviors that will not be tolerated\n\n\n\n\nFor example, the Openscapes code of conduct includes\n\nBe respectful\nBe direct but professional\nBe inclusive\nUnderstand different perspectives\nAppreciate and Accommodate Our Similarities and Differences\nLead by Example\n\n\nUnderstand Different Perspectives  Our goal should not be to “win” every disagreement or argument. A more productive goal is to be open to ideas that make our own ideas better. Strive to be an example for inclusive thinking. “Winning” is when different perspectives make our work richer and stronger. (openscapes.org)\n\nBelow are other example codes of conduct:\n\nNCEAS Code of Conduct\nCarpentries Code of Conduct\nArctic Data Center Code of Conduct\nMozilla Community Participation Guidelines\nEcological Society of America Code of Conduct",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Preferences & Social Aspects of Collaboration</span>"
    ]
  },
  {
    "objectID": "session_04.html#authorship-and-credit-policies",
    "href": "session_04.html#authorship-and-credit-policies",
    "title": "4  Thinking Preferences & Social Aspects of Collaboration",
    "section": "4.3 Authorship and Credit Policies",
    "text": "4.3 Authorship and Credit Policies\n\nNavigating issues of intellectual property and credit can be a challenge, particularly for early career researchers. Open communication is critical to avoiding misunderstandings and conflicts. Talk to your coauthors and collaborators about authorship, credit, and data sharing early and often. This is particularly important when working with new collaborators and across lab groups or disciplines which may have divergent views on authorship and data sharing. If you feel uncomfortable talking about issues surrounding credit or intellectual property, seek the advice or assistance of a mentor to support you in having these important conversations.\nThe “Publication” section of the Ecological Society of America’s Code of Ethics is a useful starting point for discussions about co-authorship, as are the International Committee of Medical Journal Editors guidelines for authorship and contribution. You should also check guidelines published by the journal(s) to which you anticipate submitting your work.\nFor collaborative research projects, develop an authorship agreement for your group early in the project and refer to it for each product. This example authorship agreement from the Arctic Data Center provides a useful template. It builds from information contained within Weltzin et al (2006) and provides a rubric for inclusion of individuals as authors. Your collaborative team may not choose to adopt the agreement in the current form, however it will prompt thought and discussion in advance of developing a consensus. Some key questions to consider as you are working with your team to develop the agreement:\n\nWhat roles do we anticipate contributors will play? e.g., the NISO Contributor Roles Taxonomy (CRediT) identifies 14 distinct roles:\n\nConceptualization\nData curation\nFormal Analysis\nFunding acquisition\nInvestigation\nMethodology\nProject administration\nResources\nSoftware\nSupervision\nValidation\nVisualization\nWriting – original draft\nWriting – review & editing\n\nWhat are our criteria for authorship? (See the ICMJE guidelines for potential criteria)\nWill we extend the opportunity for authorship to all group members on every paper or product?\nDo we want to have an opt in or opt out policy? (In an opt out policy, all group members are considered authors from the outset and must request removal from the paper if they don’t want think they meet the criteria for authorship)\nWho has the authority to make decisions about authorship? Lead author? PI? Group?\nHow will we decide authorship order?\nIn what other ways will we acknowledge contributions and extend credit to collaborators?\nHow will we resolve conflicts if they arise?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Preferences & Social Aspects of Collaboration</span>"
    ]
  },
  {
    "objectID": "session_04.html#data-sharing-and-reuse-policies",
    "href": "session_04.html#data-sharing-and-reuse-policies",
    "title": "4  Thinking Preferences & Social Aspects of Collaboration",
    "section": "4.4 Data Sharing and Reuse Policies",
    "text": "4.4 Data Sharing and Reuse Policies\nAs with authorship agreements, it is valuable to establish a shared agreement around handling of data when embarking on collaborative projects. Data collected as part of a funded research activity will typically have been managed as part of the Data Management Plan (DMP) associated with that project. However, collaborative research brings together data from across research projects with different data management plans and can include publicly accessible data from repositories where no management plan is available. For these reasons, a discussion and agreement around the handling of data brought into and resulting from the collaboration is warranted and management of this new data may benefit from going through a data management planning process. Below we discuss example data agreements.\nThe example data policy template provided by the Arctic Data Center addresses three categories of data.\n\nIndividual data not in the public domain\nIndividual data with public access\nDerived data resulting from the project\n\nFor the first category, the agreement considers conditions under which those data may be used and permissions associated with use. It also addresses access and sharing. In the case of individual, publicly accessible data, the agreement stipulates that the team will abide by the attribution and usage policies that the data were published under, noting how those requirements we met. In the case of derived data, the agreement reads similar to a DMP with consideration of making the data public; management, documentation and archiving; pre-publication sharing; and public sharing and attribution. As research data objects receive a persistent identifier (PID), often a DOI, there are citable objects and consideration should be given to authorship of data, as with articles.\nThe following example lab policy from the Wolkovich Lab combines data management practices with authorship guidelines and data sharing agreements. It provides a lot of detail about how this lab approaches data use, attribution and authorship.\nBelow an example from Wolkovich’s Lab Data Management Policies, Section 6: Co-authorship & data.\n\n\n\nData Management Policies for the Wolkovich Lab - Preview\nThis policy is communicated with all incoming lab members, from undergraduate to postdocs and visiting scholars, and is shared here with permission from Dr Elizabeth Wolkovich.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Preferences & Social Aspects of Collaboration</span>"
    ]
  },
  {
    "objectID": "session_04.html#slide-deck",
    "href": "session_04.html#slide-deck",
    "title": "4  Thinking Preferences & Social Aspects of Collaboration",
    "section": "4.5 Slide Deck",
    "text": "4.5 Slide Deck",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Preferences & Social Aspects of Collaboration</span>"
    ]
  },
  {
    "objectID": "session_05.html",
    "href": "session_05.html",
    "title": "5  Writing Data Management Plans",
    "section": "",
    "text": "5.1 Writing Good Data Management Plans",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Writing Data Management Plans</span>"
    ]
  },
  {
    "objectID": "session_05.html#writing-good-data-management-plans",
    "href": "session_05.html#writing-good-data-management-plans",
    "title": "5  Writing Data Management Plans",
    "section": "",
    "text": "5.1.1 Learning Objectives\nIn this lesson, you will learn:\n\nWhy to create data management plans\nThe major components of data management plans\nTools that can help create a data management plan\nFeatures and functionality of the DMPTool\n\n\n\n5.1.2 When to Plan: The Data Life Cycle\nShown below is one version of the Data Life Cycle that was developed by DataONE. The data life cycle provides a high level overview of the stages involved in successful management and preservation of data for use and reuse. Multiple versions of the data life cycle exist with differences attributable to variation in practices across domains or communities. It is not necessary for researchers to move through the data life cycle in a cyclical fashion and some research activities might use only part of the life cycle. For instance, a project involving meta-analysis might focus on the Discover, Integrate, and Analyze steps, while a project focused on primary data collection and analysis might bypass the Discover and Integrate steps. However, Plan is at the top of the data life cycle as it is advisable to initiate your data management planning at the beginning of your research process, before any data has been collected.\n\n\n\n5.1.3 Why Plan?\nPlanning data management in advance provides a number of benefits to the researcher.\n\nSaves time and increases efficiency: Data management planning requires that a researcher think about data handling in advance of data collection, potentially raising any challenges before they occur.\nEngages your team: Being able to plan effectively will require conversations with multiple parties, engaging project participants from the outset.\nAllows you to stay organized: It will be easier to organize your data for analysis and reuse if you’ve made a plan about what analysis you want to run, future iterations, and more.\nMeet funder requirements: Most funders require a data management plan (DMP) as part of the proposal process.\nShare data: Information in the DMP is the foundation for archiving and sharing data with community.\n\n\n\n5.1.4 How to Plan\n\nMake sure to plan from the start to avoid confusion, data loss, and increase efficiency. Given DMPs are a requirement of funding agencies, it is nearly always necessary to plan from the start. However, the same should apply to research that is being undertaken outside of a specific funded proposal.\nAs indicated above, engaging your team is a benefit of data management planning. Collaborators involved in the data collection and processing of your research data bring diverse expertise. Therefore, plan in collaboration with these individuals.\nMake sure to utilize resources that are available to assist you in helping to write a good DMP. These might include your institutional library or organization data manager, online resources or education materials such as these.\nUse tools available to you; you don’t have to reinvent the wheel.\nRevise your plan as situations change or as you potentially adapt/alter your project. Like your research projects, DMPs are not static, they require changes and updates throughout the research project process.\n\n\n\n5.1.5 What to include in a DMP\nIf you are writing a DMP as part of a solicitation proposal, the funding agency will have guidelines for the information they want to be provided in the plan. However, in general, a good plan will provide information on the:\n\nstudy design\ndata to be collected\nmetadata\npolicies for access\nsharing & reuse\nlong-term storage & data management\nand budget\n\nA note on Metadata: Both basic metadata (such as title and researcher contact information) and comprehensive metadata (such as complete methods of data collection) are critical for accurate interpretation and understanding. The full definitions of variables, especially units, inside each dataset are also critical as they relate to the methods used for creation. Knowing certain blocking or grouping methods, for example, would be necessary to understand studies for proper comparisons and synthesis.\n\n\n5.1.6 NSF DMP requirements\nIn the 2014 Proposal Preparation Instructions, Section J ‘Special Information and Supplementary Documentation’ NSF put forward the baseline requirements for a DMP. In addition, there are specific division and program requirements that provide additional detail. If you are working on a research project with funding that does not require a DMP, or are developing a plan for unfunded research, the NSF generic requirements are a good set of guidelines to follow.\nThe following questions are the prompting questions in the Arctic Data Center DMP template for NSF projects, excluding the fairly straightforward personnel section.\nFive Sections of the NSF DMP Requirements\n1. What types of data, samples, collections, software, materials, etc. will be produced during your project?\nTypes of data, samples, physical collections, software, curriculum materials, other materials produced during project\n2. What format(s) will data and metadata be collected, processed, and stored in?\nStandards to be used for data and metadata format and content (for initial data collection, as well as subsequent storage and processing)\n3. How will data be accessed and shared during the course of the project?\nProvisions for appropriate protection of privacy, confidentiality, security, intellectual property, or other rights or requirements\n4. How do you anticipate the data for this project will be used?\nIncluding re-distribution and the production of derivatives\n5. What is the long-term strategy for maintaining, curating, and archiving the data?\nPlans for archiving data, samples, research products and for preservation of access\nThe ‘NSF Public Access Plan 2.0’ recently went through public comment, and proposes that DMPs be renamed to Data Management and Sharing Plans (DMSP) and include requirements to ensure that data is submitted to data repositories. Any new requirements will be shared with researchers once this plan goes into effect.\nCurrently, the NSF is revising DMP requirements, and there will be changes coming to NSF DMPs at the release of the next Proposal and Award Policies and Procedures Guide (PAPPG). The draft revisions of the PAPPG is online and accessible for review here. The section on Data Management and Sharing Plans (DMSPs) begins on section II-31 (page 70 in the PDF) and is highlighted in yellow with an introductory comment explaining the revised section.\n\n5.1.6.1 Individual Reflection\nNow that we’ve discussed the data life cycle, how to plan, what to generally include in a DMP, and the NSF DMP requirements - take five minutes to go through each required section for a NSF DMP and write down some initial thoughts on how you would approach completing those sections. What information would you include? How would you plan to answer the questions? What do you need to answer the questions in each section?\nAnother consideration for your data management plan is the extent to which you can document ethical research practices. We’ll go over CARE and FAIR principles in future lessons, but a DMP can be a good place to include context on:\n\npermits needed for your research\ncodes of conduct the research team decided upoon prior to beginning data collection\ninstitutional or local permission needed for sampling\nthe effect on local community members and/or Indigenous lands\nwhether or not authorship expectations are clear for everyone involved in data collection\n\nAfter we’ll get into groups to further discuss.\n\n\n5.1.6.2 Group Discussion\nLet’s split up into five groups; one group for each required section of a NSF DMP. As a group, share your initial thoughts about the section you’ve been assigned to and together as a group discuss how you would complete that section. Select someone in the group to share your approach to the whole class. Take the next 10-15 minutes for group discussion.\nSome guiding questions:\n\nWhat information do you need to complete the section? Think both broadly and detailed.\nDo you need to reference outside materials to complete the section? Is this information already known / found or is additional research required?\nWhat is the relevant, key information necessary for the research to be understood for either your future self or for someone new to the data? What information would you want to know if you were given a new project to work on? Being explicit and including details are important to think about for this question.\nWhat workflows, documentation, standards, maintenance, tools / software, or roles are required?\n\n\n\n\n5.1.7 Tools in Support of Creating a DMP\n\nThe DMPTool and DMP Online are both easy-to-use web based tools that support the development of a DMP. The tools are partnered and share a code base; the DMPTool incorporates templates from US funding agencies and the DMP Online is focused on EU requirements.\n\n\n5.1.7.1 Quick Tips for DMPTool\n\nThere is no requirement to answer all questions in one sitting. Completing a DMP can require information gathering from multiple sources. Saving the plan at any point does not submit the plan, it simply saves your edits. This means you can move between sections in any order or save as you go.\nYou can collaborate in DMPTool which keeps all commentary together, saves time on collaboration, and makes it easy to access the most current version at any time since it is always available in DMPTool.\n\n\n\n\n5.1.8 Arctic Data Center Support for DMPs\nTo support researchers in creating DMPs that fulfills NSF template requirements and provides guidance on how to work with the Arctic Data Center for preservation, we have created an Arctic Data Center template within the DMPTool. This template walks researchers through the questions required by NSF and includes recommendations directly from the Arctic Data Center team.\n\nWhen creating a new plan, indicate that your funding agency is the National Science Foundation and you will then have the option to select a template. Here you can choose the Arctic Data Center.\n\nAs you answer the questions posed, guidance information from the Arctic Data Center will be visible under the ‘NSF’ tab on the right hand side. An example answer is also provided at the bottom. It is not intended that you copy and paste this verbatim. Rather, this is example prose that you can refer to for answering the question.\n\n\n\n5.1.9 Sharing Your DMP\nThe DMPTool allows you to collaborate with authorized individuals on your plan and also to publish it (make it publicly accessible) via the website. If your research activity is funded, it is also useful to share your DMP with the Arctic Data Center. This is not an NSF requirement, but can greatly help the Arctic Data Center team prepare for your data submission. Especially if you are anticipating a high volume of data.\n\n\n5.1.10 Additional Resources\nThe article Ten Simple Rules for Creating a Good Data Management Plan is a great resource for thinking about writing a data management plan and the information you should include within the plan.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Writing Data Management Plans</span>"
    ]
  },
  {
    "objectID": "session_06.html",
    "href": "session_06.html",
    "title": "6  Literate Analysis with Quarto",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_06.html#learning-objectives",
    "href": "session_06.html#learning-objectives",
    "title": "6  Literate Analysis with Quarto",
    "section": "",
    "text": "Introduce literate analysis using Quarto (an extension of RMarkdown’s features)\nLearn markdown syntax and run R code using Quarto\nBuild and render an example analysis",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_06.html#introduction-to-literate-programming",
    "href": "session_06.html#introduction-to-literate-programming",
    "title": "6  Literate Analysis with Quarto",
    "section": "6.1 Introduction to Literate Programming",
    "text": "6.1 Introduction to Literate Programming\nAll too often, computational methods are written in such a way as to be borderline incomprehensible even to the person who originally wrote the code! The reason for this is obvious, computers interpret information very differently than people do. In 1984, Donald Knuth proposed a reversal of the programming paradigm by introducing the concept of Literate Programming (Knuth 1984).\n\n“Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.”\n\nIf our aim is to make scientific research more transparent, the appeal of this paradigm reversal is immediately apparent. By switching to a literate analysis model, you help enable human understanding of what the computer is doing. As Knuth describes, in the literate analysis model, the author is an “essayist” who chooses variable names carefully, explains what they mean, and introduces concepts in the analysis in a way that facilitates understanding.\nQuarto and RMarkdown are an excellent way to generate literate analysis, and a reproducible workflow. These types of files, combine R the programming language, and markdown, a set of text formatting directives.\nIn an R script, the language assumes that you are writing R code, unless you specify that you are writing prose (using a comment, designated by #). The paradigm shift of literate analysis comes in the switch to RMarkdown or Quarto, where instead of assuming you are writing code, they assume that you are writing prose unless you specify that you are writing code. This, along with the formatting provided by markdown, encourages the “essayist” to write understandable prose to accompany the code that explains to the human-beings reading the document what the author told the computer to do. This is in contrast to writing just R code, where the author telling to the computer what to do with maybe a smattering of terse comments explaining the code to a reader.\nBefore we dive in deeper, let’s look at an example of what a rendered literate analysis can look like using a real example. Here is an example of an analysis workflow written using RMarkdown. Note that if this analysis would be in Quarto, the render version it would be similar, except for formatting and layout (eg: the default font in Quarto is different).\nThere are a few things to notice about this document, which assembles a set of similar data sources on salmon brood tables with different formatting into a single data source.\n\nIt introduces the data sources using in-line images, links, interactive tables, and interactive maps.\nAn example of data formatting from one source using R is shown.\nThe document executes a set of formatting scripts in a directory to generate a single merged file.\nSome simple quality checks are performed (and their output shown) on the merged data.\nSimple analysis and plots are shown.\n\nIn addition to achieving literate analysis, this document also represents a reproducible analysis. Because the entire merging and quality control of the data is done using the R code in the Quarto file, if a new data source and formatting script are added, the document can be run all at once with a single click to re-generate the quality control, plots, and analysis of the updated data.\n\n\n\n\n\n\nA note on reproducibility\n\n\n\nReproducible analysis allow you to automatize how the figures and the statistics in your analysis are generated. This process also helps your collaborators, your readers and your future self to follow your code trail that leads to the original data, increasing the transparency of your science.\nLiterate analysis help reduce the mistakes from copying and pasting across software, keeps results and models in sync, and allows you to provide interested readers with more information about the different approaches and analyses you tried before coming up with the final results (British Ecological Society (2017)).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_06.html#rmarkdown-and-quarto",
    "href": "session_06.html#rmarkdown-and-quarto",
    "title": "6  Literate Analysis with Quarto",
    "section": "6.2 RMarkdown and Quarto",
    "text": "6.2 RMarkdown and Quarto\nYou can identify a Quarto file with the .qmd extension. On the other hand, an RMarkdown file has a .Rmd extension. Both have similar structures and both combine prose with code. Quarto provides rich support to languages other than R such as Python, Observable, and Julia. It also excels in formatting and layout, allowing users to customize in detail the looks of the rendered documents. On the other hand, RMarkdown is compatible with some languages that Quarto is not, for example bash. Quarto and Rmarkdown are amazing tools to use for collaborative research. During this course we will spend some time learning and using the basics of Quarto and provide some comparisons to RMarkdown.\n\nNow, let’s take a look at the structure of each of these files. They both look, for the most part, the same with minor differences.\n\n\nFinally, lets compare each of these files when knitted/rendered.\n\n\nAgain, we see similar outcomes, with minor differences mainly in formatting (font, style of showing code chunks, etc.)\nBoth type of documents have three main components:\n\nYAML metadata to guide the document’s build process\nCode chunks to run\nProse (Text to display)\n\nToday we are going to use Quarto to run some analysis on data. We are specifically going to focus on the code chunk and text components. We will discuss more about the how the YAML works in a Quarto later in the course.\n\n\n\n\n\n\nThe YAML\n\n\n\nThe YAML is the document’s metadata which sets guidelines on how you want the output of your document to look like. It is located at the top of your file, delineated by three dashes (---) at the top and at the bottom of it. It can be used to specify:\n\nCharacteristics of your documents such at title, author, date of creation.\nArguments to pass on the building process to control the format of the output.\nAdditional information such as the bibliography file (and formatting of the references)\nSpecific parameters for your report (e.g.: just used a subset of the data).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_06.html#a-quarto-document",
    "href": "session_06.html#a-quarto-document",
    "title": "6  Literate Analysis with Quarto",
    "section": "6.3 A Quarto Document",
    "text": "6.3 A Quarto Document\nLet’s open a Quarto file by following the instructions below.\n\n\n\n\n\n\nSetup\n\n\n\n\nOpen a new Quarto file using the following prompts: File &gt; New File &gt; Quarto Document\nA popup window will appear.\nGive your file a new title, e.g. “Introduction to Quarto”.\nLeave the output format as HTML and Engine set to Knitr.\nThen click the “Create” button.\n\n\n\nThe first thing to notice is that by opening a file, we see the fourth pane of the RStudio pops up. This is our Quarto document, which is essentially a text editor. We also see in the upper left side that we are looking at the document under the “Visual editor”. This is probably a familiar way of looking at a text document. To introduce the markdown syntax, we are going to move to the source editor and then come back to the visual editor. In the upper left corner, click on Source. See how the formatting changed? In the Source editor we are looking at the same text, but in markdown syntax. The visual editor on the other hand, allows us to see how markdown is rendered, therefore how it is going to look in our output document.\nLet’s have a look at this file — As we saw in the examples above, it looks a little different than an R script. It’s not blank; there is some initial text already provided for you. Let’s identify the three main components we introduced before. We have the YAML at the top, in between the two sets of dashed lines. Then we also see white and grey sections. The grey sections are R code chunks, and the white sections are plain text.\nLet’s go ahead and render this file by clicking the “Render” button, next to the blue arrow at the top of the Quarto file. When you first click this button, RStudio will prompt you to save this file. Save it in into your scripts folder, and name it something that you will remember (like quarto-intro.Rmd).\n\n\n\nWhat do you notice between the two?\nFirst, the render process produced a second file (an HTML file) that popped up in a second window in the browser. You’ll also see this file in your directory with the same name as your qmd, but with the .html extension. In its simplest format, Quarto files come in pairs (same as RMarkdown files): the Quarto document, and its rendered version. In this case, we are rendering the file into HTML. You can also knit to PDF or Word files and others.\nNotice how the grey R code chunks are surrounded by 3 back-ticks and {r LABEL}. The first chunk, in this case 1+1, is evaluated and returns the output number (2). Notice the line in the second chunk that says #| echo: false? This is a code chunk option that indicates not to print the code. In the rendered version, we can see the outcome of 2*2, but not the executed code that created the outcome.\nThe table below show some of the options available for customizing outputs (Quarto.org).\n\nCode chunk options\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n#| eval:\nEvaluate the code chunk (if false, just echos the code into the output).\n\n\n#| echo:\nInclude the source code in output\n\n\n#| warning:\nInclude warnings in the output.\n\n\n#| error:\nInclude warnings in the output.\n\n\n#| include:\nCatch all for preventing any output (code or results) from being included (e.g.include: false suppresses all output from the code block).\n\n\n\nNote that you can also combine these options by adding more than one to a code chunk.\n\n\n\n\n\n\nImportant\n\n\n\nOne important difference between Quarto documents and RMarkdown documents is that in Quarto, chunk options are written in special comment format (#|) at the top of code chunks rather than within the wiggly brackets next to ```{r} at the begging of the chunk. For example:\n\nQuarto code options syntax\n\n\n\nRMarkdown code options syntax\n\n\n\n\nIt is important to emphasize one more time that in a Quarto (and RMarkdown) document, the gray areas of the document are code. In this case, it is R code because that is what is indicated in the ```{r} syntax at the start of this gray area. And the white areas of a qmd are in markdown language.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_06.html#markdown-syntax",
    "href": "session_06.html#markdown-syntax",
    "title": "6  Literate Analysis with Quarto",
    "section": "6.4 Markdown Syntax",
    "text": "6.4 Markdown Syntax\nLet’s start by talking about markdown. Markdown is a formatting language for plain text, and there are only around 15 rules to know.\nNotice the syntax in the document we just knitted:\n\nHeaders get rendered at multiple levels: #, ##\nBold: **word**\n\nThere are some good cheatsheets to get you started, and here is one built into RStudio: Go to Help &gt; Markdown Quick Reference.\n\n\n\n\n\n\nImportant\n\n\n\nThe hash symbol # is used differently in markdown and in R\n\nIn an R script or inside an R code chunk, a hash indicates a comment that will not be evaluated. You can use as many as you want: # is equivalent to ######. It’s just a matter of style.\nIn markdown, a hash indicates a level of a header. And the number you use matters: # is a “level one header”, meaning the biggest font and the top of the hierarchy. ### is a level three header, and will show up nested below the # and ## headers.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nIn markdown, write some italic text, make a numbered list, and add a few sub-headers. Use the Markdown Quick Reference (in the menu bar: Help &gt; Markdown Quick Reference).\nRe-knit your html file and observe your edits.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_06.html#the-visual-editor",
    "href": "session_06.html#the-visual-editor",
    "title": "6  Literate Analysis with Quarto",
    "section": "6.5 The Visual Editor",
    "text": "6.5 The Visual Editor\nQuarto has a “what you see is what you mean” (WYSIWYM) editor or Visual editor, which can be a nice way to write markdown without remembering all of the markdown rules. Since there aren’t many rules for markdown, we recommend just learning them especially since markdown is used in many other contexts besides Quarto and RMarkdown. For example, formatting GitHub comments and README files.\nTo access the editor, click the Visual button in the upper left hand corner of your editor pane. You’ll notice that your document is now formatted as you type, and you can change elements of the formatting using the row of icons in the top of the editor pane. Although we don’t really recommend doing all of your markdown composition in the Visual editor, there are two features to this editor that we believe are immensely helpful: adding citations, and adding tables.\n\n6.5.1 Adding citations\nTo add a citation, go to the visual editor and in the insert drop down, select “Citation.” In the window that appears, there are several options in the left hand panel for the source of your citation. If you have a citation manager, such as Zotero, installed, this would be included in that list. For now, select “From DOI”, and in the search bar enter a DOI of your choice (e.g.: 10.1038/s41467-020-17726-z), then select “Insert.”\n\nAfter selecting insert, a couple of things happen. First, the citation reference is inserted into your markdown text as [@oke2020]. Second, a file called references.bib containing the BibTex format of the citation is created. Third, that file is added to the YAML header of your Quarto document (bibliography: references.bib). Adding another citation will automatically update your references.bib file. So easy!\n\n\n6.5.2 Adding table in markdown\nThe second task that the visual editor is convenient for is generating tables. Markdown tables are a bit finicky and annoying to type, and there are a number of formatting options that are difficult to remember if you don’t use them often. In the top icon bar, the “Table” drop down gives several options for inserting, editing, and formatting tables. Experiment with this menu to insert a small table.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_06.html#code-chunks-in-quarto",
    "href": "session_06.html#code-chunks-in-quarto",
    "title": "6  Literate Analysis with Quarto",
    "section": "6.6 Code Chunks in Quarto",
    "text": "6.6 Code Chunks in Quarto\nEvery time when opening a new Quarto document, we should start by deleting all template text (everything except for the YAML). Then we save the document into the most convenient folder of our project. Now we are ready to start our work.\nYou can create a new chunk in your Quarto in one of these ways:\n\nGo to Code in the top menu bar, click “Insert Chunk”\nType by hand {r}\nUse the keyboard shortcut\n\nMac:command + option + i\nWindows: Ctrl + Alt + i\n\n\n\n\n\n\n\n\nAbout code chunks\n\n\n\nEach code chunk needs to have an opening syntax ```{r} and a closing syntax ```. Everything in between these lines will be identified as R code.\n\n\nIf I want to write some R code, this is how it would look like:\n\nx &lt;- 4 * 8\n\nheights_ft &lt;- c(5.2, 6.0, 5.7)\n\ncoef &lt;- 3.14\n\nHitting return does not execute this command; remember, it’s just a text file. To execute it, we need to get what we typed in the the R chunk (the grey R code) down into the console. How do we do it? There are several ways (let’s do each of them):\n\nCopy-paste this line into the console (generally not recommended as a primary method)\nSelect the line (or simply put the cursor there), and click “Run”. This is available from:\n\nthe bar above the file (green arrow)\nthe menu bar: Code &gt; Run Selected Line(s)\nkeyboard shortcut: command-return\n\nClick the green arrow at the right of the code chunk",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_06.html#practice-literate-analysis-with-ocean-water-samples",
    "href": "session_06.html#practice-literate-analysis-with-ocean-water-samples",
    "title": "6  Literate Analysis with Quarto",
    "section": "6.7 Practice: Literate Analysis with Ocean Water Samples",
    "text": "6.7 Practice: Literate Analysis with Ocean Water Samples\nNow that we have gone over the basics, let’s go a little deeper by building a simple Quarto document that represents a literate analysis using real data. We are going to work with the seawater chemistry data. We are going to use the BGchem2008data.csv data we downloaded in our previous session.\n\n\n6.7.1 Getting Started\n\n\n\n\n\n\n\n\nExperienced R users who have never used Quarto (or RMarkdown) often struggle a bit in the transition to developing analysis in Prose+Code format — which makes sense! It is switching the code paradigm to a new way of thinking.\nRather than starting an R chunk and putting all of your code in that single chunk, below we describe what we think is a better way.\n\nOpen a document and block out the high-level sections you know you’ll need to include using top level headers.\nAdd bullet points for some high level pseudo-code steps you know you’ll need to take.\nStart filling in, under each bullet point, the code that accomplishes each step. As you write your code, transform your bullet points into prose, and add new bullet points or sections as needed.\n\nFor this mini-analysis, we will have the following sections and code steps:\n\nIntroduction\n\nAbout the data\nSetup\nRead in data\n\nAnalysis\n\nCalculate summary statistics\nCalculate mean Redfield ratio\nPlot Redfield ratio\n\nConclusion\n\n\n\n\n\n\n\nExercise\n\n\n\nUnder “About the data”, write a sentence saying where the data set came from, including a hyperlink to the data. Also mention when the data was downloaded.\nHint: Navigate to Help &gt; Markdown Quick Reference to look-up the hyperlink syntax.\n\n\n\n\n6.7.2 Read in the data\nNow that we have outlined our document, we can start writing code! To read the data into our environment, we will use a function from the readr package.\nTo use a package in our analysis, we need to first make sure it is installed (you can install a package by running install.packages(\"name-of-package\")). Once installed you need to load it into our environment using library(package_name). Even though we have installed it, we haven’t yet told our R session to access it. Because there are so many packages (many with conflicting namespaces) R cannot automatically load every single package you have installed. Instead, you load only the ones you need for a particular analysis. Loading the package is a key part of the reproducible aspect of our literate analysis, so we will include it as an R chunk as part of our Setup.\n\n\n\n\n\n\nBest Practice\n\n\n\nIt is generally good practice to include all of your library() calls in a single, dedicated R chunk near the top of your document. This lets collaborators know what packages they might need to install before they start running your code.\n\n\nThe server should have already installed the two packages we need for now: readr and here. Let’s add a new R chunk below your Setup header that calls these libraries, and run it.\nIt should look like this:\n\nlibrary(readr)\nlibrary(here)\n\n\n\n\n\n\n\nQuarto file path and the here() function\n\n\n\nQuarto has a special way of handling relative paths that can be very handy. When working in a Quarto document, R will set all paths relative to the location of the Quarto file. This can make things easier to read in data if your Quarto document is stored in the same directory or “near” by. However, more often that not, your .qmd file will be stored in a a folder (e.g scripts) and your data in a data folder, (both folder in the main project directory).\nThe here() function helps navigate this file path mix up in a straight forward and reproducible way. This function sets the file path to the project’s directory and builds the rest of the file path from there. Making it easier to find files inside different folders in a project. In this case, because the .qmd file lives in the script folder, here() makes is easy to navigate back into the project’s directory and then into the data folder to read in our file.\n\n\nNow, under “Read data”, add a code chunk that uses the read_csv() with the here() function to read in your data file.\n\n\nRows: 70 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): Station\ndbl  (16): Latitude, Longitude, Target_Depth, CTD_Depth, CTD_Salinity, CTD_T...\ndttm  (1): Time\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nbg_chem &lt;- read_csv(here::here(\"data/BGchem2008data.csv\"))\n\n\n\nWhy read_csv() over read.csv()?\nWe chose to show read_csv() from the readr package to introduce the concept of packages, to show you how to load packages, and read_csv() has several advantages over read.csv() from base R, including:\n\nMore reasonable function defaults (no stringsAsFactors!)\nSmarter column type parsing, especially for dates\nread_csv() is much faster than read.csv(), which is helpful for large files\n\nOnce you run this line in your document, you should see the bg_chem object populate in your environment pane. It also spits out lots of text explaining what types the function parsed each column into. This text is important, and should be examined, but we might not want it in our final document.\n\n\n\n\n\n\nExercise\n\n\n\nHow would you suppress the warnings (so they don’t show in our output file) form a specific code chunk?\nHint: Code chunk options\n\n\n\n\n6.7.3 Calculate Summary Statistics\nAs our “analysis” we are going to calculate some very simple summary statistics and generate a single plot. Using water samples from the Arctic Ocean, we will examine the ratio of nitrogen to phosphate to see how closely the data match the Redfield ratio, which is the consistent 16:1 ratio of nitrogen to phosphorous atoms found in marine phytoplankton.\nLet’s start by exploring the data we just read. Every time we read a new data set, it is important to familiarize yourself with it and make sure that the data looks as expected. Below are some useful functions for exploring your data.\nLet’s start by creating a new R chunk and running the following functions. Because this is just an exploration, and we do not want this chunk to be part of our report, we will indicate that by adding #|eval: false and #| echo: false in the setup of the chunk, that way, the code in this chunk will not run and not be displayed when I knit the final document.\n\n## Prints the column names of my data frame\ncolnames(bg_chem)\n\n## General structure of the data frame - shows class of each column\nstr(bg_chem)\n\n## First 6 lines of the data frame\nhead(bg_chem)\n\n## Summary of each column of data\nsummary(bg_chem)\n\n## Prints unique values in a column (in this case Date)\nunique(bg_chem$Date)\n\nTo peek at the data frame, we can type View(bg_chem) in the console. This will open a tab with our data frame in a tabular format.\nNow that we know more about the data set we are working with, let’s do some analyses. Under the appropriate bullet point in your analysis section, create a new R chunk, and use it to calculate the mean nitrate (NO3), nitrite (NO2), ammonium (NH4), and phosphorous (P) measured.\nSave these mean values as new variables with easily understandable names, and write a (brief) description of your operation using markdown above the chunk. Remember that the $ (aka the subset operator) indicates which column of your data to look into.\n\nnitrate &lt;- mean(bg_chem$NO3)\nnitrite &lt;- mean(bg_chem$NO2)\namm &lt;- mean(bg_chem$NH4)\nphos &lt;- mean(bg_chem$P)\n\nIn another chunk, use those variables to calculate the nitrogen:phosphate ratio (Redfield ratio).\n\nratio &lt;- (nitrate + nitrite + amm)/phos\n\nYou can access this variable in your markdown text by using R in-line in your text. The syntax to call R in-line (as opposed to as a chunk) is a single back tick `, followed by the letter “r”, then whatever your simple R command is — here we will use round(ratio) to print the calculated ratio, and finally a closing back tick `. This allows us to access the value stored in this variable in our explanatory text without resorting to the evaluate-copy-paste method so commonly used for this type of task.\nSo, the text in you Quarto document should look like this:\nThe Redfield ratio for this dataset is approximately: `r round(ratio)`\nAnd the rendered text like this:\nThe Redfield ratio for this dataset is approximately 6.\nFinally, create a simple plot using base R that plots the ratio of the individual measurements, as opposed to looking at mean ratio.\n\nplot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nDecide whether or not you want the plotting code above to show up in your knitted document along with the plot, and implement your decision as a chunk option.\nRender your Quarto document (by pressing the Render button) and observe the results.\n\n\n\n\n\n\n\n\nHow do I decide when to make a new code chunk?\n\n\n\nLike many of life’s great questions, there is no clear cut answer. A rule of thumb is to have one chunk per functional unit of analysis. This functional unit could be 50 lines of code or it could be 1 line, but typically it only does one “thing.” This could be reading in data, making a plot, or defining a function. It could also mean calculating a series of related summary statistics (as we’ll see below). Ultimately, the choice is one related to personal preference and style, but generally you should ensure that code is divided up such that it is easily explainable in a literate analysis as the code is run.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_06.html#quarto-and-environments",
    "href": "session_06.html#quarto-and-environments",
    "title": "6  Literate Analysis with Quarto",
    "section": "6.8 Quarto and Environments",
    "text": "6.8 Quarto and Environments\nLet’s walk through an exercise with the document we just created to demonstrate how Quarto handles environments. We will be deliberately inducing some errors here for demonstration purposes.\nFirst, follow these steps:\n\n\n\n\n\n\nSetup\n\n\n\n\nRestart your R session (Session &gt; Restart R)\nRun the last chunk in your Quarto document by pressing the play button on the chunk\n\n\n\nPerhaps not surprisingly, we get an error:\nError in plot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4) : \n  object 'bg_chem' not found\nThis is because we have not run the chunk of code that reads in the bg_chem data. The R part of Quarto works just like a regular R script. You have to execute the code, and the order that you run it in matters. It is relatively easy to get mixed up in a large Quarto document — running chunks out of order, or forgetting to run chunks.\nTo resolve this, follow the next step:\n\n\n\n\n\n\nSetup continued\n\n\n\n\nSelect from the “Run” menu (top right of the editor pane) “Run All.”\nObserve the bg_chem variable in your environment\n\n\n\nThis is a great way to reset and re-run code when things seem to have gone sideways. It is great practice to do periodically since it helps ensure you are writing code that actually runs and it’s reproducible.\n\n\n\n\n\n\nFor the next exercise:\n\n\n\n\nClean your environment by clicking the broom in the environment pane\nRestart your R session (Session &gt; Restart R)\nPress “Render” to run all of the code in your document\nObserve the state of your environment pane\n\nAssuming your document rendered and produced an html page, your code ran. Yet, the environment pane is empty. What happened?\n\n\nThe Render button is rather special — it doesn’t just run all of the code in your document. It actually spins up a fresh R environment separate from the one you have been working in, runs all of the code in your document, generates the output, and then closes the environment. This is one of the best ways Quarto (or RMarkdown) helps ensure you have built a reproducible workflow. If, while you were developing your code, you ran a line in the console as opposed to adding it to your Quarto document, the code you develop while working actively in your environment will still work. However, when you knit your document, the environment RStudio spins up doesn’t know anything about that working environment you were in. Thus, your code may error because it doesn’t have that extra piece of information. Commonly, library() calls are the source of this kind of frustration when the author runs it in the console, but forgets to add it to the script.\nTo further clarify the point on environments, perform the following steps:\n\n\n\n\n\n\nSetup continued\n\n\n\n\nSelect from the “Run” menu (top right of editor pane) “Run All”\nObserve all of the variables in your environment\n\n\n\n\n\n\n\n\n\nWhat about all my R Scripts?\n\n\n\nSome pieces of R code are better suited for R scripts than Quarto or RMarkdown. A function you wrote yourself that you use in many different analyses is probably better to define in an R script than repeated across many Quarto or RMarkdown documents. Some analyses have mundane or repetitive tasks that don’t need to be explained very much. For example, in the document shown in the beginning of this lesson, 15 different excel files needed to be reformatted in slightly different, mundane ways, like renaming columns and removing header text. Instead of including these tasks in the primary Quarto document, the authors chose to write one R script per file and stored them all in a directory. Then, took the contents of one script and included it in the literate analysis, using it as an example to explain what the scripts did, and then used the source() function to run them all from within the Quarto document.\nSo, just because you know Quarto now, doesn’t mean you won’t be using R scripts anymore. Both .R and .qmd have their roles to play in analysis. With practice, it will become more clear what works well in Quarto or RMarkdown, and what belongs in a regular R script.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_06.html#additional-quarto-resources",
    "href": "session_06.html#additional-quarto-resources",
    "title": "6  Literate Analysis with Quarto",
    "section": "6.9 Additional Quarto Resources",
    "text": "6.9 Additional Quarto Resources\n\nPosit (the organization that developed Quarto) has great documentation, check out Quarto.org\nR for Data Science (2e) (Wickham et al, 2023), this is an awesome book for all R related things. Chapter 29 and 30 are specific to Quarto.\nQuarto Gallery: Example of different outputs created using Quarto\nHello Quarto: share, collaborate, teach, reimagine. A talk by Julia Stewart Lowndes and Mine Cetinkaya-Runde.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_06.html#troubleshooting-my-rmarkdownquarto-doc-wont-knit-to-pdf",
    "href": "session_06.html#troubleshooting-my-rmarkdownquarto-doc-wont-knit-to-pdf",
    "title": "6  Literate Analysis with Quarto",
    "section": "6.10 Troubleshooting: My RMarkdown/Quarto doc Won’t Knit to PDF",
    "text": "6.10 Troubleshooting: My RMarkdown/Quarto doc Won’t Knit to PDF\nIf you get an error when trying to knit to PDF that says your computer doesn’t have a LaTeX installation, one of two things is likely happening:\n\nYour computer doesn’t have LaTeX installed\nYou have an installation of LaTeX but RStudio cannot find it (it is not on the path)\n\nIf you already use LaTeX (like to write papers), you fall in the second category. Solving this requires directing RStudio to your installation - and isn’t covered here.\nIf you fall in the first category - you are sure you don’t have LaTeX installed - can use the R package tinytex to easily get an installation recognized by RStudio, as long as you have administrative rights to your computer.\nTo install tinytex run:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nIf you get an error that looks like destination /usr/local/bin not writable, you need to give yourself permission to write to this directory (again, only possible if you have administrative rights). To do this, run this command in the terminal:\nsudo chown -R `whoami`:admin /usr/local/bin\nand then try the above install instructions again. Learn more about tinytex from Yihui Xie’s online book TinyTeX. ````\n\n\n\n\nBritish Ecological Society, Mike, Croucher. 2017. “A Guide to Reproducible Code in Ecology and Evolution.” British Ecological Society. https://www.britishecologicalsociety.org/wp-content/uploads/2017/12/guide-to-reproducible-code.pdf.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Literate Analysis with Quarto</span>"
    ]
  },
  {
    "objectID": "session_07.html",
    "href": "session_07.html",
    "title": "7  R Practice: Literate Analysis",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Practice: Literate Analysis</span>"
    ]
  },
  {
    "objectID": "session_07.html#learning-objectives",
    "href": "session_07.html#learning-objectives",
    "title": "7  R Practice: Literate Analysis",
    "section": "",
    "text": "Practice base R skills\nPractice Markdown syntax\nWork in Quarto document",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Practice: Literate Analysis</span>"
    ]
  },
  {
    "objectID": "session_07.html#introduction",
    "href": "session_07.html#introduction",
    "title": "7  R Practice: Literate Analysis",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nIn this session of R practice, we will be working with the dataset: Tobias Schwoerer, Kevin Berry, and Jorene Joe. 2022. A household survey documenting experiences with coastal hazards in a western Alaska community (2021-2022). Arctic Data Center. doi:10.18739/A29Z90D3V..\nWe will be focusing on the initial survey results (Initial_Survey111721_ADC.csv). In this file, individual survey responses are oriented as rows, and the questions are oriented as columns. The column names are Q1, Q2, etc. Information about what question was asked and what the allowed values mean are available in the metadata for each file. You can access the metadata for each file by clicking the “more info” link next to the file name at the top of the page.\n\n\n\n\n\n\nBig Idea\n\n\n\nThe goal for this session is to practice downloading data, reading it into R in a Quarto document, using R commands to summarize a variable within the dataset, and practice formatting a Quarto document using Markdown syntax.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Practice: Literate Analysis</span>"
    ]
  },
  {
    "objectID": "session_07.html#exercise-1",
    "href": "session_07.html#exercise-1",
    "title": "7  R Practice: Literate Analysis",
    "section": "7.2 Exercise 1",
    "text": "7.2 Exercise 1\n\n\n\n\n\n\nSet up\n\n\n\n\nNavigate to the dataset archived in the Arctic Data Center repository.\nDownload the file Initial_Survey111721_ADC.csv\nUpload the file to a data folder within your training project in the server.\n\nFor this, you have to click on the Upload button on the Files Pane in RStudio.\nA pop-up window will come up. Make sure that the Target directory (directory where your file is going to be saved to) is the data folder withing your RProject.\nClick on Choose File and navigate to where Initial_Survey111721_ADC.csv was downloaded in your local computer. Click Open, then press OK.\n\n\nYou should now see the Initial_Survey111721_ADC.csv inside the data folder in your project.\n\n\n\n7.2.1 Open a new Quarto Document\n\n\n\n\n\n\nStep 1\n\n\n\n\nCreate a new Quarto document. Give it a title something along the lines of “Practice Session”.\nSave it to your R project.\nStructure your document with the following headers:\n\n\nSetup\nRead data\nExplore data\nCalculate mean\nConclusion\n\nTip: In Markdown syntax, # indicates the level of header. # is a “level one header”, meaning the biggest font and the top of the hierarchy. ### is a level three header, and will show up nested below the # and ## headers.\n\n\n\n\n7.2.2 Load necesary libraries\n\n\n\n\n\n\nStep 2\n\n\n\nUnder the Setup header load the read package.\n\n\n\n\n\n\n\n\nExample Code\n\n\n\n\n\n\nlibrary(readr)\n\n\n\n\n\n\n7.2.3 Read in the data\n\n\n\n\n\n\nStep 3\n\n\n\nRead in the data and store the data frame as survey_data.\n\n\n\n\n\n\n\n\nExample Code\n\n\n\n\n\n\nsurvey_data &lt;- read_csv(\"data/Initial_Survey111721_ADC.csv\")\n\n\n\n\n\n\n7.2.4 Explore the data\n\n\n\n\n\n\nStep 4\n\n\n\nTry functions like summary(), colnames(), str(), unique(). Feel free to use any other function you’d like to learn and explore this data set.\nYou can also try View() in the console. Note that if you include View() in your Quarto document it will cause an error when rendering.\n\n\n\n\n\n\n\n\nExample Code\n\n\n\n\n\n\nsummary(survey_data)\ncolnames(survey_data)\nstr(survey_data)\nunique(survey_data$Q6_2)\n\n\n\n\n\n\n7.2.5 Calculate the mean\n\n\n\n\n\n\nStep 5\n\n\n\nCalculate the mean of the answers to Question 3 and save this value into an object in your Global Environment.\nTip: Look at the help page if your answer isn’t what you expect, ?mean(). Does Q3 column have NA values?\n\n\n\n\n\n\n\n\nExample Code\n\n\n\n\n\n\nq3_mean &lt;- mean(survey_data$Q3, na.rm = TRUE)\n\n\n\n\n\n\n7.2.6 Write a conclusion\n\n\n\n\n\n\nStep 6\n\n\n\nInterpret this value using the metadata for the table and write a conclusion based on your interpretation calling the mean value you calculated in text.\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nYour Markdown file should look something like this\nThe average score about respondents perspective in xyz is `r round(q3_mean)\\. This means that …\n\n\n\n\n\n7.2.7 Render and Code Chunk Options\n\n\n\n\n\n\nStep 7\n\n\n\nRender your Quarto file and make sure you get the output you are expecting. Are you running into any errors? If your file is not rendering as you expect discuss with your neighbor how you can improve your file.\nDid you added any code chunk options? Review Section A Quarto Document on the Literate Analysis Lesson and decide what code chunck otion you want to include in your file.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Practice: Literate Analysis</span>"
    ]
  },
  {
    "objectID": "session_07.html#bonus",
    "href": "session_07.html#bonus",
    "title": "7  R Practice: Literate Analysis",
    "section": "7.3 Bonus",
    "text": "7.3 Bonus\n\n\n\n\n\n\nGo Further\n\n\n\nWhat other ways might you summarize the answers to question 3? Explore!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Practice: Literate Analysis</span>"
    ]
  },
  {
    "objectID": "session_08.html",
    "href": "session_08.html",
    "title": "8  Data Modeling / Tidy Data",
    "section": "",
    "text": "8.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Modeling / Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_08.html#learning-objectives",
    "href": "session_08.html#learning-objectives",
    "title": "8  Data Modeling / Tidy Data",
    "section": "",
    "text": "Understand basics of relational data models aka tidy data\nLearn how to design and create effective data tables",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Modeling / Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_08.html#introduction",
    "href": "session_08.html#introduction",
    "title": "8  Data Modeling / Tidy Data",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\n\n\n\n\n\n\nNote\n\n\n\nSlides for this lesson available here.\n\n\nIn this lesson we are going to learn what relational data models are, and how they can be used to manage and analyze data efficiently. Relational data models are what relational databases use to organize tables. However, you don’t have to be using a relational database (like mySQL, MariaDB, Oracle, or Microsoft Access) to enjoy the benefits of using a relational data model. Additionally, your data don’t have to be large or complex for you to benefit. Here are a few of the benefits of using a relational data model:\n\nPowerful search and filtering\nHandle large, complex data sets\nEnforce data integrity\nDecrease errors from redundant updates",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Modeling / Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_08.html#simple-guidelines-for-data-management",
    "href": "session_08.html#simple-guidelines-for-data-management",
    "title": "8  Data Modeling / Tidy Data",
    "section": "8.3 Simple guidelines for data management",
    "text": "8.3 Simple guidelines for data management\nA great paper called ‘Some Simple Guidelines for Effective Data Management’ (Borer et al. 2009) lays out exactly that - guidelines that make your data management, and your reproducible research, more effective.\n\nUse a scripted program (like R!)\n\nA scripted program helps to make sure your work is reproducible. Typically, point-and-click actions, such as clicking on a cell in a spreadsheet program and modifying the value, are not reproducible or easily explained. Programming allows you to both reproduce what you did, and explain it if you use a tool like Rmarkdown.\n\nNon-proprietary file formats are preferred (eg: csv, txt)\n\nUsing a file that can be opened using free and open software greatly increases the longevity and accessibility of your data, since your data do not rely on having any particular software license to open the data file.\n\nKeep a raw version of data\n\nIn conjunction with using a scripted language, keeping a raw version of your data is definitely a requirement to generate a reproducible workflow. When you keep your raw data, your scripts can read from that raw data and create as many derived data products as you need, and you will always be able to re-run your scripts and know that you will get the same output.\n\nUse descriptive file and variable names (without spaces!)\n\nWhen you use a scripted language, you will be using file and variable names as arguments to various functions. Programming languages are quite sensitive with what they are able to interpret as values, and they are particularly sensitive to spaces. So, if you are building reproducible workflows around scripting, or plan to in the future, saving your files without spaces or special characters will help you read those files and variables more easily. Additionally, making file and variables descriptive will help your future self and others more quickly understand what type of data they contain.\n\nInclude a header line in your tabular data files\n\nUsing a single header line of column names as the first row of your data table is the most common and easiest way to achieve consistency among files.\n\nUse plain ASCII text\n\nASCII (sometimes just called plain text) is a very commonly used standard for character encoding, and is far more likely to persist very far into the future than proprietary binary formats such as Excel.\nThe next three are a little more complex, but all are characteristics of the relational data model:\n\nDesign tables to add rows, not columns\nEach column should contain only one type of information\nRecord a single piece of data only once; separate information collected at different scales into different tables.\n\n\nFile and folder organization\nBefore moving on to discuss the last 3 rules, here is an example of how you might organize the files themselves following the simple rules above. Note that we have all open formats, plain text formats for data, sortable file names without special characters, scripts, and a special folder for raw files.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Modeling / Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_08.html#recognizing-untidy-data",
    "href": "session_08.html#recognizing-untidy-data",
    "title": "8  Data Modeling / Tidy Data",
    "section": "8.4 Recognizing untidy data",
    "text": "8.4 Recognizing untidy data\nBefore we learn how to create a relational data model, let’s look at how to recognize data that does not conform to the model.\n\nData Organization\nThis is a screenshot of an actual dataset that came across NCEAS. We have all seen spreadsheets that look like this - and it is fairly obvious that whatever this is, it isn’t very tidy. Let’s dive deeper in to exactly why we wouldn’t consider it tidy.\n\n\n\nMultiple tables\nYour human brain can see from the way this sheet is laid out that it has three tables within it. Although it is easy for us to see and interpret this, it is extremely difficult to get a computer to see it this way, which will create headaches down the road should you try to read in this information to R or another programming language.\n\n\n\nInconsistent observations\nRows correspond to observations. If you look across a single row, and you notice that there are clearly multiple observations in one row, the data are likely not tidy.\n\n\n\nInconsistent variables\nColumns correspond to variables. If you look down a column, and see that multiple variables exist in the table, the data are not tidy. A good test for this can be to see if you think the column consists of only one unit type.\n\n\n\nMarginal sums and statistics\nMarginal sums and statistics also are not considered tidy, and they are not the same type of observation as the other rows. Instead, they are a combination of observations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Modeling / Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_08.html#good-enough-data-modeling",
    "href": "session_08.html#good-enough-data-modeling",
    "title": "8  Data Modeling / Tidy Data",
    "section": "8.5 Good enough data modeling",
    "text": "8.5 Good enough data modeling\n\nDenormalized data\nWhen data are “denormalized” it means that observations about different entities are combined.\n\nIn the above example, each row has measurements about both the community in which observations occurred, as well as observations of two individuals surveyed in that community. This is not normalized data.\nPeople often refer to this as wide format, because the observations are spread across a wide number of columns. Note that, should one survey another individual in either community, we would have to add new columns to the table. This is difficult to analyze, understand, and maintain.\n\n\nTabular data\nObservations. A better way to model data is to organize the observations about each type of entity in its own table. This results in:\n\nSeparate tables for each type of entity measured\nEach row represents a single observation within that entity\nObservations (rows) are all unique\nThis is normalized data (aka tidy data)\n\nVariables. In addition, for normalized data, we expect the variables to be organized such that:\n\nAll values in a column are of the same type\nAll columns pertain to the same observed entity (e.g., row)\nEach column represents either an identifying variable or a measured variable\n\n\n\n\n\n\n\nChallenge\n\n\n\nTry to answer the following questions:\nWhat are the observed entities in the example above?\nWhat are the measured variables associated with those observations?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\nIf we use these questions to tidy our data, we should end up with:\n\none table for each entity observed\none column for each measured variable\nadditional columns for identifying variables (such as community)\n\nHere is what our tidy data look like:\n\nNote that this normalized version of the data meets the three guidelines set by (Borer et al. 2009):\n\nDesign tables to add rows, not columns\nEach column should contain only one type of information\nRecord a single piece of data only once; separate information collected at different scales into different tables.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Modeling / Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_08.html#using-normalized-data",
    "href": "session_08.html#using-normalized-data",
    "title": "8  Data Modeling / Tidy Data",
    "section": "8.6 Using normalized data",
    "text": "8.6 Using normalized data\nNormalizing data by separating it into multiple tables often makes researchers really uncomfortable. This is understandable! The person who designed this study collected all of this information for a reason - so that they could analyze it together. Now that our community and survey information are in separate tables, how would we use population as a predictor variable for language spoken, for example? The answer is keys - and they are the cornerstone of relational data models.\nWhen one has normalized data, we often use unique identifiers to reference particular observations, which allows us to link across tables. Two types of identifiers are common within relational data:\n\nPrimary Key: unique identifier for each observed entity, one per row\nForeign Key: reference to a primary key in another table (linkage)\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nIn our normalized tables above, identify the following:\n\nthe primary key for each table\nany foreign keys that exist\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe primary key of the top table is community. The primary key of the bottom table is id.\nThe community column is the primary key of that table because it uniquely identifies each row of the table as a unique observation of a community. In the second table, however, the community column is a foreign key that references the primary key from the first table.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Modeling / Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_08.html#entity-relationship-model-er",
    "href": "session_08.html#entity-relationship-model-er",
    "title": "8  Data Modeling / Tidy Data",
    "section": "8.7 Entity-Relationship Model (ER)",
    "text": "8.7 Entity-Relationship Model (ER)\nAn Entity-Relationship model allows us to compactly draw the structure of the tables in a relational database, including the primary and foreign keys in the tables.\n\n\n\n\n\nerDiagram\n    Responses |{--|| Communities : located_in\n    Communities {\n        string community PK\n        numeric latitude\n        numeric longitude\n        numeric population\n    }\n    Responses {\n        string id PK\n        string community FK\n        numeric age\n        string language\n    }\n\n\n\n\n\n\nIn the above model, one can see that each community in the community observations table must have one or more survey participants in the survey table, whereas each survey response has one and only one community.\nHere is a more complicated ER Model showing examples of other types of relationships.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Modeling / Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_08.html#merging-data",
    "href": "session_08.html#merging-data",
    "title": "8  Data Modeling / Tidy Data",
    "section": "8.8 Merging data",
    "text": "8.8 Merging data\nFrequently, analysis of data will require merging these separately managed tables back together. There are multiple ways to join the observations in two tables, based on how the rows of one table are merged with the rows of the other.\nWhen conceptualizing merges, one can think of two tables, one on the left and one on the right. The most common (and often useful) join is when you merge the subset of rows that have matches in both the left table and the right table: this is called an INNER JOIN. Other types of join are possible as well. A LEFT JOIN takes all of the rows from the left table, and merges on the data from matching rows in the right table. Keys that don’t match from the left table are still provided with a missing value (na) from the right table. A RIGHT JOIN is the same, except that all of the rows from the right table are included with matching data from the left, or a missing value. Finally, a FULL OUTER JOIN includes all data from all rows in both tables, and includes missing values wherever necessary.\n\nSometimes people represent these as Venn diagrams showing which parts of the left and right tables are included in the results for each join. These however, miss part of the story related to where the missing value come from in each result.\n\nIn the figure above, the blue regions show the set of rows that are included in the result. For the INNER join, the rows returned are all rows in A that have a matching row in B.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Modeling / Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_08.html#exercise-data-modeling-for-course-surveys",
    "href": "session_08.html#exercise-data-modeling-for-course-surveys",
    "title": "8  Data Modeling / Tidy Data",
    "section": "8.9 Exercise: Data modeling for course surveys",
    "text": "8.9 Exercise: Data modeling for course surveys\n\n\n\n\n\n\nExercise\n\n\n\n\nBreak into groups\n\nOur funding agency requires that we take surveys of individuals who complete our training courses so that we can report on the demographics of our trainees and how effective they find our courses to be. In your small groups, design a set of tables that will capture information collected in a participant survey that would apply to many courses.\nDon’t focus on designing a comprehensive set of questions for the survey, one or two simple stand ins (eg: “Did the course meet your expectations?”, “What could be improved?”, “To what degree did your knowledge increase?”) would be sufficient.\nInclude as variables (columns) a basic set of information not only from the surveys (such as survey question responses), but about the courses, such as the date of the course and name of the course. Try to account for the same person participating in multiple courses, multiple courses being held each year, and the same survey questions can be asked of the participants for those different courses.\nDraw your entity-relationship model for your tables.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can start by creating one box for each type of entity that we want to collection data about. Each box represents a data table in our design. As we are collecting survey responses, we might start with a table for Response that would contian one observation for each survey response that we want to store. That Response is about a particular course, so we can add another table for information about that Course, and link those two tables.\n\n\n\n\n\nerDiagram\n    Response ||--|| Course : about\n\n\n\n\n\n\n\nNext, we can change the cardinality of the relationship to indicate that each course can contain multiple responses. We can also add a new table to hold the details for each Participant that takes each Course, and that each Participant provides a Response when filling out a survey. And lastly, because questions might be reused across surveys, we create a linkage from the Response table to a new Question table that has one row for each unique question.\n\n\n\n\n\nerDiagram\n    Response |{--|| Course : about\n    Participant |{--|{ Course : takes\n    Participant ||--|| Response : provides\n    Response |{--|| Question : for\n\n\n\n\n\n\nFinally, we can add the attributes that we would have for each table, indicating which are primary keys and which are foreign keys.\n\n\n\n\n\nerDiagram\n    Response |{--|| Course : about\n    Participant |{--|{ Course : takes\n    Participant ||--|| Response : provides\n    Response |{--|| Question : for\n\n    Response {\n        string response_id PK\n        string participant_id FK\n        string course_id FK\n        string question_id FK\n        string response_value\n    }\n    Participant {\n        string participant_id PK\n        string name_first\n        string name_last\n        string email\n    }\n    Course {\n        string course_id PK\n        string course_name\n        date date_start\n        date date_end\n    }\n    Question {\n        string question_id PK\n        string question_text\n    }",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Modeling / Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_08.html#resources",
    "href": "session_08.html#resources",
    "title": "8  Data Modeling / Tidy Data",
    "section": "8.10 Resources",
    "text": "8.10 Resources\n\nBorer et al. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America.\nWhite et al. 2013. Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution 6.\nSoftware Carpentry SQL tutorial\nTidy Data\nIntro to Tidy Data slides (this lesson module)\n\n\n\n\n\nBorer, Elizabeth, Eric Seabloom, Matthew B. Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” Bulletin of the Ecological Society of America 90: 205–14. https://doi.org/10.1890/0012-9623-90.2.205.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Modeling / Tidy Data</span>"
    ]
  },
  {
    "objectID": "session_09.html",
    "href": "session_09.html",
    "title": "9  Cleaning and Wrangling Data",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#learning-objectives",
    "href": "session_09.html#learning-objectives",
    "title": "9  Cleaning and Wrangling Data",
    "section": "",
    "text": "Introduce dplyr and tidyr functions to clean and wrangle data for analysis\nLearn about the Split-Apply-Combine strategy and how it applies to data wrangling\nDescribe the difference between wide vs. long table formats and how to convert between them",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#introduction",
    "href": "session_09.html#introduction",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nThe data we get to work with are rarely, if ever, in the format we need to do our analyses. It’s often the case that one package requires data in one format, while another package requires the data to be in another format. To be efficient analysts, we should have good tools for reformatting data for our needs so we can do further work like making plots and fitting models. The dplyr and tidyr R packages provide a fairly complete and extremely powerful set of functions for us to do this reformatting quickly. Learning these tools well will greatly increase your efficiency as an analyst.\nLet’s look at two motivating examples.\n\n\n\n\n\n\nExample 1\n\n\n\nSuppose you have the following data.frame called length_data with data about salmon length and want to calculate the average length per year.\n\n\n\nyear\nlength_cm\n\n\n\n\n1990\n5.6\n\n\n1991\n3.0\n\n\n1991\n4.5\n\n\n1992\n4.3\n\n\n1992\n5.5\n\n\n1992\n4.9\n\n\n\nBefore thinking about the code, let’s think about the steps we need to take to get to the answer (aka pseudocode).\nNow, how would we code this? The dplyr R library provides a fast and powerful way to do this calculation in a few lines of code:\n\n\nAnswer\nlength_data %&gt;% \n  group_by(year) %&gt;% \n  summarize(mean_length_cm = mean(length_cm))\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\nAnother process we often need to do is to “reshape” our data. Consider the following table that is in what we call “wide” format:\n\n\n\nsite\n1990\n1991\n…\n1993\n\n\n\n\ngold\n101\n109\n…\n112\n\n\nlake\n104\n98\n…\n102\n\n\n…\n…\n…\n…\n…\n\n\ndredge\n144\n118\n…\n145\n\n\n\nYou are probably familiar with data in the above format, where values of the variable being observed are spread out across columns. In this example we have a different column per year. This wide format works well for data entry and sometimes works well for analysis but we quickly outgrow it when using R (and know it is not tidy data!). For example, how would you fit a model with year as a predictor variable? In an ideal world, we’d be able to just run lm(length ~ year). But this won’t work on our wide data because lm() needs length and year to be columns in our table.\nWhat steps would you take to get this data frame in a long format?\nThe tidyr package allows us to quickly switch between wide format and long format using the pivot_longer() function:\n\n\nAnswer\nsite_data %&gt;% \n  pivot_longer(-site, \n               names_to = \"year\", \n               values_to = \"length\")\n\n\n\n\n\nsite\nyear\nlength\n\n\n\n\ngold\n1990\n101\n\n\nlake\n1990\n104\n\n\ndredge\n1990\n144\n\n\n…\n…\n…\n\n\ndredge\n1993\n145\n\n\n\n\n\nThis lesson will cover examples to learn about the functions you’ll most commonly use from the dplyr and tidyr packages:\n\nCommon dplyr functions\n\n\n\n\n\n\nFunction name\nDescription\n\n\n\n\nmutate()\nCreates modify and deletes columns\n\n\ngroup_by()\nGroups data by one or more variables\n\n\nsummarise()\nSummaries each group down to one row\n\n\nselect()\nKeep or drop columns using their names\n\n\nfilter()\nKeeps rows that matches conditions\n\n\narrange()\norder rows using columns variable\n\n\nrename()\nRename a column\n\n\n\n\nCommon tidyr functions\n\n\n\n\n\n\nFunction name\nDescription\n\n\n\n\npivot_longer()\ntransforms data from a wide to a long format\n\n\npivot_wider()\ntransforms data from a long to a wide format\n\n\nunite()\nUnite multiple columns into one by pasting strings together\n\n\nseparate()\nSeparate a character column into multiple columns with a regular expression or numeric locations",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#data-cleaning-basics",
    "href": "session_09.html#data-cleaning-basics",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.2 Data cleaning basics",
    "text": "9.2 Data cleaning basics\nTo demonstrate, we’ll be working with a tidied up version of a data set from Alaska Department of Fish & Game containing commercial catch data from 1878-1997. The data set and reference to the original source can be found at its public archive.\n\n\n\n\n\n\nSetup\n\n\n\nFirst, open a new Quarto document. Delete everything below the setup chunk, and add a library chunk that calls dplyr, tidyr, and readr\n\n\nAnswer\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\n\n\n\n\n\n\n\n\n\n\nA note on loading packages\n\n\n\nYou may have noticed the following messages pop up when you ran your library chunk.\nAttaching package: ‘dplyr’\n\nThe following objects are masked from ‘package:stats’:\n\n    filter, lag\n\nThe following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\nThese are important messages. They are letting you know that certain functions from the stats and base packages (which are loaded by default when you start R) are masked by different functions with the same name in the dplyr package. It turns out, the order that you load the packages in matters. Since we loaded dplyr after stats, R will assume that if you call filter(), you mean the dplyr version unless you specify otherwise.\nBeing specific about which version of filter(), for example, you call is easy. To explicitly call a function by its unambiguous name, we use the syntax package_name::function_name(...). So, if we wanted to call the stats version of filter() in this Rmarkdown document, I would use the syntax stats::filter(...).\n\n\n\n\n\n\n\n\nRemove messages and warnings\n\n\n\nMessages and warnings are important, but we might not want them in our final document. After you have read the packages in, adjust the chunk settings in your library chunk to suppress warnings and messages by adding #| message: false or #| warning: false. Both of these chunk options, when set to false, prevents messages or warnings from appearing in the rendered file.\n\n\nNow that we have introduced some data wrangling libraries, let’s get the data that we are going to use for this lesson.\n\n\n\n\n\n\nSetup\n\n\n\n\nGo to KNB Data Package Alaska commercial salmon catches by management region (1886- 1997)\nFind the data file df35b.302.1. Right click the “Download” button and select “Copy Link Address”\nPaste the copied URL into the read_csv() function\n\nThe code chunk you use to read in the data should look something like this:\n\n\nAnswer\ncatch_original &lt;- read_csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1\")\n\n\n\n\n\n\n\n\n\n\n\nThis data set is relatively clean and easy to interpret as-is. While it may be clean, it’s in a shape that makes it hard to use for some types of analyses so we’ll want to fix that first.\n\n\n\n\n\n\nExercise\n\n\n\nBefore we get too much further, spend a minute or two outlining your Quarto document so that it includes the following sections and steps:\n\nData Sources\n\nRead in the data\nExplore data\n\nClean and Reshape data\n\nUsing select() function\nCheck column types\nReplace values in a column with mutate()\nReshape data with pivot_longer() and pivot_wider()\nRename columns rename()\nAdd columns with mutate()\nSummary stats using group_by() and summarize()\nFiltering rows using filter()\nSort data using arrange()\nSplit and combine values in columns with separate() and unite()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#data-exploration",
    "href": "session_09.html#data-exploration",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.3 Data exploration",
    "text": "9.3 Data exploration\nSimilar to what we did in our Literate Analysis lesson, it is good practice to skim through the data you just read in.\nDoing so is important to make sure the data is read as you were expecting and to familiarize yourself with the data.\nSome of the basic ways to explore your data are:\n\n\nAnswer\n## Prints the column names of my data frame\ncolnames(catch_original)\n\n## First 6 lines of the data frame\nhead(catch_original)\n\n## Summary of each column of data\nsummary(catch_original)\n\n## Prints unique values in a column (in this case, the region)\nunique(catch_original$Region)\n\n## Opens data frame in its own tab to see each row and column of the data (do in console)\nView(catch_original)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#about-the-pipe-operator",
    "href": "session_09.html#about-the-pipe-operator",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.4 About the pipe (%>%) operator",
    "text": "9.4 About the pipe (%&gt;%) operator\nBefore we jump into learning tidyr and dplyr, we first need to explain the pipeline operator %&gt;%.\nBoth the tidyr and the dplyr packages use the pipe operator (%&gt;%), which may look unfamiliar. The pipe is a powerful way to efficiently chain together operations. The pipe will take the output of a previous statement, and use it as the input to the next statement.\nSay you want to both filter() out rows of a data set, and select() certain columns.\nInstead of writing:\n\n\nAnswer\ndf_filtered &lt;- filter(df, ...)\ndf_selected &lt;- select(df_filtered, ...)\n\n\nYou can write:\n\n\nAnswer\ndf_cleaned &lt;- df %&gt;% \n    filter(...) %&gt;%\n    select(...)\n\n\nIf you think of the assignment operator (&lt;-) as reading like “gets”, then the pipe operator would read like “then”.\nSo you might think of the above chunk being translated as:\n\nThe cleaned data frame gets the original data, and then a filter (of the original data), and then a select (of the filtered data).\n\nThe benefits to using pipes are that you don’t have to keep track of (or overwrite) intermediate data frames. The drawbacks are that it can be more difficult to explain the reasoning behind each step, especially when many operations are chained together. It is good to strike a balance between writing efficient code (chaining operations), while ensuring that you are still clearly explaining, both to your future self and others, what you are doing and why you are doing it.\n\n\n\n\n\n\nQuick Tip\n\n\n\nRStudio has a keyboard shortcut for %&gt;%\n\nWindows: Ctrl + Shift + M\nMac: cmd + shift + M",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#selecting-or-removing-columns-using-select",
    "href": "session_09.html#selecting-or-removing-columns-using-select",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.5 Selecting or removing columns using select()",
    "text": "9.5 Selecting or removing columns using select()\nWe’re ready to go back to our salmon dataset. The first issue is the extra columns All and notesRegCode. Let’s select only the columns we want, and assign this to a variable called catch_data.\n\n\nAnswer\ncatch_data &lt;- catch_original %&gt;%\n    select(Region, Year, Chinook, Sockeye, Coho, Pink, Chum)\n\nhead(catch_data)\n\n\nMuch better!\nThe select() function also allows you to say which columns you don’t want, by passing unquoted column names preceded by minus (-) signs:\n\n\nAnswer\ncatch_data &lt;- catch_original %&gt;%\n    select(-All,-notesRegCode)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#quality-check",
    "href": "session_09.html#quality-check",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.6 Quality check",
    "text": "9.6 Quality check\nNow that we have the data we are interested in using, we should do a little quality check to see that everything seems as expected. One nice way of doing this is the glimpse() function.\n\n\nAnswer\ndplyr::glimpse(catch_data)\n\n\n\n\n\n\n\n\nExercise\n\n\n\nExamine the output of the glimpse() function call. Does anything seem amiss with this data set that might warrant fixing?\n\n\nAnswer:\n\nThe Chinook catch data are character class. Let’s fix it using the function mutate() before moving on.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#changing-column-content-using-mutate",
    "href": "session_09.html#changing-column-content-using-mutate",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.7 Changing column content using mutate()",
    "text": "9.7 Changing column content using mutate()\nWe can use the mutate() function to change a column, or to create a new column. First, let’s try to convert the Chinook catch values to numeric type using the as.numeric() function, and overwrite the old Chinook column.\n\n\nAnswer\ncatch_clean &lt;- catch_data %&gt;%\n    mutate(Chinook = as.numeric(Chinook))\n\nhead(catch_clean)\n\n\nWe get a warning \"NAs introduced by coercion\" which is R telling us that it couldn’t convert every value to an integer and, for those values it couldn’t convert, it put an NA in its place. This is behavior we commonly experience when cleaning data sets and it’s important to have the skills to deal with it when it comes up.\nTo investigate, let’s isolate the issue. We can find out which values are NAs with a combination of is.na() and which(), and save that to a variable called i.\n\n\nAnswer\ni &lt;- which(is.na(catch_clean$Chinook))\ni\n\n\nIt looks like there is only one problem row, lets have a look at it in the original data.\n\n\nAnswer\ncatch_data[i,]\n\n\nWell that’s odd: The value in Chinook is the letter I. It turns out that this data set is from a PDF which was automatically converted into a csv and this value of I is actually a 1.\nLet’s fix it by incorporating the if_else() function to our mutate() call, which will change the value of the Chinook column to 1 if the value is equal to I, then will use as.numeric() to turn the character representations of numbers into numeric typed values.\n\n\nAnswer\ncatch_clean &lt;- catch_data %&gt;%\n    mutate(Chinook = if_else(condition = Chinook == \"I\", \n                             true = \"1\", \n                             false = Chinook),\n           Chinook = as.numeric(Chinook))\n\n##check\ncatch_clean[i, ]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#changing-shape-using-pivot_longer-and-pivot_wider",
    "href": "session_09.html#changing-shape-using-pivot_longer-and-pivot_wider",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.8 Changing shape using pivot_longer() and pivot_wider()",
    "text": "9.8 Changing shape using pivot_longer() and pivot_wider()\nThe next issue is that the data are in a wide format and we want the data in a long format instead. The function pivot_longer() from the tidyr package helps us do this conversion. If you do not remember all the arguments that go into pivot_longer() you can always call the help page by typing ?pivot_longer in the console.\n\n\nAnswer\ncatch_long &lt;- catch_clean %&gt;% \n    #pivot longer all columns except Region and Year\n    pivot_longer(\n        cols = -c(Region, Year),\n        names_to = \"species\",\n        values_to = \"catch\"\n    )\n\nhead(catch_long)\n\n\nThe syntax we used above for pivot_longer() might be a bit confusing so let’s walk though it.\n\nThe first argument to pivot_longer is the columns over which we are pivoting. You can select these by listing either the names of the columns you do want to pivot, or in this case, the names of the columns you are not pivoting over.\nThe names_to argument: this is the name of the column that you are creating from the column names of the columns you are pivoting over.\nThe values_to argument: the name of the column that you are creating from the values in the columns you are pivoting over.\n\nThe opposite of pivot_longer() is the pivot_wider() function. It works in a similar declarative fashion:\n\n\nAnswer\ncatch_wide &lt;- catch_long %&gt;%\n    pivot_wider(names_from = species,\n                values_from = catch)\n\nhead(catch_wide)\n\n\nSame than we did above we can pull up the documentation of the function to remind ourselves what goes in which argument. Type ?pivot_wider in the console.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#renaming-columns-with-rename",
    "href": "session_09.html#renaming-columns-with-rename",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.9 Renaming columns with rename()",
    "text": "9.9 Renaming columns with rename()\nIf you scan through the data, you may notice the values in the catch column are very small (these are supposed to be annual catches). If we look at the metadata we can see that the catch column is in thousands of fish, so let’s convert it before moving on.\nLet’s first rename the catch column to be called catch_thousands:\n\n\nAnswer\ncatch_long &lt;- catch_long %&gt;%\n    rename(catch_thousands = catch)\n\nhead(catch_long)\n\n\n\n\n\n\n\n\nnames() versus rename()\n\n\n\nMany people use the base R function names() to rename columns, often in combination with column indexing that relies on columns being in a particular order. Column indexing is often also used to select columns instead of the select() function from dplyr. Although these methods work just fine, they do have one major drawback: in most implementations they rely on you knowing exactly the column order your data is in.\nTo illustrate why your knowledge of column order isn’t reliable enough for these operations, considering the following scenario:\nYour colleague emails you letting you know that she has an updated version of the conductivity-temperature-depth data from this year’s research cruise, and sends it along. Excited, you re-run your scripts that use this data for your phytoplankton research. You run the script and suddenly all of your numbers seem off. You spend hours trying to figure out what is going on.\nUnbeknownst to you, your colleagues bought a new sensor this year that measures dissolved oxygen. Because of the new variables in the data set, the column order is different. Your script which previously renamed the fourth column, SAL_PSU to salinity now renames the fourth column, O2_MGpL to salinity. No wonder your results looked so weird, good thing you caught it!\nIf you had written your code so that it doesn’t rely on column order, but instead renames columns using the rename() function, the code would have run just fine (assuming the name of the original salinity column didn’t change, in which case the code would have thrown an error in an obvious way). This is an example of a defensive coding strategy, where you try to anticipate issues before they arise, and write your code in such a way as to keep the issues from happening.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#adding-columns-using-mutate",
    "href": "session_09.html#adding-columns-using-mutate",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.10 Adding columns using mutate()",
    "text": "9.10 Adding columns using mutate()\nNow let’s use mutate() again to create a new column called catch with units of fish (instead of thousands of fish).\n\n\nAnswer\ncatch_long &lt;- catch_long %&gt;%\n    mutate(catch = catch_thousands * 1000)\n\nhead(catch_long)\n\n\nLet’s remove the catch_thousands column for now since we don’t need it. Note that here we have added to the expression we wrote above by adding another function call (mutate) to our expression. This takes advantage of the pipe operator by grouping together a similar set of statements, which all aim to clean up the catch_clean data frame.\n\n\nAnswer\ncatch_long &lt;- catch_long %&gt;%\n    mutate(catch = catch_thousands * 1000) %&gt;%\n    select(-catch_thousands)\n\nhead(catch_long)\n\n\nWe’re now ready to start analyzing the data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#summary-statistics-using-group_by-and-summarize",
    "href": "session_09.html#summary-statistics-using-group_by-and-summarize",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.11 Summary statistics using group_by() and summarize()",
    "text": "9.11 Summary statistics using group_by() and summarize()\nSuppose we are now interested in getting the average catch per region. In our initial data exploration we saw there are 18 regions, we can easily see their names again:\n\n\nAnswer\nunique(catch_original$Region)\n\n\nThink about how we would calculate the average catch per region “by hand”. It would be something like this:\n\nWe start with our table and notice there are multiple regions in the “Regions” column.\nWe split our original table to group all observations from the same region together.\nWe calculate the average catch for each of the groups we form.\nThen we combine the values for average catch per region into a single table.\n\n\n\n\n\n\n\n\nAnalyses like this conform to what is known as the Split-Apply-Combine strategy. This strategy follows the three steps we explained above:\n\nSplit: Split the data into logical groups (e.g., region, species, etc.)\nApply: Calculate some summary statistic on each group (e.g. mean catch by year, number of individuals per species)\nCombine: Combine the statistic calculated on each group back together into a single table\n\nThe dplyr library lets us easily employ the Split-Apply-Combine strategy by using the group_by() and summarize() functions:\n\n\nAnswer\nmean_region &lt;- catch_long %&gt;%\n    group_by(Region) %&gt;%\n    summarize(mean_catch = mean(catch))\n\nhead(mean_region)\n\n\nLet’s see how the previous code implements the Split-Apply-Combine strategy:\n\ngroup_by(Region): this is telling R to split the dataframe and create a group for each different value in the column Region. R just keeps track of the groups, it doesn’t return separate dataframes per region.\nmean(catch): here mean is the function we want to apply to the column catch in each group.\nsummarize(catch = mean(catch)) the function summarize() is used to combine the results of mean(catch) in each group into a single table. The argument mean_catch = mean(catch) indicates that the column having the results of mean(catch) will be named mean_catch.\n\nAnother common use of group_by() followed by summarize() is to count the number of rows in each group. We have to use a special function from dplyr, n().\n\n\nAnswer\nn_region &lt;- catch_long %&gt;%\n    group_by(Region) %&gt;%\n    summarize(n = n())\n\nhead(n_region)\n\n\n\n\n\n\n\n\nTry using count()\n\n\n\nIf you are finding that you are reaching for this combination of group_by(), summarize() and n() a lot, there is a helpful dplyr function count() that accomplishes this in one function!\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nFind another grouping and statistic to calculate for each group.\nFind out if you can group by multiple variables.\n\n\n\nAnswer\n## for example:\ncatch_year_sp &lt;- catch_long %&gt;%\n    group_by(Year, species) %&gt;%\n    summarize(total_year = sum(catch, na.rm = T))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#filtering-rows-using-filter",
    "href": "session_09.html#filtering-rows-using-filter",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.12 Filtering rows using filter()",
    "text": "9.12 Filtering rows using filter()\nWe use the filter() function to filter our data.frame to rows matching some condition. It’s similar to subset() from base R.\nLet’s go back to our original data.frame and do some filter()ing:\n\n\nAnswer\nsse_catch &lt;- catch_long %&gt;%\n    filter(Region == \"SSE\")\n\nhead(sse_catch)\n\n\n\n\n\n\n\n\n== and %in% operators\n\n\n\nThe filter() function performs a logical test across all the rows of a dataframe, and if that test is TRUE for a given row, it keeps that row. The == operator tests whether the left hand side and right hand side match - in the example above, does the value of the Region variable match the value \"SSE\"?\nBut if you want to test whether a variable’s value is within a set of possible values, do not use the == operator - it will very likely give false results! Instead, use the %in% operator:\n\n\nAnswer\ncatch_long %&gt;%\n  filter(Region == c(\"SSE\", \"ALU\")) %&gt;%\n  nrow()\n\ncatch_long %&gt;%\n  filter(Region %in% c(\"SSE\", \"ALU\")) %&gt;%\n  nrow()\n\n\nThis is because the == version “recycles” the vector of allowed values, so it tests whether the first row matches \"SSE\" (yep!), whether the second matches \"ALU\" (nope! this row gets dropped!), and then whether the third is \"SSE\" again and so on.\nNote that the %in% operator actually works for single values too, so you can never go wrong with that!\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nFilter to just catches of over one million fish\nFilter to just Chinook from the SSE region\n\n\n\nAnswer\n## Catches over a million fish\ncatch_million &lt;- catch_long %&gt;%\n    filter(catch &gt; 1000000)\n\n## Chinook from SSE data\nchinook_sse &lt;- catch_long %&gt;%\n    filter(Region == \"SSE\",\n           species == \"Chinook\")\n\n## OR combine tests with & (\"and\") or | (\"or\")... also, we can swap == for %in%\nchinook_sse &lt;- catch_long %&gt;%\n    filter(Region %in% \"SSE\" & species %in% \"Chinook\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#sorting-your-data-using-arrange",
    "href": "session_09.html#sorting-your-data-using-arrange",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.13 Sorting your data using arrange()",
    "text": "9.13 Sorting your data using arrange()\nThe arrange() function is used to sort the rows of a data.frame. Two common cases to use arrange() are:\n\nTo calculate a cumulative sum (with cumsum()) so row order matters\nTo display a table (like in an .qmd document) in sorted order\n\nLet’s re-calculate mean catch by region, and then arrange() the output by mean catch:\n\n\nAnswer\nmean_region &lt;- catch_long %&gt;%\n    group_by(Region) %&gt;%\n    summarize(mean_catch = mean(catch)) %&gt;%\n    arrange(mean_catch)\n\nhead(mean_region)\n\n\nThe default sorting order of arrange() is to sort in ascending order. To reverse the sort order, wrap the column name inside the desc() function:\n\n\nAnswer\nmean_region &lt;- catch_long %&gt;%\n    group_by(Region) %&gt;%\n    summarize(mean_catch = mean(catch)) %&gt;%\n    arrange(desc(mean_catch))\n\nhead(mean_region)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#splitting-a-column-using-separate-and-unite",
    "href": "session_09.html#splitting-a-column-using-separate-and-unite",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.14 Splitting a column using separate() and unite()",
    "text": "9.14 Splitting a column using separate() and unite()\nThe separate() function allow us to easily split a single column into numerous. Its complement, the unite() function, allows us to combine multiple columns into a single one.\nThis can come in really handy when we need to split a column into two pieces by a consistent separator (like a dash).\nLet’s make a new data.frame with fake data to illustrate this. Here we have a set of site identification codes with information about the island where the site is (the first 3 letters) and a site number (the 3 numbers). If we want to group and summarize by island, we need a column with just the island information.\n\n\nAnswer\nsites_df &lt;- data.frame(site = c(\"HAW-101\",\n                                \"HAW-103\",\n                                \"OAH-320\",\n                                \"OAH-219\",\n                                \"MAU-039\"))\n\nsites_df %&gt;%\n    separate(site, c(\"island\", \"site_number\"), \"-\")\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSplit the city column in the data frame cities_df into city and state_code columns\n\n\nAnswer\n## create `cities_df`\ncities_df &lt;- data.frame(city = c(\"Juneau AK\",\n                                 \"Sitka AK\",\n                                 \"Anchorage AK\"))\n\n\n\n\nAnswer\ncolnames(cities_df)\n\ncities_clean &lt;- cities_df %&gt;%\n    separate(city, c(\"city\", \"state_code\"), \" \")\n\n\n\n\nThe unite() function does just the reverse of separate(). If we have a data.frame that contains columns for year, month, and day, we might want to unite these into a single date column.\n\n\nAnswer\ndates_df &lt;- data.frame(\n    year = c(\"1930\",\n             \"1930\",\n             \"1930\"),\n    month = c(\"12\",\n              \"12\",\n              \"12\"),\n    day = c(\"14\",\n            \"15\",\n            \"16\")\n)\n\ndates_df %&gt;%\n    unite(date, year, month, day, sep = \"-\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#now-all-together",
    "href": "session_09.html#now-all-together",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.15 Now, all together!",
    "text": "9.15 Now, all together!\nWe just ran through the various things we can do with dplyr and tidyr but if you’re wondering how this might look in a real analysis. Let’s look at that now:\n\n\nAnswer\ncatch_original &lt;- read_csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1\")\n\nmean_region &lt;- catch_original %&gt;%\n  select(-All, -notesRegCode) %&gt;% \n  mutate(Chinook = if_else(Chinook == \"I\", \"1\", Chinook)) %&gt;% \n  mutate(Chinook = as.numeric(Chinook)) %&gt;% \n  pivot_longer(-c(Region, Year), \n               names_to = \"species\", \n               values_to = \"catch\") %&gt;%\n  mutate(catch = catch * 1000) %&gt;% \n  group_by(Region) %&gt;% \n  summarize(mean_catch = mean(catch)) %&gt;% \n  arrange(desc(mean_catch))\n\nhead(mean_region)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_09.html#write-out-the-results-with-readrwrite_csv",
    "href": "session_09.html#write-out-the-results-with-readrwrite_csv",
    "title": "9  Cleaning and Wrangling Data",
    "section": "9.16 Write out the results with readr::write_csv()",
    "text": "9.16 Write out the results with readr::write_csv()\nNow that we have performed all this data wrangling, we can save out the results for future use using readr::write_csv().\n\n\nAnswer\nwrite_csv(mean_region, here::here(\"data/mean_catch_by_region.csv\"))\n\n\nWe have completed our lesson on Cleaning and Wrangling data. Before we break, let’s practice our Git workflow.\n\n\n\n\n\n\nSteps\n\n\n\n\nSave the .qmd you have been working on for this lesson.\nRender the Quarto file. This is a way to test everything in your code is working.\nStage (Add) &gt; Commit &gt; Pull &gt; Push",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_10.html",
    "href": "session_10.html",
    "title": "10  Intro to Data Visualization",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Intro to Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#learning-objectives",
    "href": "session_10.html#learning-objectives",
    "title": "10  Intro to Data Visualization",
    "section": "",
    "text": "Understand the fundamentals of how the ggplot2 package works\nUse ggplot2’s theme and other customization functions create publication-grade graphics\nIntroduce the leaflet and DT package to create interactive maps and tables respectively",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Intro to Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#overview",
    "href": "session_10.html#overview",
    "title": "10  Intro to Data Visualization",
    "section": "10.1 Overview",
    "text": "10.1 Overview\nggplot2 is a popular package for visualizing data in R. From the home page:\n\nggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.\n\nIt’s been around for years and has pretty good documentation and tons of example code around the web (like on StackOverflow). The goal of this lesson is to introduce you to the basic components of working with ggplot2 and inspire you to go and explore this awesome resource for visualizing your data.\n\n\n\n\n\n\nggplot2 vs base graphics in R vs others\n\n\n\nThere are many different ways to plot your data in R. All of them work! However, ggplot2 excels at making complicated plots easy and easy plots simple enough.\nBase R graphics (plot(), hist(), etc) can be helpful for simple, quick and dirty plots. ggplot2 can be used for almost everything else.\n\n\nLet’s dive into creating and customizing plots with ggplot2.\n\n\n\n\n\n\nSetup\n\n\n\n\nMake sure you’re in the right project (training_{USERNAME}), create a new Quarto document, delete the default text, and save this document. \nLoad the packages we’ll need:\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(janitor) # expedite cleaning and exploring data\nlibrary(scales) # scale functions for visualization\nlibrary(leaflet) # interactive maps\nlibrary(DT) # interactive tables\n\n\nLoad the data table directly from the KNB Data Repository: Daily salmon escapement counts from the OceanAK database, Alaska, 1921-2017. Navigate to the link above, hover over the “Download” button for the ADFG_firstAttempt_reformatted.csv, right click, and select “Copy Link”.\n\n\nescape_raw &lt;- read_csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Af119a05b-bbe7-4aea-93c6-85434dcb1c5e\")\n\n\nLearn about the data. For this session we are going to be working with data on daily salmon escapement counts in Alaska. Check out the documentation.\nFinally, let’s explore the data we just read into our working environment.\n\n\n## Check out column names\ncolnames(escape_raw)\n\n## Peak at each column and class\nglimpse(escape_raw)\n\n## From when to when\nrange(escape_raw$sampleDate)\n\n## Which species?\nunique(escape_raw$Species)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Intro to Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#getting-the-data-ready",
    "href": "session_10.html#getting-the-data-ready",
    "title": "10  Intro to Data Visualization",
    "section": "10.2 Getting the data ready",
    "text": "10.2 Getting the data ready\nIn most cases, we need to do some wrangling before we can plot our data the way we want to. Now that we have read in the data and have done some exploration, we’ll put our data wrangling skills to practice to get our data in the desired format.\n\n\n\n\n\n\nSide note on clean column names\n\n\n\njanitor::clean_names() is an awesome function to transform all column names into the same format. The default format for this function is snake_case_format. We highly recommend having clear well formatted column names. It makes your life easier down the line.\nHow it works?\n\nescape &lt;- escape_raw %&gt;% \n    janitor::clean_names()\n\nAnd that’s it! If we look at the column names of the object escape, we can see the columns are now all in a lowercase, snake format.\n\ncolnames(escape)\n\n[1] \"location\"     \"sasap_region\" \"sample_date\"  \"species\"      \"daily_count\" \n[6] \"method\"       \"latitude\"     \"longitude\"    \"source\"      \n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nCalculate the annual escapement by species and sasap_region\nFilter the main 5 salmon species (Chinook, Sockeye, Chum, Coho and Pink)\n\n\n\n\nannual_esc &lt;- escape %&gt;%\n    filter(species %in% c(\"Chinook\", \"Sockeye\", \"Chum\", \"Coho\", \"Pink\")) %&gt;%\n    mutate(year = lubridate::year(sample_date)) %&gt;%\n    group_by(species, sasap_region, year) %&gt;%\n    summarize(escapement = sum(daily_count))\n\nhead(annual_esc)\n\n# A tibble: 6 × 4\n# Groups:   species, sasap_region [1]\n  species sasap_region                           year escapement\n  &lt;chr&gt;   &lt;chr&gt;                                 &lt;dbl&gt;      &lt;dbl&gt;\n1 Chinook Alaska Peninsula and Aleutian Islands  1974       1092\n2 Chinook Alaska Peninsula and Aleutian Islands  1975       1917\n3 Chinook Alaska Peninsula and Aleutian Islands  1976       3045\n4 Chinook Alaska Peninsula and Aleutian Islands  1977       4844\n5 Chinook Alaska Peninsula and Aleutian Islands  1978       3901\n6 Chinook Alaska Peninsula and Aleutian Islands  1979      10463\n\n\nThe chunk above used some dplyr commands that we’ve used previously, and some that are new. First, we use a filter with the %in% operator to select only the salmon species. Although we would get the same result if we ran this filter operation later in the sequence of steps, it’s good practice to apply filters as early as possible because they reduce the size of the dataset and can make the subsequent operations faster. The mutate() function then adds a new column containing the year, which we extract from sample_date using the year() function in the helpful lubridate package. Next we use group_by() to indicate that we want to apply subsequent operations separately for each unique combination of species, region, and year. Finally, we apply summarize() to calculate the total escapement for each of these groups.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Intro to Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#plotting-with-ggplot2",
    "href": "session_10.html#plotting-with-ggplot2",
    "title": "10  Intro to Data Visualization",
    "section": "10.3 Plotting with ggplot2",
    "text": "10.3 Plotting with ggplot2\n\n10.3.1 Essential components\nFirst, we’ll cover some ggplot2 basics to create the foundation of our plot. Then, we’ll add on to make our great customized data visualization.\n\n\n\n\n\n\nThe basics\n\n\n\n\nInitialize a ggplot() visualization, by calling the ggplot2::ggplot() function.\nSpecify the data we want to plot, by setting data = my_data.\nSpecify how columns in our data should map to aesthetics (i.e., visual elements) in the plot, by using aes() function.\nDefine one or more plot geometries, i.e. specific data visualization layers (e.g., scatterplot, histogram, boxplot), by using relevant geom_*() functions.\n\nNote To add layers, theme elements, and various customizations to the ggplot object initialized in the first step above, we use the + operator. You can think of this as a process of building out your desired visualization by starting with a blank canvas and incrementally adding the desired details.\n\n\nFor example, let’s plot total escapement by species. We will show this by creating the same plot in 3 slightly different ways. Each of the options below have the essential pieces of a ggplot.\n\n## Option 1 - data and mapping called in the ggplot() function\nggplot(data = annual_esc,\n       aes(x = species, y = escapement)) +\n    geom_col()\n\n## Option 2 - data called in ggplot function; mapping called in geom\nggplot(data = annual_esc) +\n    geom_col(aes(x = species, y = escapement))\n\n\n## Option 3 - data and mapping called in geom\nggplot() +\n    geom_col(data = annual_esc,\n             aes(x = species, y = escapement))\n\nThey all will create the same plot:\n\n\n\n\n\n\n\n\n\nLet’s take a minute to review a few of the core components of a ggplot visualization, and see how they relate back to the data that we are plotting. Consider this small subset of 12 records from our escapement data, and a corresponding scatterplot of daily fish counts across 5 days for a particular region.\n\n\n\n\n\n\n\n\n\n\n10.3.2 Looking at different geoms\nHaving the basic structure with the essential components in mind, we can easily change the type of graph by updating the geom_*().\n\n\n\n\n\n\nggplot2 and the pipe operator\n\n\n\nRemember that the first argument of ggplot is the data input. Just like in dplyr and tidyr, we can also pipe a data.frame directly into the first argument of the ggplot function using the %&gt;% operator. This means we can create expressions that start with data, pipe it through various tidying and restructuring operations, then pipe into a ggplot call which we then build up as described above.\nThis can certainly be convenient, but use it carefully! Combining too many data-tidying or subsetting operations with your ggplot call can make your code more difficult to debug and understand. In general, operations included in the pipe sequence preceding a ggplot call should be those needed to prepare the data for that particular visualization, whereas pre-processing steps that more generally prepare your data for analysis and visualization should be done separately, producing a new named data.frame object.\n\n\nNext, we will use the pipe operator to pass into ggplot() a filtered version of annual_esc, and make a plot with different geometries.\nLine and point\nLet’s start with a line and point visualization.\n\nannual_esc %&gt;%\n    filter(species == \"Sockeye\",\n           sasap_region == \"Bristol Bay\") %&gt;%\n    ggplot(aes(x = year, y = escapement)) +\n    geom_line() +\n    geom_point()\n\n\n\n\n\n\n\n\nHere we are added two layers to the plot. They are drawn in the order given in the expression, which means the points are drawn on top of the lines. In this example, it doesn’t make a difference, but if you were to use different colors for the lines and points, you could look closely and see that the order matters where the lines overlap with the points.\nAlso notice how we included the aesthetic mapping in the ggplot call, which means this mapping was used by both of the layers. If we didn’t supply the mapping in the ggplot call, we would need to pass it into both of the layers separately.\nBoxplot\nNow let’s try a box plot.\n\nannual_esc %&gt;%\n    filter(year == 1974,\n          species %in% c(\"Chum\", \"Pink\")) %&gt;%\n    ggplot(aes(x = species, y = escapement)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nNotice how we again provided aesthetic mappings for both x and y. Let’s think about how the behavior of these two aesthetics is different in this plot compared with the point and line blot.\nWhereas in the previous example we set x to the continuous variable year, here we use the discrete variable species. But it still worked! This is handled by a so-called scale, another component of the ggplot grammar. In short, scales convert the values in your data to corresponding aesthetic values in the plot. You typically don’t have to worry about this yourself, because ggplot chooses the relevant scale based on your input data type, the corresponding aesthetic, and the type of geom. In the plot above, ggplot applied its discrete x-axis scale, which is responsible for deciding how to order and space out the discrete set of species values along the x axis. Later we’ll see how we can customize the scale in some cases.\nThe y aesthetic also seems different in this plot. We mapped the y aesthetic to the escapement variable, but unlike in the point & line diagram, the plot does not have raw escapement values on the y-axis. Instead, it’s showing statistical properties of the data! How does that happen? The answer involves stats, one of the final key components of the ggplot grammar. A stat is a statistical summarization that ggplot applies to your data, producing a new dataframe that it then uses internally to produce the plot. For geom_boxplot, ggplot internally applies a stat_boxplot to your input data, producing the statistics needed to draw the boxplots. Fortunately, you will rarely ever need to invoke a stat_* function yourself, as each geom_* has a corresponding stat_* that will almost always do the job. But it’s still useful to know this is happening under the hood.\n\n\n\n\n\n\nThe identity stat\n\n\n\nIf ggplot applies a statistical transformation to your data before plotting it, then how does the geom_point plot only your raw values? The answer is that this and other geometries use stat_identity, which leaves the data unchanged! Although this might seem unnecessary, it allows ggplot to have a consistent behavior where all input dataframes are processed by a stat_* function before plotting, even if sometimes that step doesn’t actually modify the data.\n\n\nViolin plot\nFinally, let’s look at a violin plot. Other than the geom name itself, this expression is identical to what we used to produce the boxplot above. This is a nice example of how we can quickly switch visualization types – and in this case, the corresponding statistical summarization of our data – with a small change to the code.\n\nannual_esc %&gt;%\n    filter(year == 1974,\n           species %in% c(\"Chum\", \"Pink\")) %&gt;%\n    ggplot(aes(x = species, y = escapement)) +\n    geom_violin()\n\n\n\n\n\n\n\n\n\n\n10.3.3 Customizing our plot\nLet’s go back to our base bar graph. What if we want our bars to be blue instead of gray? You might think we could run this:\n\nggplot(annual_esc,\n       aes(x = species, y = escapement,\n           fill = \"blue\")) +\n    geom_col()\n\n\n\n\n\n\n\n\nWhy did that happen?\nNotice that we tried to set the fill color of the plot inside the mapping aesthetic call. What we have done, behind the scenes, is create a column filled with the word “blue” in our data frame, and then mapped it to the fill aesthetic, which then chose the default fill color of red.\nWhat we really wanted to do was just change the color of the bars. If we want do do that, we can call the color option in the geom_col() function, outside of the mapping aesthetics function call.\n\nggplot(annual_esc,\n       aes(x = species, y = escapement)) +\n    geom_col(fill = \"blue\")\n\n\n\n\n\n\n\n\nWhat if we did want to map the color of the bars to a variable, such as region? ggplot() is really powerful because we can easily get this plot to visualize more aspects of our data.\n\nggplot(annual_esc,\n       aes(x = species, y = escapement,\n           fill = sasap_region)) +\n    geom_col()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeep in mind\n\n\n\n\nIf you want to map a plot aesthetic to a variable in your data (e.g., point color should be based on a specific region), put it within the aes() expression passed to the geom_ layer via its mapping argument.\nIf you want to set a plot aesthetic to a constant value (e.g., “Make ALL the points BLUE”), pass it as an argument directly to the relevant geom_ layer.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThe geom_col plot above introduces another useful ggplot layer control: position adjustments. Consider that at each position along the y axis, our data has potentially many escapement values to plot. Why don’t we simply see many overlapping bars? This is handled by the position argument of geom_col, which takes either a position_* function or its corresponding string alias. The default value for column charts is position_stack(), which is equivalent to calling geom_col(..., position = \"stack\"), and produces the stacked bar chart you see above.\nTry repeating the plot with other available position adjustments relevant for this type of geom: “dodge”, “dodge2”, and “fill”.\n\n\n\n10.3.3.1 Creating multiple plots\nWe know that in the graph we just plotted, each bar includes escapements for multiple years. Let’s leverage the power of ggplot to plot more aspects of our data in one plot.\nWe are going to plot escapement by species over time, from 2000 to 2016, for each region.\nAn easy way to plot another aspect of your data is using the function facet_wrap(). This function takes a mapping to a variable using the syntax ~{variable_name}. The ~ (tilde) is a model operator which tells facet_wrap() to model each unique value within variable_name to a facet in the plot.\nThe default behavior of facet wrap is to put all facets on the same x and y scale. You can use the scales argument to specify whether to allow different scales between facet plots (e.g scales = \"free_y\" to free the y axis scale). You can also specify the number of columns using the ncol = argument or number of rows using nrow =.\n\n## Subset with data from years 2000 to 2016\n\nannual_esc_2000s &lt;- annual_esc %&gt;%\n    filter(year %in% c(2000:2016))\n\n## Quick check\nunique(annual_esc_2000s$year)\n\n [1] 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014\n[16] 2015 2016\n\n## Plot with facets\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\n10.3.3.2 Setting ggplot themes\nNow let’s work on making this plot look a bit nicer. We are going to:\n\nAdd a title using labs()\nAdjust labels using labs()\nInclude a built in theme using theme_bw()\n\nThere are a wide variety of built in themes in ggplot that help quickly set the look of the plot. Use the RStudio autocomplete theme_ &lt;TAB&gt; to view a list of theme functions.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nYou can see that the theme_bw() function changed a lot of the aspects of our plot! The background is white, the grid is a different color, etc. There are lots of other built in themes like this that come with the ggplot2 package.\n\n\n\n\n\n\nExercise\n\n\n\nUse the RStudio auto complete, the ggplot2 documentation, a cheat sheet, or good old Google to find other built in themes. Pick out your favorite one and add it to your plot.\n\n\n\n\nThemes\n## Useful baseline themes are\ntheme_minimal()\ntheme_light()\ntheme_classic()\n\n\nThe built in theme functions (theme_*()) change the default settings for many elements that can also be changed individually using thetheme() function. The theme() function is a way to further fine-tune the look of your plot. This function takes MANY arguments (just have a look at ?theme). Luckily there are many great ggplot resources online so we don’t have to remember all of these, just Google “ggplot cheat sheet” and find one you like.\nLet’s look at an example of a theme() call, where we change the position of the legend from the right side to the bottom, and remove its title.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    theme_light() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank())\n\n\n\n\n\n\n\n\nNote that the theme() call needs to come after any built-in themes like theme_bw() are used. Otherwise, theme_bw() will likely override any theme elements that you changed using theme().\nYou can also save the result of a series of theme() function calls to an object to use on multiple plots. This prevents needing to copy paste the same lines over and over again!\n\nmy_theme &lt;- theme_light() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank())\n\nSo now our code will look like this:\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    my_theme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nUsing whatever method you like, figure out how to rotate the x-axis tick labels to a 45-degree angle.\n\nHint: You can start by looking at the documentation of the function by typing ?theme() in the console. And googling is a great way to figure out how to do the modifications you want to your plot.\n\nWhat changes do you expect to see in your plot by adding the following line of code? Discuss with your neighbor and then try it out!\n\nscale_x_continuous(breaks = seq(2000, 2016, 2))\n\n\n\n\nAnswer\n## Useful baseline themes are\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    scale_x_continuous(breaks = seq(2000, 2016, 2)) +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    my_theme +\n    guides(x=guide_axis(angle = 45))\n\n\n\n\n10.3.3.3 Smarter tick labels using scales\nFixing tick labels in ggplot can be super annoying. The y-axis labels in the plot above don’t look great. We could manually fix them, but it would likely be tedious and error prone.\nThe scales package provides some nice helper functions to easily rescale and relabel your plots. Here, we use scale_y_continuous() from ggplot2, with the argument labels, which is assigned to the function name comma, from the scales package. This will format all of the labels on the y-axis of our plot with comma-formatted numbers.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    scale_x_continuous(breaks = seq(2000, 2016, 2),\n        guide=guide_axis(angle = 45)) +\n    scale_y_continuous(labels = comma) +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    my_theme\n\n\n\n\n\n\n\n\nLet’s look at one more version of this graphic, with even fancier label customization. First, we’ll supply names to the scale_*() calls, which ggplot then uses as the default axis titles. Secondly, we’ll add an even fancier label specification for the y axis, dynamically adjusting the units. Finally, we’ll fix the issue with truncated text in the long facet title, by using a labeller function. While we’re at it, will also use a smaller size for the points.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point(size=1) +\n    scale_x_continuous(\"Year\",\n        breaks = seq(2000, 2016, 4),\n        guide = guide_axis(angle = 45)) +\n    scale_y_continuous(\"Escapement\",\n        label = label_comma(scale_cut = cut_short_scale())) +\n    facet_wrap( ~ sasap_region, scales = \"free_y\",\n        labeller = labeller(sasap_region = label_wrap_gen())) +\n    labs(title = \"Annual Salmon Escapement by Region\") +\n    my_theme\n\n\n\n\n\n\n\n\n\n\n\n10.3.3.4 Saving plots\nSaving plots using ggplot is easy! The ggsave() function will save either the last plot you created, or any plot that you have saved to a variable. You can specify what output format you want, size, resolution, etc. See ?ggsave() for documentation.\n\nggsave(\"figures/annualsalmon_esc_region.jpg\", width = 8, height = 6, units = \"in\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Intro to Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#interactive-visualization",
    "href": "session_10.html#interactive-visualization",
    "title": "10  Intro to Data Visualization",
    "section": "10.4 Interactive visualization",
    "text": "10.4 Interactive visualization\n\n10.4.1 Tables with DT\nNow that we know how to make great static visualizations, let’s introduce two other packages that allow us to display our data in interactive ways.\n\nFirst let’s show an interactive table of unique sampling locations using DT. Write a data.frame containing unique sampling locations with no missing values using two new functions from dplyr and tidyr: distinct() and drop_na().\n\nlocations &lt;- escape %&gt;%\n    distinct(location, latitude, longitude) %&gt;%\n    drop_na()\n\nAnd display it as an interactive table using datatable() from the DT package.\n\ndatatable(locations)\n\n\n\n\n\n\n\n10.4.2 Maps with leaflet\nSimilar to ggplot2, you can make a basic leaflet map using just a couple lines of code. Note that unlike ggplot2, the leaflet package uses pipe operators (%&gt;%) and not the additive operator (+).\nThe addTiles() function without arguments will add base tiles to your map from OpenStreetMap. addMarkers() will add a marker at each location specified by the latitude and longitude arguments. Note that the ~ symbol is used here to model the coordinates to the map (similar to facet_wrap() in ggplot).\n\nleaflet(locations) %&gt;%\n    addTiles() %&gt;%\n    addMarkers(\n        lng = ~ longitude,\n        lat = ~ latitude,\n        popup = ~ location\n    )\n\n\n\n\n\n\nYou can also use leaflet to import Web Map Service (WMS) tiles. Here is an example that utilizes the General Bathymetric Map of the Oceans (GEBCO) WMS tiles. In this example, we also demonstrate how to create a more simple circle marker, the look of which is explicitly set using a series of style-related arguments.\n\nleaflet(locations) %&gt;%\n    addWMSTiles(\n        \"https://www.gebco.net/data_and_products/gebco_web_services/web_map_service/mapserv?request=getmap&service=wms&BBOX=-90,-180,90,360&crs=EPSG:4326&format=image/jpeg&layers=gebco_latest&width=1200&height=600&version=1.3.0\",\n        layers = 'GEBCO_LATEST',\n        attribution = \"Imagery reproduced from the GEBCO_2022 Grid, WMS 1.3.0 GetMap, www.gebco.net\"\n    ) %&gt;%\n    addCircleMarkers(\n        lng = ~ longitude,\n        lat = ~ latitude,\n        popup = ~ location,\n        radius = 5,\n        # set fill properties\n        fillColor = \"salmon\",\n        fillOpacity = 1,\n        # set stroke properties\n        stroke = T,\n        weight = 0.5,\n        color = \"white\",\n        opacity = 1\n    )\n\n\n\n\n\n\nLeaflet has a ton of functionality that can enable you to create some beautiful, functional maps with relative ease. Here is an example of some we created as part of the State of Alaskan Salmon and People (SASAP) project, created using the same tools we showed you here.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Intro to Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#ggplot2-resources",
    "href": "session_10.html#ggplot2-resources",
    "title": "10  Intro to Data Visualization",
    "section": "10.5 ggplot2 Resources",
    "text": "10.5 ggplot2 Resources\n\nWhy not to use two axes, and what to use instead: The case against dual axis charts by Lisa Charlotte Rost.\nCustomized Data Visualization in ggplot2 by Allison Horst.\nA ggplot2 tutorial for beautiful plotting in R by Cedric Scherer.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Intro to Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_11.html",
    "href": "session_11.html",
    "title": "11  Ethical Data Collection",
    "section": "",
    "text": "11.1 Introduction\nThis part of the course was developed with input from ELOKA and the NNA-CO, and is a work-in-progress. The training introduces ethics issues in a broad way and includes discussion of social science data and open science, but the majority of the section focuses on issues related to research with, by, and for Indigenous communities. We recognize that there is a need for more in-depth training and focus on open science for social scientists and others who are not engaging with Indigenous Knowledge holders and Indigenous communities, and hope to develop further resources in this area in the future. Many of the data stewardship practices that have been identified as good practices through Indigenous Data Sovereignty framework development are also relevant for those working with Arctic communities that are not Indigenous, although the rights frameworks and collective ownership is specific to the Indigenous context.\nThe examples we include in this training are primarily drawn from the North American research context. In future trainings, we plan to expand and include examples from other Indigenous Arctic contexts. We welcome suggestions and resources that would strengthen this training for audiences outside of North America.\nWe also recognize the importance of trainings on Indigenous data sovereignty and ethics that are being developed and facilitated by Indigenous organizations and facilitators. In this training we offer some introductory material but there is much more depth offered in IDS specific trainings.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ethical Data Collection</span>"
    ]
  },
  {
    "objectID": "session_11.html#introduction-to-eloka",
    "href": "session_11.html#introduction-to-eloka",
    "title": "11  Ethical Data Collection",
    "section": "11.2 Introduction to ELOKA",
    "text": "11.2 Introduction to ELOKA\nThe Exchange for Local Observations and Knowledge of the Arctic is an NSF funded project. ELOKA partners with Indigenous communities in the Arctic to create online products that facilitate the collection, preservation, exchange, and use of local observations and Indigenous Knowledge of the Arctic. ELOKA fosters collaboration between resident Arctic experts and visiting researchers, provides data management and user support, and develops digital tools for Indigenous Knowledge in collaboration with our partners. By working together, Arctic residents and researchers can make significant contributions to a deeper understanding of the Arctic and the social and environmental changes ongoing in the region.\nArctic residents and Indigenous peoples have been increasingly involved in, and taking control of, research. Through Local and Indigenous Knowledge and community-based monitoring, Arctic communities have made, and continue to make, significant contributions to understanding recent Arctic change. In ELOKA’s work, we subscribe to ideas of information and data sovereignty, in that we want our projects to be community-driven with communities having control over how their data, information, and knowledge are shared in an ethical manner.\nA key challenge of Local and Indigenous Knowledge research and community-based monitoring to date is having an effective and appropriate means of recording, storing, representing, and managing data and information in an ethical manner. Another challenge is to find an effective means of making such data and information available to Arctic residents and researchers, as well as other interested groups such as teachers, students, and decision-makers. Without a network and data management system to support Indigenous Knowledge and community-based research, a number of problems have arisen, such as misplacement or loss of extremely precious data, information, and stories from Elders who have passed away, a lack of awareness of previous studies causing repetition of research and wasted resources occurring in the same communities, and a reluctance or inability to initiate or maintain community-based research without an available data management system. Thus, there is an urgent need for effective and appropriate means of recording, preserving, and sharing the information collected in Arctic communities. ELOKA aims to fill this gap by partnering with Indigenous communities to ensure their knowledge and data are stored in an ethical way, thus ensuring sovereignty over these valuable sources of information.\nELOKA’s overarching philosophy is that Local and Indigenous Knowledge and scientific expertise are complementary and reinforcing ways of understanding the Arctic system. Collecting, documenting, preserving, and sharing knowledge is a cooperative endeavor, and ELOKA is dedicated to fostering that shared knowledge between Arctic residents, scientists, educators, policy makers, and the general public. ELOKA operates on the principle that all knowledge should be treated ethically, and intellectual property rights should be respected.\nELOKA is a service available for research projects, communities, organizations, schools, and individuals who need help storing, protecting, and sharing Local and Indigenous Knowledge. ELOKA works with many different types of data and information, including:\n\nWritten interview transcripts\nAudio or video tapes and files\nPhotographs, artwork, illustrations, and maps\nDigital geographic information such as GPS tracks, and data created using Geographic Information Systems\nQuantitative data such as temperature, snow thickness, wind data, etc.\nMany other types of Indigenous Knowledge and local observations, including place names\n\nELOKA collaborates with other organizations engaged in addressing data management issues for community-based research. Together, we are working to build a community that facilitates international knowledge exchange, development of resources, and collaboration focused on local communities and stewardship of their data, information, and knowledge.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ethical Data Collection</span>"
    ]
  },
  {
    "objectID": "session_11.html#working-with-arctic-communities",
    "href": "session_11.html#working-with-arctic-communities",
    "title": "11  Ethical Data Collection",
    "section": "11.3 Working With Arctic Communities",
    "text": "11.3 Working With Arctic Communities\nArctic communities (defined as a place and the people who live there, based on geographic location in the Arctic/sub-Arctic) are involved in research in diverse ways - as hosts to visiting or non-local researchers, as well as “home” to community researchers who are leading or collaborating on research projects. Over the past decades, community voices of discontent with standard research practices that are often exclusive and perpetuate inequities have grown stronger. The Arctic research community (defined more broadly as the range of institutions, organizations, researchers and local communities involved in research) is in the midst of a complex conversation about equity in research aimed at transforming research practice to make it more equitable and inclusive.\nOne of the drivers of community concerns is the colonial practice of extracting knowledge from a place or group of people without respect for local norms of relationship with people and place, and without an ethical commitment to sharing and making benefits of knowledge accessible and accountable to that place. Such approaches to knowledge and data extraction follow hundreds of years of exploration and research that viewed science as a tool of “Enlightenment” yet focused exclusively on benefits to White, European (or “southern” from an Arctic community perspective) researchers and scientists. This prioritization of non-local perspectives and needs (to Arctic communities) continues in Arctic research.\nOne result of this approach to research has been a lack of access for Arctic residents to the data and knowledge that have resulted from research conducted in their own communities. Much of this data was stored in the personal files or hard drives of researchers, or in archives located in urban centers far from the Arctic.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ethical Data Collection</span>"
    ]
  },
  {
    "objectID": "session_11.html#indigenous-data-governance-and-sovereignty",
    "href": "session_11.html#indigenous-data-governance-and-sovereignty",
    "title": "11  Ethical Data Collection",
    "section": "11.4 Indigenous Data Governance and Sovereignty",
    "text": "11.4 Indigenous Data Governance and Sovereignty\n\nAll governing entities, whether national, state, local, or tribal, need access to good, current, relevant data in order to make policy, planning, and programmatic decisions. Indigenous nations and organizations have had to push for data about their peoples and communities to be collected and shared in ethical and culturally appropriate ways, and they have also had to fight for resources and capacity to develop and lead their own research programs.\n\n11.4.0.1 Indigenous Data Definitions\nIndigenous data sovereignty “…refers to the right of Indigenous peoples to govern the collection, ownership, and application of data about Indigenous communities, peoples, lands, and resources (Rainie et al. 2019). These governance rights apply”regardless of where/by whom data is held (Rainie et al. 2019).\nSome Indigenous individuals and communities have expressed dissatisfaction with the term “data” as being too narrowly focused and abstract to represent the embedded and holistic nature of knowledge in Indigenous communities. Knowledge sovereignty is a related term that has a similar meaning but is framed more broadly, and has been defined as:\n“Tribal communities having control over the documentation and production of knowledge (such as through research activities) which relate to Alaska Native people and the resources they steward and depend on” (Kawerak 2021).\nIndigenous data is “data in a wide variety of formats inclusive of digital data and data as knowledge and information. It encompasses data, information, and knowledge about Indigenous individuals, collectives, entities, lifeways, cultures, lands, and resources.” (Rainie et al. 2019)\nIndigenous data governance is “The entitlement to determine how Indigenous data is governed and stewarded” (Rainie et al. 2019)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ethical Data Collection</span>"
    ]
  },
  {
    "objectID": "session_11.html#care-principles",
    "href": "session_11.html#care-principles",
    "title": "11  Ethical Data Collection",
    "section": "11.5 CARE Principles",
    "text": "11.5 CARE Principles\nIn facilitating use of data resources, the data stewardship community have converged on principles surrounding best practices for open data management One set of these principles is the FAIR principles. FAIR stands for Findable, Accessible, Interoperable, and Reproducible.\nThe FAIR (Findable, Accessible, Interoperable, Reproducible) principles for data management are widely known and broadly endorsed. \nFAIR principles and open science are overlapping concepts, but are distinctive concepts. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. The FAIR principles place emphasis on machine readability, “distinct from peer initiatives that focus on the human scholar” (Wilkinson et al 2016) and as such, do not fully engage with sensitive data considerations and with Indigenous rights and interests (Research Data Alliance International Indigenous Data Sovereignty Interest Group, 2019). Research has historically perpetuated colonialism and represented extractive practices, meaning that the research results were not mutually beneficial. These issues also related to how data was owned, shared, and used.\nTo address issues like these, the Global Indigenous Data Alliance (GIDA) introduced CARE Principles for Indigenous Data Governance to support Indigenous data sovereignty. To many, the FAIR and CARE principles are viewed by many as complementary: CARE aligns with FAIR by outlining guidelines for publishing data that contributes to open-science and at the same time, accounts for Indigenous’ Peoples rights and interests. CARE Principles for Indigenous Data Governance stand for Collective Benefit, Authority to Control, Responsibility, Ethics. The CARE principles for Indigenous Data Governance complement the more data-centric approach of the FAIR principles, introducing social responsibility to open data management practices. These principles ask researchers to put human well-being at the forefront of open-science and data sharing (Carroll et al., 2021; Research Data Alliance International Indigenous Data Sovereignty Interest Group, September 2019).\nIndigenous data sovereignty and considerations related to working with Indigenous communities are particularly relevant to the Arctic. The CARE Principles stand for:\n\nCollective Benefit - Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data for:\n\nInclusive development/innovation\nImproved governance and citizen engagement\nEquitable outcomes\n\nAuthority to Control - Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data.\n\nRecognizing Indigenous rights (individual and collective) and interests\nData for governance\nGovernance of data\n\nResponsibility - Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples.\n\nFor positive relationships\nFor expanding capability and capacity (enhancing digital literacy and digital infrastructure)\nFor Indigenous languages and worldviews (sharing data in Indigenous languages)\n\nEthics - Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n\nMinimizing harm/maximizing benefit - not using a “deficit lens” that conceives of and portrays Indigenous communities as dysfunctional, lacking solutions, and in need of intervention. For researchers, adopting a deficit lens can lead to collection of only a subset of data while excluding other data and information that might identify solutions, innovations, and sources of resilience from within Indigenous communities. For policy makers, a deficit lens can lead to harmful interventions framed as “helping.”\nFor justice - addressing power imbalances and equity\nFor future use - acknowledging potential future use/future harm. Metadata should acknowledge provenance and purpose and any limitations in secondary use inclusive of issues of consent.\n\n\n\nSharing sensitive data introduces unique ethical considerations, and FAIR and CARE principles speak to this by recommending sharing anonymized metadata to encourage discover ability and reduce duplicate research efforts, following consent of rights holders (Puebla & Lowenberg, 2021). While initially designed to support Indigenous data sovereignty, CARE principles are being adopted more broadly and researchers argue they are relevant across all disciplines (Carroll et al., 2021). As such, these principles introduce a “game changing perspective” for all researchers that encourages transparency in data ethics, and encourages data reuse that is both purposeful and intentional and that aligns with human well-being (Carroll et al., 2021). Hence, to enable the research community to articulate and document the degree of data sensitivity, and ethical research practices, the Arctic Data Center has introduced new submission requirements.\n\n\n\n\n11.5.1 Discussion Questions:\n\nDo any of the practices of your data management workflow reflect the CARE Principles, or incorporate aspects of them?\n\n\n\n\n\n11.5.2 Examples from ELOKA\n\n11.5.2.1 Nunaput Atlas\nhttps://eloka-arctic.org/communities/nunaput/atlas/index.html \n\n\nNunaput translates to “our land” in Cup’ik, the Indigenous language of Chevak, Alaska. Chevak or Cev’aq means “cut through channel” and refers to the creation of a short cut between two rivers. Chevak is a Cup’ik community, distinct from the Yup’ik communities that surround it, located in the Yukon-Kuskokwim Delta region of Western Alaska.\nThe Nunaput Atlas is a community driven, interactive, online atlas for the Chevak Traditional Council and Chevak community members to create a record of observations, knowledge, and share stories about their land. The Nunaput Atlas is being developed in collaboration with the Exchange for Local Observations and Knowledge of the Arctic (ELOKA) and the U.S. Geological Survey (USGS). The community of Chevak has been involved in a number of community-based monitoring and research projects with the USGS and the Yukon River Inter-Tribal Watershed Council (YRITWC) over the years. The monitoring data collected by the Chevak Traditional Council’s Environmental department as well as results from research projects are also presented in this atlas.\nAll atlases are created uniquely and data ethics issues and privacy are presented. There is no standard template for user agreements that all atlases adopt: each user agreement is designed by the community, and it isdesigned specific to their needs. Nunaput Atlas has a public view, but is primarily used on the private, password protected side. \n\n\n11.5.2.2 Yup’ik Atlas\nhttps://eloka-arctic.org/communities/yupik/atlas/index.html \nYup’ik Atlas is a great example of the data owners and community wanting the data to be available and public. Part of Indigenous governance and data governance is about having good data, to enable decision making. The Yup’ik atlas has many aspects and uses, and one of the primary uses is for it to be integrated into the regional curriculum to engage youth. \n\n\n11.5.2.3 AAOKH\nhttps://arctic-aok.org/ \nThe Alaska Arctic Observatory and Knowledge Hub is a great example of the continual correspondence and communication between researchers and community members/knowledge holders once the data is collected. Specifically, AAOKH has dedicated a lot of time and in person meetings to creating a data citation that best reflects what the community members want. \n\n\n\n\n\n11.5.3 Final Questions\n\nDo CARE Principles apply to your research? Why or why not?\nAre there any limitations or barriers to adopting CARE Principles?\n\n\n\n11.5.4 Data Ethics Resources \nTrainings:\nFundamentals of OCAP (online training - for working with First Nations in Canada): https://fnigc.ca/ocap-training/take-the-course/\nNative Nations Institute trainings on Indigenous Data Sovereignty and Indigenous Data Governance: https://igp.arizona.edu/jit\nThe Alaska Indigenous Research Program, is a collaboration between the Alaska Native Tribal Health Consortium (ANTHC) and Alaska Pacific University (APU) to increase capacity for conducting culturally responsive and respectful health research that addresses the unique settings and health needs of Alaska Native and American Indian People. The 2024 program runs for three weeks (May 6 - May 24), with specific topics covered each week. Week two (Research Ethics) may be of particular interest. Registration is fee-based the first two weeks, and free for week three. Check out the registration, here.\nThe r-ETHICS training (Ethics Training for Health in Indigenous Communities Study) is starting to become an acceptable, recognizable CITI addition for IRB training by tribal entities.\nKawerak, Inc and First Alaskans Institute have offered trainings in research ethics and Indigenous Data Sovereignty. Keep an eye out for further opportunities from these Alaska-based organizations.\nOn open science and ethics:\nhttps://www.nature.com/articles/d41586-022-00724-0?WT.ec_id=NATURE-20220317&utm_source=nature_etoc&utm_medium=email&utm_campaign=20220317&sap-outbound-id=B4EAD742973291804C3AEF5A15DD13806C9F2C30\nON-MERRIT recommendations for maximizing equity in open and responsible research https://zenodo.org/record/6276753#.YjjgC3XMLCI\nhttps://link.springer.com/article/10.1007/s10677-019-10053-3\nhttps://sagebionetworks.org/in-the-news/on-the-ethics-of-open-science-2/\nArctic social science and data management:\nArctic Horizons report: Anderson, S., Strawhacker, C., Presnall, A., et al. (2018). Arctic Horizons: Final Report. Washington D.C.: Jefferson Institute. https://www.jeffersoninst.org/sites/default/files/Arctic%20Horizons%20Final%20Report%281%29.pdf\nArctic Data Center workshop report: https://arcticdata.io/social-scientific-data-workshop/\nArctic Indigenous research and knowledge sovereignty frameworks, strategies and reports:\nKawerak, Inc. (2021) Knowledge & Research Sovereignty Workshop May 18-21, 2021 Workshop Report. Prepared by Sandhill.Culture. Craft and Kawerak Inc. Social Science Program. Nome, Alaska.\nInuit Circumpolar Council. 2021. Ethical and Equitable Engagement Synthesis Report: A collection of Inuit rules, guidelines, protocols, and values for the engagement of Inuit Communities and Indigenous Knowledge from Across Inuit Nunaat. Synthesis Report. International.\nInuit Tapiriit Kanatami. 2018. National Inuit Strategy on Research. Accessed at: https://www.inuitcircumpolar.com/project/icc-ethical-and-equitable-engagement-synthesis-report/\nIndigenous Data Governance and Sovereignty:\nMcBride, K. Data Resources and Challenges for First Nations Communities. Document Review and Position Paper. Prepared for the Alberta First Nations Information Governance Centre.\nCarroll, S.R., Garba, I., Figueroa-Rodríguez, O.L., Holbrook, J., Lovett, R., Materechera, S., Parsons, M., Raseroka, K., Rodriguez-Lonebear, D., Rowe, R., Sara, R., Walker, J.D., Anderson, J. and Hudson, M., 2020. The CARE Principles for Indigenous Data Governance. Data Science Journal, 19(1), p.43. DOI: http://doi.org/10.5334/dsj-2020-043\nKornei, K. (2021), Academic citations evolve to include Indigenous oral teachings, Eos, 102, https://doi.org/10.1029/2021EO210595. Published on 9 November 2021.\nKukutai, T. & Taylor, J. (Eds.). (2016). Indigenous data sovereignty: Toward an agenda. Canberra: Australian National University Press. See the editors’ Introduction and Chapter 7.\nKukutai, T. & Walter, M. (2015). Indigenising statistics: Meeting in the recognition space. Statistical Journal of the IAOS, 31(2), 317–326.\nMiaim nayri Wingara Indigenous Data Sovereignty Collective and the Australian Indigenous Governance Institute. (2018). Indigenous data sovereignty communique. Indigenous Data Sovereignty Summit, 20 June 2018, Canberra. http://www.aigi.com.au/wp-content/uploads/2018/07/Communique-Indigenous-Data-Sovereignty-Summit.pdf\nNational Congress of American Indians. (2018). Resolution KAN-18-011: Support of US Indigenous data sovereignty and inclusion of tribes in the development of tribal data governance principles. http://www.ncai.org/attachments/Resolution_gbuJbEHWpkOgcwCICRtgMJHMsUNofqYvuMSnzLFzOdxBlMlRjij_KAN-18-011%20Final.pdf\nRainie, S., Kukutai, T., Walter, M., Figueroa-Rodriguez, O., Walker, J., & Axelsson, P. (2019) Issues in Open Data - Indigenous Data Sovereignty. In T. Davies, S. Walker, M. Rubinstein, & F. Perini (Eds.), The State of Open Data: Histories and Horizons. Cape Town and Ottawa: African Minds and International Development Research Centre. https://zenodo.org/record/2677801#.YjqOFDfMLPY\nSchultz, Jennifer Lee, and Stephanie Carroll Rainie. 2014. “The Strategic Power of Data : A Key Aspect of Sovereignty.” 5(4).\nTrudgett, Skye, Kalinda Griffiths, Sara Farnbach, and Anthony Shakeshaft. 2022. “A Framework for Operationalising Aboriginal and Torres Strait Islander Data Sovereignty in Australia: Results of a Systematic Literature Review of Published Studies.” eClinicalMedicine 45: 1–23.\nIRBs/Tribal IRBs:\nAround Him D, Aguilar TA, Frederick A, Larsen H, Seiber M, Angal J. Tribal IRBs: A Framework for Understanding Research Oversight in American Indian and Alaska Native Communities. Am Indian Alsk Native Ment Health Res. 2019;26(2):71-95. doi: 10.5820/aian.2602.2019.71. PMID: 31550379.\nKuhn NS, Parker M, Lefthand-Begay C. Indigenous Research Ethics Requirements: An Examination of Six Tribal Institutional Review Board Applications and Processes in the United States. Journal of Empirical Research on Human Research Ethics. 2020;15(4):279-291. doi:10.1177/1556264620912103\nMarley TL. Indigenous Data Sovereignty: University Institutional Review Board Policies and Guidelines and Research with American Indian and Alaska Native Communities. American Behavioral Scientist. 2019;63(6):722-742. doi:10.1177/0002764218799130\nMarley TL. Indigenous Data Sovereignty: University Institutional Review Board Policies and Guidelines and Research with American Indian and Alaska Native Communities. American Behavioral Scientist. 2019;63(6):722-742. doi:10.1177/0002764218799130\nEthical research with Sami communities:\nEriksen, H., Rautio, A., Johnson, R. et al. Ethical considerations for community-based participatory research with Sami communities in North Finland. Ambio 50, 1222–1236 (2021). https://doi.org/10.1007/s13280-020-01459-w\nJonsson, Å.N. Ethical guidelines for the documentation of árbediehtu, Sami traditional knowledge. In Working with Traditional Knowledge: Communities, Institutions, Information Systems, Law and Ethics. Writings from the Árbediehtu Pilot Project on Documentation and Protection of Sami Traditional Knowledge. Dieđut 1/2011. Sámi allaskuvla / Sámi University College 2011: 97–125. https://samas.brage.unit.no/samas-xmlui/bitstream/handle/11250/177065/Diedut-1-2011_AasaNordinJonsson.pdf?sequence=8&isAllowed=y",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ethical Data Collection</span>"
    ]
  },
  {
    "objectID": "session_12.html",
    "href": "session_12.html",
    "title": "12  R Practice: Cleaning and Wrangling",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Practice: Cleaning and Wrangling</span>"
    ]
  },
  {
    "objectID": "session_12.html#learning-objectives",
    "href": "session_12.html#learning-objectives",
    "title": "12  R Practice: Cleaning and Wrangling",
    "section": "",
    "text": "Practice cleaning and wrangling data\nPractice using dplyr and tidyr functions\nLearn how to use the tibble::tibble() function to create a data frame\nApply concepts learned about data visualization to plot data using ggplot2",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Practice: Cleaning and Wrangling</span>"
    ]
  },
  {
    "objectID": "session_12.html#introduction",
    "href": "session_12.html#introduction",
    "title": "12  R Practice: Cleaning and Wrangling",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nIn this session of R practice, we will continue working with the dataset: Tobias Schwoerer, Kevin Berry, and Jorene Joe. 2022. A household survey documenting experiences with coastal hazards in a western Alaska community (2021-2022). Arctic Data Center. doi:10.18739/A29Z90D3V.\n\n\n\n\n\n\nBig Idea\n\n\n\nIn this practice session, we will build upon the previous session by using dplyr, tidyr, and other packages form the tidyverse to summarize answers of the survey.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Practice: Cleaning and Wrangling</span>"
    ]
  },
  {
    "objectID": "session_12.html#exercise-2",
    "href": "session_12.html#exercise-2",
    "title": "12  R Practice: Cleaning and Wrangling",
    "section": "12.2 Exercise 2",
    "text": "12.2 Exercise 2\n\n\n\n\n\n\nSet up\n\n\n\n\nWork in the same Qmd you did during R practice 1.\nAdd necessary headers and text to describe what you are doing during this practice.\n\nUsing Split-Apply-Combine strategy\nCreating a Data Frame\nJoining Data Frames\nPlotting Q3 responses\n\nAt the top of your document, under the Setup header, load the necessary packages for this practice: dplyr, tidyr, tibble, and ggplot2. .\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\n\n\n\n\n12.2.1 Using Split-Apply-Combine strategy\n\n\n\n\n\n\nStep 1\n\n\n\nUse group_by and summarize to calculate how many responses there were to each unique answer for question 3.\n\n\n\n\n\n\n\n\nExample Code\n\n\n\n\n\n\nq3_tally &lt;- survey_data %&gt;% \n    group_by(Q3) %&gt;% \n    summarize(n_responses = n())\n\n\n\n\n\n\n12.2.2 Creating a Data Frame\n\n\n\n\n\n\nStep 2\n\n\n\nCreate a data.frame containing the definitions to the answer codes in Question 3. Use the metadata to get code-definition pairs.\nOne way of creating a new data frame is by using the tribble() or tibble() functions from the tibble package.\nTip: Search either in the help page or on the web for information about tribble() or tibble(). Then decide which on to use to create a new data frame.\n\n\n\n\n\n\n\n\nExample Code\n\n\n\n\n\n\n## tribble\nq3_definitions &lt;- tribble(\n  ~Q3, ~definition,\n  1,   \"definition of 1\",\n  2,   \"definition of 2\",\n  3,   \"definition of 3\",\n  4, \"definition of 4\",\n  5, \"definition of 5\",\n  NA, \"definition of NA\")\n\n##tibble\nQ3 &lt;- c(1,2,3,4,5,NA)\n\ndefinition &lt;- c(\"definition 1\", \"definition 2\", \"definition 3\", \"definition 4\", \"definition 5\", \"definition NA\")\n\nq3_definitions &lt;- tibble(Q3, definition)\n\n\n\n\n\n\n12.2.3 Joining Data Frames\n\n\n\n\n\n\nStep 3\n\n\n\nUse a left_join to join your definitions table to the summarized answers\n\n\n\n\n\n\n\n\nExample Code\n\n\n\n\n\n\n## Option 1\nq3_summary &lt;- left_join(q3_tally, q3_definitions,\n                        by = \"Q3\")\n\n\n## Option 2\n\nq3_summary &lt;- q3_tally %&gt;% \n    left_join(q3_definitions, by = \"Q3\")\n\n\n\n\n\n\n12.2.4 Data Visualization\n\n\n\n\n\n\nStep 4\n\n\n\nUse ggplot() to create a bar graph (geom_col) comparing the total number of responses for each option in Q3.\nNote: The Example Code provides only the base plot. Reference the Data Visualization lesson to custom your plot. Add a theme_, change the labels, add a title, maybe flip the coords to plot the bars horizontally? Feel free to use other functions you know or discover by searching on the web.\n\n\n\n\n\n\n\n\nExample Code\n\n\n\n\n\n\nggplot(q3_summary,\n       aes(x = Q3,\n           y = n_responses))+\n  geom_col()",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Practice: Cleaning and Wrangling</span>"
    ]
  },
  {
    "objectID": "session_12.html#bonus",
    "href": "session_12.html#bonus",
    "title": "12  R Practice: Cleaning and Wrangling",
    "section": "12.3 Bonus",
    "text": "12.3 Bonus\n\n\n\n\n\n\nGo Further\n\n\n\nExplore how you might summarize other questions in these survey results.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Practice: Cleaning and Wrangling</span>"
    ]
  },
  {
    "objectID": "session_13.html",
    "href": "session_13.html",
    "title": "13  Working with Text Data in R",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Text Data in R</span>"
    ]
  },
  {
    "objectID": "session_13.html#learning-objectives",
    "href": "session_13.html#learning-objectives",
    "title": "13  Working with Text Data in R",
    "section": "",
    "text": "Describe principles of tidy text data\nEmploy strategies to wrangle unstructured text data into a tidy text format using the tidytext package\nDescribe non-tidy text formats and how to convert between tidy text and non-tidy text formats\nBecome familiar with text analysis (or text mining) methods and when to use them\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThis lesson has been adapted from the following resources:\n\nWelcome to Text Mining with R by Julia Silge and David Robinson. Julia and David are also the developers of the tidytext package.\nSection 7.3: (R-) Workflow for Text Analysis from Computational Social Science: Theory & Application, Version: 17 June, 2021 by Paul C. Bauer",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Text Data in R</span>"
    ]
  },
  {
    "objectID": "session_13.html#what-is-text-data",
    "href": "session_13.html#what-is-text-data",
    "title": "13  Working with Text Data in R",
    "section": "13.1 What is text data?",
    "text": "13.1 What is text data?\nText data is information stored as character or string data types. It comes in various different forms including books, research articles, social media posts, interview transcripts, newspapers, government reports, and much more.\n\n\n\n\n\n\nRaw text data is often unstructured and quite “messy”. This could include wrong grammar, missing words, spelling issues, ambiguous language, humor, emojis, symbols, etc. Investing time into carefully cleaning and preparing text data is crucial for your ultimate analysis.\n\n\n\n\n13.1.1 How do we talk about text data?\nHere is a list of text data or text analysis terms we’ll be referring to throughout this lesson. Note this is not a comprehensive list of text analysis terms that are used beyond this lesson.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nCorpus (corpora, plural)\nCollection or database of text or multiple texts. These types of objects typically contain raw strings annotated with additional metadata and details.\n\n\nDocument-feature matrix (DFM)\nRepresents the relationship between features and documents, where each row is a document and each column corresponds to a set of features. Features are not limited to terms and can include a variety of attributes that describe the documents. Each cell in the matrix contains a value that represents the presence, absence, or some quantitative measure of a specific feature in a particular document.\n\n\nDocument-term matrix (DTM)\nRepresents the relationship between terms and documents, where each row stands for a document and each column represents a term, and an entry is the number of occurrences of the term in the document.\n\n\nNatural Language Processing (NLP)\nNLP is an interdisciplinary field used in computer science, data science, linguistics, and others to analyze, categorize, and work with computerized text.\n\n\nSparse matrix\nA matrix where the majority of the entries are zero.\nBoth DFM and DTM are sparse matrices and this is normal. Typically, the bigger the DTM or DFM, the more zeros you’ll see.\n\n\nString\nSpecific type of data whose values are enclosed within a set of quotes. Typically values or elements are characters (e.g. “Hello World!”).\n\n\nText analysis\nThe process of deriving high-quality information or patterns from text through evaluation and interpretation of the output. Also referred to as “text mining” or “text analytics”.\n\n\nToken\nA meaningful unit of text, such as a word, to use for analysis.\n\n\nTokenization\nThe process of splitting text into tokens.\n\n\n\n\n\n13.1.2 How is text data used in the environmental field?\nAs our knowledge about the environmental world grows, researchers will need new computational approaches for working with text data because reading and identifying all the relevant literature for literature syntheses is becoming an increasingly difficult task.\nBeyond literature syntheses, quantitative text analysis tools are extremely valuable for efficiently extracting information from texts and other text mining or text analysis tasks.\n\n\n\n\n\n\nEnvironmental researchers have used text data to:\n\n\n\n\nUnderstand how text mining is used in the field (Farrell et al. 2022)\nGain insights into public perception of a particular subject (Froehlich et al. 2017)\nHighlight the power of NLP models to advance research (Van Houtan et al. 2020)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Text Data in R</span>"
    ]
  },
  {
    "objectID": "session_13.html#what-is-tidy-text-data",
    "href": "session_13.html#what-is-tidy-text-data",
    "title": "13  Working with Text Data in R",
    "section": "13.2 What is tidy text data?",
    "text": "13.2 What is tidy text data?\nLet’s recall the tidy data principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nKeeping that in mind, Silge and Robinson define the tidy text format as being a table with one-token-per-row.\nThis one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix.\nFor tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph.\nBy using tidy data principles, we can apply many “tidy” R packages including dpylr, tidyr, ggplot2, and more.\n\n13.2.1 What is the tidytext R package?\n\n\n\n\n\n\n\n\ntidytext is a package that applies the tidy principles to analyzing text. \nThe package contains many useful functions to wrangle text data into tidy formats. It has been built considering other text mining R packages so that it’s easy to switch between text mining tools (e.g. tm, quanteda, stringr, wordcloud2).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Text Data in R</span>"
    ]
  },
  {
    "objectID": "session_13.html#exercise-tidy-text-workflow",
    "href": "session_13.html#exercise-tidy-text-workflow",
    "title": "13  Working with Text Data in R",
    "section": "13.3 Exercise: Tidy Text Workflow",
    "text": "13.3 Exercise: Tidy Text Workflow\n\n\n\nSource: Silge & Robinson\n\n\nWe are going to use the gutenbergr package to access public domain texts from Project Gutenberg (a library of free eBooks). We’ll then use the tidytext, dyplr and ggplot2 packages to practice the tidy text workflow.\n\n\n\n\n\n\nSetup\n\n\n\n\nCreate a new qmd file and title it “Intro to Text Data”, name yourself as the author, and then save the file as intro-text-data.qmd.\nCreate a new code chunk and attach the following libraries:\n\n\nlibrary(gutenbergr) # access public domain texts from Project Gutenberg\nlibrary(tidytext) # text mining using tidy tools\nlibrary(dplyr) # wrangle data\nlibrary(ggplot2) # plot data\n\n\nDepending on which group you’re in, use one of the following public domain texts:\n\n\n# Group A\ngutenberg_works(title == \"Emma\")  # id: 158\n\n# Group B\ngutenberg_works(title == \"The Wonderful Wizard of Oz\")  # id: 55\n\n# Group C\ngutenberg_works(title == \"The Phantom of the Opera\") # phantom text\n\n\n\n\n13.3.1 Questions\nThe answers in the code chunks are using the text of Heidi.\n\n\n\n\n\n\nQuestion 1\n\n\n\nGet the id number from the gutenberg_works() function so that you can download the text as a corpus using the function gutenberg_download(). Save the corpus to an object called {book-title}_corp. View the object - is the data in a tidy format?\n\n\n\n\nAnswer\n# get id number\ngutenberg_works(title == \"Heidi\")  # id: 1448\n\n# access text data using id number from `gutenberg_works()`\ntxt_corp &lt;- gutenberg_download(1448)\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nTokenize the corpus data using unnest_tokens(). Take a look at the data - do we need every single token for our analysis?\n\n\n\n\nAnswer\n# tidy text data - unnest and remove stop words\ntxt_tidy &lt;- txt_corp %&gt;%\n    unnest_tokens(word, text)\n\nhead(txt_tidy)\n\n\n# A tibble: 6 × 2\n  gutenberg_id word    \n         &lt;int&gt; &lt;chr&gt;   \n1         1448 heidi   \n2         1448 by      \n3         1448 johanna \n4         1448 spyri   \n5         1448 contents\n6         1448 i       \n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nRemove “stop words” or words that can be safely removed or ignored without sacrificing the meaning of the sentence (e.g. “to”, “in”, “and”) using anti_join().\nTake a look at the data - are you satisfied with your data? We won’t conduct any additional cleaning steps here, but consider how you would further clean the data.\n\n\n\n\nAnswer\n# remove stop words\ntxt_tidy &lt;- txt_tidy %&gt;%\n    dplyr::anti_join(stop_words, by = \"word\")\n\nhead(txt_tidy)\n\n\n# A tibble: 6 × 2\n  gutenberg_id word    \n         &lt;int&gt; &lt;chr&gt;   \n1         1448 heidi   \n2         1448 johanna \n3         1448 spyri   \n4         1448 contents\n5         1448 mountain\n6         1448 alm     \n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nCalculate the top 10 most frequent words using the functions count() and slice_max().\n\n\n\n\nAnswer\n# calculate top 10 most frequent words\ntxt_count &lt;- txt_tidy %&gt;%\n    count(word) %&gt;%\n    slice_max(n = 10, order_by = n)\n\ntxt_count\n\n\n# A tibble: 10 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 heidi         924\n 2 peter         335\n 3 child         291\n 4 grandfather   281\n 5 clara         270\n 6 grandmother   242\n 7 day           239\n 8 time          195\n 9 mountain      181\n10 uncle         151\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nPlot the top 10 most frequent words using ggplot().\nWe recommend creating either a bar plot using geom_col() or a lollipop plot using both geom_point() and geom_segment().\n\n\n\n\nBar Plot Code\n# bar plot\nggplot(data = txt_count, aes(n, reorder(word, n))) +\n  geom_col() +\n    labs(x = \"Count\",\n         y = \"Token\")\n\n\n\n\n\n\n\n\n\n\n\nBase Lollipop Plot Code\n# initial lollipop plot\nggplot(data = txt_count, aes(x=word, y=n)) +\n    geom_point() +\n    geom_segment(aes(x=word, xend=word, y=0, yend=n)) +\n    coord_flip() +\n    labs(x = \"Token\",\n         y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\n13.3.2 Bonus Question\n\n\n\n\n\n\nQuestion 6\n\n\n\nConsider elements in theme() and improve your plot.\n\n\n\n\nCustom Lollipop Plot Code\n# ascending order pretty lollipop plot\nggplot(data = txt_count, aes(x=reorder(word, n), y=n)) +\n    geom_point(color=\"cyan4\") +\n    geom_segment(aes(x=word, xend=word, y=0, yend=n), color=\"cyan4\") +\n    coord_flip() +\n    labs(title = \"Top Ten Words in The Phantom of the Opera\",\n         x = NULL,\n         y = \"Count\") +\n    theme_minimal() +\n    theme(\n        panel.grid.major.y = element_blank()\n    )",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Text Data in R</span>"
    ]
  },
  {
    "objectID": "session_13.html#tidy-text-to-non-tidy-text-workflows",
    "href": "session_13.html#tidy-text-to-non-tidy-text-workflows",
    "title": "13  Working with Text Data in R",
    "section": "13.4 Tidy Text to Non-tidy Text Workflows",
    "text": "13.4 Tidy Text to Non-tidy Text Workflows\n\n\n\nA flowchart of a typical text analysis that combines tidytext with other tools and data formats, particularly the tm or quanteda packages. Source: Silge & Robinson\n\n\nIn the Tidy Text Workflow Exercise, we converted our corpus into a data table that has “one-token-per-row”. However, the tidy text format of one-token-per-row is not a common format for other R packages that work with text data or perform text analysis. Packages like tm, quanteda, topicmodels.\nMany text analysis methods, in particular NLP techniques (e.g. topic models) require text data to be stored in a mathematical format. A common approach is to create a matrix, such as a: sparse matrix, a document term matrix (DTM), or a document-feature matrix (DFM). In a matrix format, algorithms are able to more easily compare one document to many other documents to identify patterns.\nSilge and Robinson kept this in mind as they built the tidytext package, and included helpful cast() functions to turn a tidy text object (again a table with one-token-per-row) into a matrix.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Text Data in R</span>"
    ]
  },
  {
    "objectID": "session_13.html#exercise-explore-unstructured-text-data-from-a-pdf",
    "href": "session_13.html#exercise-explore-unstructured-text-data-from-a-pdf",
    "title": "13  Working with Text Data in R",
    "section": "13.5 Exercise: Explore Unstructured Text Data from a PDF",
    "text": "13.5 Exercise: Explore Unstructured Text Data from a PDF\nFrequently the text data we want to analyzed is in PDF format. In the next exercise we walk through how to read in a PDF file into R to be able to programmatically analyze the text.\n\n\n\n\n\n\nSetup\n\n\n\n\nIn the intro-text-data.qmd file, create a new header for this exercise (e.g. “Explore Unstructured Text Data from a PDF”).\nCreate a new code chunk and attach the following libraries:\n\n\nlibrary(tidytext) # tidy text tools\nlibrary(quanteda) # create a corpus\nlibrary(pdftools) # read in data\nlibrary(dplyr) # wrangle data\nlibrary(stringr) # string manipulation\nlibrary(ggplot2) # plots\nlibrary(wordcloud)\n\n\nDepending on which group you’re in, read in one of the following chapters of the Delta Plan. Access and download the chapters of the Delta Plan from Delta Stewardship Council website.\n\nNotes for quick exploration of data:\n\nCheck the class() of the pdf you just read in - is it what you expected? How does the object appear in the Global Environment?\nCall the object in the Console. What does your data look like? What can you infer from how it’s structured?\n\n\n# ch 3\npath_df &lt;- \"data/dsc-plan-ch3.pdf\"\ndp_ch3 &lt;- pdftools::pdf_text(path_df)\n\n# ch 4\npath_df &lt;- \"data/dsc-plan-ch4.pdf\"\ndp_ch4 &lt;- pdftools::pdf_text(path_df)\n\n# ch 5\npath_df &lt;- \"data/dsc-plan-ch5.pdf\"\ndp_ch5 &lt;- pdftools::pdf_text(path_df)\n\n# ch 6\npath_df &lt;- \"data/dsc-plan-ch6.pdf\"\ndp_ch6 &lt;- pdftools::pdf_text(path_df)\n\nWe will demonstrate using Chapter 8.\n\npath_df &lt;- \"data/text/dsc-plan-ch8.pdf\"\ndp_ch8 &lt;- pdftools::pdf_text(path_df)\n\n\nUsing the quanteda package, turn the unstructured pdf text data into a corpus.\n\nNotes for quick exploration of data:\n\nCheck the class() of the corpus you created - is it what you expected? How does the object appear in the Global Environment?\nCall the object in the Console. What does your data look like? How does this structure compare to the pdf object?\nRun summary() of the corpus in the Console. What insights can you glean?\n\n\ndp_ch8_corpus &lt;- quanteda::corpus(dp_ch8)\n\n\nUsing tidy() from tidytext, make the corpus a tidy object.\n\nNotes for quick exploration of data:\n\nCheck the class() of the corpus you created - is it what you expected? How does the object appear in the Global Environment?\nCall the object in the Console or use View(). What does your data look like? Is it what you expected?\n\n\n# ch 8 is used for demonstration and testing\ndp_ch8_tidy &lt;- tidy(dp_ch8_corpus)\n\n\n\n\n13.5.1 Questions\nWork independently or in groups for Question 1-5. The code solutions are based on the text data from Chapter 8 of the Delta Plan.\n\n\n\n\n\n\nQuestion 1\n\n\n\nTokenize the tidy text data using unnest_tokens()\n\n\n\n\nAnswer\ndp_ch8_unnest &lt;- dp_ch8_tidy %&gt;%\n    unnest_tokens(output = word,\n                  input = text)\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nRemove stop words using anti_join() and the stop_words data frame from tidytext.\n\n\n\n\nAnswer\ndp_ch8_words &lt;- dp_ch8_unnest %&gt;%\n    dplyr::anti_join(stop_words)\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCalculate the top 10 most frequently occurring words. Consider using count() and slice_max().\n\n\n\n\nAnswer\ndp_ch8_count &lt;- dp_ch8_words %&gt;%\n    count(word) %&gt;%\n    slice_max(n = 10, order_by = n)\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nVisualize the results using a plot of your choice (e.g. bar plot, lollipop plot, or wordcloud).\n\n\n\n\nBar Plot Code\n# bar plot\nggplot(dp_ch8_count, aes(x = reorder(word, n), y = n)) +\n    geom_col() +\n    coord_flip() +\n    labs(title = \"Top 10 Most Frequently Occurring Words in Chapter 8 of the Delta Plan\",\n         x = NULL,\n         y = \"count\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nLollipop Plot Code\n# lollipop plot\nggplot(data = dp_ch8_count, aes(x=reorder(word, n), y=n)) +\n    geom_point() +\n    geom_segment(aes(x=word, xend=word, y=0, yend=n)) +\n    coord_flip() +\n    labs(title = \"Top 10 Most Frequently Occurring Words in Chapter 8 of the Delta Plan\",\n         x = NULL,\n         y = \"Count\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nWordcloud Plot Code\n# wordcloud\nwordcloud(words = dp_ch8_count$word,\n          freq = dp_ch8_count$n)\n\n\n\n\n\n\n\n\n\n\n\n13.5.2 Bonus Question\n\n\n\n\n\n\nQuestion 5\n\n\n\nWhat do you think of your plots? Are they helpful? Consider other techniques like adding custom stop words or stemming to improve your results.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Text Data in R</span>"
    ]
  },
  {
    "objectID": "session_13.html#common-text-analysis-and-text-mining-methods",
    "href": "session_13.html#common-text-analysis-and-text-mining-methods",
    "title": "13  Working with Text Data in R",
    "section": "13.6 Common Text Analysis and Text Mining Methods",
    "text": "13.6 Common Text Analysis and Text Mining Methods\nThe text analysis tools, methods, and packages depend greatly on your specific text analysis goals and the nature of your text data. You may end up only using one method or one package, or many in combination.\n\n\n\n\n\n\n\n\nText Analysis Method\nR Package\nWhen to Use\n\n\n\n\nDocument-Term Matrix (DTM)\ntm, quanteda, tm.plugin.webmining\nRepresent text data as a matrix of word frequencies.\n\n\nNamed Entity Recognition (NER)\nopenNLP, spacyr, udpipe\nIdentify entities like names, locations, etc. in text.\n\n\nSentiment Analysis\ntidytext, sentimentr, syuzhet\nDetermine sentiment (positive/negative) in text.\n\n\nStopword Removal\ntm, quanteda, tidytext\nRemove common and irrelevant words.\n\n\nText Classification\ncaret, tm, quanteda\nCategorize text into predefined classes/categories.\n\n\nText Clustering\ntm, text2vec, tm.plugin.clustering\nGroup similar documents together.\n\n\nText Summarization\ntm, textclean, textrank\nCondense the main points of a text into a summary.\n\n\nTF-IDF\ntm, tm.plugin.webmining, tidytext\nMeasure word importance in a document corpus.\n\n\nTokenization\ntm, quanteda, text2vec\nSplit text into individual words/tokens.\n\n\nTopic Modeling\ntm, topicmodels, lda2vec\nDiscover hidden topics in a collection of documents.\n\n\nWord Embeddings\nword2vec, text2vec, fastText\nRepresent words as dense vectors for semantic analysis.\n\n\nDependency Parsing\nudpipe, spaCy, openNLP\nAnalyze grammatical structure and word dependencies.\n\n\n\n\n13.6.1 Exercise: Basic topic modeling\n\nlibrary(topicmodels)\n\n\n\n13.6.2 Steps\n\n\n\n\n\n\nStep 1\n\n\n\nGet some PDFs from the Delta Stewardship Council\n\n\n\n\nAnswer\ndelta_url &lt;- \"https://deltacouncil.ca.gov/pdf/delta-plan/\"\ndp_ch3 &lt;- pdftools::pdf_text(paste0(delta_url, \"2018-04-26-amended-chapter-3.pdf\"))\ndp_ch4 &lt;- pdftools::pdf_text(paste0(delta_url, \"2022-06-29-chapter-4-protect-restore-and-enhance-the-delta-ecosystem.pdf\"))\n\n\n\n\n\n\n\n\nStep 2\n\n\n\nTidy the data using word tokens, remove stop words, and combine both into one dataframe.\n\n\n\n\nAnswer\ndp_ch3_words &lt;- dp_ch3 %&gt;%\n  quanteda::corpus() %&gt;%\n  tidy() %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\ndp_ch4_words &lt;- dp_ch4 %&gt;%\n  quanteda::corpus() %&gt;%\n  tidy() %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\ndp_words &lt;- bind_rows(\n  dp_ch3_words %&gt;% mutate(document=\"Chapter3\"),\n  dp_ch4_words %&gt;% mutate(document=\"Chapter4\")\n)\n\n\n\n\n\n\n\n\nStep 3\n\n\n\nCreate a document-term matrix.\n\n\n\n\nAnswer\ndp_dtm &lt;- dp_words %&gt;%\n  count(document, word, sort = TRUE) %&gt;%\n  cast_dtm(document, word, n)\n\n\n\n\n\n\n\n\nStep 4\n\n\n\nRun topic modeling using Latent Dirichlet allocation (LDA).\n\n\n\n\nAnswer\ndp_lda &lt;- LDA(dp_dtm, k = 2, control = list(seed = 1234))\ndp_topics &lt;- tidy(dp_lda, matrix = \"beta\")\n\ndp_top_terms &lt;- dp_topics %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 20) %&gt;%\n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\n\n\n\n\n\n\n\nStep 4\n\n\n\nWhat are the top terms?\n\n\n\n\nAnswer\ndp_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5\n\n\n\nWhat are these two topics, as expressed using the words most strongly associated with each one?\n\n\n\n\nAnswer\nbeta_wide &lt;- dp_topics %&gt;%\n  mutate(topic = paste0(\"topic\", topic)) %&gt;%\n  tidyr::pivot_wider(names_from = topic, values_from = beta) %&gt;%\n  filter(topic1 &gt; .001 | topic2 &gt; .001) %&gt;%\n  mutate(log_ratio = log2(topic2 / topic1))\n\nbeta_wide %&gt;%\n  mutate(term=reorder(term, log_ratio)) %&gt;%\n  slice_max(abs(log_ratio), n=20) %&gt;%\n  ggplot +\n    geom_col(aes(term, log_ratio)) +\n    coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Text Mining Tools and Analysis Packages Resources\n\n\n\n\nCRAN Task View: Natural Language Processing\nPenn Libraries Guides: Text Analysis\nQuanteda Tutorials By Kohei Watanabe and Stefan Müller\n\n\n\n\n\n\n\n\nFarrell, Maxwell J., Liam Brierley, Anna Willoughby, Andrew Yates, and Nicole Mideo. 2022. “Past and Future Uses of Text Mining in Ecology and Evolution.” Proceedings of the Royal Society B: Biological Sciences 289 (1975). https://doi.org/10.1098/rspb.2021.2721.\n\n\nFroehlich, Halley E., Rebecca R. Gentry, Michael B. Rust, Dietmar Grimm, and Benjamin S. Halpern. 2017. “Public Perceptions of Aquaculture: Evaluating Spatiotemporal Patterns of Sentiment Around the World.” Edited by Christopher M. Somers. PLOS ONE 12 (1): e0169281. https://doi.org/10.1371/journal.pone.0169281.\n\n\nVan Houtan, Kyle S., Tyler Gagne, Clinton N. Jenkins, and Lucas Joppa. 2020. “Sentiment Analysis of Conservation Studies Captures Successes of Species Reintroductions.” Patterns 1 (1): 100005. https://doi.org/10.1016/j.patter.2020.100005.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Text Data in R</span>"
    ]
  },
  {
    "objectID": "session_14.html",
    "href": "session_14.html",
    "title": "14  U.S Census Data in R",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>U.S Census Data in R</span>"
    ]
  },
  {
    "objectID": "session_14.html#learning-objectives",
    "href": "session_14.html#learning-objectives",
    "title": "14  U.S Census Data in R",
    "section": "",
    "text": "Provide an overview of US Census data\nIntroduce the main functions of the tidycensus package to be able to work with census data\nReview data wrangling function to get census data ready for analysis\nPlot census data using ggplot2\n\n\n\n\n\n\n\nAcknowledgement\n\n\n\nThis lesson is based on Analyzing US Census Data: Methods, Maps, and Models in R, (Chapter 1 through 4) by Kyle Walker. And Walker’s workshop “Analyzing 2020 Census Data with R and tidycensus” for University of Michigan (2022). GitHub repository here.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>U.S Census Data in R</span>"
    ]
  },
  {
    "objectID": "session_14.html#introduction-to-tidycensus",
    "href": "session_14.html#introduction-to-tidycensus",
    "title": "14  U.S Census Data in R",
    "section": "14.1 Introduction to tidycensus",
    "text": "14.1 Introduction to tidycensus\n\n\n\n\n\n\n\nWhen working with census data, the general workflow would be to go to data.census.gov, filter data and queries based on variables and geographies, and then download the data into a spreadsheet. People that work with census data do this over and over again.\nThe tidycensus package (Walker and Matt (2021)) was developed to systematize this process and do this systematization using R. In 2012 the US Census Bureau released the Census Application Programming Interface (API) as a way to disseminate government data resources to the public. This interface now provides programmatic access to hundreds of data resources from the Census Bureau. The vision behind this package was to incorporate the API access into an R package to facilitate access to census data using R.\nNote there are other R packages that interact with the census APIs, such as censusapi and acs. In this lesson, we are going to focus on the tidycensus package, given that is continuously maintained by the authors, has good documentation and it makes it easy to work with other tidyverse functions.\nFrom Walkers Book:\n\n“The tidycensus is an R package that provides an interface to access and work with the United States Census Bureau data. It simplifies the process of retrieving and analyzing census data by allowing users to query data directly from the Census Bureau’s APIs and then organize the data into a tidy format for easy manipulation and analysis.”\n\nNote that this lesson is just an introduction to this package. Here we aim to providing the basic tools for you to understand how the core functions of this package work so you can adapt to your specific census data needs.\n\n14.1.1 General Structure\ntidycensus takes an opinionated approach to accessing a selected number of census APIs. The main goal is to facilitate access to a few census APIs through R.\nData files you can access through this package are named and described in the table below.\n\n\n\n\n\n\n\nSurvey Name\nDescription\n\n\n\n\nDecennial census\nComplete enumeration of the US population to assist with apportionment. It asks a limited set of questions on race, ethnicity, age, sex, and housing tenure. Data from 2000, 2010, available data from 2020\n\n\nAmerican Community Survey (ACS)\nDetailed demographic information about US population. Annual data updates. 1-year ACS greater, and the 5-year ACS, which is a moving average of data over a 5-year period that covers geographies down to the Census block group. ACS data represent estimates rather than precise counts. Data includes margin of error.\n\n\nPopulation estimate program\nThese datasets include yearly estimates of population characteristics by state, county, and metropolitan area, along with components of change demographic estimates like births, deaths, and migration rates.\n\n\nACS Public Use Microdata\nAnonymized individual-level records from the ACS organized by households\n\n\nMigration Flows\nInformation about in and outflows from several geographies from the 5-year ACS samples.\n\n\n\nThe idea behind this package is to make the tedious process of working with Census data more concise. It pulls data from the census API and returns it to the user in a “tidy” format.\nCan easily merge census geometries to data for mapping. Which apparently can be a very time-consuming task. Unfortunately, for this session, we will not get into mapping. Check out Analyzing Census Data Chapter 6: Mapping Census Data with R for more details on this subject.\nOther features of this package:\n\nIncludes tools for handling margins of errors in the ACS and working with survey weights in the ACS Public Use Microdata.\nYou can request data from states and counties by name instead of FIPS codes.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>U.S Census Data in R</span>"
    ]
  },
  {
    "objectID": "session_14.html#getting-started",
    "href": "session_14.html#getting-started",
    "title": "14  U.S Census Data in R",
    "section": "14.2 Getting Started",
    "text": "14.2 Getting Started\n\n\n\n\n\n\nSetup\n\n\n\n\nMake sure you’re in the right project (training_{USERNAME}) and use the Git workflow by Pulling to check for any changes. Then, create a new Quarto document, delete the default text, and save this document.\nLoad the packages we’ll need:\n\n\nlibrary(tidycensus)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n\nGet an API key to connect our session with the census data and be able to retrieve data\n\n\nGo to https://api.census.gov/data/key_signup.html\nFill out the form\nCheck your email for your key.\nMake sure to ACTIVATE your key\n\n\nUse the census_api_key() function to set your key. Note: install = TRUE forces r to write this key to a file in our R environment that will be read every time you use R. This means, by setting this argument to TRUE, you only have to do it once in any computer you are working. If you see this argument as FALSE, R will not remember this key next time you come back.\n\n\ncensus_api_key(\"YOUR KEY GOES HERE\", install = TRUE)\n\n\nRestart R\nRun the library chunk again.\n\n\n\nNote 1: As we are working on the server, the package is already installed in our system. But when you go work on your own computers, you will have to install the package using install.package(\"tidycensus\"), if you haven’t done that already.\nNote 2: The API key you received in your email is good to use in other instances too. Keep this email! We are using the key to connect your included-crab session with the US Census data. You can use the same key to connect your personal/work computer too.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>U.S Census Data in R</span>"
    ]
  },
  {
    "objectID": "session_14.html#quering-data-focus-on-2020-decennial-data",
    "href": "session_14.html#quering-data-focus-on-2020-decennial-data",
    "title": "14  U.S Census Data in R",
    "section": "14.3 Quering data (focus on 2020 decennial data)",
    "text": "14.3 Quering data (focus on 2020 decennial data)\n\n14.3.1 The 2020 Decennial Data\nBefore we dive into retrieving data from the 2020 decennial census, we need to mention a couple of things. Based on all the challenges of running a decennial census during a pandemic, the Census Bureau had to make decisions and provide new and different functionalists. The pandemic also delayed the release of the 2020 census data. You can check here what data products have been released until now, and the Bureau’s timeline to release more data.\nOne of the main files from the 2020 census is the PL94-171 Redistricting Summary File which is used for congressional appointments and redistricting. Variable available in this file are:\n\nTotal counts (population & households)\nOccupied/vacant housing unit\nTotal and voting age population breakdown by race & ethnicity\nGroup quarter status\n\nDemographic and Housing Characteristics Summary Files (Different to summary file 1 form 2010). Contains age and sex breakdowns and detailed race and ethnicity data.\n\n\n14.3.2 Getting census data\nThe main functions from tidycensus represent the select number of datasets that this package provides access to. The following table provides the description for each of the core functions from Walker’s book.\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nget_decennial()\nRetrieves data from the US Decennial Census APIs for 2000, 2010, and 2020.\n\n\nget_acs()\nRequests data from the 1-year and 5-year American Community Survey samples. Data are available from the 1-year ACS back to 2005 and the 5-year ACS back to 2005-2009.\n\n\nget_estimates()\nAllows you to get the Population Estimates. These datasets include yearly estimates of population characteristics by state, county, and metropolitan area, along with components of change demographic estimates like births, deaths, and migration rates.\n\n\nget_pums()\nAccesses data from the ACS Public Use Microdata Sample APIs. These samples include anonymized individual-level records from the ACS organized by household and are highly useful for many different social science analyses\n\n\nget_flows()\nan interface to the ACS Migration Flows APIs. Includes information on in- and out-flows from various geographies for the 5-year ACS samples, enabling origin-destination analyses.\n\n\n\nHere we are going to focus on getting started with using get_decennial() as a way of understanding how this package works.\nWith get_decennial(), you can query data from 2000, 2010 and 2020 decennial census. The 3 necessary arguments you need to provide are:\n\nGeography\nVariable\nYear\n\n\n\n\n\n\n\nSpecifying a summary file\n\n\n\nAnother argument form get_decennial() is sumfile =.\n“The Census summary file; if NULL, defaults to”pl” when the year is 2020 and “sf1” for 2000 and 2010. Not all summary files are available for each decennial Census year. Make sure you are using the correct summary file for your requested variables, as variable IDs may be repeated across summary files and represent different topics”\n\n\nSo, to get the total population for 2020 by state the code would look like this.\n\npop_2020 &lt;- get_decennial(\n    geography = \"state\",\n    variable = \"P1_001N\",\n    year = 2020)\n\nThe resulting data frame you get from this query is a tibble in “tidy” format with 4 columns:\n\nGEOID = Code for the geographic unit (in this case FIP codes)\nvariable = census code for the variable (eg. P1_001N is the code for the total population in the 2020 population redistricting file)\nvalue = population count\n\nThis table provides data for one single variable. In this case the variable is “P1_001N” which refers to total population.\n\n\n\n\n\n\nMessage\n\n\n\n\nYou get this message the first time you run get_decennial() in your session. It first makes sure your are retrieving the correct data. Then it mentions the fact that the 2020 census “Introduces errors differential privacy”.\nIn the past other privacy methods have been used to preserve confidentiality. Differential privacy is a method that purposely introduces noise or error into the data in order to make it impossible or at least very difficult to reverse engineer the census and track where the responses are coming from. This has an impact on small area counts (e.g.: block with children but not adults). This is something to be aware if you are working with small population geographies.\nOnly the population data is differentially infused. The household data are not.\n\n\nNote that to get total population data for the 2010 decennial census the variable code is different than the 2020 census (even though the arguments of the functions are the same). In this case the code would look like this.\n\npop_2010 &lt;- get_decennial(\n  geography = \"state\", \n  variables = \"P001001\",\n  year = 2010)\n\nWhen using get_decennial(), you can also specify a specific table instead of only one variable. A table of data contains multiple variables. The code for getting the “P2” table from 2020 would look like this.\n\ntable_p2_2020 &lt;- get_decennial(\n    geography = \"state\",\n    table = \"P2\",\n    year = 2020)\n\nTable P2 is one of the tables of the 2020 census, which provides counts by Race with Hispanic origins.\nLet’s check which variables we have in the P2 table.\n\nunique(table_p2_2020$variable)\n\nOne of the hardest things to wrap your head around when using tidycensus is understanding the difference between files, tables, and variables and how they work.\nLet’s take a look on how this works. Data is released in files. Each file is a data product from the Census Bureau (find details of the Census data products here), for example, PL 94-171 is one of the files for the 2020 Decennial Census. Each of these files contains many tables (P1, P2 .. P5). Each table covers a specific topic. For example, P2 provides counts by race by Hispanic origins. Then each table has multiple variables. The variable code seems to, for the most part, start with the table code. For example, the unique codes values we printed from table P2 all start with P2_.\nHow do we navigate all these codes? There is no straightforward way. In part, this dilemma is part of working with census data, which in itself is complicated. However, tidycensus provides some help with a function called load_variables()\nThis function scrapes the variables’ names from the census website and gives you a data frame that you can interact with. This variable requires you to input a year and a file name.\nSo let’s run this function and create an object named vars_pl_2020 This will give us all the variable codes and definitions for the PL 94-171 redistricting file.\n\nvars_pl_2020 &lt;-  load_variables(2020, \"pl\")\n\n## for 20210\nvars_pl_2010 &lt;-  load_variables(2010, \"pl\")\n\nNow, let’s take a look at this data frame. And interactively search for the variables in this file. This data frame has 3 columns, variable’s name, label, and concept, which represent the table’s name.\nFor example, here we can see the variable “P1_001N” that we used earlier to retrieve the total population by state. We can also scroll down and see that all variables that start with P2, belong to the P2 table “HISPANIC OR LATINO, AND NOT HISPANIC OR LATINO BY RACE”. We can also see the variable codes for the H1 table for Occupancy or Housing.\nTables available in the 2020 Census PL file:\n\n\n\n\n\n\n\nTable Name\nDescription\n\n\n\n\nH1\nOccupancy status (housing)\n\n\nP1\nRace by Hispanic origin\n\n\nP2\nHispanic or Latino, and not Hispanic or Latino by Race\n\n\nP3\nRace for the population 18+\n\n\nP4\nHispanic or Latino, and not Hispanic or Latino by Race for the Population 18 Years and Over\n\n\nP5\nGroup quarters status\n\n\n\nNote: “Group quarters are places where people live or stay, in a group living arrangement, that is owned or managed by an entity or organization providing housing and/or services for the residents.” (US Census Bureau Glossary)\nWe won’t get into much more detail, but, you can query any of the available files for the 2020 census data. Like the Demographic and Housing Characteristics File (DHC) by running the following code. This is a much larger file with much more tables than the PL file.\n\nvars_dhc_2020 &lt;-  load_variables(2020, \"dhc\")\n\n## Note you have to specify the file with sumfile =\nhousehold_2020 &lt;- get_decennial(\n    geography = \"state\",\n    variable = \"H10_001N\",\n    year = 2020,\n    sumfile = \"dhc\")\n\nThe idea behind load_variables() is for you to be able to search for the variable code for the variable you need.\nNow that we’ve talked about variables let’s talk a little bit about geography and how tidycensus makes it easy to query data within census geographies. Census data is tabulated in enumeration units. These units are specific geographies including legal entities such as states and counties, and statistical entities that are not official jurisdictions but used to standardize data. The graphic below, provided by census.gov shows the standard hierarchy of census geographic entities.\n\nThe parameter geography = in get_acs() and get_decennial() allows us to request data from common enumeration units. This mean we can name the specific geography we want data from. For example, let’s get data for Native population the different counties in Alaska.\n\nalaska_native &lt;- get_decennial(\n  geography = \"county\",\n  state = \"AK\",\n  county = c(\"Anchorage\", \"Bristol Bay\", \"Juneau\", \"Bethel\"),\n  variables = \"P2_007N\",\n  year = 2020)\n\nTo learn more about the arguments for geography for each core function of tidycensus, check out the documentation here.\n\n14.3.2.1 Quering for multiple variables\nThe varaible argument can take a vector of variables as an input, allowing to query for more than one variable at the time. We can create a vector and then call that vector as the input of the variable argument, or we can use the concatenate function c() and request data from multiple variables.\n\n## Vector with race variables codes\nrace_vars &lt;- c(\n  Hispanic = \"P2_002N\",\n  White = \"P2_005N\",\n  Black = \"P2_006N\",\n  Native = \"P2_007N\",\n  Asian = \"P2_008N\",\n  HIPI = \"P2_009N\") ## Native Hawaiian and other Pacific Islander\n\nalaska_race &lt;- get_decennial(\n  geography = \"county\",\n  state = \"AK\",\n  county = c(\"Anchorage\", \"Bristol Bay\", \"Juneau\", \"Bethel\"),\n  variables = race_vars,\n  summary_var = \"P2_001N\",\n  year = 2020)\n\nNote how this table returns the variable name we assigned in the vector above instead of the variable code! This is a handy option, given that the codes can be hard to remember what is what.\nAlso, note that we added one more argument to our request summary_var = \"P2_001N. This adds one more column to our output data frame, one with the summary variable value.\nIn every table you can generally find a variable that is an appropriate denominators for a group of variables. Following the example above, the P2 table, which provides population by race with Hispanic origin, the variable “P2001N” represents the total population. Because this variable is an an appropriate denominator for the other variables in the table, it helps to have it in a different column to make it easier to calculate proportions or percentage.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>U.S Census Data in R</span>"
    ]
  },
  {
    "objectID": "session_14.html#getting-census-data-ready-for-analysis",
    "href": "session_14.html#getting-census-data-ready-for-analysis",
    "title": "14  U.S Census Data in R",
    "section": "14.4 Getting Census Data ready for analysis",
    "text": "14.4 Getting Census Data ready for analysis\nOnce we access the data we want, we can apply our data wrangling skills to get the data in the format that we want.\nLet’s demonstrate this with an example. Let’s compare the distribution of percentage White population and percentage Native population by census track infour Alaska counties.\nThe first step is to get the data.\n\n\n\n\n\n\nExercise 1: get_decennial()\n\n\n\n\nMake a query to get White and Native population data for 3 Alaska counties by tract from the 2020 Decennial Census. Include the total population summary variable (summary_var = \"P2_001N\").\n\nHint: variable codes are:\n\nTotal Native population = P2_007N\nTotal White population = P2_005N\n\n\n\nAnswer\nalaska_tract_nw &lt;- get_decennial(\n  geography = \"tract\",\n  variables = c(native = \"P2_007N\",\n                white = \"P2_005N\"),\n  summary_var = \"P2_001N\",\n  state = \"AK\",\n  county = c(\"Anchorage\", \"Bristol Bay\", \"Juneau\", \"Bethel\"),\n  year = 2020)\n\n\nWe can check our data by calling the View(alaska_tract_nw) function in the console.\n\nNow that we have our data, next thing we will do is calculate the percentage of White and Native population in each track. Given that we have the summary variable within our data set we can easily add a new column with the percentage. And then, we will also clean the NAMES column and separate track, county and state into it’s own column (hint: tidyr::separate()).\n\n\n\nAnswer\nalaska_tract_nw_clean &lt;- alaska_tract_nw %&gt;% \n    mutate(percent = 100 * (value / summary_value)) %&gt;% \n    separate(NAME, into = c(\"tract\", \"county\", \"state\"),\n           sep = \", \")\n\n\nNote that we can apply all other dplyr functions we have learned to this dataset depending on what we want to achieve. One of the main goals of tidycensus is to make the output data frames compatible with tidyverse functions.\n\nNow that we have or “clean” data, with all the variables we need. Let’s plot this data to compare the distribution of percentage White population and percentage Native population by census tract vary among Counties in Alaska.\n\n\n\nAnswer\nggplot(alaska_tract_nw_clean,\n       aes(x = county, y = value, fill = variable)) +\n    geom_bar(position = \"fill\", stat = \"identity\") +\n    scale_y_continuous(labels = scales::percent) +\n    scale_fill_manual(guide = guide_legend(reverse = TRUE),\n                      values = c(\"lightblue2\", \"gold2\")) +\n    labs(\n        title = \"Native/White Population\",\n        subtitle = \"Subset of 4 Alaska Counties\",\n        fill = \"Race\",\n        caption = \"Decennial Census 2020 | tidycensus R package\",\n        x = \"\",\n        y = \"\"\n    ) +\n    theme_minimal() +\n    coord_flip() +\n    theme(legend.position = \"top\")\n \n    \n## Another geom to check out (note: Bristol Bay has only one tract therefore is not plotted)\nggplot(alaska_tract_nw_clean, \n       aes(x = percent, fill = county)) + \n    geom_density(alpha = 0.5)+\n    facet_wrap(~variable)+\n    theme_light()",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>U.S Census Data in R</span>"
    ]
  },
  {
    "objectID": "session_14.html#notes-on-get_acs",
    "href": "session_14.html#notes-on-get_acs",
    "title": "14  U.S Census Data in R",
    "section": "14.5 Notes on get_acs()",
    "text": "14.5 Notes on get_acs()\n\nThe functions operates very similar to get_decennial()\nThe main differences is that is access a different survey so the options for each argument change.\nThe two required arguments are geography and variables. The function defaults to the 2017-2021 5-year ACS\n1-year ACS data are more current, but are only available for geographies of population 65,000 and greater\nAccess 1-year ACS data with the argument survey = \"acs1\"; defaults to “acs5”\nExample code to get median income for Alaska by county\n\n\n## 1-year survey\nmedian_income_1yr &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  state = \"AK\",\n  year = 2021,\n  survey = \"acs1\")\n\n## 5-year survey. Defaults to the 2017-2021 5-year ACS\nmedian_income_5yr &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  state = \"AK\")\n\n\nYou can access to different variables in a survey with the load_variables() function\n\n\n## variables for 5-year 2017-2021 ACS\nvars &lt;- load_variables(2021, \"acs5\")\n\n\n\n\n\n\n\nExercise 2: get_acs()\n\n\n\n\nLoad variables for “acs5”, year 2021.\n\n\n\nAnswer\nvars_acs5_21 &lt;- load_variables(2021, \"acs5\")\n\n\n\nFind code for total median gross rent.\nGet acs data for median gross rent by county in Alaska.\n\n\n\nAnswer\nak_rent &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B25031_001\",\n  state = \"AK\",\n  year = 2021)\n\n\n\nVisualize estimate by county using geom_point. (Hint: use y = reorder(NAME, estimate) to order counties by estimates)\n\n\n\nAnswer\nggplot(ak_rent, aes(x = estimate, y = reorder(NAME, estimate))) + \n  geom_point()\n\n\n\nAdd the following layer, one by one, to your plot. Discuss with your neighbor what each line of code is doing.\n\n\n\nAnswer\ngeom_errorbar(aes(xmin = estimate - moe, xmax = estimate + moe),\n                width = 0.5, linewidth = 0.5)\n\nscale_x_continuous(labels = label_dollar()) \n\n\n\nEnhance you plot adding a theme_*, changing the color of the points, renaming the labels, adding a title, or any other modification you want to make.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>U.S Census Data in R</span>"
    ]
  },
  {
    "objectID": "session_14.html#resources",
    "href": "session_14.html#resources",
    "title": "14  U.S Census Data in R",
    "section": "14.6 Resources",
    "text": "14.6 Resources\n\nLesson’s slides\nU.S Census Bureau\nAccess Canadian Census data with cancensus R package\n\n\n\n\n\nWalker, Kyle, and Herman Matt. 2021. “Tidycensus: Load US Census Boundary and Attribute Data as ’Tidyverse’ and ’Sf’ -Ready Data Frames.” https://github.com/walkerke/tidycensus.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>U.S Census Data in R</span>"
    ]
  },
  {
    "objectID": "session_15.html",
    "href": "session_15.html",
    "title": "15  Metadata Best Practices and Data Publishing",
    "section": "",
    "text": "15.1 Learning Objectives\nIn this lesson, you will learn:",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Metadata Best Practices and Data Publishing</span>"
    ]
  },
  {
    "objectID": "session_15.html#learning-objectives",
    "href": "session_15.html#learning-objectives",
    "title": "15  Metadata Best Practices and Data Publishing",
    "section": "",
    "text": "About open data archives, especially the Arctic Data Center\nWhat science metadata are and how they can be used\nHow data and code can be documented and published in open data archives\n\nWeb-based submission",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Metadata Best Practices and Data Publishing</span>"
    ]
  },
  {
    "objectID": "session_15.html#data-sharing-and-preservation",
    "href": "session_15.html#data-sharing-and-preservation",
    "title": "15  Metadata Best Practices and Data Publishing",
    "section": "15.2 Data sharing and preservation",
    "text": "15.2 Data sharing and preservation\n\n\n15.2.1 Data repositories: built for data (and code)\n\nGitHub is not an archival location\nExamples of dedicated data repositories: KNB, Arctic Data Center, tDAR, EDI, Zenodo\n\nRich metadata\nArchival in their mission\nCertification for repositories: https://www.coretrustseal.org/\n\nData papers, e.g., Scientific Data\nList of data repositories: http://re3data.org\n\nRepository finder tool: https://repositoryfinder.datacite.org/\n\n\n\n\n\n15.2.2 DataONE Federation\nDataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses.\nDataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the (currently 44!) repositories one at a time.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Metadata Best Practices and Data Publishing</span>"
    ]
  },
  {
    "objectID": "session_15.html#metadata",
    "href": "session_15.html#metadata",
    "title": "15  Metadata Best Practices and Data Publishing",
    "section": "15.3 Metadata",
    "text": "15.3 Metadata\nMetadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where they were collected, and why they were collected.\nFor consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the metadata for a sockeye salmon dataset:\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;eml:eml packageId=\"df35d.442.6\" system=\"knb\" \n    xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.1\"&gt;\n    &lt;dataset&gt;\n        &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through \n            Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt;\n        &lt;creator id=\"1385594069457\"&gt;\n            &lt;individualName&gt;\n                &lt;givenName&gt;Mark&lt;/givenName&gt;\n                &lt;surName&gt;Willette&lt;/surName&gt;\n            &lt;/individualName&gt;\n            &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt;\n            &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt;\n            &lt;address&gt;\n                &lt;city&gt;Soldotna&lt;/city&gt;\n                &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt;\n                &lt;country&gt;USA&lt;/country&gt;\n            &lt;/address&gt;\n            &lt;phone phonetype=\"voice\"&gt;(907)260-2911&lt;/phone&gt;\n            &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt;\n        &lt;/creator&gt;\n        ...\n    &lt;/dataset&gt;\n&lt;/eml:eml&gt;\nThat same metadata document can be converted to HTML format and displayed in a much more readable form on the web: https://knb.ecoinformatics.org/#view/doi:10.5063/F1F18WN4\n And, as you can see, the whole dataset or its components can be downloaded and reused.\nAlso note that the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data.\n\n15.3.1 Overall Guidelines\nAnother way to think about metadata is to answer the following questions with the documentation:\n\nWhat was measured?\nWho measured it?\nWhen was it measured?\nWhere was it measured?\nHow was it measured?\nHow is the data structured?\nWhy was the data collected?\nWho should get credit for this data (researcher AND funding agency)?\nHow can this data be reused (licensing)?\n\n\n\n15.3.2 Bibliographic Guidelines\nThe details that will help your data be cited correctly are:\n\nGlobal identifier like a digital object identifier (DOI)\nDescriptive title that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data\nDescriptive abstract that serves as a brief overview off the specific contents and purpose of the data package\nFunding information like the award number and the sponsor\nPeople and organizations like the creator of the dataset (i.e. who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset\n\n\n\n15.3.3 Discovery Guidelines\nThe details that will help your data be discovered correctly are:\n\nGeospatial coverage of the data, including the field and laboratory sampling locations, place names and precise coordinates\nTemporal coverage of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to\nTaxonomic coverage of the data, including what species were measured and what taxonomy standards and procedures were followed\nAny other contextual information as needed\n\n\n\n15.3.4 Interpretation Guidelines\nThe details that will help your data be interpreted correctly are:\n\nCollection methods for both field and laboratory data the full experimental and project design as well as how the data in the dataset fits into the overall project\nProcessing methods for both field and laboratory samples\nAll sample quality control procedures\nProvenance information to support your analysis and modelling methods\nInformation about the hardware and software used to process your data, including the make, model, and version\nComputing quality control procedures like testing or code review\n\n\n\n15.3.5 Data Structure and Contents\n\nEverything needs a description: the data model, the data objects (like tables, images, matrices, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation.\nVariable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (i.e. 0 = not collected), and any missing values (i.e. 999 = NA).\n\nNot only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable).\nFor example, if you were to search for the character string “carbon dioxide flux” in a data repository, not all relevant results will be shown due to varying vocabulary conventions (i.e., “CO2 flux” instead of “carbon dioxide flux”) across disciplines — only datasets containing the exact words “carbon dioxide flux” are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search.\n\n\n15.3.6 Rights and Attribution\nCorrectly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like:\n\nCitation format to be used when giving credit for the data\nAttribution expectations for the dataset\nReuse rights, which describe who may use the data and for what purpose\nRedistribution rights, which describe who may copy and redistribute the metadata and the data\nLegal terms and conditions like how the data are licensed for reuse.\n\n\n\n15.3.7 Metadata Standards\nSo, how does a computer organize all this information? There are a number of metadata standards that make your metadata machine readable and therefore easier for data curators to publish your data.\n\nEcological Metadata Language (EML)\nGeospatial Metadata Standards (ISO 19115 and ISO 19139)\n\nSee NOAA’s ISO Workbook\n\nDublin Core\nDarwin Core\nPREservation Metadata: Implementation Strategies (PREMIS)\nMetadata Encoding Transmission Standard (METS)\n\nNote this is not an exhaustive list.\n\n\n15.3.8 Data Identifiers\nMany journals require a DOI (a digital object identifier) be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to.\nSome data repositories assign a DOI for each dataset you publish on their repository. But, if you need to update the datasets, check the policy of the data repository. Some repositories assign a new DOI after you update the dataset. If this is the case, researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available.\n\n\n15.3.9 Data Citation\nResearchers should get in the habit of citing the data that they use (even if it’s their own data!) in each publication that uses that data.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Metadata Best Practices and Data Publishing</span>"
    ]
  },
  {
    "objectID": "session_15.html#structure-of-a-data-package",
    "href": "session_15.html#structure-of-a-data-package",
    "title": "15  Metadata Best Practices and Data Publishing",
    "section": "15.4 Structure of a data package",
    "text": "15.4 Structure of a data package\nNote that the dataset above lists a collection of files that are contained within the dataset. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\n\nThese data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4,and each of the individual files have their own identifiers as well.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Metadata Best Practices and Data Publishing</span>"
    ]
  },
  {
    "objectID": "session_15.html#publishing-data-from-the-web",
    "href": "session_15.html#publishing-data-from-the-web",
    "title": "15  Metadata Best Practices and Data Publishing",
    "section": "15.5 Publishing data from the web",
    "text": "15.5 Publishing data from the web\nEach data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the Arctic Data Center, we provide some easy to use web forms for editing and submitting a data package. This section provides a brief overview of some highlights within the data submission process, in advance of a more comprehensive hands-on activity.\nORCiDs\nWe will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button.\n\n\nORCID is a non-profit organization made up of research institutions, funders, publishers and other stakeholders in the research space. ORCID stands for Open Researcher and Contributor ID. The purpose of ORCID is to give researchers a unique identifier which then helps highlight and give credit to researchers for their work. If you click on someone’s ORCID, their work and research contributions will show up (as long as the researcher used ORCID to publish or post their work).\nAfter signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata.\n\nSensitive Data Handling\nUnderneath the Title field, you will see a section titled “Data Sensitivity”. As the primary repository for the NSF Office of Polar Programs Arctic Section, the Arctic Data Center accepts data from all disciplines. This includes data from social science research that may include sensitive data, meaning data that contains personal or identifiable information. Sharing sensitive data can pose challenges to researchers, however sharing metadata or anonymized data contributes to discovery, supports open science principles, and helps reduce duplicate research efforts.\nTo help mitigate the challenges of sharing sensitive data, the Arctic Data Center has added new features to the data submission process influenced by the CARE Principles for Indigenous Data Governance (Collective benefit, Authority to control, Responsibility, Ethics). Researchers submitting data now have the option to choose between varying levels of sensitivity that best represent their dataset. Data submitters can select one of three sensitivity level data tags that best fit their data and/or metadata. Based on the level of sensitivity, guidelines for submission are provided. The data tags range from non-confidential information to maximally sensitive information.\nThe purpose of these tags is to ethically contribute to open science by making the richest set of data available for future research. The first tag, “non-sensitive data”, represents data that does not contain potentially harmful information, and can be submitted without further precaution. Data or metadata that is “sensitive with minimal risk” means that either the sensitive data has been anonymized and shared with consent, or that publishing it will not cause any harm. The third option, “some or all data is sensitive with significant risk” represents data that contains potentially harmful or identifiable information, and the data submitter will be asked to hold off submitting the data until further notice. In the case where sharing anonymized sensitive data is not possible due to ethical considerations, sharing anonymized metadata still aligns with FAIR (Findable, Accessible, Interoperable, Reproducible) principles because it increases the visibility of the research which helps reduce duplicate research efforts. Hence, it is important to share metadata, and to publish or share sensitive data only when consent from participants is given, in alignment with the CARE principles and any IRB requirements.\nYou will continue to be prompted to enter information about your research, and in doing so, create your metadata record. We recommend taking your time because the richer your metadata is, the more easily reproducible and usable your data and research will be for both your future self and other researchers. Detailed instructions are provided below for the hands-on activity.\nResearch Methods\nMethods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software.\n\nAs part of a recent update, researchers are now asked to describe the ethical data practices used throughout their research. The information provided will be visible as part of the metadata record. This feature was added to the data submission process to encourage transparency in data ethics. Transparency in data ethics is a vital part of open science and sharing ethical practices encourages deeper discussion about data reuse and ethics.\nWe encourage you to think about the ethical data and research practices that were utilized during your research, even if they don’t seem obvious at first.\nFile and Variable Level Metadata\nIn addition to providing information about, (or a description of) your dataset, you can also provide information about each file and the variables within the file. By clicking the “Describe” button you can add comprehensive information about each of your measurements, such as the name, measurement type, standard units etc.\n\nProvenance\nThe data submission system also provides the opportunity for you to provide provenance information, describe the relationship between package elements. When viewing your dataset followinng submission, After completing your data description and submitting your dataset you will see the option to add source data and code, and derived data and code.\n\nThese are just some of the features and functionality of the Arctic Data Center submission system and we will go through them in more detail below as part of a hands-on activity.\n\n15.5.1 Download the data to be used for the tutorial\nI’ve already uploaded the test data package, and so you can access the data here:\n\nhttps://demo.arcticdata.io/view/urn%3Auuid%3A98c799ef-d7c9-4658-b432-4d486221fca3\n\nGrab both CSV files, and the R script, and store them in a convenient folder.\n\n\n15.5.2 Login via ORCID\nWe will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button.\n When you sign in, you will be redirected to orcid.org, where you can either provide your existing ORCID credentials or create a new account. ORCID provides multiple ways to login, including using your email address, an institutional login from many universities, and/or a login from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login.\n\n\n\n15.5.3 Create and submit the dataset\nAfter signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata.\n\n15.5.3.1 Click Add Files to choose the data files for your package\nYou can select multiple files at a time to efficiently upload many files.\n\nThe files will upload showing a progress indicator. You can continue editing metadata while they upload.\n\n\n\n15.5.3.2 Enter Overview information\nThis includes a descriptive title, abstract, and keywords.\n  You also must enter a funding award number and choose a license. The funding field will search for an NSF award identifier based on words in its title or the number itself. The licensing options are CC-0 and CC-BY, which both allow your data to be downloaded and re-used by other researchers.\n\nCC-0 Public Domain Dedication: “…can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.”\nCC-BY: Attribution 4.0 International License: “…free to…copy,…redistribute,…remix, transform, and build upon the material for any purpose, even commercially,…[but] must give appropriate credit, provide a link to the license, and indicate if changes were made.”\n\n\n\n\n15.5.3.3 People Information\nInformation about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people:\n\nCreators - all the people who should be in the citation for the dataset\nContacts - one is required, but defaults to the first Creator if omitted\nPrincipal Investigators\nAny others that are relevant\n\nFor each, please provide their ORCID identifier, which helps link this dataset to their other scholarly works.\n\n\n\n15.5.3.4 Location Information\nThe geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that spatial search will be more precise.\n\nNote that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global:\n\nMission Canyon, Santa Barbara, California, USA\n\n\n\n\n15.5.3.5 Temporal Information\nAdd the temporal coverage of the data, which represents the time period to which data apply. Again, use multiple date ranges if your sampling was discontinuous.\n\n\n\n15.5.3.6 Methods\nMethods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data.\n\n\n\n15.5.3.7 Save a first version with Submit\nWhen finished, click the Submit Dataset button at the bottom.\nIf there are errors or missing fields, they will be highlighted.\nCorrect those, and then try submitting again. When you are successful, you should see a large green banner with a link to the current dataset view. Click the X to close that banner if you want to continue editing metadata.\n Success!\n\n\n\n15.5.4 File and variable level metadata\nThe final major section of metadata concerns the structure and content of your data files. In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure.\nFor example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information.\n The Attributes tab is where you enter variable (aka attribute) information, including:\n\nvariable name (for programs)\nvariable label (for display)\n\n - variable definition (be specific) - type of measurement  - units & code definitions\n You’ll need to add these definitions for every variable (column) in the file. When done, click Done. Now, the list of data files will show a green checkbox indicating that you have fully described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes.\n After you get the big green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes.\n\n\n15.5.5 Add workflow provenance\nUnderstanding the relationships between files (aka provenance) in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, which are often then used in analysis and visualization code to produce final outputs. In the DataONE network, we support structured descriptions of these relationships, so researchers can see the flow of data from raw data to derived to outputs.\nYou add provenance by navigating to the data table descriptions and selecting the Add buttons to link the data and scripts that were used in your computational workflow. On the left side, select the Add circle to add an input data source to the events.csv file. This starts building the provenance graph to explain the origin and history of each data object.\n The linkage to the source dataset should appear.\n\nThen you can add the link to the file that was the source for the derived data files by clicking on the Add arrow and selecting the file:\n   The diagram now shows the relationships among the data files, so click Submit to save another version of the package.\n Et voilà! A beautifully preserved data package!",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Metadata Best Practices and Data Publishing</span>"
    ]
  },
  {
    "objectID": "session_15.html#exercise-evaluate-a-data-package-on-the-adc-repository",
    "href": "session_15.html#exercise-evaluate-a-data-package-on-the-adc-repository",
    "title": "15  Metadata Best Practices and Data Publishing",
    "section": "15.6 Exercise: Evaluate a Data Package on the ADC Repository",
    "text": "15.6 Exercise: Evaluate a Data Package on the ADC Repository\nExplore data packages published on the ADC assess the quality of their metadata. Imagine you’re a data curator!\n\n\n\n\n\n\nSetup\n\n\n\n\nBreak into groups and use the following data packages:\n\nGroup A: ADC Data Portal White spruce (Picea glauca) densities at Brooks Range treelines, Alaska (2019-2022)\nGroup B: ADC Data Portal A 2022 household survey about impacts and human response to climate-related multi-hazards in Anchorage, Fairbanks, and Whitehorse\nGroup C: ADC Data Portal Summertime water quality measurements at beaver ponds and associated locations in Arctic Alaska, 2022-2026\n\n\n\n\nYou and your group will evaluate a data package for its: (1) metadata quality, (2) data documentation quality for reproducibility, and (3) FAIRness and CAREness.\n\n\n\n\n\n\nExercise: Evaluate a data package on the ADC Data Portal\n\n\n\n\nView our Data Package Assessment Rubric and make a copy of it to:\n\nInvestigate the metadata in the provided data\n\nDoes the metadata meet the standards we talked about? How so?\nIf not, how would you improve the metadata based on the standards we talked about?\n\nInvestigate the overall data documentation in the data package\n\nIs the documentation sufficient enough for reproducibility? Why or why not?\nIf not, how would you improve the data documentation? What’s missing?\n\nIdentify elements of FAIR and CARE\n\nIs it clear that the data package used a FAIR and CARE lens?\nIf not, what documentation or considerations would you add?\n\n\nElect someone to share back to the group the following:\n\nHow easy or challenging was it to find the metadata and other data documentation you were evaluating? Why or why not?\nWhat documentation stood out to you? What did you like or not like about it?\nHow well did these data packages uphold FAIR and CARE Principles?\nDo you feel like you understand the research project enough to use the data yourself (aka reproducibility?",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Metadata Best Practices and Data Publishing</span>"
    ]
  },
  {
    "objectID": "session_16.html",
    "href": "session_16.html",
    "title": "16  Creating Data Portals",
    "section": "",
    "text": "16.1 What is a Portal?\nData portals are a new feature on the Arctic Data Center. Researchers can now easily view project information and datasets all in one place.\nA portal is a collection of Arctic Data Center data packages on a unique webpage.\nTypically, a research project’s website won’t be maintained beyond the life of the project, and all the information on the website that provides context for the data collection is lost. Arctic Data Center portals can provide a means to preserve information regarding the projects’ objectives, scopes, and organization and couple this with the data files so it’s clear how to use and interpret the data for years to come.\nPortals also leverage Arctic Data Center’s metric features, which create statistics describing the project’s data packages. Information such as total size of data, proportion of data file types, and data collection periods are immediately available from the portal webpage.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Creating Data Portals</span>"
    ]
  },
  {
    "objectID": "session_16.html#portal-uses",
    "href": "session_16.html#portal-uses",
    "title": "16  Creating Data Portals",
    "section": "16.2 Portal Uses",
    "text": "16.2 Portal Uses\nPortals allow users to bundle supplementary information about their group, data, or project along with the data packages. Data contributors can organize their project-specific data packages into a unique portal and customize the portal’s theme and structure according to the needs of that project.\nResearchers can also use portals to compare their public data packages and highlight or share them with other teams, as well as the broader Arctic research audience.\nTo see an example of a portal, please view the Toolik Field Station’s portal.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Creating Data Portals</span>"
    ]
  },
  {
    "objectID": "session_16.html#portal-features",
    "href": "session_16.html#portal-features",
    "title": "16  Creating Data Portals",
    "section": "16.3 Portal Features",
    "text": "16.3 Portal Features\nPublished portals vary in their design according to the needs and preferences of the individual or group. However, when constructing a portal there are three core elements: a data page, a metrics page, and customizable free-form pages.\n\nFlexible Content Creation\nPortals can be constructed as a website providing information about the research and products. Using our flexible ‘free-form’ pages (written in markdown), you can add and re-order pages to meet your needs. These pages might be used as a home, or ‘landing’ page with general project information. They are also used to showcase research products, and communicate news and upcoming events.\n\n\n\n\nCurated Collections of Data\nThe data page is arguably the most important component of the Arctic Data Center portal system. This is where users will display the data packages of their choice. Whether these data reflect research products from your research group (collated based on contributor IDs), or thematic areas of research interest (collated based on keyword and search terms), will depend upon the intended use of the portal but in all cases, you are refining the suite of data viewed by your audience. The data page looks and performs just like the main Arctic Data Center catalog - with some added bonuses, see below.\n\n\n\nCustomized Search Capabilities\nYou can also build more precise search capabilities into your portal, leveraging the rich metadata associated with data products preserved at the Arctic Data Center. For example, in the example below the research group have identified seven primary search categories and within these, enable users to search within specific metadata fields or to select from a drop down list of options. In doing so, your audience can construct refined search queries drilling down to the data of interest. (Note that although the SASAP portal is hosted by DataONE, the same functionality exists at the Arctic Data Center. More on this later).\n\n\n\n\nMetrics, Metrics, Metrics\nAs with the full Arctic Data Center catalog, we aggregate metrics for the collection of data packages within a portal. This page is not customizable - it comes as a default with the portal - but you can choose to delete it. The metrics provided include a summary of the holdings: number, volume, time period, format of datasets, metadata assessment scores, citations across all packages, and counts of downloads and views. These latter metrics can be particularly useful if wanting to track the usage or reach of your project or group’s activities.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Creating Data Portals</span>"
    ]
  },
  {
    "objectID": "session_16.html#enhancing-access-to-social-science-research-data",
    "href": "session_16.html#enhancing-access-to-social-science-research-data",
    "title": "16  Creating Data Portals",
    "section": "16.4 Enhancing Access to Social Science Research Data",
    "text": "16.4 Enhancing Access to Social Science Research Data\nMany of the portal examples provided above are organizational, individual or project portals created by members of the Arctic research community. The ability to group relevant datasets and customize search criteria increases data discoverability and accessibility among target audiences. The Arctic Data Center has leveraged these features to create a specific portal for social science data.\n\nWithin this portal, users can subset by social science discipline, data type and other metadata fields built into the portal. These search features depend on sufficient user contributed metadata describing the dataset, and as can be seen from the ‘with data’ toggle, the portal does not require the data themselves to be uploaded to the Arctic Data Center.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Creating Data Portals</span>"
    ]
  },
  {
    "objectID": "session_16.html#relationship-between-arctic-data-center-portals-and-dataone",
    "href": "session_16.html#relationship-between-arctic-data-center-portals-and-dataone",
    "title": "16  Creating Data Portals",
    "section": "16.5 Relationship between Arctic Data Center portals and DataONE",
    "text": "16.5 Relationship between Arctic Data Center portals and DataONE\nBoth DataONE and the Arctic Data Center use Metacat and MetacatUI software and both have the capability for individuals and groups to develop portals. This is true of other repositories running this software. The difference between a portal at the Arctic Data Center and one through DataONE is the corpus of data that can be pulled into your portal. Arctic Data Center portals expose only data held in the Arctic Data Center repository. A portal in DataONE can expose data from across the full network of data repositories - including DataONE. This is particularly useful for interdisciplinary research projects, labs that have published data to multiple repositories etc. However, unlike at the Arctic Data Center, there is a cost associated with a DataONE portal as they are part of the organizations sustainability model.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Creating Data Portals</span>"
    ]
  },
  {
    "objectID": "session_16.html#creating-portals",
    "href": "session_16.html#creating-portals",
    "title": "16  Creating Data Portals",
    "section": "16.6 Creating Portals",
    "text": "16.6 Creating Portals\nA step-by-step guide on how to navigate the Arctic Data Center and create a new portal. For video tutorials on how to create your first portal, please visit the Arctic Data Center’s website.\n\nGetting Started with Portals\nIf you are on the Arctic Data Center’s primary website, select the button on the top right titled ‘Create Portal’. This will take you to sign in with your ORCID id if you are not already signed in. Sign in with your ORCID, which will then take you directly to the page where you can start customizing your portal.\n\nYou can also get to the page to create a new portal by clicking on your name in the upper right hand corner when you are signed in to the Arctic Data Center with your ORCID. A dropdown will appear, and you would select ‘My Portals’.\n\nOn your profile settings page, select ‘My Portals’. After the page loads, select the green button ‘+ New Portal’ to add a new portal. You’ll automatically be directed to a fresh edit session.\n\n\n\nPortal Settings Page\nIn a new edit session, the first page you’ll be taken to is the settings page where you’ll be able to add details about your portal.\n\n\nPortal URL\n\nIdentify a short name for your portal that will become part of the URL. If the name is available, a label will indicate it’s available and if the name is taken already, it will note that the name is already taken. This feature ensures the portals are unique.\n\nPortal description\nSharing options\n\nFor the purposes of this training, please leave your portal in ‘Private’ status. You are welcome to return and make the portal public when the portal is complete and is useful to you and others.\n\nPermissions\n\nAdding collaborators to help you create your portal is as straightforward as copying and pasting in their ORCID into the box below the permissions section. You can choose whether the collaborator can view, edit, or is an owner of the portal. You can have multiples of each role.\n\nPartner Logos\n\n\n\n\nAdding Data to Portals\nWhen selecting the data tab you will see a page with two sections. These are titled with instructive statements to help explain their function:\n\nAdd filters to help people find data within your collection\n\nbuild search features for others to use within your portal\n\nAdd data to your collection\n\nconstruct a search to populate your portal with data\n\n\n We’re going to start with the second section: Adding data to your collection.\nWhen adding data to your collection, you can include any of the datasets that are available at the Arctic Data Center. You build rules based on metadata to define which datasets should be included. Of course, where metadata is incomplete or abbreviated, this will impact your results. Data added to the network in the future that match these rules will also be added to your collection.\nThe first thing we notice is the ability to include/exclude data based on all/any metadata criteria. This setting applies across all rules.\nIn the default view, you have the starting point for a single rule, and a view showing 0 datasets.\n\nAs we build rules, the page will refresh to show how many datasets are included based on your rule structure. You can continue to add rules (and rule groups) to create complex queries that are specific to your needs.\n\nThe metadata fields available for rule construction are easily visible in the dropdown option, and grouped for ease of use. You also have the option to define ‘any metadata field’, though your results may be less precise.\n\n\n\nBuilding Search Options for Your Audience\nThis section covers the first part of the ‘data’ page: “Add filters to help people find data within your collection”. Although it appears first, I recommend constructing these filters after you have defined your data, as you will have a better understanding of the metadata fields that are relevant to your portal collection. It appears at the top of this editor page, as when published, it will be at the top for users. Hence, the editor page reflects the layout of the published page.\nWhen selecting “add a search filter” you will be presented with a pop-up that comprises three primary elements:\n\nThe metadata filed you will be querying\nThe way in which you want the user to interact with that metadata\nLanguage settings for the filter you are building\n\n\nAs you select options for number 1 - the metadata field, the pop-up will refresh to show only those relevant options. Save this filter to close the pop-up, return to the main editor and add another search filter.\n\n\n\nData Package Metrics\nAs stated above, the metrics page is a default function provided by the Arctic Data Center. This page cannot be edited and cannot be viewed while editing. Users do have the option to delete the page if they’d like. To delete the page, select the arrow next to the word ‘Metrics’ in the tab and choose ‘Delete’ from the dropdown list. You can always change your mind and add a metrics page with the ‘+’ tab.\nTo see metric summaries, navigate to your portal in view mode. See ‘Saving and Editing Portals’ for more information on how to view portals.\n\n\n\nCreating Unique Freeform Pages\nTo watch a tutorial on creating a new freeform page see this video:Creating a Freeform Text Page\nTo add a freeform page to a portal, select the ‘+’ tab next to the data and metric tabs and then choose the freeform option that appears on screen. A freeform page will then populate.\n\nEasily customize your banner with a unique image, title, and page description. To change the name of the tab, click on the arrow in the ‘Untitled’ tab and select ‘Rename’ from the dropdown list.\n\nBelow the banner, there is a markdown text box with some examples on how to use the markdown formatting directives to customize the text display. There is also a formatting header at the top to assist if you’re unfamiliar with markdown. As you write, toggle through the Edit and Preview modes in the markdown text box to make sure your information is displaying as intended. Portals are flexible and can accommodate as many additional freeform pages as needed.\nThe markdown header structure helps to generate the table of contents for the page.\nPlease see these additional resources for help with markdown:\n\nMarkdown reference\nTen minute tutorial\nFor a longer example where you can also preview the results, checkout the Showdown Live Editor\n\n\n\nSaving and Editing Portals\nBe sure to save your portal when you complete a page to ensure your progress is retained.\n\nWhenever a portal is saved, a dialogue box will pop up at the top of the page prompting users to view their private portal in view mode. You can choose to ignore this and continue editing.\n\nTo delete a page from your portal, select the arrow in the tab and choose ‘Delete’ from the dropdown.\n\nUsers can view and edit their portal from their ‘My Portals’ tab.\nFirst, click the arrow your name in the top-right corner to drop down your menu options. Then, select ‘My Portals’ from the dropdown underneath your name. See the section on Getting Started with Portals for more details.\n \nClick on the portal title to view it or select the edit button to make changes.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Creating Data Portals</span>"
    ]
  },
  {
    "objectID": "session_16.html#how-to-publish-portals",
    "href": "session_16.html#how-to-publish-portals",
    "title": "16  Creating Data Portals",
    "section": "16.7 How to Publish Portals",
    "text": "16.7 How to Publish Portals\nNew portals are automatically set to private and only visible to the portal creator. The portal will remain private until the owner decides to make it public.\nTo make your portal public, go into the settings of your portal. Under the description, you’ll see a new section called ‘Sharing Options’. You can toggle between your portal being private and your portal being public there.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Creating Data Portals</span>"
    ]
  },
  {
    "objectID": "session_16.html#sharing-portals",
    "href": "session_16.html#sharing-portals",
    "title": "16  Creating Data Portals",
    "section": "16.8 Sharing Portals",
    "text": "16.8 Sharing Portals\nIn order to share a published portal, users can simply direct recipients to their unique identifier. Portal identifiers are embedded into the Arctic Data Center’s portal URL: https://arcticdata.io/catalog/portals/portal-identifier\nTo view or edit your portal identifier, go into edit mode in your portal. In the settings page, your portal URL will be the first item on the page.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Creating Data Portals</span>"
    ]
  },
  {
    "objectID": "session_16.html#tutorial-videos",
    "href": "session_16.html#tutorial-videos",
    "title": "16  Creating Data Portals",
    "section": "16.9 Tutorial Videos",
    "text": "16.9 Tutorial Videos\nFor video tutorials on how to create your first portal, please visit the Arctic Data Center video tutorial page.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Creating Data Portals</span>"
    ]
  },
  {
    "objectID": "session_16.html#acknowledgements",
    "href": "session_16.html#acknowledgements",
    "title": "16  Creating Data Portals",
    "section": "16.10 Acknowledgements",
    "text": "16.10 Acknowledgements\nMuch of this documentation was composed by ESS-DIVE, which can be found here.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Creating Data Portals</span>"
    ]
  },
  {
    "objectID": "session_17.html",
    "href": "session_17.html",
    "title": "17  Reproducible Survey Workflow",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reproducible Survey Workflow</span>"
    ]
  },
  {
    "objectID": "session_17.html#learning-objectives",
    "href": "session_17.html#learning-objectives",
    "title": "17  Reproducible Survey Workflow",
    "section": "",
    "text": "Overview of survey tools\nGenerating a reproducible survey report with Qualtrics",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reproducible Survey Workflow</span>"
    ]
  },
  {
    "objectID": "session_17.html#introduction",
    "href": "session_17.html#introduction",
    "title": "17  Reproducible Survey Workflow",
    "section": "17.1 Introduction",
    "text": "17.1 Introduction\nSurveys and questionnaires are commonly used research methods within social science and other fields. For example, understanding regional and national population demographics, income, and education as part of the National Census activity, assessing audience perspectives on specific topics of research interest (e.g. the work by Tenopir and colleagues on Data Sharing by Scientists), evaluation of learning deliverable and outcomes, and consumer feedback on new and upcoming products. These are distinct from the use of the term survey within natural sciences, which might include geographical surveys (“the making of measurement in the field from which maps are drawn”), ecological surveys (“the process whereby a proposed development site is assess to establish any environmental impact the development may have”) or biodiversity surveys (“provide detailed information about biodiversity and community structure”) among others.\nAlthough surveys can be conducted on paper or verbally, here we focus on surveys done via software tools. Needs will vary according to the nature of the research being undertaken. However, there is fundamental functionality that survey software should provide including:\n\nThe ability to create and customize questions\nThe ability to include different types of questions\nThe ability to distribute the survey and manage response collection\nThe ability to collect, summarize, and (securely) store response data\n\nMore advanced features can include:\n\nVisual design and templates - custom design might include institutional branding or aesthetic elements. Templates allow you to save these designs and apply to other surveys\nQuestion piping - piping inserts answers from previous questions into upcoming questions and can personalize the survey experience for users\nSurvey logic - with question logic and skip logic you can control the inclusion / exclusion of questions based on previous responses\nRandomization - the ability to randomize the presentation of questions within (blocks of) the survey\nBranching - this allows for different users to take different paths through the survey. Similar to question logic but at a bigger scale\nLanguage support - automated translation or multi-language presentation support\nShared administration - enables collaboration on the survey and response analysis\nSurvey export - ability to download (export) the survey instrument\nReports - survey response visualization and reporting tools\nInstitutional IRB approved - institutional IRB policy may require certain software be used for research purposes\n\nCommonly used survey software within academic (vs market) research include Qualtrics, Survey Monkey and Google Forms. Both Qualtrics and Survey Monkey are licensed (with limited functionality available at no cost) and Google forms is free.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reproducible Survey Workflow</span>"
    ]
  },
  {
    "objectID": "session_17.html#building-workflows-using-qualtrics",
    "href": "session_17.html#building-workflows-using-qualtrics",
    "title": "17  Reproducible Survey Workflow",
    "section": "17.2 Building workflows using Qualtrics",
    "text": "17.2 Building workflows using Qualtrics\nIn this lesson we will use the qualtRics package to reproducible access some survey results set up for this course.\n\n17.2.1 Survey Instrument\nThe survey is very short, only four questions. The first question is on its own page and is a consent question, after a couple of short paragraphs describing what the survey is, it’s purpose, how long it will take to complete, and who is conducting it. This type of information is required if the survey is governed by an IRB, and the content will depend on the type of research being conducted. In this case, this survey is not for research purposes, and thus is not governed by IRB, but we still include this information as it conforms to the Belmont Principles. The Belmont Principles identify the basic ethical principles that should underlie research involving human subjects.\n\nThe three main questions of the survey have three types of responses: a multiple choice answer, a multiple choice answer which also includes an “other” write in option, and a free text answer. We’ll use the results of this survey, which was sent out to NCEAS staff to fill out, to learn about how to create a reproducible survey report.\n\n\n\n17.2.2 Working with qualtiRcs\n\n\n\n\n\n\nSet up\n\n\n\nFirst, open a new Quarto document and add a chunk to load the libraries we’ll need for this lesson:\n\n\nAnswer\nlibrary(qualtRics)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n\nNext, we need to set the API credentials. The qualtrics_api_credentials function creates environment variables to hold your Qualtrics account information. The function can either temporarily store this information for just this session, or it can modify the .Renviron file to set your API key and base URL so that you can access Qualtrics programmatically from any session.\nThe API key is as good as a password, so care should be taken to not share it publicly. For example, you would never want to save it in a script. The function below is the rare exception of code that should be run in the console and not saved. It works in a way that you only need to run it once, unless you are working on a new computer or your credentials changed. Note that in this book, we have not shared the actual API key, for the reasons outlined above. For the course, we will share the key via a file or by e-mail. Provide the key as a string to the api_key argument in the function below:\n\n\nAnswer\nkey_file &lt;- read_lines(\"/tmp/qualtrics-key.txt\")\nqualtrics_api_credentials(api_key = key_file[1], base_url = \"ucsb.co1.qualtrics.com\", install = FALSE, overwrite = FALSE)\n\n\n\n\n\n\n\n\n\n\nAside note\n\n\n\nThe .Renviron file is a special user controlled file that can create environment variables. Every time you open Rstudio, the variables in your environment file are loaded as…environment variables! Environment variables are named values that are accessible by your R process. They will not show up in your environment pane, but you can get a list of all of them using Sys.getenv(). Many are system defaults.\nTo view or edit your .Renviron file, you can use usethis::edit_r_environ().\n\n\nTo get a list of all the surveys in your Qualtrics instance, use the all_surveys function.\n\n\nAnswer\nsurveys &lt;- all_surveys()\nglimpse(surveys)\n\n\nThis function returns a list of surveys, in this case only one, and information about each, including an identifier and it’s name. We’ll need that identifier later, so let’s go ahead and extract it using base R from the data frame.\n\n\nAnswer\nid &lt;- surveys %&gt;%\n    filter(name == \"Survey for Data Science Training\") %&gt;%\n    pull(id)\n\n\nYou can retrieve a list of the questions the survey asked using the survey_questions function and the survey id.\n\n\nAnswer\nquestions &lt;- survey_questions(id)\nquestions\n\n\nThis returns a data.frame with one row per question with columns for question id, question name, question text, and whether the question was required. This is helpful to have as a reference for when you are looking at the full survey results.\nTo get the full survey results, run fetch_survey with the survey id.\n\n\nAnswer\nsurvey_results &lt;- fetch_survey(id)\nsurvey_results %&gt;% head(1) %&gt;% glimpse\n\n\nThe survey results table has tons of information in it, not all of which will be relevant depending on your survey. The table has identifying information for the respondents (eg: ResponseID, IPaddress, RecipientEmail, RecipientFirstName, etc), much of which will be empty for this survey since it is anonymous. It also has information about the process of taking the survey, such as the StartDate, EndDate, Progress, and Duration. Finally, there are the answers to the questions asked, with columns labeled according to the qname column in the questions table (eg: Q1, Q2, Q3). Depending on the type of question, some questions might have multiple columns associated with them. We’ll have a look at this more closely in a later example.\n\n17.2.2.1 Response metadata\nAs mentioned above, Qualtrics helpfully provides some metadata with each response. Let’s use some of this information now. First, you’ll noticed a column named Finished, containing TRUE or FALSE values. This indicates whether the respondent fully completed and submitted the survey. In many cases, you will want to filter out unfinished survey responses to avoid analyzing incomplete answers. Let’s go ahead and remove those now.\n\n\nAnswer\n# Report the frequency of finished vs unfinished responses\nsurvey_results %&gt;% count(Finished)\n\n# Remove unfinished responses\nsurvey_results &lt;- survey_results %&gt;% filter(Finished)\n\n\nSecond, you’ll noticed columns named LocationLongitude and LocationLatitude. These give an approximate best-guess location of the respondent based on IP address. This is usually accurate to a city level in the United States, but perhaps only to a country for many international respondents, and can be completely wrong in some cases (e.g., if the respondent is using a VPN). Nevertheless, it can be useful to give at least a rough sense of where your respondents are located. Let’s use leaflet to draw a quick map of our survey respondents.\n\n\nAnswer\nlibrary(leaflet)\nsurvey_results %&gt;%\n    leaflet %&gt;%\n    addTiles() %&gt;%\n    addMarkers(\n        lng = ~ LocationLongitude,\n        lat = ~ LocationLatitude,\n        popup = ~ Q2\n    )\n\n\n\n\n17.2.2.2 Question 2\nLet’s look at the responses to the second question in the survey, “How long have you been programming?” Remember, the first question was the consent question.\nWe’ll use the dplyr and tidyr tools we learned earlier to extract the information. Here are the steps:\n\nselect the column we want (Q2)\ngroup_by and summarize the values\n\n\n\nAnswer\nq2 &lt;- survey_results %&gt;% \n    select(Q2) %&gt;% \n    group_by(Q2) %&gt;% \n    summarise(n = n())\n\n\nWe can show these results in a table using the kable function from the knitr package:\n\n\nAnswer\nkable(q2, col.names = c(\"How long have you been programming?\",\n                        \"Number of responses\")) %&gt;%\n    kable_styling()\n\n\n\n\n17.2.2.3 Question 3\nFor question 3, we’ll use a similar workflow. For this question, however there are two columns containing survey answers. One contains the answers from the controlled vocabulary, the other contains any free text answers users entered.\nTo present this information, we’ll first show the results of the controlled answers as a plot. Below the plot, we’ll include a table showing all of the free text answers for the “other” option.\n\n\nAnswer\nq3 &lt;- survey_results %&gt;% \n    select(Q3) %&gt;% \n    group_by(Q3) %&gt;% \n    summarise(n = n())\n\n\n\n\nAnswer\nggplot(data = q3, \n       mapping = aes(x = Q3, y = n)) +\n    geom_col() +\n    labs(x = \"What language do you currently use most frequently?\", y = \"Number of reponses\") +\n    theme_minimal()\n\n\nNow we’ll extract the free text responses:\n\n\nAnswer\nq3_text &lt;- survey_results %&gt;% \n    select(Q3_7_TEXT) %&gt;% \n    drop_na()\n\nkable(q3_text, col.names = c(\"Other responses to 'What language do you currently use mose frequently?'\")) %&gt;% \n    kable_styling()\n\n\n\n\n17.2.2.4 Question 4\nThe last question is just a free text question, so we can just display the results as is.\n\n\nAnswer\nq4 &lt;- survey_results %&gt;% \n    select(Q4) %&gt;% \n    rename(`What data science tool or language are you most excited to learn next?` = Q4) %&gt;% \n    drop_na()\n\nkable(q4, col.names = \"What data science tool or language are you most excited to learn next?\") %&gt;% \n    kable_styling()",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reproducible Survey Workflow</span>"
    ]
  },
  {
    "objectID": "session_17.html#other-survey-tools",
    "href": "session_17.html#other-survey-tools",
    "title": "17  Reproducible Survey Workflow",
    "section": "17.3 Other survey tools",
    "text": "17.3 Other survey tools\n\n17.3.1 Google forms\nGoogle forms can be a great way to set up surveys, and it is very easy to interact with the results using R. The benefits of using google forms are a simple interface and easy sharing between collaborators, especially when writing the survey instrument.\nThe downside is that google forms has far fewer features than Qualtrics in terms of survey flow and appearance.\nTo show how we can link R into our survey workflows, I’ve set up a simple example survey here.\nI’ve set up the results so that they are in a new spreadsheet here:. To access them, we will use the googlesheets4 package.\nFirst, open up a new Quarto doc and load the googlesheets4 library:\n\n\nAnswer\nlibrary(googlesheets4)\n\n\nNext, we can read the sheet in using the same URL that you would use to share the sheet with someone else. Right now, this sheet is public\n\n\nAnswer\nresponses &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1CSG__ejXQNZdwXc1QK8dKouxphP520bjUOnZ5SzOVP8/edit?usp=sharing\")\n\n\nThe first time you run this, you should get a popup window in your web browser asking you to confirm that you want to provide access to your google sheets via the tidyverse (googlesheets) package.\nMy dialog box looked like this:\n\nMake sure you click the third check box enabling the Tidyverse API to see, edit, create, and delete your sheets. Note that you will have to tell it to do any of these actions via the R code you write.\nWhen you come back to your R environment, you should have a data frame containing the data in your sheet! Let’s take a quick look at the structure of that sheet.\n\n\nAnswer\ndplyr::glimpse(responses)\n\n\nSo, now that we have the data in a standard R data.frame, we can easily summarize it and plot results. By default, the column names in the sheet are the long fully descriptive questions that were asked, which can be hard to type. We can save those questions into a vector for later reference, like when we want to use the question text for plot titles.\n\n\nAnswer\nquestions &lt;- colnames(responses)[2:5]\ndplyr::glimpse(questions)\n\n\nWe can make the responses data frame more compact by renaming the columns of the vector with short numbered names of the form Q1. Note that, by using a sequence, this should work for sheets from just a few columns to many hundreds of columns, and provides a consistent question naming convention.\n\n\nAnswer\nnames(questions) &lt;- paste0(\"Q\", seq(1,4))\n\nquestions\n\ncolnames(responses) &lt;- c(\"Timestamp\", names(questions))\ndplyr::glimpse(responses)\n\n\nNow that we’ve renamed our columns, let’s summarize the responses for the first question. We can use the same pattern that we usually do to split the data from Q1 into groups, then summarize it by counting the number of records in each group, and then merge the count of each group back together into a summarized data frame. We can then plot the Q1 results using ggplot:\n\n\nAnswer\nq1 &lt;- responses %&gt;% \n    dplyr::select(Q1) %&gt;% \n    dplyr::group_by(Q1) %&gt;% \n    dplyr::summarise(n = dplyr::n())\n\nggplot2::ggplot(data = q1, mapping = aes(x = Q1, y = n)) +\n    geom_col() +\n    labs(x = questions[1], \n         y = \"Number of reponses\",\n         title = \"To what degree did the course meet expectations?\") +\n    theme_minimal()\n\n\n\nBypassing authentication for public sheets\nIf you don’t want to go through a little interactive dialog every time you read in a sheet, and your sheet is public, you can run the function gs4_deauth() to access the sheet as a public user. This is helpful for cases when you want to run your code non-interactively. This is actually how I set it up for this book to build!\n\n\n\n\n17.3.2 Survey Monkey\nSimilar to Qualtrics and qualtRics, there is an open source R package for working with data in Survey Monkey called svmkR. This package provides a suite of tools to work with Survey Monkey surveys. Note that this package is only available to install through GitHub. This is great, it makes is open source and easy to use. However, it is important tot make sure that the package is being maintain. In this case if we look at the GitHub repository with this package we can see that last updates were done somewhat recent. We are not going to cover this package here but fell free to explore and try it out if you use Survey Monkey as a platform to collect data. We hope that the principles describe in this lesson help you navigate how to access the data (in a reproducible way!).",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reproducible Survey Workflow</span>"
    ]
  },
  {
    "objectID": "session_17.html#resourcese",
    "href": "session_17.html#resourcese",
    "title": "17  Reproducible Survey Workflow",
    "section": "17.4 Resourcese",
    "text": "17.4 Resourcese\n\nHow to get your Qualtrics API key",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reproducible Survey Workflow</span>"
    ]
  },
  {
    "objectID": "session_18.html",
    "href": "session_18.html",
    "title": "18  Provenance and Reproducibility",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Provenance and Reproducibility</span>"
    ]
  },
  {
    "objectID": "session_18.html#learning-objectives",
    "href": "session_18.html#learning-objectives",
    "title": "18  Provenance and Reproducibility",
    "section": "",
    "text": "Discuss the concept of reproducible workflows including computational reproducibility and provenance metadata\nLearn how to use R to package your work by building a reproducible paper in RMarkdown/Quarto\nIntroduce tools and techniques for reproducibility supported by the NCEAS and DataONE\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn this lesson, we will be leveraging RMarkdown instead of Quarto so that we can use a very cool R package called rticles. Quarto has the same functionality as RMarkdown with rticles - making journal formatted articles from a code notebook - but it is done from the command line without additional R packages. See the Quarto documentation for details\n\n\n\n18.0.1 Reproducible Research: Recap\nWorking in a reproducible manner:\n\nIncreases research efficiency, accelerating the pace of your research and collaborations.\nProvides transparency by capturing and communicating scientific workflows.\nEnables research to stand on the shoulders of giants (build on work that came before).\nAllows credit for secondary usage and supports easy attribution.\nIncreases trust in science.\n\nTo enable others to fully interpret, reproduce or build upon our research, we need to provide more comprehensive information than is typically included in a figure or publication. The methods sections of papers are typically inadequate to fully reproduce the work described in the paper.\n\nFor example, the figure above conveys multiple messages. But, by looking at the figure we don’t get the full story of the process the scientist used to make this plot. What data were used in this study? What methods were applied? What were the parameter settings? What documentation or code are available to us to evaluate the results? Can we trust these data and methods? Are the results reproducible?\nComputational reproducibility is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions.\nPractically speaking, reproducibility includes:\n\nPreserving the data\nPreserving the software workflow\nDocumenting what you did\nDescribing how to interpret it all\n\nA recent study of publicly-available datasets in the Harvard Database repository containing R files found that only 26% of R files ran without error in the initial execution. 44% were able to be run after code cleaning, showing the importance of good programming practice (Trisovic et al. 2022). The figure below from Trisovic et al. shows a sankey diagram of how code cleaning was able to fix common errors.\n\n\n\n18.0.2 Computational Provenance and Workflows\nComputational provenance refers to the origin and processing history of data including:\n\nInput data\nWorkflow/scripts\nOutput data\nFigures\nMethods, dataflow, and dependencies\n\nWhen we put these all together with formal documentation, we create a computational workflow that captures all of the steps from initial data cleaning and integration, through analysis, modeling, and visualization. In other words, computational provenance is a formalized description of a workflow from the origin of the data to its final outcome.\nHere’s an example of a computational workflow from Mark Carls: Mark Carls. Analysis of hydrocarbons following the Exxon Valdez oil spill, Gulf of Alaska, 1989 - 2014. Gulf of Alaska Data Portal. urn:uuid:3249ada0-afe3-4dd6-875e-0f7928a4c171., that represents a three step workflow comprising four source data files and two output visualizations.\n\n\nThis image is a screenshot of an interactive user interface of a workflow built by DataONE. You can clearly see which data files were inputs to the process, the scripts that are used to process and visualize the data, and the final output objects that are produced, in this case two graphical maps of Prince William Sound in Alaska.\n\n\n18.0.3 From Provenance to Reproducibility\n\nDataONE provides a tool to track and visualize provenance. It facilitates reproducible science through provenance by:\n\nTracking data derivation history\nTracking data inputs and outputs of analyses\nPreserving and documenting software workflows\nTracking analysis and model executions\nLinking all of these to publications\n\n\nOne way to illustrate this is to look into the structure of a data package. A data package is the unit of publication of your data, including datasets, metadata, software and provenance. The image below represents a data package and all its components and how these components relate to one another.\n\n\n\n\n18.0.4 Data Citation and Transitive Credit\nWe want to move towards a model such that when a user cites a research publication we will also know:\n\nWhich data produced it\nWhat software produced it\nWhat was derived from it\nWho to credit down the attribution stack\n\n\nThis is transitive credit. And it changes the way in which we think about science communication and traditional publications.\n\n\n18.0.5 Reproducible Papers with rrtools\nA great overview of this approach to reproducible papers comes from:\n\nBen Marwick, Carl Boettiger & Lincoln Mullen (2018) Packaging Data Analytical Work Reproducibly Using R (and Friends), The American Statistician, 72:1, 80-88, doi:10.1080/00031305.2017.1375986\n\nThe key idea in Marwick et al. (2018) is that of the research compendium: A single container for not just the journal article associated with your research but also the underlying analysis, data, and even the required software environment required to reproduce your work.\nResearch compendium makes it easy for researchers to do their work but also for others to inspect or even reproduce the work because all necessary materials are readily at hand due to being kept in one place. Rather than a constrained set of rules, the research compendium is a scaffold upon which to conduct reproducible research using open science tools such as:\n\nR\nRMarkdown\nQuarto\ngit and GitHub\n\nFortunately for us, Ben Marwick (and others) have written an R package called rrtools that helps us create a research compendium from scratch.\n\n\n\n\n\n\nSet up\n\n\n\nTo start a reproducible paper with rrtools:\n\nClose your username-training project. Go to the project switcher dropdown, just click “close project.” This will set your working directory back to your home directory.\nIn console run the following line of code\n\n\n## \"mypaper\" is the name of the Rproj with my research compendia\nrrtools::use_compendium(\"mypaper\")\n\nrrtools has created the beginnings of a research compendium for us. The structure of this compendium is similar to the one needed to built an R package. That’s because it uses the same underlying folder structure and metadata and therefore it technically is an R package (called mypaper). And this means our research compendium could be easy to install in someone elses’ computer, similar to an R package.\n\nrrtools also helps you set up some key information:\n\n\nSet up a README file in the RMarkdown format\nCreate an analysis folder to hold our reproducible paper\n\n\nrrtools::use_readme_rmd()\nrrtools::use_analysis()\n\nThis creates a standard, predictable layout for our code and data and outputs that multiple people can understand. At this point, we’re technically ready to start writing the paper. But.. What about GitHub?\n\n\n\n18.0.5.1 Creating a git and GitHub repository with usethis\n\nusethis is a package that facilitates interactive workflows for R project creation and development. It automates repetitive tasks that arise during project setup and development.\n\nWe are going to use two functions to start tracking our work in git, create a remote repository in GitHub and be able to push and pull between the local version and the remote. To learn more about this package checkout the package documentation.\n\n\n\n\n\n\nSet up\n\n\n\n\nMake sure your are in “mypaper” Rproj.\nIn the Console run usethis::use_git() to create a local git repo. Choose yes to both questions when prompted (to commit files, and to restart R).\nThen, in the Console, run usethis::use_github() to create an upstream remote repo (in GitHub).\n\nAnd that’s it! Now your have your research compendium in your local computer and your changes are being tracked by git and your can pull and push to GitHub.\n\n\nLet’s explore the structure rrtools has put in place for us. Inside the analysis folder we have 5 folders. Different parts of our project will go into this different folders. Our data into the data folder, when the time comes to save any figure, we should save them into the figures folder, and so on.\n\n\n\nResearch compendia from Marwick et al.\n\n\nYou’ll notice a analysis/templates directory that contains journal citation style language (CSL) files which set the style of citations and reference list for the journal (the Journal of Archaeological Science, in this example). The template.Rmd renders into the template.docx. This document is called in the paper.qmd YAML to style the output of the paper created in paper.qmd.\nWhat if I want a template from another journal, different from the Journal of Archeological Science? We can create other journal’s template with the rticles package. This package will provide the templates and necessary information to render your paper in the journal of your choice (note: not all journal are in the rticles package). With that in mind, we will delete the existing paper directory and create a new one shortly.\n\n\n\n18.0.6 RMarkdown templates with rticles\nThe rticles package provides a lot of other great templates for formatting your paper specifically to the requirements of many journals. In addition to a custom CSL file for reference customization, rticles supports custom LATeX templates that fit the formatting requirements of each journals.\n\n\n\n\n\n\nTinytex and rendering to PDF\n\n\n\nTo be able to render your document to PDF you need to have tinytex installed in your machine.\nIn the console run:\n\ninstall.packages('tinytex') ## this package is already installed in our server\n\ntinytex::install_tinytex() ## this may take several minutes\n\n\n\n\n\n\n\n\n\nSet up\n\n\n\n\nIf you do not have rticle installed, go ahead and install calling the following function in the console: install.packages('rticles') Restart your RStudio session.\nTo create a new file from rticlescustom templates, got to File | New File | R Markdown... menu, which shows the following dialog:\n\n\n\nGo to “From Template” in the left side menu.\nSelect the “PNAS” template, give the file a name and set the location of the files to be mypaper/analysis, and click “OK”.\nYou can now Knit the Rmd file to see a highly-targeted article format, like this one for PNAS:\n\n\n\n\n\n\n18.0.7 Workflow in a nutshell\n\n\n\n\n\n\nSummary\n\n\n\n\nUse rrtools to generate the core directory layout and approach to data handling.\nThen use rticles to create the structure of the paper itself. The combination is incredibly flexible.\n\n\n\nThings we can do with our research compendium:\n\nEdit ./analysis/paper/paper.Rmd to begin writing your paper and your analysis in the same document\nAdd any citations to ./analysis/paper/pnas-sample.bib\nAdd any longer R scripts that don’t fit in your paper in an R folder at the top level\nAdd raw data to ./data/raw_data\nWrite out any derived data (generated in paper.Rmd) to ./data/derived_data\nWrite out any figures in ./analysis/figures\n\nYou can then write all of your R code in your RMarkdown/Quarto, and generate your manuscript all in the format needed for your journal (using its .csl file, stored in the paper directory).\n\n\n18.0.8 Adding renv to conserve your environment\n\nrrtools has a couple more tricks up its sleeve to help your compendium be as reproducible and portable as possible.\nTo capture the R packages and versions this project depends on, we can use the renv package.\nRunning renv::init(), will initiate tracking of the R packages in your project.\nThis action will create a new folder called renv in your top directory.\nrenv::init() automatically detects dependencies in your code (by looking for library calls, at the DESCRIPTION file, etc.) and installs them to a private project specific library. This means that your project mypaper can use a different version of dplyr than another project which may need an older version without any hassle.\nrenv also write the package dependencies to a special file in the repository called renv.lock.\nIf any of your packages you are using is updated, while your are working on your project, you can run renv::snapshot() to update the renv.lock file and your project-installed packages.\nYou can read the renv.lock file using renv::restore(), when needed. This will install the versions of the packages needed.\n\n\n\n18.0.9 Conserve your computational environement with Docker\n\nThe rrtools package then uses this renv.lock file to build what is called a Dockerfile.\nDocker allows you to build containers, a standard unit of software that packages up code and all its dependencies so an application runs quickly and reliably from one computing environment to another.\nA container is an “image” of all the software specified, and this image can be run on other computers such that the software stack looks exactly as you specify.\nThis is important when it comes to reproducibility, because when running someone else code, you may get different results or errors if you are using different versions of software (like an old version of dplyr).\nA Dockerfile contains the instructions for how to recreate the computational environment where your analysis was run.\n\nIn practice\n\nOnce you have your research compendium, you can called rrtools::use_dockerfile(). If needed, re-install rrtools directly from GitHub remotes::install_github(\"benmarwick/rrtools\")\nThis first creates a Dockerfile that loads a standard image for using R with the tidyverse.\nAnd then has more instructions for how to create the environment so that it has the very specific R packages and versions you need.\nIf we look at the Dockerfile (example below), it calls to renv::restore(), as described above.\nThe last line of the docker file renders our Quarto/RMarkdown reproducible paper!\n\n# get the base image, the rocker/verse has R, RStudio and pandoc\nFROM rocker/verse:4.2.2\n\n# required\nMAINTAINER Your Name &lt;your_email@somewhere.com&gt;\n\nCOPY . /&lt;REPO&gt;\n\n# go into the repo directory\nRUN . /etc/environment \\\n  # Install linux depedendencies here\n  # e.g. need this for ggforce::geom_sina\n  && sudo apt-get update \\\n  && sudo apt-get install libudunits2-dev -y \\\n  # build this compendium package\n  && R -e \"install.packages('remotes', repos = c(CRAN = 'https://cloud.r-project.org'))\" \\\n  && R -e \"remotes::install_github(c('rstudio/renv', 'quarto-dev/quarto-r'))\" \\\n  # install pkgs we need\n  && R -e \"renv::restore()\" \\\n  # render the manuscript into a docx, you'll need to edit this if you've\n  # customised the location and name of your main qmd file\n  && R -e \"quarto::quarto_render('/&lt;REPO&gt;/analysis/paper/paper.qmd')\"\n\nAfter running rrtools::use_dockerfile(), the package also sets up GitHub Actions for you.\nActions are processes that are triggered in GitHub events (like a push) and run automatically.\nIn this case, the Action that is set up will build your Docker image on GitHub.\nThis means that the code that knits your paper is run, and an updated version of your paper is knit.\nThis is called continuous integration, and is extremely convenient for developing products like this, since the build step can be taken care of automatically as you push to your repository.\n\n\n\n\n18.0.10 The 5th Generation of Reproducible Papers\n\nWhole Tale is a project that aims to simplify computational reproducibility. It enables researchers to easily package and share ‘Tales’. Tales are executable research objects captured in a standards-based tale format complete with metadata. They can contain:\n\nData (references)\nCode (computational methods)\nNarrative (traditional science story)\nCompute environment (e.g. RStudio, Jupyter)\n\n\nBy combining data, code and the compute environment, Tales allow researchers to:\n\nRe-create the computational results from a scientific study\nAchieve computational reproducibility\n“Set the default to reproducible.”\n\nFull circle reproducibility can be achieved by publishing data, code AND the computational environment.\n\n\n18.0.11 Resources\n\nrrtools documentation\nThe rticles\nusethis documentation\n\n\n\n\n\nTrisovic, Ana, Matthew K. Lau, Thomas Pasquier, and Mercè Crosas. 2022. “A Large-Scale Study on Research Code Quality and Execution.” Scientific Data 9 (1). https://doi.org/10.1038/s41597-022-01143-6.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Provenance and Reproducibility</span>"
    ]
  }
]