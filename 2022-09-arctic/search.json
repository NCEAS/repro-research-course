[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "",
    "text": "This 5-day in-person workshop will provide researchers with an introduction to advanced topics in computationally reproducible research in python and R, including software and techniques for working with very large datasets. This includes working in cloud computing environments, docker containers, and parallel processing using tools like parsl and dask. The workshop will also cover concrete methods for documenting and uploading data to the Arctic Data Center, advanced approaches to tracking data provenance, responsible research and data management practices including data sovereignty and the CARE principles, and ethical concerns with data-intensive modeling and analysis.\n\n\n\n\n\n\n\n\n\n\nPlease note that by participating in this activity you agree to abide by the NCEAS Code of Conduct.\n\n\n\n\nIn this course, we will be using Python (3.9.13) as our primary language, and VS Code as our IDE. Below are instructions on how to get VS Code set up to work for the course. If you are already a regular Python user, you may already have another IDE set up. We strongly encourage you to set up VS Code with us, because we will use your local VS Code instance to write and execute code on one of the NCEAS servers.\n\n\nFirst, download VS Code if you do not already have it installed.\nYou’ll also need to download the Remote - SSH extension.\n\n\n\nTo connect to the server using VS Code follow these steps, from the VS Code window:\n\nopen the command pallette (Cmd + Shift + P)\nenter “Remote SSH: Connect to Host”\n\n\n\nselect “Add New SSH Host”\nenter the ssh command to connect to the host as if in a terminal (ssh username@included-crab.nceas.ucsb.edu)\n\nNote: you will only need to do this step once\n\n\n\n\nselect the SSH config file to update with the name of the host. You should select the one in your user directory (eg: /Users/jclark/.ssh/config)\nclick “Connect” in the popup in the lower right hand corner\n\nNote: If the dialog box does not appear, reopen the command palette (Cmd + Shift + P), type in “Remote-SSH: Connect to Host…”, choose included-crab.nceas.ucsb.edu from the options of configured SSH hosts, then enter your password into the dialog box that appears\n\nenter your password in the dialog box that pops up\n\nWhen you are connected, you will see in the lower left hand corner of the window a green bar that says “SSH: included-crab.nceas.ucsb.edu.”\n\n\n\n\nAfter connecting to the server, in the extensions pane (View > Extensions) search for, and install, the following extensions:\n- Python\n- Jupyter\n- Jupyter Keymap\nNote that these extensions will be installed on the server, and not locally.\n\n\n\nWe are going to be working on the server exclusively, but if you are interested in setting up VS Code to work for you locally with Python, you can follow these instructions. This local setup section summarizes the official VS Code tutorial. For more detailed instructions and screenshots, see the source material. This step is 100% optional, if you already have an IDE set up to work locally that you like, or already have VS code set up to work locally, you are welcome to skip this.\nLocally (not connected to the server), check to make sure you have Python installed if you aren’t sure you do. File > New Window will open up a new VS Code window locally.\nTo check your python, from the terminal run:\npython3 --version\nIf you get an error, it means you need to install Python. Here are instructions for getting installed, depending on your operating system. Note: There are many ways to install and manage your Python installations, and advantages and drawbacks to each. If you are unsure about how to proceed, feel free to reach out to the instructor team for guidance.\n\nWindows: Download and run an installer from Python.org.\nMac: Install using homebrew. If you don’t have homebrew installed, follow the instructions from their webpage.\n\nbrew install python3\n\n\nAfter you run your install, make sure you check that the install is on your system PATH by running python3 --version again.\nNext, install the Python extension for VS Code.\nOpen a terminal window in VS Code from the Terminal drop down in the main window. Run the following commands to initialize a project workspace in a directory called training. This example will show you how to do this locally. Later, we will show you how to set it up on the remote server with only one additional step.\nmkdir training\ncd training\ncode .\nNext, select the Python interpreter for the project. Open the Command Palette using Command + Shift + P (Control + Shift + P for windows). The Command Palette is a handy tool in VS Code that allows you to quickly find commands to VS Code, like editor commands, file edit and open commands, settings, etc. In the Command Palette, type “Python: Select Interpreter.” Push return to select the command, and then select the interpreter you want to use (your Python 3.X installation).\nTo make sure you can write and execute code in your project, create a Hello World test file.\n\nFrom the File Explorer toolbar, or using the terminal, create a file called hello.py\nAdd some test code to the file, and save\n\nmsg = \"Hello World\"\nprint(msg)\n\nExecute the script using either the Play button in the upper-right hand side of your window, or by running python3 hello.py in the terminal.\n\nFor more ways to run code in VS Code, see the tutorial\n\n\nFinally, to test Jupyter, download the Jupyter extension. You’ll also need to install ipykernel. From the terminal, run pip install ipykernel.\nYou can create a test Jupyter Notebook document from the command pallete by typing “Create: New Jupyter Notebook” and selecting the command. This will open up a code editor pane with a notebook that you can test.\n\n\n\nIn order to code along during the Google Earth Engine lesson (Ch 15) on Thursday, you’ll need to sign up for an account at https://signup.earthengine.google.com. Following the link above will take you to a form that looks like this: \n\n\n\n Once submitted, you’ll receive an email with some helpful links and a message that it may take a few days for your account to be up and running. Please be sure to do this a few days ahead of needing to use GEE.\n\n\n\n\n\n\nImportant\n\n\n\nGEE authentication (more on that in Lesson 15) uses Cloud Projects. Some organizations control who can create Cloud Projects, which may prevent you from completing the authentication process. To circumvent authentication issues, we recommend creating your GEE account using a non-organizational account (e.g. a personal email account). Check out GEE’s authentication troubleshooting recommendations if you continue to run into issues.\n\n\n\n\n\n\nThese written materials reflect the continuous development of learning materials at the Arctic Data Center and NCEAS to support individuals to understand, adopt, and apply ethical open science practices. In bringing these materials together we recognize that many individuals have contributed to their development. The primary authors are listed alphabetically in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Matthew B. Jones, Bryce Mecum, S. Jeanette Clark, Samantha Csik. 2022. Scalable and Computationally Reproducible Approaches to Arctic Research.\nAdditional contributors: Amber E. Budden, Natasha Haycock-Chavez, Noor Johnson, Stephanie Hampton, Jim Regetz, Bryce Mecum, Julien Brun, Julie Lowndes, Erin McLean, Andrew Barrett, David LeBauer, Jessica Guo.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "sections/01-adc-intro.html",
    "href": "sections/01-adc-intro.html",
    "title": "1  Welcome and Introductions",
    "section": "",
    "text": "This course is one of three that we are currently offering, covering fundamentals of open data sharing, reproducible research, ethical data use and reuse, and scalable computing for reusing large data sets."
  },
  {
    "objectID": "sections/02-remote-computing.html",
    "href": "sections/02-remote-computing.html",
    "title": "2  Remote Computing",
    "section": "",
    "text": "Understand the basic architecture of computer networks\nLearn how to connect to a remote computer via a shell\nBecome familiarized with Bash Shell programming to navigate your computer’s file system, manipulate files and directories, and automate processes"
  },
  {
    "objectID": "sections/02-remote-computing.html#introduction",
    "href": "sections/02-remote-computing.html#introduction",
    "title": "2  Remote Computing",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nScientific synthesis and our ability to effectively and efficiently work with big data depends on the use of computers and the internet. Working on a personal computer may be sufficient for many tasks, but as data get larger and analyses more computationally intensive, scientists often find themselves needing more computing resources than they have available locally. Remote computing, or the process of connecting to a computer(s) in another location via a network link is becoming more and more common in overcoming big data challenges.\nIn this lesson, we’ll learn about the architecture of computer networks and explore some of the different remote computing configurations that you may encounter, we’ll learn how to securely connect to a remote computer via a shell, and we’ll become familiarized with using Bash Shell to efficiently manipulate files and directories. We will begin working in the VS Code IDE (integrated development environment), which is a versatile code editor that supports many different languages."
  },
  {
    "objectID": "sections/02-remote-computing.html#servers-networking",
    "href": "sections/02-remote-computing.html#servers-networking",
    "title": "2  Remote Computing",
    "section": "2.3 Servers & Networking",
    "text": "2.3 Servers & Networking\nRemote computing typically involves communication between two or more “host” computers. Host computers connect via networking equipment and can send messages to each other over communication protocols (aka an Internet Protocol, or IP). Host computers can take the role of client or server, where servers share their resources with the client. Importantly, these client and server roles are not inherent properties of a host (i.e. the same machine can play either role).\n\nClient: the host computer intiating a request\nServer: the host computer responding to a request\n\n\n\n\nFig 1. Examples of different remote computing configurations. (a) A client uses secure shell protocol (SSH) to login/connect to a server over the internet. (b) A client uses SSH to login/connect to a computing cluster (i.e. a set of computers (nodes) that work together so that they can be viewed as a single system) over the internet. In this example, servers A - I are each nodes on this single cluster. The connection is first made through a gateway node (i.e. a computer that routes traffic from one network to another). (c) A client uses SSH to login/connect to a computing cluser where each node is a virtual machine (VM). In this example, the cluster comprises three servers (A, B, and C). VM1 (i.e. node 1) runs on server A while VM4 runs on server B, etc. The connection is first made through a gateway node."
  },
  {
    "objectID": "sections/02-remote-computing.html#ip-addressing",
    "href": "sections/02-remote-computing.html#ip-addressing",
    "title": "2  Remote Computing",
    "section": "2.4 IP addressing",
    "text": "2.4 IP addressing\nHosts are assigned a unique numerical address used for all communication and routing called an Internet Protocol Address (IP Address). They look something like this: 128.111.220.7. Each IP Address can be used to communicate over various “ports”, which allow multiple applications to communicate with a host without mixing up traffic.\n\n\n\n\n\n\nPort numbers are divided into three ranges:\n\n\n\n\nwell-known ports, range from 0 through 1023 and are reserved for the most commonly used services (see table below for examples of some well-known port numbers)\nregistered ports, range from 1024 through 49151 and are not assigned or controlled, but can be registered (e.g. by a vendor for use with thier own server application) to prevent duplication\ndynamic ports, range from 49152 through 65535 and are not assigned, controlled, or registered but may instead be used as temporary or private ports\n\n\n\n\n\n\n\n\nwell-known port\nassignment\n\n\n\n\n20, 21\nFile Transfer Protocol (FTP), for transfering files between a client & server\n\n\n22\nsecure shell (SSH), to create secure network conections\n\n\n53\nDomain Name System (DNS) service, to match domain names to IP addresses\n\n\n80\nHypertext Transfer Protocol (HTTP), used in the World Wide Web\n\n\n443\nHTTP Secure (HTTPS), an encrypted version of HTTP\n\n\n\n\n\nBecause IP addresses can be difficult to remember, they are also assigned hostnames, which are handled through the global Domain Name System (DNS). Clients first look up a hostname in the DNS to find the IP address, then open a connection to the IP address.\n\n\n\n\n\n\nIn order to connect to remote servers, computing clusters, virtual machines, etc., you need to know their IP address (or hostname)\n\n\n\nA couple important ones: 1. Throughout this course, we’ll be working on a server with the hostname, included-crab and IP address, 128.111.85.1 (in just a little bit, we’ll learn how to connect to included-crab using SSH) 2. localhost is a hostname that refers to your local computer and is assigned the IP address 127.0.0.1 – the concept of localhost is important for tasks such as website testing and also is important to understand when provisioning local execution resources (e.g. we’ll practice this in during the section 6 exercise when working with Parsl.)"
  },
  {
    "objectID": "sections/02-remote-computing.html#bash-shell-programming",
    "href": "sections/02-remote-computing.html#bash-shell-programming",
    "title": "2  Remote Computing",
    "section": "2.5 Bash Shell Programming",
    "text": "2.5 Bash Shell Programming\nWhat is a shell? From Wikipedia:\n\n“a computer program which exposes an operating system’s services to a human user or other programs. In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), depending on a computer’s role and particular operation.”\n\n\n\n\nWhat is Bash? Bash, or Bourne-again Shell, is a command line tool (language) commonly used to manipulate files and directories. Accessing and using bash is slightly different depending on what type of machine you work on:\n\nMac: bash via the Terminal, which comes ready-to-use with all Macs and Linux machines\nWindows: running bash depends on which version of Windows you have – newer versions may ship with bash or may require a separate install (e.g. Windows Subsystem for Linux (WSL) or Git Bash), however there are a number of different (non-bash) shell options as well (they all vary slightly; e.g. PowerShell, Command Prompt).\n\n\n\n\n\n\n\nNote\n\n\n\nMac users may have to switch from Z Shell, or zsh, to bash. Use the command exec bash to switch your default shell to bash (or exec zsh to switch back).\n\n\n\n2.5.1 Some commonly used (and very helpful) bash commands:\nBelow are just a few bash commands that you’re likely to use. Some may be extended with options (more on that in the next section) or even piped together (i.e. where the output of one command gets sent to the next command, using the | operator). You can also find some nice bash cheat sheets online, like this one. Alternatively, the Bash Reference Manual has all the content you need, albeit a bit dense.\n\n\n\n\n\n\n\nbash command\nwhat it does\n\n\n\n\npwd\nprint your current working directory\n\n\ncd\nchange directory\n\n\nls\nlist contents of a directory\n\n\ntree\ndisplay the contents of a directory in the form of a tree structure (not installed by default)\n\n\necho\nprint text that is passed in as an argument\n\n\nmv\nmove or rename a file\n\n\ncp\ncopy a file(s) or directory(ies)\n\n\ntouch\ncreate a new empty file\n\n\nmkdir\ncreate a new directory\n\n\nrm/rmdir\nremove a file/ empty directory (be careful – there is no “trash” folder!)\n\n\ngrep\nsearches a given file(s) for lines containing a match to a given pattern list\n\n\nawk\na text processing language that can be used in shell scripts or at a shell prompt for actions like pattern matching, printing specified fields, etc.\n\n\nsed\nstands for Stream Editor; a versatile command for editing files\n\n\ncut\nextract a specific portion of text in a file\n\n\njoin\njoin two files based on a key field present in both\n\n\ntop, htop\nview running processes in a Linux system (press Q to quit)\n\n\n\n\n\n2.5.2 General command syntax\nBash commands are typically are written as: command [options] [arguments] where the command must be an executable on your PATH and where options (settings that change the shell and/or script behavior) take one of two forms: short form (e.g. command -option-abbrev) or long form (e.g. command --option-name or command -o option-name). An example:\n# the `ls` command lists the files in a directory\nls file/path/to/directory\n\n# adding on the `-a` or `--all` option lists all files (including hidden files) in a directory\nls -a file/path/to/directory # short form\nls --all file/path/to/directory # long form\nls -o all file/path/to/directory # long form\n\n\n2.5.3 Some useful keyboard shortcuts\nIt can sometimes feel messy working on the command line. These keyboard shortcuts can make it a little easier:\n\nCtrl + L: clear your terminal window\nCtrl + U: delete the current line\nCtrl + C: abort a command\nup & down arrow keys: recall previously executed commands in chronological order\nTAB key: autocompletion"
  },
  {
    "objectID": "sections/02-remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "href": "sections/02-remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "title": "2  Remote Computing",
    "section": "2.6 Connecting to a remote computer via a shell",
    "text": "2.6 Connecting to a remote computer via a shell\nIn addition to navigating your computer/manipulating your files, you can also use a shell to gain accesss to and remotely control other computers. To do so, you’ll need the following:\n\na remote computer (e.g. server) which is turned on\nclient and server ssh clients installed/enabled\nthe IP address or name of the remote computer\nthe necessary permissions to access the remote computer\n\nSecure Shell, or SSH, is a network communication protocol that is often used for securely connecting to and running shell commands on a remote host, tremendously simplifying remote computing."
  },
  {
    "objectID": "sections/02-remote-computing.html#git-via-a-shell",
    "href": "sections/02-remote-computing.html#git-via-a-shell",
    "title": "2  Remote Computing",
    "section": "2.7 Git via a shell",
    "text": "2.7 Git via a shell\nGit, a popular version control systemm and command line tool can be accessed via a shell. While there are lots of graphical user interfaces (GUIs) that faciliatate version control with Git, they often only implement a small subset of Git’s most-used functionality. By interacting with Git via the command line, you have access to all Git commands. While all-things Git is outside the scope of this workshop, we will use some basic Git commands in the shell to clone GitHub (remote) repositories to the server and save/store our changes to files. A few important Git commands:\n\n\n\n\n\n\n\nGit command\nwhat it does\n\n\n\n\ngit clone\ncreate a copy (clone) of repository in a new directory in a different location\n\n\ngit add\nadd a change in the working directory to the staging area\n\n\ngit commit\nrecord a snapshot of a repository; the -m option adds a commit message\n\n\ngit push\nsend commits from a local repository to a remote repository\n\n\ngit fetch\ndownloads contents (e.g. files, commits, refs) from a remote repo to a local repo\n\n\ngit pull\nfetches contents of a remote repo and merges chnanges into the local repo"
  },
  {
    "objectID": "sections/02-remote-computing.html#lets-practice",
    "href": "sections/02-remote-computing.html#lets-practice",
    "title": "2  Remote Computing",
    "section": "2.8 Let’s practice!",
    "text": "2.8 Let’s practice!\nWe’ll now use bash commands to do the following:\n\nconnect to the server (included-crab) that we’ll be working on for the remainder of this course\nnavigate through directories on the server and add/change/manipulate files\nclone a GitHub repository to the server\nautomate some of the above processes by writing a bash script\n\n\n2.8.1 Exercise 1: Connect to a server using the ssh command (or using VS Code’s command palette)\nLet’s connect to a remote computer (included-crab) and practice using some of above commands.\n\nLaunch a terminal in VS Code\n\n\nThere are two options to open a terminal window, if a terminal isn’t already an open pane at the bottom of VS Code\n\nClick on Terminal > New Terminal in top menu bar\nClick on the + (dropdown menu) > bash in the bottom right corner\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou don’t need to use the VS Code terminal to ssh into a remote computer, but it’s conveniently located in the same window as your code when working in the VS Code IDE.\n\n\n\nConnect to a remote server\n\n\nYou can choose to SSH into the server (included-crab.nceas.ucsb.edu) through (a) the command line by using the ssh command, or (b) through VS Code’s command palette. If you prefer the latter, please refer back to the Log in to the server section. To do so via the command line, use the ssh command followed by yourusername@included-crab.nceas.ucsb.edu. You’ll be prompted to type/paste your password to complete the login. It should look something like this:\n\nyourusername:~$ ssh yourusername@included-crab.nceas.ucsb.edu \nyourusername@included-crab.nceas.ucsb.edu's password: \nyourusername@included-crab:~$ \n\n\n\n\n\n\nImportant\n\n\n\nYou won’t see anything appear as you type or paste your password – this is a security feature! Type or paste your password and press enter/return when done to finish connecting to the server.\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo log out of the server, type exit – it should look something like this:\nyourusername@included-crab.nceas.ucsb.edu:$ exit\nlogout\nConnection to included-crab.nceas.ucsb.edu closed.\n(base) .....\n\n\n\n\n2.8.2 Exercise 2: Practice using some common bash commands\n\nUse the pwd command to print your current location, or working directory. You should be in your home directory on the server (e.g. /home/yourusername).\nUse the ls command to list the contents (any files or subdirectories) of your home directory\nUse the mkdir command to create a new directory named bash_practice:\n\nmkdir bash_practice\n\nUse the cd command to move into your new bash_practice directory:\n\n# move from /home/yourusername to home/yourusername/bash_practice\ncd bash_practice\n\nTo move up a directory level, use two dots, .. :\n\n# move from /home/yourusername/bash_practice back to /home/yourusername\n$ cd ..\n\n\n\n\n\n\nNote\n\n\n\nTo quickly navigate back to your home directory from wherever you may be on your computer, use a tilde, ~ :\n# e.g. to move from from some subdirectory, /home/yourusername/Projects/project1/data, back to your home directory, home/yourusername\n$ cd ~\n\n# or use .. to back out three subdirectories\n$ cd ../../..\n\n\n\nAdd some .txt files (file1.txt, file2.txt, file3.txt) to your bash_practice subdirectory using the touch command (Note: be sure to cd into bash_practice if you’re not already there):\n\n# add one file at a time\ntouch file1.txt\ntouch file2.txt\ntouch file3.txt\n\n# or add all files simultanously like this:\ntouch file{1..3}.txt\n\n# or like this:\ntouch file1.txt file2.txt file3.txt\n\nYou can also add other file types (e.g. .py, .csv, etc.)\n\ntouch mypython.py mycsv.csv\n\nPrint out all the .txt files in bash_practice using a wildcard, *:\n\nls *.txt\n\nCount the number of .txt files in bash_practice by combining the ls and wc (word count) funtions using the pipe, |, operator:\n\n# `wc` returns a word count (lines, words, chrs)\n# the `-l` option only returns the number of lines\n# use a pipe, `|`, to send the output from `ls *.txt` to `wc -l`\nls *.txt | wc -l\n\nDelete mypython.py using the rm command:\n\nrm mypython.py \n\nCreate a new directory inside bash_practice called data and move mycsv.csv into it.\n\nmkdir data\nmv mycsv.csv ~/bash_practice/data\n\n# add the --interactive option (-i for short) to prevent a file from being overwritten by accident (e.g. in case there's a file with the same name in the destination location)\nmv -i mycsv.csv ~/bash_practice/data\n\nUse mv to rename mycsv.csv to mydata.csv\n\nmv mycsv.csv mydata.csv\n\nAdd column headers col1, col2, col3 to mydata.csv using echo + the > operator\n\necho \"col1, col2, col3\" > mydata.csv\n\n\n\n\n\n\nTip\n\n\n\nYou can check to see that mydata.csv was updated using GNU nano, a text editor for the command line that comes preinstalled on Linux machines (you can edit your file in nano as well). To do so, use the nano command followed by the file you want to open/edit:\nnano mydata.csv\nTo save and quit out of nano, use the control + X keyboard shortcut.\nYou can also create and open a file in nano in just one line of code. For example, running nano hello_world.sh is the same as creating the file first using touch hello_world.sh, then opening it with nano using nano hello_world.sh.\n\n\n\nAppend a row of data to mydata.csv using echo + the >> operator\n\n# using `>` will overwrite the contents of an existing file; `>>` appends new information to an existing file\necho \"1, 2, 3\" >> mydata.csv\n\n\n2.8.3 Exercise 3: Clone a GitHub repository to the server\nIDEs commonly have helper buttons for cloning (i.e. creating a copy of) remote repositories to your local computer (or in this case, a server), but using git commands in a terminal can be just as easy. We can practice that now, following the steps below:\n\nGo to the scalable-computing-exercises repository on GitHub at https://github.com/NCEAS/scalable-computing-examples – this repo contains example files for you to edit and practice in throughout this course. Fork (make your own copy of the repository) this repo by clicking on the Fork button (top right corner of the repository’s page).\n\n\n\n\n\n\nOnce forked, click on the green Code button (from the forked version of the GitHub repo) and copy the URL to your clipboard.\n\n\n\n\n\n\nIn the VS Code terminal, use the git clone command to create a copy of the scalable-computing-examples repository in the top level of your user directory (i.e. your home directory) on the server (Note: use pwd to check where you are; use cd ~/ to navigate back to your home directory if you find that you’re somewhere else).\n\ngit clone <url-of-forked-repo>\n\nTo open the project, open the folder into your workpace using File > Open Folder. Enter your password, if prompted.\nYou should now have a copy of the scalable-computing-examples repository to work on on the server. Use the tree command to see the structure of the repo (you need to be in the scalable-computing-examples directory for this to work) – there should be a subdirectory called bash-babynames that contains (i) a README.MD file, (ii) a KEY.sh file (this is a functioning bash script available for reference; we’ll be recreating it together in the next exercise) and (iii) a namesbystate folder containing 51 .TXT files and a StateReadMe.pdf file with some metadata.\n\n\n\n2.8.4 Bonus Exercise: Automate data processing with a Bash script\nAs we just demonstrated, we can use bash commands in the terminal to accomplish a variety of tasks like navigating our computer’s directories, manipulating/creating/adding files, and much more. However, writing a bash script allows us to gather and save our code for automated execusion.\nWe just cloned the scalable-computing-examples GitHub repository to the server in Exercise 3 above. This contains a bash-babynames folder with 51 .TXT files (one for each of the 50 US states + The District of Columbia), each with the top 1000 most popular baby names in that state. We’re going to use some of the bash commands we learned in Exercise 2 to concatenate all rows of data from these 51 files into a single babynames_allstates.csv file.\nLet’s begin by creating a simple bash script that when executed, will print out the message, “Hello, World!” This simple script will help us determine whether or not things are working as expected before writing some more complex (and interesting) code.\n\nOpen a terminal window and determine where you are by using the pwd command – we want to be in scalable-computing-examples/bash-babynames. If necessary, navigate here using the cd command.\nNext, we’ll create a shell script called mybash.sh using the touch command:\n\n$ touch bybash.sh\n\nThere are a number of ways to edit a file or script – we’ll use Nano, a terminal-based text editor, as we did earlier. Open your mybash.sh with nano by running the following in your terminal:\n\n$ nano mybash.sh\n\nWe can now start to write our script. Some important considerations:\n\n\nAnything following a # will not be executed as code – these are useful for adding comments to your scripts\nThe first line of a Bash script starts with a shebang, #!, followed by a path to the Bash interpreter – this is used to tell the operating system which interpreter to use to parse the rest of the file. There are two ways to use the shebang to set your interpreter (read up on the pros & cons of both methods on this Stack Overflow post):\n\n# (option a): use the absolute path to the bash binary\n#!/bin/bash\n\n# (option b): use the env untility to search for the bash executable in the user's $PATH environmental variable\n#!/usr/bin/env bash\n\nWe’ll first specify our bash interpreter using the shebang, which indicates the start of our script. Then, we’ll use the echo command, which when executed, will print whatever text is passed as an argument. Type the following into your script (which should be opened with nano), then save (Use the keyboard shortcut control + X to exit, then type Y when it asks if you’d like to save your work. Press enter/return to exit nano).\n\n# specify bash as the interpreter\n#!/bin/bash\n\n# print \"Hello, World!\"\necho \"Hello, World!\"\n\nTo execute your script, use the bash command followed by the name of your bash script (be sure that you’re in the same working directory as your mybash.sh file or specify the file path to it). If successful, “Hello, World!” should be printed in your terminal window.\n\nbash mybash.sh\n\nNow let’s write our script. Re-open your script in nano by running nano mybash.sh. Using what we practiced above and the hints below, write a bash script that does the following:\n\n\nprints the number of .TXT files in the namesbystate subdirectory\nprints the first 10 rows of data from the CA.TXT file (HINT: use the head command)\nprints the last 10 rows of data from the CA.TXT file (HINT: use the tail command)\ncreates an empty babynames_allstates.csv file in the namesbystate subdirectory (this is where the concatenated data will be saved to)\nadds the column names, state, gender, year, firstname, count, in that order, to the babynames_allstates.csv file\nconcatenates data from all .TXT files in the namesbystate subdirectory and appends those data to the babynames_allstates.csv file (HINT: use the cat command to concatenate files)\n\nHere’s a script outline to fill in (Note: The echo statements below are not necessary but can be included as progress indicators for when the bash script is executed – these also make it easier to diagnose where any errors occur during execution):\n#!bin/bash\necho \"THIS IS THE START OF MY SCRIPT!\"\n\necho \"-----Verify that we have .TXT files for all 50 states + DC-----\"\n<add your code here>\n\necho \"-----Printing head of CA.TXT-----\"\n<add your code here>\n\necho \"-----Printing tail of CA.TXT-----\"\n<add your code here>\n\necho \"-----Creating empty .csv file to concatenate all data-----\"\n<add your code here>\n\necho \"-----Adding column headers to csv file-----\"\n<add your code here>\n\necho \"-----Concatenating files-----\"\n<add your code here>\n\necho \"DONE!\"\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n#!bin/bash\necho \"THIS IS THE START OF MY SCRIPT!\"\n\necho \"-----Verify that we have .TXT files for all 50 states + DC-----\"\nls namesbystate/*.TXT | wc -l\n\necho \"-----Printing head of CA.TXT-----\"\nhead namesbystate/CA.TXT\n\necho \"-----Printing tail of CA.TXT-----\"\ntail namesbystate/CA.TXT\n\necho \"-----Creating empty .csv file to concatenate all data-----\"\ntouch namesbystate/babynames_allstates.csv\n\necho \"-----Adding column headers to csv file-----\"\necho \"state, gender, year, firstname, count\" > namesbystate/babynames_allstates.csv\n\necho \"-----Concatenating files-----\"\ncat namesbystate/*.TXT >> namesbystate/babynames_allstates.csv\n\necho \"DONE!\""
  },
  {
    "objectID": "sections/03-python-intro.html",
    "href": "sections/03-python-intro.html",
    "title": "3  Python Programming on Clusters",
    "section": "",
    "text": "Basic Python review\nUsing virtual environments\nWriting in Jupyter notebooks\nWriting functions in Python"
  },
  {
    "objectID": "sections/03-python-intro.html#introduction",
    "href": "sections/03-python-intro.html#introduction",
    "title": "3  Python Programming on Clusters",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nWe’ve chosen to use VS Code in this training, in part, because it has great support for developing on remote machines. Hopefully, your VS Code setup went easily, and you were able to connect to our server included-crab. Once connected, the VS Code interface looks just like you were working locally, and connection to the server is seamless.\nOther aspects of VS Code that we like: it supports all languages thanks to the extensive free extension library, it has built in version control integration, and it is highly flexible/configurable.\nWe will also be working quite a bit in Jupyter notebooks in this course. Notebooks are great ways to interleave rich text (markdown formatted text, equations, images, links) and code in a way that a ‘literate analysis’ is generated. Although Jupyter notebooks are not subsitutes for python scripts, they can be great communication tools, and can also be convenient for code development."
  },
  {
    "objectID": "sections/03-python-intro.html#connect-to-the-server-if-you-arent-already-connected",
    "href": "sections/03-python-intro.html#connect-to-the-server-if-you-arent-already-connected",
    "title": "3  Python Programming on Clusters",
    "section": "3.3 Connect to the server (if you aren’t already connected)",
    "text": "3.3 Connect to the server (if you aren’t already connected)\nTo get set up for the course, let’s connect to the server again. If you were able to work through the setup for the lesson without difficulty, follow these steps to connect:\n\nopen VS Code\nopen the command pallette (Cmd + Shift + P)\nenter “Remote SSH: Connect to Host”\nselect included-crab\nenter your password in the dialog box that pops up\n\nAlternatively, you may see a popup window that says “Cannot reconnect. Please reload the window”. Choose the blue Reload Window button and enter your password, if prompted.\nTo open the scalable-computing-examples workspace, select File > Open Workspace from File. Then navigate to and select ~/scalable-computing-examples/scalable-computing-examples.code-workspace. If you cannot find the scalable-computing-examples directory, in a terminal you can use the ls command to list out the contents of your home directory and ensure that you have a clone of the respository. If not, follow the instructions in Exercise 3 in the Remote Computing session).\nOnce connected and in the workslace, use the pwd command in the terminal to make sure you’re in your project directory (/home/yourusername/scalable-computing-examples)."
  },
  {
    "objectID": "sections/03-python-intro.html#virtual-environments",
    "href": "sections/03-python-intro.html#virtual-environments",
    "title": "3  Python Programming on Clusters",
    "section": "3.4 Virtual Environments",
    "text": "3.4 Virtual Environments\n\nWhen you install a python library, let’s say pandas, via pip, unless you specify otherwise, pip will go out and grab the most recent version of the library, and install it somewhere on your system path (where, exactly, depends highly on how you install python originally, and other factors). This is all great, untl you realize that as part of a new project, a new library you are starting to work with requires a older version of pandas, what do you do? You need both pandas versions for each of your projects. Virtual environments help to solve this issue without making the all to common situation in the comic above even more complicated.\nA virtual environment is a folder structure which creates a symlink (pointer) to all of the libraries that you need into the folder. The three main components will be: the python distribution itself, its configuration, and a site-packages directory (where your libraries like pandas live). So the folder is a self contained directory of all the version-specific python software you need for your project.\nVirtual environments are very helpful to create reproducible workflows, and we’ll talk more about this concept of reproducible environements later in the course. Perhaps most importantly though, virtual environments also help you maintain your sanity when python programming. Because they are just folders, you can create and delete new ones at will, without worrying about bungling your underlying python setup.\nIn this course, we are going to use virtualenv as our tool to create and manage virtual environments. Other virtual environment tools used commonly are conda and pipenv. One reason we like using virtualenv is there is an extension to it called virtualenvwrapper, which provides easy to remember wrappers around common virtualenv operations that make creating, activating, and deactivating a virtual environment very easy.\nFirst we will create a .bash_profile file to create variables that point to the install locations of python and virtualenvwrapper. .bash_profile is just a text file that contains bash commands that are run every time you start up a new terminal. Although setting up this file is not required to use virtualenvwrapper, it is convenient because it allows you to set up some reasonable defaults to the commands (meaning less typing, overall), and it makes sure that the package is available every time you start a new terminal.\n\n3.4.0.1 Setup\n\nIn VS Code, select ‘File > New Text File’\nPaste this text into the file:\n\nexport VIRTUALENVWRAPPER_VIRTUAWORKON_HOME=$HOME/.virtualenvs\nsource /usr/share/virtualenvwrapper/virtualenvwrapper.sh\nThe first line points virtualenvwrapper to the directory where your virtual environments will be stored. We point it to a hidden directory (.virtualenvs) in your home directory. The last line sources a bash script that ships with virtualenvwrapper, which makes all of virtualenvwrapper commands available in your terminal session.\n\nSave the file in the top of your home directory as .bash_profile.\nRestart your terminal (Terminal > New Terminal)\nCheck to make sure it was installed and configured correctly by running this in the terminal:\n\nmkvirtualenv --version\nIt should return some content that looks like this (with more output, potentially).\nvirtualenv 20.13.0+ds from /usr/lib/python3/dist-packages/virtualenv/__init__.py\n\n\n3.4.0.2 Course environment\nNow we can create the virtual environment we will use for the course. In the terminal run:\nmkvirtualenv -p python3.9 scomp\nHere, we’ve specified explicitly which python version to use by using the -p flag, and the path to the python 3.9 installation on the server. After making a virtual environment, it will automatically be activated. You’ll see the name of the env you are working in on the left side of your terminal prompt in parentheses. To deactivate your environment (like if you want to work on a different project), just run deactivate. To activate it again, run:\nworkon scomp\nYou can get a list of all available environments by just running:\nworkon\nNow let’s install the dependencies for this course into that environment. To install our libraries we’ll use pip. As of Python 3.4, pip is automatically included with your python installation. pip is a package manager for python, and you might have used it already to install common python libraries like pandas or numpy. pip goes out to PyPI, the Python Package Index, to download the code and put it in your site-packages directory. Note that on this shared server, your user directory will ahve a site-packages directory, in addition to one that our systems administrator manages as the root of the system.\npip install -r requirements.txt\n\n\n3.4.0.3 Installing locally (optional)\nvirtualenvwrapper was already installed on the server we are working on. To install on your local computer, run:\npip3 install virtualenvwrapper\nAnd then follow the instructions as described above, making sure that you have the correct paths set when you edit your .bash_profile."
  },
  {
    "objectID": "sections/03-python-intro.html#brief-overview-of-python-syntax",
    "href": "sections/03-python-intro.html#brief-overview-of-python-syntax",
    "title": "3  Python Programming on Clusters",
    "section": "3.5 Brief overview of python syntax",
    "text": "3.5 Brief overview of python syntax\nWe’ll very briefly go over some basic python syntax and the base variable types. First, open a python script. From the File menu, select New File, type “python”, then save it as ‘python-intro.py’ in the top level of your directory.\nIn your file, assign a value to a variable using = and print the result.\n\nx = 4\nprint(x)\n\n4\n\n\nTo run this code in python we can:\n\nexecute python python-intro.py in the terminal\nclick the Play button in the upper right hand corner of the file editor\nright click any line and select: “Run to line in interactive terminal”\n\nIn that interactive window you can then run python code interactively, which is what we’ll use for the next bit of exploring data types.\nThere are 5 standard data types in python\n\nNumber (int, long, float, complex)\nString\nList\nTuple\nDictionary\n\nWe already saw a number type, here is a string:\n\nstr = 'Hello World!'\nprint(str)\n\nHello World!\n\n\nLists in python are very versatile, and are created using square brackets []. Items in a list can be of different data types.\n\nlist = [100, 50, -20, 'text']\nprint(list)\n\n[100, 50, -20, 'text']\n\n\nYou can access items in a list by index using the square brackets. Note indexing starts with 0 in python. The slice operator enables you to easily access a portion of the list without needing to specify every index.\n\nlist[0] # print first element\nlist[1:3] # print 2nd until 4th elements\nlist[:2] # print first until the 3rd\nlist[2:] # print last elements from 3rd\n\n100\n\n\n[50, -20]\n\n\n[100, 50]\n\n\n[-20, 'text']\n\n\nThe + and * operators work on lists by creating a new list using either concatenation (+) or repetition (*).\n\nlist2 = ['more', 'things']\n\nlist + list2\nlist * 3\n\n[100, 50, -20, 'text', 'more', 'things']\n\n\n[100, 50, -20, 'text', 100, 50, -20, 'text', 100, 50, -20, 'text']\n\n\nTuples are similar to lists, except the values cannot be changed in place. They are constructed with parentheses.\n\ntuple = ('a', 'b', 'c', 'd')\ntuple[0]\ntuple * 3\ntuple + tuple\n\n'a'\n\n\n('a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'a', 'b', 'c', 'd')\n\n\n('a', 'b', 'c', 'd', 'a', 'b', 'c', 'd')\n\n\nObserve the difference when we try to change the first value. It works for a list:\n\nlist[0] = 'new value'\nlist\n\n['new value', 50, -20, 'text']\n\n\n…and errors for a tuple.\n\ntuple[0] = 'new value'\n\nTypeError: 'tuple' object does not support item assignment\nDictionaries consist of key-value pairs, and are created using the syntax {key: value}. Keys are usually numbers or strings, and values can be any data type.\n\ndict = {'name': ['Jeanette', 'Matt'],\n        'location': ['Tucson', 'Juneau']}\n\ndict['name']\ndict.keys()\n\n['Jeanette', 'Matt']\n\n\ndict_keys(['name', 'location'])\n\n\nTo determine the type of an object, you can use the type() method.\n\ntype(list)\ntype(tuple)\ntype(dict)\n\nlist\n\n\ntuple\n\n\ndict"
  },
  {
    "objectID": "sections/03-python-intro.html#jupyter-notebooks",
    "href": "sections/03-python-intro.html#jupyter-notebooks",
    "title": "3  Python Programming on Clusters",
    "section": "3.6 Jupyter notebooks",
    "text": "3.6 Jupyter notebooks\nTo create a new notebook, from the file menu select File > New File > Jupyter Notebook. Go ahead and save this notebook at the top level of your scalable-computing-examples directory.\nAt the top of your notebook, add a first level header using a single hash. Practice some markdown text by creating:\n\na list\nbold text\na link\n\nUse the Markdown cheat sheet if needed.\nYou can click the plus button below any chunk to add a chunk of either markdown or python.\n\n3.6.1 Load libraries\nIn your first code chunk, lets load in some modules. We’ll use pandas, numpy, matplotlib.pyplot, requests, skimpy, and exists from os.path.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport requests\nimport skimpy\nimport os\n\nA note on style: There are a few ways to construct import statements. The above code uses three of the most common:\nimport module\nimport module as m\nfrom module import function\nThe first way of importing will make the module a function comes from more explicitly clear, and is the simplest. However for very long module names, or ones that are used very frequently (like pandas, numpy, and matplotlib.plot), the code in the notebook will be more cluttered with constant calls to longer module names. So module.function() instead is written as m.function()\nThe second way of importing a module is a good style to use in cases where modules are used frequently, or have extremely long names. If you import every single module with a short name, however, you might have a hard time remembering which modules are named what, and it might be more confusing for others trying to read your code. Many of the most commonly used libraries for python data science have community-driven styling for how they are abbreviated in import statements, and these community norms are generally best followed.\nFinally, the last way to import a single object from a module can be helpful if you only need that one piece from a larger module, but again, like the first case, results in less explicit code and therefore runs the risk of your or someone else misremembering the usage and source.\n\n\n3.6.2 Read in a csv\nCreate a new code chunk that will download the csv that we are going to use for this tutorial.\n\nNavigate to Rohi Muthyala, Åsa Rennermalm, Sasha Leidman, Matthew Cooper, Sarah Cooley, et al. 2022. 62 days of Supraglacial streamflow from June-August, 2016 over southwest Greenland. Arctic Data Center. doi:10.18739/A2XW47X5F.\nRight click the download button for ‘Discharge_timeseries.csv’\nClick ‘copy link address’\n\nCreate a variable called URL and assign it the link copied to your clipboard. Then use requests.get to download the file, and open to write it to disk, to a directory called data/. We’ll write this bundled in an if statement so that we only download the file if it doesn’t yet exist. First, we create the directory if it doesn’t exist:\n\nif not os.path.exists ('data/'):\n        os.mkdir('data/')\n\n\nif not os.path.exists('data/discharge_timeseries.csv'):\n\n        url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3Ae248467d-e1f9-4a32-9e38-a9b4fb17cefb'\n\n        data = requests.get(url)\n        a = open('data/discharge_timeseries.csv', 'wb').write(data.content)\n\nNow we can read in the data from the file.\n\ndf = pd.read_csv('data/discharge_timeseries.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Total Pressure [m]\n      Air Pressure [m]\n      Stage [m]\n      Discharge [m3/s]\n      temperature [degrees C]\n    \n  \n  \n    \n      0\n      6/13/2016 0:00\n      9.816\n      9.609775\n      0.206225\n      0.083531\n      -0.1\n    \n    \n      1\n      6/13/2016 0:05\n      9.810\n      9.609715\n      0.200285\n      0.077785\n      -0.1\n    \n    \n      2\n      6/13/2016 0:10\n      9.804\n      9.609656\n      0.194344\n      0.072278\n      -0.1\n    \n    \n      3\n      6/13/2016 0:15\n      9.800\n      9.609596\n      0.190404\n      0.068756\n      -0.1\n    \n    \n      4\n      6/13/2016 0:20\n      9.793\n      9.609537\n      0.183463\n      0.062804\n      -0.1\n    \n  \n\n\n\n\nThe column names are a bit messy so we can use clean_columns from skimpy to make them cleaner for programming very quickly. We can also use the skim function to get a quick summary of the data.\n\n\nclean_df = skimpy.clean_columns(df)\nskimpy.skim(clean_df)\n\n6 column names have been cleaned\n\n\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 17856  │ │ float64     │ 5     │                                                          │\n│ │ Number of columns │ 6      │ │ string      │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name              ┃ NA  ┃ NA %    ┃ mean     ┃ sd     ┃ p0        ┃ p25    ┃ p75   ┃ p100  ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━┩  │\n│ │ total_pressure_m         │   0 │       0 │      9.9 │   0.12 │       9.6 │    9.8 │    10 │    10 │ ▁▅█▇▅▂ │  │\n│ │ air_pressure_m           │   0 │       0 │      9.6 │   0.06 │       9.5 │    9.6 │   9.7 │   9.7 │ ▂▅▄▆█▃ │  │\n│ │ stage_m                  │   0 │       0 │     0.28 │   0.12 │   0.00056 │   0.17 │  0.37 │  0.56 │ ▁█▇▇▆▁ │  │\n│ │ discharge_m_3_s          │   0 │       0 │     0.22 │   0.19 │   4.7e-08 │  0.055 │  0.35 │  0.96 │  █▄▃▁  │  │\n│ │ temperature_degrees_     │   8 │   0.045 │   -0.034 │  0.053 │      -0.1 │   -0.1 │     0 │   0.2 │   ▅█   │  │\n│ └──────────────────────────┴─────┴─────────┴──────────┴────────┴───────────┴────────┴───────┴───────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name               ┃ NA      ┃ NA %       ┃ words per row                ┃ total words              ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ date                      │       0 │          0 │                            2 │                    36000 │  │\n│ └───────────────────────────┴─────────┴────────────┴──────────────────────────────┴──────────────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nWe can see that the date column is classed as a string, and not a date, so let’s fix that.\n\n\nclean_df['date'] = pd.to_datetime(clean_df['date'])\nskimpy.skim(clean_df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 17856  │ │ float64     │ 5     │                                                          │\n│ │ Number of columns │ 6      │ │ datetime64  │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name              ┃ NA  ┃ NA %    ┃ mean     ┃ sd     ┃ p0        ┃ p25    ┃ p75   ┃ p100  ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━┩  │\n│ │ total_pressure_m         │   0 │       0 │      9.9 │   0.12 │       9.6 │    9.8 │    10 │    10 │ ▁▅█▇▅▂ │  │\n│ │ air_pressure_m           │   0 │       0 │      9.6 │   0.06 │       9.5 │    9.6 │   9.7 │   9.7 │ ▂▅▄▆█▃ │  │\n│ │ stage_m                  │   0 │       0 │     0.28 │   0.12 │   0.00056 │   0.17 │  0.37 │  0.56 │ ▁█▇▇▆▁ │  │\n│ │ discharge_m_3_s          │   0 │       0 │     0.22 │   0.19 │   4.7e-08 │  0.055 │  0.35 │  0.96 │  █▄▃▁  │  │\n│ │ temperature_degrees_     │   8 │   0.045 │   -0.034 │  0.053 │      -0.1 │   -0.1 │     0 │   0.2 │   ▅█   │  │\n│ └──────────────────────────┴─────┴─────────┴──────────┴────────┴───────────┴────────┴───────┴───────┴────────┘  │\n│                                                    datetime                                                     │\n│ ┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name        ┃ NA    ┃ NA %     ┃ first            ┃ last                           ┃ frequency      ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩  │\n│ │ date               │     0 │        0 │    2016-06-13    │      2016-08-13 23:55:00       │ 5T             │  │\n│ └────────────────────┴───────┴──────────┴──────────────────┴────────────────────────────────┴────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nIf we wanted to calculate the daily mean flow (as opposed to the flow every 5 minutes), we need to:\n\ncreate a new column with only the date\ngroup by that variable\nsummarize over it by taking the mean\n\nFirst we should probably rename our existing date/time column to prevent from getting confused.\n\nclean_df = clean_df.rename(columns = {'date': 'datetime'})\n\nNow create the new date column\n\nclean_df['date'] = clean_df['datetime'].dt.date\n\nFinally, we use group by to split the data into groups according to the date. We can then apply the mean method to calculate the mean value across all of the columns. Note that there are other methods you can use to calculate different statistics across different columns (eg: clean_df.groupby('date').agg({'discharge_m_3_s': 'max'})).\n\ndaily_flow = clean_df.groupby('date', as_index = False).mean()\n\n\ncreate a simple plot\n\n\nvar = 'discharge_m_3_s'\nvar_labs = {'discharge_m_3_s': 'Total Discharge'}\n\nfig, ax = plt.subplots(figsize=(7, 7))\nplt.plot(daily_flow['date'], daily_flow[var])\nplt.xticks(rotation = 45)\nax.set_ylabel(var_labs.get('discharge_m_3_s'))\n\n(array([16967., 16974., 16983., 16990., 16997., 17004., 17014., 17021.,\n        17028.]),\n [Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, '')])\n\n\nText(0, 0.5, 'Total Discharge')"
  },
  {
    "objectID": "sections/03-python-intro.html#functions",
    "href": "sections/03-python-intro.html#functions",
    "title": "3  Python Programming on Clusters",
    "section": "3.7 Functions",
    "text": "3.7 Functions\nThe plot we made above is great, but what if we wanted to make it for each variable? We could copy paste it and replace some things, but this violates a core tenet of programming: Don’t Repeat Yourself! Instead, we’ll create a function called myplot that accepts the data frame and variable as arguments.\n\ncreate myplot.py\n\n\nimport matplotlib.pyplot as plt\n\ndef myplot(df, var):\n\n        var_labs = {'discharge_m_3_s': 'Total Discharge (m^3/s)',\n                    'total_pressure_m': 'Total Pressure (m)',\n                    'air_pressure_m': 'Air Pressure (m)',\n                    'stage_m': 'Stage (m)',\n                    'temperature_degrees_c': 'Temperature (C)'}\n\n        fig, ax = plt.subplots(figsize=(7, 7))\n        plt.plot(df['date'], df[var])\n        plt.xticks(rotation = 45)\n        ax.set_ylabel(var_labs.get(var))\n\n\nload myplot into jupyter notebook (from myplot import myplot)\nadd libraries\nreplace old plot method with new function\n\n\nmyplot(daily_flow, 'temperature_degrees_c')\n\n\n\n\nWe’ll have more on functions in the software design sections."
  },
  {
    "objectID": "sections/03-python-intro.html#summary",
    "href": "sections/03-python-intro.html#summary",
    "title": "3  Python Programming on Clusters",
    "section": "3.8 Summary",
    "text": "3.8 Summary\nIn this lesson we learned all about virtual environments, how to use them, and why. We got our environments set up for the course, did a brief python syntax review, and then jumped into Jupyter notebooks, pandas, and writing functions."
  },
  {
    "objectID": "sections/04-parallel-programming.html",
    "href": "sections/04-parallel-programming.html",
    "title": "4  Pleasingly Parallel Programming",
    "section": "",
    "text": "Understand what parallel computing is and when it may be useful\nUnderstand how parallelism can work\nReview sequential loops and map functions\nBuild a parallel program using concurrent.futures\nBuild a parallel program using parsl\nUnderstand Thread Pools and Process pools"
  },
  {
    "objectID": "sections/04-parallel-programming.html#introduction",
    "href": "sections/04-parallel-programming.html#introduction",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nProcessing large amounts of data with complex models can be time consuming. New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America:\n\n\n\nLevy et al. 2016. doi:10.5063/F1Z899CZ\n\n\nThere are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization.\nAlternatively, think of remote sensing data. Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years.\n\n\n\nNEON Data Cube"
  },
  {
    "objectID": "sections/04-parallel-programming.html#why-parallelism",
    "href": "sections/04-parallel-programming.html#why-parallelism",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.3 Why parallelism?",
    "text": "4.3 Why parallelism?\nMuch R code runs fast and fine on a single processor. But at times, computations can be:\n\ncpu-bound: Take too much cpu time\nmemory-bound: Take too much memory\nI/O-bound: Take too much time to read/write from disk\nnetwork-bound: Take too much time to transfer\n\nTo help with cpu-bound computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire cluster of those computers. Plus, these machines also have large amounts of memory to avoid memory-bound computing jobs."
  },
  {
    "objectID": "sections/04-parallel-programming.html#processors-cpus-and-cores",
    "href": "sections/04-parallel-programming.html#processors-cpus-and-cores",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.4 Processors (CPUs) and Cores",
    "text": "4.4 Processors (CPUs) and Cores\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with mutliple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations.\nA computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time.\n\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here we show four quad-core processors for a total of 16 cores in this machine.\n\nYou can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later).\nHistorically, R has only utilized one processor, which makes it single-threaded. Which is a shame, because the 2017 MacBook Pro that I am writing this on is much more powerful than that:\njones@powder:~$ sysctl hw.ncpu hw.physicalcpu\nhw.ncpu: 12\nhw.physicalcpu: 6\nTo interpret that output, this machine powder has 6 physical CPUs, each of which has two processing cores, for a total of 12 cores for computation. I’d sure like my computations to use all of that processing power. Because its all on one machine, we can easily use multicore processing tools to make use of those cores. Now let’s look at the computational server included-crab at NCEAS:\njones@included-crab:~$ lscpu | egrep 'CPU\\(s\\)|per core|per socket'\nCPU(s):                          88\nOn-line CPU(s) list:             0-87\nThread(s) per core:              1\nCore(s) per socket:              1\nNUMA node0 CPU(s):               0-87\nNow that’s more compute power! included-crab has 384 GB of RAM, and ample storage. All still under the control of a single operating system.\nHowever, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now:\n\nJetStream\n\n640 nodes, 15,360 cores, 80TB RAM\n\nStampede2 at TACC\n\n4200 KNL nodes: 285,600 cores\n1736 SKX nodes: 83,328 cores\n224 ICX nodes: 17,920 cores\nTOTAL: 386,848 cores\n\n\nNote that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster. One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine. But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster."
  },
  {
    "objectID": "sections/04-parallel-programming.html#modes-of-parallelization",
    "href": "sections/04-parallel-programming.html#modes-of-parallelization",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.5 Modes of parallelization",
    "text": "4.5 Modes of parallelization\n\nTODO: describe these modes\n\n\n\n\nSerial and parallel execution of tasks on using thre4ads and cluster processes."
  },
  {
    "objectID": "sections/04-parallel-programming.html#simple-task-parallelization",
    "href": "sections/04-parallel-programming.html#simple-task-parallelization",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.6 Simple task parallelization",
    "text": "4.6 Simple task parallelization\nStart with a task that is a little expensive:\n\ndef task(x):\n    import numpy as np\n    result = np.arange(x*10**8).sum()\n    return result\n\n\nimport numpy as np\n\n@timethis\ndef run_tasks_s(task_list):\n    return [task(x) for x in task_list]\n\nrun_tasks_s(np.arange(10))\n\nrun_tasks_s: 95014.92166519165 ms\n\n\n[0,\n 4999999950000000,\n 19999999900000000,\n 44999999850000000,\n 79999999800000000,\n 124999999750000000,\n 179999999700000000,\n 244999999650000000,\n 319999999600000000,\n 404999999550000000]\n\n\n\nfrom concurrent.futures import ThreadPoolExecutor\n\n@timethis\ndef run_tasks_p(task_list):\n    with ThreadPoolExecutor(max_workers=20) as cf_executor:\n        return cf_executor.map(task, task_list)\n\nresults = run_tasks_p(np.arange(10))\n[x for x in results]"
  },
  {
    "objectID": "sections/04-parallel-programming.html#setup-downloading-data-for-staging",
    "href": "sections/04-parallel-programming.html#setup-downloading-data-for-staging",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.7 Setup – downloading data for staging",
    "text": "4.7 Setup – downloading data for staging\nIn this exercise, we’re going to parallelize a simple task that is often very time consuming – downloading data. And we’ll compare performance of simple downloads using first a serial loop, and then using two parallel execution libraries: concurrent.futures and parsl. More on those later. But note that parallel execution won’t always speed up this task, as this is likely an I/O bound task if you’re downloading a lot of data. But we still should be able to speed things up a lot until we hit the limits of our disk arrays.\nThe data we are downloading is a pan-Arctic time series of TIF images containing rasterized Arctic surface water indices from:\n\nElizabeth Webb. 2022. Pan-Arctic surface water (yearly and trend over time) 2000-2022. Arctic Data Center doi:10.18739/A2NK3665N.\n\n\n\n\nWebb water data\n\n\nFirst, let’s download the data serially to set a benchmark. The data files are listed in a table with their filename and identifier, and can be downloaded directly from the Arctic Data Center using the identifier:\n\n\n\n\n\n\n  \n    \n      \n      filename\n      identifier\n    \n  \n  \n    \n      0\n      SWI_2007.tif\n      urn:uuid:5ee72c9c-789d-4a1c-95d8-cb2b24a20662\n    \n    \n      1\n      SWI_2019.tif\n      urn:uuid:9cd1cdc3-0792-4e61-afff-c11f86d3a9be\n    \n    \n      2\n      SWI_2021.tif\n      urn:uuid:14e1e509-77c0-4646-9cc3-d05f8d84977c\n    \n    \n      3\n      SWI_2020.tif\n      urn:uuid:1ba473ff-8f03-470b-90d1-7be667995ea1\n    \n    \n      4\n      SWI_2001.tif\n      urn:uuid:85150557-05fd-4f52-8bbd-ec5a2c27e23d"
  },
  {
    "objectID": "sections/04-parallel-programming.html#task-parallelism-serial-downloads",
    "href": "sections/04-parallel-programming.html#task-parallelism-serial-downloads",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.8 Task parallelism: serial downloads",
    "text": "4.8 Task parallelism: serial downloads\nWhen you have a list of repetitive tasks, you may be able to speed it up by adding more computing power. If each task is completely independent of the others, then it is a prime candidate for executing those tasks in parallel, each on its own core. For example, let’s build a simple loop that downloads the data files that we need for an analysis. First, we start with the serial implementation.\n\nimport urllib\n\ndef download_file(row):\n    service = \"https://arcticdata.io/metacat/d1/mn/v2/object/\"\n    pid = row[1]['identifier']\n    filename = row[1]['filename']\n    url = service + pid\n    print(\"Downloading: \" + filename)\n    msg = urllib.request.urlretrieve(url, filename)\n    return filename\n\n@timethis\ndef download_list(dl_list):\n    return [download_file(row) for row in dl_list.iterrows()]\n    \nresults = download_list(id_list[0:5])\nprint(results)\n\nDownloading: SWI_2007.tif\n\n\nDownloading: SWI_2019.tif\n\n\nDownloading: SWI_2021.tif\n\n\nDownloading: SWI_2020.tif\n\n\nDownloading: SWI_2001.tif\n\n\ndownload_list: 88664.64900970459 ms\n['SWI_2007.tif', 'SWI_2019.tif', 'SWI_2021.tif', 'SWI_2020.tif', 'SWI_2001.tif']\n\n\nIn this code, I have one function (download_file) that downloads a single data file and saves it to disk. It is called iteratively from the function download_list. The serial execution takes about 15.2 seconds, but can vary by a few seconds.\nThe issue with this loop is that we execute each trial sequentially, which means that only one of our processors on this machine is in use. In order to exploit parallelism, we need to be able to dispatch our tasks and allow each to run at the same time, with one task going to each core. To do that, we can use one of the many parallelization libraries in python to help us out."
  },
  {
    "objectID": "sections/04-parallel-programming.html#task-parallelism-with-concurrent.futures",
    "href": "sections/04-parallel-programming.html#task-parallelism-with-concurrent.futures",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.9 Task parallelism with concurrent.futures",
    "text": "4.9 Task parallelism with concurrent.futures\nIn this case, we’ll use the same download_file function from previously, but will switch up the download_list to use concurrent.futures.\n\nfrom concurrent.futures import ThreadPoolExecutor\n\n@timethis\ndef download_list(dl_list):\n    with ThreadPoolExecutor(max_workers=15) as cf_executor:\n        return cf_executor.map(download_file, dl_list.iterrows(), timeout=60)\n\nresults = download_list(id_list[0:5])\nfor result in results:\n    print(result)\n\nDownloading: SWI_2007.tifDownloading: SWI_2019.tif\n\nDownloading: SWI_2021.tif\nDownloading: SWI_2020.tif\nDownloading: SWI_2001.tif\n\n\ndownload_list: 22249.897003173828 ms\nSWI_2007.tif\nSWI_2019.tif\nSWI_2021.tif\nSWI_2020.tif\nSWI_2001.tif\n\n\n\nfrom concurrent.futures import ProcessPoolExecutor\n\n@timethis\ndef download_list(dl_list):\n    with ProcessPoolExecutor(max_workers=15) as cf_executor:\n        return cf_executor.map(download_file, dl_list.iterrows(), timeout=60)\n\nresults = download_list(id_list[0:5])\nfor result in results:\n    print(result)\n\nDownloading: SWI_2021.tif\n\n\nDownloading: SWI_2007.tif\n\n\nDownloading: SWI_2019.tif\n\n\nDownloading: SWI_2020.tif\n\n\nDownloading: SWI_2001.tif\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndownload_list: 16459.52844619751 ms\nSWI_2007.tif\nSWI_2019.tif\nSWI_2021.tif\nSWI_2020.tif\nSWI_2001.tif"
  },
  {
    "objectID": "sections/04-parallel-programming.html#parsl",
    "href": "sections/04-parallel-programming.html#parsl",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.10 parsl",
    "text": "4.10 parsl\n\nOverview of parsl and it’s use of python decorators.\n\n\n# Required packages\nimport parsl\nfrom parsl import python_app\nfrom parsl.config import Config\nfrom parsl.executors import HighThroughputExecutor\nfrom parsl.providers import LocalProvider\n\n# Configure the parsl executor\nactivate_env = 'workon scomp'\nhtex_local = Config(\n    executors=[\n        HighThroughputExecutor(\n            max_workers=15,\n            provider=LocalProvider(\n                worker_init=activate_env\n            )\n        )\n    ],\n)\nparsl.clear()\nparsl.load(htex_local)\n\n# Define a parsl_app for our download function using a decorator\n@python_app\ndef download_parsl(row):\n    import urllib\n    service = \"https://arcticdata.io/metacat/d1/mn/v2/object/\"\n    pid = row[1]['identifier']\n    filename = row[1]['filename']\n    url = service + pid\n    print(\"Downloading: \" + filename)\n    msg = urllib.request.urlretrieve(url, filename)\n    return filename\n\n@timethis\ndef download_list(dl_list):\n    results = []\n    for row in dl_list.iterrows():\n        result = download_parsl(row)\n        results.append(result)\n    return(results)\n\n@timethis\ndef wait_for_futures():\n    results = download_list(id_list[0:5])\n    done = [app_future.result() for app_future in results]\n    print(done)\n\nwait_for_futures()\nhtex_local.executors[0].shutdown()\nparsl.clear()\n\ndownload_list: 104.13622856140137 ms\n\n\n['SWI_2007.tif', 'SWI_2019.tif', 'SWI_2021.tif', 'SWI_2020.tif', 'SWI_2001.tif']\nwait_for_futures: 62854.185342788696 ms"
  },
  {
    "objectID": "sections/04-parallel-programming.html#when-to-parallelize",
    "href": "sections/04-parallel-programming.html#when-to-parallelize",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.11 When to parallelize",
    "text": "4.11 When to parallelize\nIt’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!\nIn addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced. Some propose that this may follow Amdahl’s Law, where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines):\n#| eval: false\nlibrary(ggplot2)\nlibrary(tidyr)\namdahl <- function(p, s) {\n  return(1 / ( (1-p) + p/s  ))\n}\ndoubles <- 2^(seq(0,16))\ncpu_perf <- cbind(cpus = doubles, p50 = amdahl(.5, doubles))\ncpu_perf <- cbind(cpu_perf, p75 = amdahl(.75, doubles))\ncpu_perf <- cbind(cpu_perf, p85 = amdahl(.85, doubles))\ncpu_perf <- cbind(cpu_perf, p90 = amdahl(.90, doubles))\ncpu_perf <- cbind(cpu_perf, p95 = amdahl(.95, doubles))\n#cpu_perf <- cbind(cpu_perf, p99 = amdahl(.99, doubles))\ncpu_perf <- as.data.frame(cpu_perf)\ncpu_perf <- cpu_perf %>% gather(prop, speedup, -cpus)\nggplot(cpu_perf, aes(cpus, speedup, color=prop)) + \n  geom_line() +\n  scale_x_continuous(trans='log2') +\n  theme_bw() +\n  labs(title = \"Amdahl's Law\")\nSo, its important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done. With that, let’s do some parallel computing…"
  },
  {
    "objectID": "sections/04-parallel-programming.html#summary",
    "href": "sections/04-parallel-programming.html#summary",
    "title": "4  Pleasingly Parallel Programming",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nIn this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores. Next, we reviewed the way in which traditional for loops in R can be rewritten as functions that are applied to a list serially using lapply, and then how the parallel package mclapply function can be substituted in order to utilize multiple cores on the local computer to speed up computations. Finally, we installed and reviewed the use of the foreach package with the %dopar operator to accomplish a similar parallelization using multiple cores."
  },
  {
    "objectID": "sections/04-parallel-programming.html#further-reading",
    "href": "sections/04-parallel-programming.html#further-reading",
    "title": "4  Pleasingly Parallel Programming",
    "section": "5.2 Further Reading",
    "text": "5.2 Further Reading\nRyan Abernathey & Joe Hamman. 2020. Closed Platforms vs. Open Architectures for Cloud-Native Earth System Analytics. Medium."
  },
  {
    "objectID": "sections/05-adc-data-publishing.html",
    "href": "sections/05-adc-data-publishing.html",
    "title": "5  Documenting and Publishing Data",
    "section": "",
    "text": "Become familiar with the submission process\nUnderstand what constitutes as “large data”\n\nKnow when to reach out to support team for help\n\nLean how data & code can be documented and published in open data archives"
  },
  {
    "objectID": "sections/05-adc-data-publishing.html#introduction",
    "href": "sections/05-adc-data-publishing.html#introduction",
    "title": "5  Documenting and Publishing Data",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nA data repository is a database infrastructure that collects, manages, and stores data. In addition to the Arctic Data Center, there are many other repositories dedicated to archiving data, code, and creating rich metadata. The Knowledge Network for Biocomplexity (KNB), the Digital Archaeological Record (tDAR), Environmental Data Initiative (EDI), and Zenodo are all examples of dedicated data repositories."
  },
  {
    "objectID": "sections/05-adc-data-publishing.html#metadata",
    "href": "sections/05-adc-data-publishing.html#metadata",
    "title": "5  Documenting and Publishing Data",
    "section": "5.3 Metadata",
    "text": "5.3 Metadata\nMetadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where they were collected, and why they were collected.\nFor consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the machine-readable version of the metadata for a sockeye salmon dataset:\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<eml:eml packageId=\"df35d.442.6\" system=\"knb\" \n    xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.1\">\n    <dataset>\n        <title>Improving Preseason Forecasts of Sockeye Salmon Runs through \n            Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007</title>\n        <creator id=\"1385594069457\">\n            <individualName>\n                <givenName>Mark</givenName>\n                <surName>Willette</surName>\n            </individualName>\n            <organizationName>Alaska Department of Fish and Game</organizationName>\n            <positionName>Fishery Biologist</positionName>\n            <address>\n                <city>Soldotna</city>\n                <administrativeArea>Alaska</administrativeArea>\n                <country>USA</country>\n            </address>\n            <phone phonetype=\"voice\">(907)260-2911</phone>\n            <electronicMailAddress>mark.willette@alaska.gov</electronicMailAddress>\n        </creator>\n        ...\n    </dataset>\n</eml:eml>\n\nAlternatively, the same metadata document can be converted to HTML format and displayed in a more readable form on the web:\n\nAs you can see from the picture above, users can download either the whole dataset or its individual components. This makes the dataset and its associated data resuable.\nAdditionally, the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data."
  },
  {
    "objectID": "sections/05-adc-data-publishing.html#data-package-structure",
    "href": "sections/05-adc-data-publishing.html#data-package-structure",
    "title": "5  Documenting and Publishing Data",
    "section": "5.4 Data Package Structure",
    "text": "5.4 Data Package Structure\nNote that the dataset above lists a collection of files that are contained within the dataset. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\n\nThese data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI (digital object identifier) identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID (universally unique identifier) that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4, and each of the individual files have their own identifiers as well."
  },
  {
    "objectID": "sections/05-adc-data-publishing.html#submitting-data-from-the-large-data-perspective",
    "href": "sections/05-adc-data-publishing.html#submitting-data-from-the-large-data-perspective",
    "title": "5  Documenting and Publishing Data",
    "section": "5.5 Submitting Data from the Large Data Perspective",
    "text": "5.5 Submitting Data from the Large Data Perspective"
  },
  {
    "objectID": "sections/05-adc-data-publishing.html#archiving-data-the-large-data-perspective",
    "href": "sections/05-adc-data-publishing.html#archiving-data-the-large-data-perspective",
    "title": "5  Documenting and Publishing Data",
    "section": "5.6 Archiving Data: The Large Data Perspective",
    "text": "5.6 Archiving Data: The Large Data Perspective\nThere are two components to any data package archived with the Arctic Data Center: the metadata & the data themselves. Data can be images, plain text documents, tabular data, spatial data, scripts used to analyze the data, a readme file, and more. To the best of your ability, please make sure that the data uploaded are in an open format, rather than proprietary. This means uploading a .pdf rather than a .docx file, or .csv rather than .xlsx. That being said, we understand that this is not always possible or reasonable for users, as is the case sometimes if they’ve used ESRI products rather than QGIS.\nThis section provides an overview of some highlights within the data submission process, and will specifically address issues related to datasets with large amounts of data, whether that be in number of files or cumulative file size.\nFirst we’ll go over the metadata submission; then learn how to upload the data using a secure File Transfer Protocol; and finally how to add attribute information to the data.\n\n5.6.1 Step 1: The Metadata Submission\n\n5.6.1.1 ORCiDs\nIn order to archive data with the Arctic Data Center, you must log in with your ORCID account. If you do not have one, you can create at https://orcid.org/. ORCID is a non-profit organization made up of research institutions, funders, publishers and other stakeholders in the research space. ORCID stands for Open Researcher and Contributor ID. The purpose of ORCID is to give researchers a unique identifier which then helps highlight and give credit to researchers for their work. If you click on someone’s ORCID, their work and research contributions will show up (as long as the researcher used ORCID to publish or post their work).\nOnce you’re logged into the Arctic Data Center with your ORCID, you can access the data submission form by clicking “Submit Data” in the navigation bar. For most dataset submissions, you would submit your data and metadata at the same using the “Add Files” buttons seen in the image below. However, when you know you have a large quantity of files or large cumulative file size, you should focus only on submitting metadata through the web form. We’ll discuss how to submit large quantities of data in the next section.\n\n\n\n5.6.1.2 Overview Section\nIn the overview section, you will include a descriptive title of your data set, select the appropriate data sensitivity tag, an abstract of the data set, keywords, funding information, and a license.\nIn general, if your data has been anonymized or de-identified in any way, your submission is no longer considered to have “Non-sensitive data”. If you have not had to de-identify your data or through an Instituional Review Board process, you should select the “Non-sensitive data” tag. You can find a more in-depth review of the data sensitivity tag in Chapter 12 of our Fundamentals in Data Management coursebook.\n\nYou also must enter a funding award number and choose a license. The funding field will search for an NSF award identifier based on words in its title or the number itself. When including funding information not from an NSF award, please make sure to add an award number, title, and organization if possible.\nThe licensing options are CC-0 and CC-BY, both of which allow your data to be downloaded and re-used by other researchers.\n\nCC-0 Public Domain Dedication: “…can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.”\nCC-BY: Attribution 4.0 International License: “…free to…copy,…redistribute,…remix, transform, and build upon the material for any purpose, even commercially,…[but] must give appropriate credit, provide a link to the license, and indicate if changes were made.”\n\n\n\n\n5.6.1.3 People Information\nInformation about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people:\n\nCreators - all the people who should be in the citation for the dataset\nContacts - one is required, but defaults to the dataset submitter if omitted\nPrincipal Investigators\nAny others that are relevant\n\nFor each, please provide their ORCID identifier, which helps link this dataset to their other scholarly works.\n\n\n\n5.6.1.4 Temporal Information\nAdd the temporal coverage of the data, which represents the time period to which data apply. You may notice as you begin to fill in the date information that a second box automatically appears. The editor allows you to enter multiple dates. Please use multiple date ranges if your sampling was discontinuous.\n\n\n\n\n5.6.1.5 Location Information\nThe geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that spatial search will be more precise.\n\nNote that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global:\n\nMission Canyon, Santa Barbara, California, USA\n\n\n\n\n5.6.1.6 Methods\nMethods are critical to the accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data. Ideally, enough detail should be provided such that a reasonable scientist could interpret the study and data for reuse without needing to consult the researchers or any other resources.\nIncluded in the methods section is a question that asks users about the ethical research practices that may or may not have been considered throughout the research process. You will learn more about this in Chapter 14, and can find more in-depth information on our website’s data ethics page.\nThe ethical research practices response box must be filled out in order to save your dataset. If users feel as though this question is not applicable to their research, we encourage them to discuss why that is rather than simply stating “Not Applicable,” or some variation thereof. For example, say a researcher has compiled satellite imagery of weather patterns over the open ocean. Rather than respond with “N/A”, a user should instead include something to the effect of “Dataset contains satellite imagery of weather patterns over the Atlantic Ocean. Imagery was downloaded from NASA and NOAA, and does not contain any individual’s identifiable information.” When in doubt, you can always email the support team at support@arcticdata.io.\n\n\n\n5.6.1.7 Save Metadata Submission\nWhen you’re finished editing the narrative metadata, click the Save Dataset button at the bottom right of your screen.\nIf there are errors or missing fields, they will be highlighted with a red banner as seen earlier. Correct those, and then try submitting again. If the save button disappears after making corrections, add a space in the abstract and the save button should reappear. If not, please reach out to the support team for assistance.\nWhen you are successful, you should see a large green banner with a link to the current dataset view. Click the X to close that banner if you want to continue editing metadata.\n\n\n\n\n5.6.2 Step 2: Uploading Large Data\nIn order to submit your large data files to the Arctic Data Center repository, we encourage users to directly upload their data to the Data Team’s servers using a secure file transfer protocol (FTP). Our team uses and recommends the free program Cyberduck.\nBefore we begin, let’s answer the following question: Why would a user want to upload their data through a separate process, rather than the web form when they submit their metadata?\nDepending on your internet connection, the number of files you have, and the cumulative size of the data, users may experience difficulty uploading their data through the submission form. These difficulties are most often significantly reduced upload speeds and submission malfunctions. As such, it is best for all parties for large quantities of data to be uploaded directly to our server through an FTP.\nBefore you can upload your data to the Data Team’s server, make sure to email us at support@arcticdata.io to retrieve the login password. Once you have that, you can proceed through the following steps.\n\n\n\n\n\n\nNote\n\n\n\nPlease note, if you have multiple terabytes of data, it may be best to arrange a shipment of an external hard drive.\n\n\nWhen you start Cyberduck, you should see a screen similar to the image below.\n\nClick the arrows of the drop down menu that says “FTP”, and select the “SFTP” option. Make sure you enter datateam.nceas.ucsb.edu as the server, and visitor as the username. Enter the password, and click connect.\n\n\n\n\n\n\nNote\n\n\n\nIf prompted about an unknown fingerprint, click allow.\n\n\n\nOnce you have successfully logged into the visitor folder, you will be able to see the folders of other users who have also uploaded their data. Notice the folders are almost exclusively the user’s last name.\n\nWhen you email the Data Team asking for the login password, they will instruct you to add a folder with your last name (assuming one does not already exist). You can add a folder by clicking the cog icon labeled “Action”, and then selecting “New Folder” from the drop down menu. Once you have your own folder, you can upload your data by again clicking the “Action” button, and selecting “Upload” from the menu.\n\n\n\n\n\n\n\nTip\n\n\n\nIf you know that you will need to use this process for more than one dataset, we suggest creating folders with the same name as the associated dataset’s title. This way, it will be clear as to which submission the data should be associated with.\n\n\nOnce you have finished uploading your data to our servers, please let the Data Team know via email so that we can continue associate your uploaded data with your metadata submission.\nAs mentioned in Step 1: The Metadata Submission section above, when the data package is finalized and made public, there will be a sentence in the abstract that directs users to a separate page where your data will live. The following image is an example of where the data from this dataset live.\n\n\n\n5.6.3 Step 3: Adding File & Variable Level Metadata\nThe final major section of metadata concerns the structure and content of your data files. Assuming there are many files (and not a few very large ones), it would be unreasonable for users to input file and variable level metadata for each file. When this situation occurs, we encourage users to fill out as much information as possible for each unique type of file. Once that is completed, usually with some assistance from the Data Team, we will then programmatically carry over the information to other relevant files.\nWhen you’re data are associated with your metadata submission, they will appear in the data section at the top of the page when you go to edit your dataset. Choose which file you would like to begin editing by selecting the “Describe” button to the right of the file name.\n\nOnce there, you will see the following screen. In the Overview section, we recommend not editing the file name, and instead add a descriptive overview of the file. Once done, click the Attributes tab."
  },
  {
    "objectID": "sections/06-group-project-1.html",
    "href": "sections/06-group-project-1.html",
    "title": "6  Group Project: Staging and Preprocessing",
    "section": "",
    "text": "Get familiarized with the overall group project workflow\nWrite a parsl app that will stage and tile the IWP example data in parallel"
  },
  {
    "objectID": "sections/06-group-project-1.html#introduction",
    "href": "sections/06-group-project-1.html#introduction",
    "title": "6  Group Project: Staging and Preprocessing",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nThe Permafrost Discovery Gateway is an online platform for archiving, processing, analysis, and visualization of permafrost big imagery products to enable discovery and knowledge-generation. The PDG utilizes and makes available products derived from high resolution satellite imagrey from the Polar Geospatial Center, Planet (3m), Sentinel (10 m), Landsat (30 m), and MODIS (250 m). One of these products is a dataset showing Ice Wedge Polygons (IWP) that form in melting permafrost.\nIce wedges form as a result of thermal contraction during melt/freeze cycles of permafrost. They can form very distinctive geometries clearly visible in satellite images. The PDG is using advanced analysis and computational tools to take high resolution satellite imagery and automatically dectect where ice wedge polygons form. Below is an example of a satellite image (left) and the detected ice wedge polygons in geospatial vector format (right) of that same image.\n\nIn the group project, we are going to use a subset of the high resolution dataset of these detected ice wedge polygons in order to learn some of the reproducible, scalable techniques that will allow us to process it. Our workflow will start with a set of large geopackage files that contain the detected ice wedge polygons. These files all have irregular extents due to the variation in satellite coverage, clouds, etc. Our first processing step will take these files and “tile” them into smaller files which have regular extents.\n\nIn step two of the workflow, we will take those regularly tiled geopackage files and rasterize them. The files will be regularly gridded, and a summary statistic will be calculated for each grid cell (such as the proportion of pixel area covered by polygons).\n\nIn the final step of the workflow, we will take the raster files and resample them to create a set of raster tiles at different resolutions. This last step is what will enable us to visualize our raster data dynamically, such that we look at lower resolutions when very zoomed out (and high resolution data would take too long to load), and higher resolution data when zoomed in and the extent is smaller."
  },
  {
    "objectID": "sections/06-group-project-1.html#staging-and-tiling",
    "href": "sections/06-group-project-1.html#staging-and-tiling",
    "title": "6  Group Project: Staging and Preprocessing",
    "section": "6.3 Staging and Tiling",
    "text": "6.3 Staging and Tiling\nToday we will undertake the first step of the workflow, staging and tiling the data.\n\nIn your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-06.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills."
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html",
    "href": "sections/08-data-structures-netcdf.html",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "",
    "text": "Learn about the NetCDF data format:\n\nCharacteristics: self-describing, scalable, portable, appendable, shareable, and archivable\nUnderstand the NetCDF data model: what are dimensions, variables, and attributes\nAdvantages and differences between NetCDF and tabular data formats\n\nLearn how to use the xarray Python package to work with NetCDF files:\n\nDescribe the core xarray data structures, the xarray.DataArray and the xarray.Dataset, and their components, including data variables, dimensions, coordinates, and attributes\nCreate xarray.DataArrays and xarra.DataSets out of raw numpy arrays and save them as netCDF files\nLoad xarray.DataSets from netCDF files and understand the attributes view\nPerform basic indexing, processing, and reduction of xarray.DataArrays\nConvert pandas.DataFrames into xarray.DataSets"
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#introduction",
    "href": "sections/08-data-structures-netcdf.html#introduction",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nEfficient and reproducible data analysis begins with choosing a proper format to store our data, particularly when working with large, complex, multi-dimensional datasets. Consider, for example, the following Earth System Data Cube from Mahecha et al. 2020, which measures nine environmental variables at high resolution across space and time. We can consider this dataset large (high-resolution means we have a big file), complex (multiple variables), and multi-dimensional (each variable is measured along three dimensions: latitude, longitude, and time). Additionally, necessary metadata must accompany the dataset to make it functional, such as units of measurement for variables, information about the authors, and processing software used.\n\n\n\nMahecha et al. 2020 . Visualization of the implemented Earth system data cube. The figure shows from the top left to bottom right the variables sensible heat (H), latent heat (LE), gross primary production (GPP), surface moisture (SM), land surface temperature (LST), air temperature (Tair), cloudiness (C), precipitation (P), and water vapour (V). The resolution in space is 0.25° and 8 d in time, and we are inspecting the time from May 2008 to May 2010; the spatial range is from 15° S to 60° N, and 10° E to 65° W.\n\n\nKeeping complex datasets in a format that facilitates access, processing, sharing, and archiving can be at least as important as how we parallelize the code we use to analyze them. In practice, it is common to convert our data from less efficient formats into more efficient ones before we parallelize any processing. In this lesson, we will\n\nfamiliarize ourselves with the NetCDF data format, which enables us to store large, complex, multi-dimensional data efficiently, and\nlearn to use the xarray Python package to read, process, and create NetCDF files."
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#netcdf-data-format",
    "href": "sections/08-data-structures-netcdf.html#netcdf-data-format",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.3 NetCDF Data Format",
    "text": "8.3 NetCDF Data Format\nNetCDF (network Common Data Form) is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. NetCDF was initially developed at the Unidata Program Center and is supported on almost all platforms, and parsers exist for most scientific programming languages.\nThe NetCDF documentation outlines that this data format is desgined to be:\n\n\nSelf-describing: Information describing the data contents of the file is embedded within the data file itself. This means that there is a header describing the layout of the rest of the file and arbitrary file metadata.\nScalable: Small subsets of large datasets may be accessed efficiently through netCDF interfaces, even from remote servers.\nPortable: A NetCDF file is machine-independent i.e. it can be accessed by computers with different ways of storing integers, characters, and floating-point numbers.\nAppendable: Data may be appended to a properly structured NetCDF file without copying the dataset or redefining its structure.\nSharable: One writer and multiple readers may simultaneously access the same NetCDF file.\nArchivable: Access to all earlier forms of NetCDF data will be supported by current and future versions of the software.\n\n\n\n8.3.1 Data Model\nThe NetCDF data model is the way that NetCDF organizes data. This lesson will follow the Classic NetCDF Data Model, which is at the core of all netCDF files. \nThe model consists of three key components: variables, dimensions, and attributes.\n\nVariables are N-dimensional arrays of data. We can think of these as varying/measured/dependent quantities.\nDimensions describe the axes of the data arrays. A dimension has a name and a length. We can think of these as the constant/fixed/independent quantities at which we measure the variables.\nAttributes are small notes or supplementary metadata to annotate a variable or the file as a whole.\n\n\n\n\nClassic NetCDF Data Model (NetCDF documentation)\n\n\n\n\n8.3.2 Metadata Standards\nThe most commonly used metadata standard for geospatial data is the Climate and Forecast metadata standard, also called the CF conventions.\n\nThe CF conventions are specifically designed to promote the processing and sharing of files created with the NetCDF API. Principles of CF include self-describing data (no external tables needed for understanding), metadata equally readable by humans and software, minimum redundancy, and maximum simplicity. (CF conventions FAQ)\n\nThe CF conventions provide a unique standardized name and precise description of over 1,000 physical variables. To maximize the reusability of our data, it is best to include a variable’s standardized name as an attribute called standard_name. Variables should also include a units attribute. This attribute should be a string that can be recognized by UNIDATA’s UDUNITS package. In these links you can find:\n\na table with all of the CF convention’s standard names, and\na list of the units found in the UDUNITS database maintained by the North Carolina Institute for Climate Studies.\n\n\n\n8.3.3 Exercise\nLet’s do a short practice now that we have reviewed the classic NetCDF model and know a bit about metadata best practices.\n\nPart 1\n\nImagine the following scenario: we have a network of 25 weather stations. They are located in a square grid: starting at 30°0′N 60°0′E, there is a station every 10° North and every 10° East. Each station measures the air temperature at a set time for three days, starting on September 1st, 2022. On the first day, all stations record a temperature of 0°C. On the second day, all temperatures are 1°C, and on the third day, all temperatures are 2°C. What are the variables, dimensions and attributes for this data?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nVariables: There is a single variable being measured: temperature. The variable values can be represented as a 5x5x3 array, with constant values for each day.\nDimensions: This dataset has three dimensions: time, latitude, and longitude. Time indicates when the measurement happened, we can encode it as the dates 2022-09-01, 2022-09-02, and 2022-09-03. The pairs of latitude and longitude values indicate the positions of the weather stations. Latitude has values 30, 40, 50, 60, and 70, measured in degrees North. Longitude has values 60, 70, 80, 90, and 100, measured in degrees East.\n\nAttributes: Let’s divide these into attributes for the variable, the dimensions, and the whole dataset:\n\nVariable attributes:\n\nTemperature attributes:\n\nstandard_name: air_temperature\nunits: degree_C\n\n\nDimension attributes:\n\nTime attributes:\n\ndescription: date of measurement\n\nLatitude attributes:\n\nstandard_name: grid_latitude\nunits: degrees_N\n\nLongitude attributes:\n\nsatandard_name: grid_longitude\nunits: degree_E\n\n\nDataset attributes:\n\ntitle: Temperature Measurements at Weather Stations\nsummary: an example of NetCDF data format\n\n\n\n\n\n\nPart 2\n\nNow imagine we calculate the average temperature over time at each weather station, and we wish to incorporate this data into the same dataset. How will adding the average temperature data change the dataset’s variables, attributes, and dimensions?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nVariables: Now we are measuring two variables: temperature and average temperature. The temperature data stays the same. We can represent the average temperature as a single 5x5 array with value 1 at each cell.\nDimensions: This dataset still has three dimensions: time, latitude, and longitude. The temperature variable uses all three dimensions, and the average temperature variable only uses two (latitude and longitude). This is ok! The dataset’s dimensions are the union of the dimensions of all the variables in the dataset. Variables in the same dataset may have all, some, or no dimensions in common.\n\nAttributes: To begin with, we need to keep all the previous attributes. Notice that the dataset’s title is general enough that we don’t need to update it. The only update we need to do is add the attributes for our new average temperature variable:\n\nAverage temperature attributes:\n\nstandard_name: average_air_temperature\ndescription: average temperature over three days\n\n\n\n\n\nOur next step is to see how we can translate all this information into something we can store and handle on our computers."
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#xarray",
    "href": "sections/08-data-structures-netcdf.html#xarray",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.4 xarray",
    "text": "8.4 xarray\nxarray is an open source project and Python package that augments NumPy arrays by adding labeled dimensions, coordinates and attributes. xarray is based on the netCDF data model, making it the appropriate tool to open, process, and create datasets in netCDF format.\n\n\n\n\n\nxarray’s development portal\n\n\n\n8.4.1 xarray.DataArray\nThe xarray.DataArray is the primary data structure of the xarray package. It is an n-dimensional array with labeled dimensions. We can think of it as representing a single variable in the NetCDF data format: it holds the variable’s values, dimensions, and attributes.\nApart from variables, dimensions, and attributes, xarray introduces one more piece of information to keep track of a dataset’s content: in xarray each dimension has at least one set of coordinates. A dimension’s coordinates indicate the dimension’s values. We can think of the coordinate’s values as the tick labels along a dimension. For example, in our previous exercise about temperature measured in weather stations, latitude is a dimension, and the latitude’s coordinates are 30, 40, 50, 60, and 70 because those are the latitude values at which we are collecting temperature data. In that same exercise, time is a dimension, and its coordinates are 2022-09-1, 2022-09-02, and 2022-09-03.\nHere you can read more about the xarray terminology.\n\n8.4.1.1 Create an xarray.DataArray\nLet’s suppose we want to make an xarray.DataArray that includes the information from our previous exercise about measuring temperature across three days. First, we import all the necessary libraries.\n\nimport os              \nimport requests \nimport pandas as pd\nimport numpy as np\n\nimport xarray as xr   # This is the package we'll explore\n\nThe underlying data in the xarray.DataArray is a numpy.ndarray that holds the variable values. So we can start by making a numpy.ndarray with our mock temperature data:\n\n# values of a single variable at each point of the coords \ntemp_data = np.array([np.zeros((5,5)), \n                      np.ones((5,5)), \n                      np.ones((5,5))*2]).astype(int)\ntemp_data\n\narray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])\n\n\nWe could think this is “all” we need to represent our data. But if we stopped at this point, we would need to\n\nremember that the numbers in this array represent the temperature in degrees Celsius (doesn’t seem too bad),\nremember that the first dimension of the array represents time, the second latitude and the third longitude (maybe ok), and\nkeep track of the range of values that time, latitude, and longitude take (not so good).\n\nKeeping track of all this information separately could quickly get messy and could make it challenging to share our data and analyses with others. This is what the netCDF data model and xarray aim to simplify. We can get data and its descriptors together in an xarray.DataArray by adding the dimensions over which the variable is being measured and including attributes that appropriately describe dimensions and variables.\n\n# names of the dimensions\ndims = ('time', 'lat', 'lon')\n\n# coordinates (tick labels) to use for indexing along each dimension \ncoords = {'time' : pd.date_range(\"2022-09-01\", \"2022-09-03\"),\n          'lat' : np.arange(30,80,10),\n          'lon' : np.arange(60,110,10)}  \n\n# attributes (metadata) of the data array \nattrs = { 'title' : 'temperature across weather stations',\n          'standard_name' : 'air_temperature',\n          'units' : 'degree_c'}\n\n# initialize xarray.DataArray\ntemp = xr.DataArray(data = temp_data, \n                    dims = dims,\n                    coords = coords,\n                    attrs = attrs)\ntemp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 3, lat: 5, lon: 5)>\narray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])\nCoordinates:\n  * time     (time) datetime64[ns] 2022-09-01 2022-09-02 2022-09-03\n  * lat      (lat) int64 30 40 50 60 70\n  * lon      (lon) int64 60 70 80 90 100\nAttributes:\n    title:          temperature across weather stations\n    standard_name:  air_temperature\n    units:          degree_cxarray.DataArraytime: 3lat: 5lon: 50 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2array([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Coordinates: (3)time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03array(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')lat(lat)int6430 40 50 60 70array([30, 40, 50, 60, 70])lon(lon)int6460 70 80 90 100array([ 60,  70,  80,  90, 100])Attributes: (3)title :temperature across weather stationsstandard_name :air_temperatureunits :degree_c\n\n\nWe can also update the variable’s attributes after creating the object. Notice that each of the coordinates is also an xarray.DataArray, so we can add attributes to them.\n\n# update attributes\ntemp.attrs['description'] = 'simple example of an xarray.DataArray'\n\n# add attributes to coordinates \ntemp.time.attrs = {'description':'date of measurement'}\n\ntemp.lat.attrs['standard_name']= 'grid_latitude'\ntemp.lat.attrs['units'] = 'degree_N'\n\ntemp.lon.attrs['standard_name']= 'grid_longitude'\ntemp.lon.attrs['units'] = 'degree_E'\ntemp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 3, lat: 5, lon: 5)>\narray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])\nCoordinates:\n  * time     (time) datetime64[ns] 2022-09-01 2022-09-02 2022-09-03\n  * lat      (lat) int64 30 40 50 60 70\n  * lon      (lon) int64 60 70 80 90 100\nAttributes:\n    title:          temperature across weather stations\n    standard_name:  air_temperature\n    units:          degree_c\n    description:    simple example of an xarray.DataArrayxarray.DataArraytime: 3lat: 5lon: 50 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2array([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Coordinates: (3)time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03description :date of measurementarray(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')lat(lat)int6430 40 50 60 70standard_name :grid_latitudeunits :degree_Narray([30, 40, 50, 60, 70])lon(lon)int6460 70 80 90 100standard_name :grid_longitudeunits :degree_Earray([ 60,  70,  80,  90, 100])Attributes: (4)title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArray\n\n\nAt this point, since we have a single variable, the dataset attributes and the variable attributes are the same.\n\n\n8.4.1.2 Indexing\nAn xarray.DataArray allows both positional indexing (like numpy) and label-based indexing (like pandas). Positional indexing is the most basic, and it’s done using Python’s [] syntax, as in array[i,j] with i and j both integers. Label-based indexing takes advantage of dimensions in the array having names and coordinate values that we can use to access data instead of remembering the positional order of each dimension.\nAs an example, suppose we want to know what was the temperature recorded by the weather station located at 40°0′N 80°0′E on September 1st, 2022. By recalling all the information about how the array is setup with respect to the dimensions and coordinates, we can access this data positionally:\n\ntemp[0,1,2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray ()>\narray(0)\nCoordinates:\n    time     datetime64[ns] 2022-09-01\n    lat      int64 40\n    lon      int64 80\nAttributes:\n    title:          temperature across weather stations\n    standard_name:  air_temperature\n    units:          degree_c\n    description:    simple example of an xarray.DataArrayxarray.DataArray0array(0)Coordinates: (3)time()datetime64[ns]2022-09-01description :date of measurementarray('2022-09-01T00:00:00.000000000', dtype='datetime64[ns]')lat()int6440standard_name :grid_latitudeunits :degree_Narray(40)lon()int6480standard_name :grid_longitudeunits :degree_Earray(80)Attributes: (4)title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArray\n\n\nOr, we can use the dimensions names and their coordinates to access the same value:\n\ntemp.sel(time='2022-09-01', lat=40, lon=80)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray ()>\narray(0)\nCoordinates:\n    time     datetime64[ns] 2022-09-01\n    lat      int64 40\n    lon      int64 80\nAttributes:\n    title:          temperature across weather stations\n    standard_name:  air_temperature\n    units:          degree_c\n    description:    simple example of an xarray.DataArrayxarray.DataArray0array(0)Coordinates: (3)time()datetime64[ns]2022-09-01description :date of measurementarray('2022-09-01T00:00:00.000000000', dtype='datetime64[ns]')lat()int6440standard_name :grid_latitudeunits :degree_Narray(40)lon()int6480standard_name :grid_longitudeunits :degree_Earray(80)Attributes: (4)title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArray\n\n\nNotice that the result of this indexing is a 1x1 xarray.DataArray. This is because operations on an xarray.DataArray (resp. xarray.DataSet) always return another xarray.DataArray (resp. xarray.DataSet). In particular, operations returning scalar values will also produce xarray objects, so we need to cast them as numbers manually. See xarray.DataArray.item.\nMore about xarray indexing.\n\n\n8.4.1.3 Reduction\nxarray has implemented several methods to reduce an xarray.DataArray along any number of dimensions. One of the advantages of xarray.DataArray is that, if we choose to, it can carry over attributes when doing calculations. For example, we can calculate the average temperature at each weather station over time and obtain a new xarray.DataArray.\n\navg_temp = temp.mean(dim = 'time') \n# to keep attributes add keep_attrs = True\n\navg_temp.attrs = {'title':'average temperature over three days'}\navg_temp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (lat: 5, lon: 5)>\narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])\nCoordinates:\n  * lat      (lat) int64 30 40 50 60 70\n  * lon      (lon) int64 60 70 80 90 100\nAttributes:\n    title:    average temperature over three daysxarray.DataArraylat: 5lon: 51.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0array([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])Coordinates: (2)lat(lat)int6430 40 50 60 70standard_name :grid_latitudeunits :degree_Narray([30, 40, 50, 60, 70])lon(lon)int6460 70 80 90 100standard_name :grid_longitudeunits :degree_Earray([ 60,  70,  80,  90, 100])Attributes: (1)title :average temperature over three days\n\n\nMore about xarray computations.\n\n\n\n8.4.2 xarray.DataSet\nAn xarray.DataSet resembles an in-memory representation of a NetCDF file and consists of multiple variables (each being an xarray.DataArray), with dimensions, coordinates, and attributes, forming a self-describing dataset. Attributes can be specific to each variable, each dimension, or they can describe the whole dataset. The variables in an xarray.DataSet can have the same dimensions, share some dimensions, or have no dimensions in common. Let’s see an example of this.\n\n8.4.2.1 Create an xarray.DataSet\nFollowing our previous example, we can create an xarray.DataSet by combining the temperature data with the average temperature data. We also add some attributes that now describe the whole dataset, not only each variable.\n\n# make dictionaries with variables and attributes\ndata_vars = {'avg_temp': avg_temp,\n            'temp': temp}\n\nattrs = {'title':'temperature data at weather stations: daily and and average',\n        'description':'simple example of an xarray.Dataset'}\n\n# create xarray.Dataset\ntemp_dataset = xr.Dataset( data_vars = data_vars,\n                        attrs = attrs)\n\nTake some time to click through the data viewer and read through the variables and metadata in the dataset. Notice the following:\n\ntemp_dataset is a dataset with three dimensions (time, latitude, and longitude),\ntemp is a variable that uses all three dimensions in the dataset, and\naveg_temp is a variable that only uses two dimensions (latitude and longitude).\n\n\ntemp_dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (lat: 5, lon: 5, time: 3)\nCoordinates:\n  * lat       (lat) int64 30 40 50 60 70\n  * lon       (lon) int64 60 70 80 90 100\n  * time      (time) datetime64[ns] 2022-09-01 2022-09-02 2022-09-03\nData variables:\n    avg_temp  (lat, lon) float64 1.0 1.0 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0 1.0\n    temp      (time, lat, lon) int64 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2\nAttributes:\n    title:        temperature data at weather stations: daily and and average\n    description:  simple example of an xarray.Datasetxarray.DatasetDimensions:lat: 5lon: 5time: 3Coordinates: (3)lat(lat)int6430 40 50 60 70standard_name :grid_latitudeunits :degree_Narray([30, 40, 50, 60, 70])lon(lon)int6460 70 80 90 100standard_name :grid_longitudeunits :degree_Earray([ 60,  70,  80,  90, 100])time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03description :date of measurementarray(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (2)avg_temp(lat, lon)float641.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0title :average temperature over three daysarray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])temp(time, lat, lon)int640 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArrayarray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Attributes: (2)title :temperature data at weather stations: daily and and averagedescription :simple example of an xarray.Dataset\n\n\n\n\n8.4.2.2 Save and Reopen\nFinally, we want to save our dataset as a NetCDF file. To do this, specify the file path and use the .nc extension for the file name. Then save the dataset using the to_netcdf method with your file path. Opening NetCDF is similarly straightforward using xarray.open_dataset().\n\n# specify file path: don't forget the .nc extension!\nfp = os.path.join(os.getcwd(),'temp_dataset.nc') \n# save file\ntemp_dataset.to_netcdf(fp)\n\n# open to check:\ncheck = xr.open_dataset(fp)\ncheck\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (lat: 5, lon: 5, time: 3)\nCoordinates:\n  * lat       (lat) int64 30 40 50 60 70\n  * lon       (lon) int64 60 70 80 90 100\n  * time      (time) datetime64[ns] 2022-09-01 2022-09-02 2022-09-03\nData variables:\n    avg_temp  (lat, lon) float64 1.0 1.0 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0 1.0\n    temp      (time, lat, lon) int64 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2\nAttributes:\n    title:        temperature data at weather stations: daily and and average\n    description:  simple example of an xarray.Datasetxarray.DatasetDimensions:lat: 5lon: 5time: 3Coordinates: (3)lat(lat)int6430 40 50 60 70standard_name :grid_latitudeunits :degree_Narray([30, 40, 50, 60, 70])lon(lon)int6460 70 80 90 100standard_name :grid_longitudeunits :degree_Earray([ 60,  70,  80,  90, 100])time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03description :date of measurementarray(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (2)avg_temp(lat, lon)float64...title :average temperature over three daysarray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])temp(time, lat, lon)int64...title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArrayarray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Attributes: (2)title :temperature data at weather stations: daily and and averagedescription :simple example of an xarray.Dataset\n\n\n\n\n\n8.4.3 Exercise\nFor this exercise, we will use a dataset including time series of annual Arctic freshwater fluxes and storage terms. The data was produced for the publication Jahn and Laiho, 2020 about changes in the Arctic freshwater budget and is archived at the Arctic Data Center doi:10.18739/A2280504J\n\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A792bfc37-416e-409e-80b1-fdef8ab60033'\n\nresponse = requests.get(url)\nopen(\"FW_data_CESM_LW_2006_2100.nc\", \"wb\").write(response.content)\n\n208086\n\n\n\nfp = os.path.join(os.getcwd(),'FW_data_CESM_LW_2006_2100.nc')\nfw_data = xr.open_dataset(fp)\nfw_data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                          (time: 95, member: 11)\nCoordinates:\n  * time                             (time) float64 2.006e+03 ... 2.1e+03\n  * member                           (member) float64 1.0 2.0 3.0 ... 10.0 11.0\nData variables: (12/16)\n    FW_flux_Fram_annual_net          (time, member) float64 -1.26e+03 ... -2....\n    FW_flux_Barrow_annual_net        (time, member) float64 -600.7 ... -537.2\n    FW_flux_Nares_annual_net         (time, member) float64 -1.805e+03 ... -2...\n    FW_flux_Davis_annual_net         (time, member) float64 -2.313e+03 ... -3...\n    FW_flux_BSO_annual_net           (time, member) float64 -859.2 ... -993.2\n    FW_flux_Bering_annual_net        (time, member) float64 2.351e+03 ... 3.1...\n    ...                               ...\n    Solid_FW_flux_BSO_annual_net     (time, member) float64 -26.77 ... -35.43\n    Solid_FW_flux_Bering_annual_net  (time, member) float64 56.3 86.62 ... 22.87\n    runoff_annual                    (time, member) float64 3.39e+03 ... 3.97...\n    netPrec_annual                   (time, member) float64 2.019e+03 ... 2.1...\n    Liquid_FW_storage_Arctic_annual  (time, member) float64 8.125e+04 ... 9.7...\n    Solid_FW_storage_Arctic_annual   (time, member) float64 1.828e+04 ... 7.6...\nAttributes:\n    creation_date:   02-Jun-2020 15:38:31\n    author:          Alexandra Jahn, CU Boulder, alexandra.jahn@colorado.edu\n    title:           Annual timeseries of freshwater data from the CESM Low W...\n    description:     Annual mean Freshwater (FW) fluxes and storage relative ...\n    data_structure:  The data structure is |Ensemble member | Time (in years)...xarray.DatasetDimensions:time: 95member: 11Coordinates: (2)time(time)float642.006e+03 2.007e+03 ... 2.1e+03long_name :time in years, 1920-2100array([2006., 2007., 2008., 2009., 2010., 2011., 2012., 2013., 2014., 2015.,\n       2016., 2017., 2018., 2019., 2020., 2021., 2022., 2023., 2024., 2025.,\n       2026., 2027., 2028., 2029., 2030., 2031., 2032., 2033., 2034., 2035.,\n       2036., 2037., 2038., 2039., 2040., 2041., 2042., 2043., 2044., 2045.,\n       2046., 2047., 2048., 2049., 2050., 2051., 2052., 2053., 2054., 2055.,\n       2056., 2057., 2058., 2059., 2060., 2061., 2062., 2063., 2064., 2065.,\n       2066., 2067., 2068., 2069., 2070., 2071., 2072., 2073., 2074., 2075.,\n       2076., 2077., 2078., 2079., 2080., 2081., 2082., 2083., 2084., 2085.,\n       2086., 2087., 2088., 2089., 2090., 2091., 2092., 2093., 2094., 2095.,\n       2096., 2097., 2098., 2099., 2100.])member(member)float641.0 2.0 3.0 4.0 ... 9.0 10.0 11.0long_name :Ensemble member, 1-11array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])Data variables: (16)FW_flux_Fram_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Fram Strait, relative to 34.8array([[-1259.970567, -1264.007662, -1043.912842, ..., -1127.209397,\n        -1094.937548, -1068.404606],\n       [-1211.286894, -1479.531102, -1018.166211, ..., -1243.711036,\n        -1267.207286, -1114.552581],\n       [-1280.228564, -1385.553358,  -880.902707, ..., -1289.849198,\n        -1050.289977, -1041.592238],\n       ...,\n       [-1994.723559, -2192.810731, -1867.436333, ..., -1801.993229,\n        -2151.202886, -2616.74365 ],\n       [-2312.975878, -1882.595866, -1626.129161, ..., -2050.419246,\n        -2007.994165, -2369.86382 ],\n       [-1869.903721, -1941.767155, -1944.454371, ..., -1797.444957,\n        -2291.658043, -2133.186992]])FW_flux_Barrow_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Barrow Strait, relative to 34.8array([[-600.656931, -591.794563, -623.46547 , ..., -530.954288, -557.466782,\n        -540.397683],\n       [-545.381073, -651.15272 , -647.759741, ..., -554.62912 , -614.254495,\n        -681.241813],\n       [-676.367103, -597.249584, -526.308636, ..., -458.975897, -539.056379,\n        -600.418649],\n       ...,\n       [-495.781816, -343.416049, -572.034808, ..., -547.660529, -503.001068,\n        -575.476408],\n       [-508.571693, -385.524045, -505.368339, ..., -560.393935, -432.867012,\n        -501.710328],\n       [-478.525844, -380.776262, -525.255738, ..., -418.526205, -514.73683 ,\n        -537.165685]])FW_flux_Nares_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Nares Strait, relative to 34.8array([[-1805.094645, -1750.974399, -1736.428853, ..., -1790.293088,\n        -1630.279788, -1698.111863],\n       [-1661.738712, -1906.896808, -1756.856956, ..., -1846.616134,\n        -1754.973196, -1978.856594],\n       [-1956.029601, -1729.47722 , -1506.906015, ..., -1742.673838,\n        -1613.488436, -1857.85344 ],\n       ...,\n       [-2835.663916, -2685.863919, -3123.0961  , ..., -2930.78795 ,\n        -3075.907956, -2954.079147],\n       [-2767.255034, -2435.987932, -2852.725589, ..., -3132.779162,\n        -2879.811506, -2758.960293],\n       [-2801.006622, -2418.965807, -2907.765449, ..., -2699.143264,\n        -3153.186248, -2825.880317]])FW_flux_Davis_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Davis Strait, relative to 34.8array([[-2312.903562, -2206.193327, -2513.938315, ..., -2452.741479,\n        -2178.565682, -2496.419307],\n       [-2442.875007, -2302.612039, -2550.956543, ..., -2594.189567,\n        -2411.537129, -2491.906155],\n       [-2380.667757, -2460.765939, -2355.580677, ..., -2566.93496 ,\n        -2292.468786, -2443.485248],\n       ...,\n       [-3222.803208, -3277.824803, -3791.472809, ..., -3864.113295,\n        -3704.395263, -3775.769167],\n       [-3091.625499, -3293.624915, -3857.535966, ..., -3733.405415,\n        -3404.497911, -3538.640386],\n       [-3657.58114 , -3137.665787, -3346.385689, ..., -3778.231506,\n        -3182.004745, -3294.19718 ]])FW_flux_BSO_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through the Barents Sea Opening, relative to 34.8array([[ -859.243059,  -942.211781,  -723.703298, ..., -1036.046746,\n        -1055.237416,  -736.846205],\n       [ -878.919322,  -862.165165,  -760.842132, ...,  -981.459231,\n        -1017.759096,  -719.537064],\n       [ -867.125354,  -771.0794  ,  -883.947051, ...,  -823.744405,\n        -1084.024653,  -832.384503],\n       ...,\n       [-1201.528063,  -956.321947,  -989.990348, ...,  -928.170124,\n        -1522.813339, -1333.944058],\n       [-1175.496328,  -992.809814, -1042.238636, ..., -1027.30427 ,\n        -1012.655464, -1161.633369],\n       [-1108.045221, -1149.213528,  -974.059705, ...,  -944.651881,\n        -1351.228427,  -993.196776]])FW_flux_Bering_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Bering Strait, relative to 34.8array([[2351.369668, 2345.677436, 2046.439566, ..., 2298.623674, 2036.719667,\n        1907.280919],\n       [2481.513182, 2178.085803, 2490.061059, ..., 2760.976043, 2139.010713,\n        2320.373448],\n       [2514.857782, 2488.589714, 2864.883346, ..., 2666.687224, 2805.529561,\n        1831.068287],\n       ...,\n       [2212.93228 , 2305.747855, 3165.795471, ..., 2682.178162, 2137.858094,\n        2769.764568],\n       [2402.817047, 2871.272846, 2241.49669 , ..., 2899.779166, 2921.583788,\n        3124.232717],\n       [2553.009217, 2738.646198, 2402.378728, ..., 2832.505199, 2564.175895,\n        3110.720966]])Solid_FW_flux_Fram_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Fram Strait, relative to 34.8array([[-2355.201527, -3079.918585, -1900.235976, ..., -2379.143692,\n        -2263.860412, -1740.013499],\n       [-2072.011081, -2483.281102, -2397.287222, ..., -2044.468084,\n        -2757.085258, -2187.036132],\n       [-2495.74312 , -1468.060247, -2391.734318, ..., -2991.861612,\n        -2881.262317, -2262.942851],\n       ...,\n       [ -989.394195, -1285.473222,  -763.780509, ...,  -657.173252,\n         -940.635132, -2216.186576],\n       [-1176.377381,  -715.543546,  -763.626743, ...,  -876.188872,\n         -550.202924, -1673.306079],\n       [ -842.80755 , -1236.592837,  -685.109519, ...,  -585.170298,\n        -1213.069558, -1393.501459]])Solid_FW_flux_Barrow_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Barrow Strait, relative to 34.8array([[ -5.907839, -11.016486,   2.898278, ...,  -6.772   ,  -2.346885,\n         -4.972206],\n       [ -5.405102, -12.325443,  -2.837828, ...,  -1.54938 ,  -1.524433,\n         -3.239178],\n       [-13.615343,  -3.321843,   3.297263, ...,  17.846225,  -8.849867,\n         -1.40733 ],\n       ...,\n       [ -4.252253,  -4.74888 ,  -2.318929, ...,  -0.692743,  -3.516814,\n         -7.24111 ],\n       [ -2.473455,  -2.43267 ,  -4.119768, ...,  -3.041222,  -0.642326,\n         -3.151623],\n       [ -4.023888,  -4.630743,  -6.034809, ...,  -3.946639,  -2.759937,\n         -3.67783 ]])Solid_FW_flux_Nares_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Nares Strait, relative to 34.8array([[-443.523805, -369.69004 , -319.816739, ..., -359.434137, -331.597514,\n        -351.161639],\n       [-402.060722, -365.66951 , -455.727733, ..., -359.163976, -398.079664,\n        -312.769165],\n       [-309.118123, -407.329009, -518.392379, ..., -380.334965, -392.379044,\n        -384.005149],\n       ...,\n       [-322.681791, -285.266534, -360.379804, ..., -249.28949 , -291.504876,\n        -301.008009],\n       [-214.401449, -252.253729, -303.416623, ..., -295.322261, -282.239789,\n        -357.222407],\n       [-361.437905, -301.694338, -259.185245, ..., -248.62952 , -255.963657,\n        -396.696598]])Solid_FW_flux_Davis_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Davis Strait, relative to 34.8array([[-647.548166, -690.769077, -803.010762, ..., -637.388108, -669.678922,\n        -636.923716],\n       [-738.654959, -712.727442, -787.469841, ..., -741.271782, -734.0404  ,\n        -652.683568],\n       [-607.804625, -582.211375, -735.954593, ..., -633.625357, -651.638696,\n        -628.742061],\n       ...,\n       [-547.210914, -467.312263, -533.798556, ..., -565.575028, -573.092124,\n        -579.833192],\n       [-381.382969, -369.539971, -560.781192, ..., -423.884627, -600.9458  ,\n        -596.006766],\n       [-544.597115, -384.419366, -492.503528, ..., -482.520345, -479.724188,\n        -568.860609]])Solid_FW_flux_BSO_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through the Barents Sea Opening, relative to 34.8array([[-2.677222e+01, -2.198672e+02, -7.108708e+01, ..., -8.265375e+00,\n        -7.027574e+01, -2.478501e-01],\n       [-1.119710e+00, -1.611613e+01, -9.113674e+01, ..., -7.502441e+01,\n        -4.567288e+01, -8.217717e+01],\n       [-1.862617e+01, -4.320776e+00, -1.400749e+02, ..., -1.114686e+01,\n        -2.599572e+02, -2.824174e+01],\n       ...,\n       [-2.107002e+01, -1.972472e+01, -1.322291e+00, ..., -1.250803e+01,\n         0.000000e+00, -8.715960e+00],\n       [-1.129319e-01, -3.020100e+00, -9.972180e+00, ..., -6.321711e+00,\n        -1.357067e-01, -5.525681e-01],\n       [-1.324610e-02, -1.583726e+01, -2.125017e-01, ..., -2.465729e+00,\n        -3.163840e-01, -3.543094e+01]])Solid_FW_flux_Bering_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Bering Strait, relative to 34.8array([[ 56.300783,  86.619422,  21.629406, ...,  45.369409, 120.205643,\n          6.023335],\n       [ 52.854622,  68.339477, 139.181478, ...,  79.513795,  82.273825,\n        216.946026],\n       [ 64.470058,  16.432358,  33.79051 , ...,  36.5111  ,  51.494807,\n        -83.580959],\n       ...,\n       [ 58.127421,  22.637266,  79.579919, ...,  48.221805,  29.201449,\n        146.166502],\n       [ 46.223662,  44.331598,   9.747562, ...,  44.638335,  63.846993,\n         49.628882],\n       [  4.125923,  26.376146,  44.08245 , ...,  35.386517, 144.93525 ,\n         22.865989]])runoff_annual(time, member)float64...units :km3/yrlong_name :FW flux from river runoff into the Arctic Ocean domain, relative to 34.8array([[3389.793118, 3653.250059, 3255.904314, ..., 3484.324351, 3762.057224,\n        3233.230548],\n       [2991.152992, 3493.827835, 3384.707049, ..., 3571.643922, 3489.125416,\n        3641.709685],\n       [3534.397219, 3128.514068, 3354.798516, ..., 3304.537755, 3539.266607,\n        3084.447612],\n       ...,\n       [3665.258107, 3551.863606, 3809.485292, ..., 3493.525529, 3663.667382,\n        4321.328755],\n       [3743.83261 , 3378.368684, 3672.490382, ..., 3739.293615, 3739.029349,\n        3973.012085],\n       [3552.160713, 3480.78009 , 3710.884772, ..., 3580.675064, 4001.506981,\n        3973.911416]])netPrec_annual(time, member)float64...units :km3/yrlong_name :Net FW flux from precipitation minus evaporation over the Arctic Ocean domain, relative to 34.8array([[2019.436111, 2208.638737, 1936.136054, ..., 2049.989982, 1959.873063,\n        1750.749748],\n       [1945.582104, 2039.416637, 2085.04094 , ..., 1870.845855, 1965.883031,\n        2057.552438],\n       [2032.998228, 2047.057141, 1904.70621 , ..., 1894.083722, 1825.828287,\n        2026.627389],\n       ...,\n       [1858.080937, 1946.276296, 2100.604079, ..., 1994.116693, 2114.804976,\n        2423.484939],\n       [1915.662721, 1961.74668 , 1920.628415, ..., 1993.625762, 1744.708491,\n        2245.915281],\n       [1767.588737, 1904.765559, 1953.973994, ..., 1760.192971, 2384.175174,\n        2118.712754]])Liquid_FW_storage_Arctic_annual(time, member)float64...units :km3long_name :FW storage in the Arctic Ocean domain, , relative to 34.8. Ignoring any negative FW (water above reference salinity)array([[ 81250.414556,  85443.803218,  80863.782256, ...,  86120.03671 ,\n         81999.322618,  87066.701325],\n       [ 82567.489008,  85437.534745,  81016.233259, ...,  86801.295397,\n         81720.408045,  85405.906043],\n       [ 83435.689171,  84300.641386,  81701.536079, ...,  88202.733212,\n         82363.689791,  84802.874562],\n       ...,\n       [ 99179.037787, 100587.947588,  99713.659894, ..., 105329.776774,\n        101183.194676,  96731.099064],\n       [ 99022.701221, 100071.990872, 100428.151173, ..., 106023.385394,\n        101243.934883,  97353.008726],\n       [ 99884.660244, 101526.449779, 100904.197851, ..., 106283.670855,\n        101106.948969,  97309.828159]])Solid_FW_storage_Arctic_annual(time, member)float64...units :km3long_name :FW storage in sea ice and snow in the Arctic Ocean domain, relative to 34.8array([[18283.302344, 16436.957815, 18942.480543, ..., 19105.369531,\n        17193.142364, 16026.957959],\n       [17228.15868 , 16184.248407, 19493.271233, ..., 18736.727461,\n        17731.125586, 18033.048197],\n       [16582.390706, 17113.875851, 19081.541068, ..., 18073.074623,\n        16677.630513, 18713.913839],\n       ...,\n       [ 8286.152773,  7618.147979,  7159.460183, ...,  7615.242652,\n         7314.728945,  8332.238818],\n       [ 7925.266   ,  8366.732533,  7583.306716, ...,  7247.856691,\n         6969.390301,  7445.873087],\n       [ 6850.743786,  7492.329279,  7275.546906, ...,  7656.442802,\n         7476.367423,  7656.280716]])Attributes: (5)creation_date :02-Jun-2020 15:38:31author :Alexandra Jahn, CU Boulder, alexandra.jahn@colorado.edutitle :Annual timeseries of freshwater data from the CESM Low Warming Ensembledescription :Annual mean Freshwater (FW) fluxes and storage relative to 34.8 shown in Jahn and Laiho, GRL, 2020, calculated from the 11-member Community Earth System Model (CESM) Low Warming Ensemble output (Sanderson et al., 2017, Earth Syst. Dynam., 8, 827-847. doi: 10.5194/esd-8-827-2017). These 11 ensemble members were branched from the first 11 ensemble members of the CESM Large Ensemble (companion data file) at the end of 2005. Convention for the fluxes is that positive fluxes signify a source of FW to the Arctic and negative fluxes are a sink/export of FW for the Arctic. FW fluxes are the net fluxes through a strait over the full ocean depth, adding up any positive and negative fluxes. Liquid FW storage is calculated over the full depth of the ocean but ignoring any negative FW (resulting from salinties over 34.8). Solid FW storage includes FW stored in sea ice and FW stored in snow on sea ice. Surface fluxes and FW storage is calculated over the Arctic domain bounded by Barrow Strait, Nares Strait, Bering Strait, Fram Strait, and the Barents Sea Opeing (BSO). Davis Strait fluxes are included for reference only and are outside of the Arctic domain. A map showing the domain and the location of the straits can be found in Jahn and Laiho, GRL, 2020.data_structure :The data structure is |Ensemble member | Time (in years)|. All data are annual means. The data covers 2006-2100. There are 11 ensemble members.\n\n\n\nHow many dimensions does the runoff_annual variable have? What are the coordinates for the second dimension of this variable?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe can see in the object viewer that the runoff_annual variable has two dimensions: time and member, in that order. We can also access the dimensions by calling:\n\nfw_data.runoff_annual.dims\n\nThe second dimensions is member. Near the top of the object viewer, under coordinates, we can see that that member’s coordinates is an array from 1 to 11. We can directly see this array by calling:\n\nfw_data.member\n\n\n\n\n\nSelect the values for the second member of the netPrec_annual variable.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmember2 = fw_data.netPrec_annual.sel(member=2)\n\n\n\n\n\nWhat is the maximum value of the second member of the netPrec_annual variable in the time period 2022 to 2100? Hint.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBased on our previous answer, this maximum is:\n\nx_max = member2.loc[2022:2100].max()\nxmax.item(0)\n\nNotice we had to use item to transform the array into a number."
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#tabular-data-and-netcdf",
    "href": "sections/08-data-structures-netcdf.html#tabular-data-and-netcdf",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.5 Tabular Data and NetCDF",
    "text": "8.5 Tabular Data and NetCDF\nUndoubtedly, tabular data is one of the most popular data formats. In this last section, we will discuss the relation between tabular data and the NetCDF data format and how to transform a pandas.DataFrame into an xarray.DataSet.\n\n8.5.1 Tabular to NetCDF\nWe assume our starting point is tabular data that meets the criteria for tidy data, which means:\n\nEach column holds a different variable.\nEach row holds a different observation.\n\nTake, for example, this tidy data subset from our exercise about weather stations measuring temperature: \nTo understand how this will transform into NetCDF format, we first need to identify which columns will act as dimensions and which as variables. We can also think of the values of the dimension columns as the coordinates in the xarray.DataSet. The diagram below shows how these columns transform into variables, dimensions, and coordinates.\n\nTabular formats like csv do not offer an intrinsic way to encode attributes for the dimensions or variables, this is why we don’t see any attributes in the resulting NetCDF data. One of the most significant advantages of NetCDF is its self-describing properties.\n\n\n8.5.2 pandas to xarray\nWhat does the previous example look like when working with pandas and xarray?\nLet’s work with a csv file containing the previous temperature measurements. Essentially, we need to read this file as a pandas.DataFrame and then use the pandas.DataFrame.to_xarray() method, taking into account that the dimensions of the resulting xarray.DataSet will be formed using the index column(s) of the pandas.DataFrame. In this case, we know the first three columns will be our dimension columns, so we need to group them as a multindex for the pandas.DataFrame. We can do this by using the index_col argument directly when we read in the csv file.\n\nfp = os.path.join(os.getcwd(),'netcdf_temp_data.csv') \n\n\n# specify columns representing dimensions\ndimension_columns = [0,1,2]\n\n# read file\ntemp = pd.read_csv(fp, index_col=dimension_columns)\ntemp\n\n\n\n\n\n  \n    \n      \n      \n      \n      temperature\n    \n    \n      longitude\n      latitude\n      date\n      \n    \n  \n  \n    \n      60\n      30\n      2022-09-01\n      0\n    \n    \n      2022-09-02\n      1\n    \n    \n      40\n      2022-09-01\n      0\n    \n    \n      2022-09-02\n      1\n    \n    \n      70\n      30\n      2022-09-01\n      0\n    \n    \n      2022-09-02\n      1\n    \n    \n      40\n      2022-09-01\n      0\n    \n    \n      2022-09-02\n      1\n    \n  \n\n\n\n\nAnd this is our resulting xarray.DataSet:\n\ntemp.to_xarray()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:      (longitude: 2, latitude: 2, date: 2)\nCoordinates:\n  * longitude    (longitude) int64 60 70\n  * latitude     (latitude) int64 30 40\n  * date         (date) object '2022-09-01' '2022-09-02'\nData variables:\n    temperature  (longitude, latitude, date) int64 0 1 0 1 0 1 0 1xarray.DatasetDimensions:longitude: 2latitude: 2date: 2Coordinates: (3)longitude(longitude)int6460 70array([60, 70])latitude(latitude)int6430 40array([30, 40])date(date)object'2022-09-01' '2022-09-02'array(['2022-09-01', '2022-09-02'], dtype=object)Data variables: (1)temperature(longitude, latitude, date)int640 1 0 1 0 1 0 1array([[[0, 1],\n        [0, 1]],\n\n       [[0, 1],\n        [0, 1]]])Attributes: (0)\n\n\nFor further reading and examples about switching between pandas and xarray you can visit the following:\n\nxarray’s Frequently Asked Questions\nxarray’s documentation about working with pandas\npandas.DataFrame.to_xarray documentation"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html",
    "href": "sections/09-parallel-with-dask.html",
    "title": "9  Parallelization with Dask",
    "section": "",
    "text": "Become familiar with the Dask processing workflow:\n\nWhat are the client, scheduler, workers, and cluster\nUnderstand delayed computations and “lazy” evaluation\nObtain information about computations via the Dask dashboard\n\nLearn to load data and specify partition/chunk sizes of dask.arrays/dask.dataframes \nIntegrate xarray and rioxarray with Dask for geospatial computations\nShare best practices and resources for further reading"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#introduction",
    "href": "sections/09-parallel-with-dask.html#introduction",
    "title": "9  Parallelization with Dask",
    "section": "9.2 Introduction",
    "text": "9.2 Introduction\nDask is a library for parallel computing in Python. It can scale up code to use your personal computer’s full capacity or distribute work in a cloud cluster. By mirroring APIs of other commonly used Python libraries, such as Pandas and NumPy, Dask provides a familiar interface that makes it easier to parallelize your code. In this lesson, we will get acquainted with some of Dask’s most commonly used objects and Dask’s way of distributing and evaluating computations."
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#dask-cluster",
    "href": "sections/09-parallel-with-dask.html#dask-cluster",
    "title": "9  Parallelization with Dask",
    "section": "9.3 Dask Cluster",
    "text": "9.3 Dask Cluster\nWe can deploy a Dask cluster on a single machine or an actual cluster with multiple machines. The cluster has three main components for processing computations in parallel. These are the client, the scheduler and the workers.\n\nWhen we code, we communicate directly with the client, which is responsible for submitting tasks to be executed to the scheduler.\nAfter receiving the tasks from the client, the scheduler determines how tasks will be distributed among the workers and coordinates them to process tasks in parallel.\nFinally, the workers compute tasks and store and return computations results. Workers can be threads, processes, or separate machines in a cluster. We won’t go into how to choose between threads and processes, but there are best practices to select the right one. You can read more about threads and processes here.\n\nTo interact with the client and generate tasks that can be processed in parallel we need to use Dasks’ objects to read our data. In this lesson, we will see examples of how to use dask.dataframes and dask.arrays. Apart from data frames and arrays, other Dask objects can be used to parallelize your workflow.\n\n\n\nM. Schmitt, Understanding Dask Architecture: The Client, Scheduler and Workers\n\n\n\n9.3.1 Setting up a Local Cluster\nWe can create a local cluster as follows:\n\nfrom dask.distributed import LocalCluster, Client\n\n\ncluster = LocalCluster(n_workers=5, memory_limit=0.1, processes=False)\ncluster\n\n\nAnd then we create a client to connect to our cluster:\n\nclient = Client(cluster)\nclient\n\n\nThis is a good place to learn more about different Dask clusters.\n\n\n9.3.2 Dask Dashboard\nWe chose to use the Dask cluster in this lesson instead of the default Dask scheduler to take advantage of the cluster dashboard, which offers live monitoring of the performance and progress of our computations. When we set up a cluster, we can see the dashboard’s address by looking at either the client or the cluster. The dashboard’s main page shows diagnostics about:\n\nthe cluster’s and individual worker’s memory usage,\nnumber of tasks being processed by each worker,\nindividual tasks being processed across workers, and\nprogress towards completion of individual tasks.\n\nThere’s much to talk about interpreting the Dask dashboard’s diagnostics. We recommend this documentation to understand the basics of the dashboard diagnostics and this video as a deeper dive into the dashboard’s functions.\n\n\n\nA Dask dashboard."
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#dask.dataframes",
    "href": "sections/09-parallel-with-dask.html#dask.dataframes",
    "title": "9  Parallelization with Dask",
    "section": "9.4 dask.dataframes",
    "text": "9.4 dask.dataframes\nWhen we analyze tabular data, we usually start our analysis by loading it into memory as a Pandas DataFrame. But what if this data does not fit in memory? Or maybe our analyzes crash because we run out of memory. These scenarios are typical entry points into parallel computing. In such cases, Dask’s scalable alternative to a Pandas DataFrame is the dask.dataframe. A dask.dataframe comprises many pd.DataFrames, each containing a subset of rows of the original dataset. We call each of these pandas pieces a partition of the dask.dataframe.\n\n\n\nDask Array design (dask documentation)\n\n\n\n9.4.1 Reading a csv\nTo get familiar with dask.dataframes, we will use tabular data of soil moisture measurements at six forest stands in northeastern Siberia. The data has been collected since 2014 and is archived at the Arctic Data Center (Loranty & Alexander, doi:10.18739/A24B2X59C). Just as we did in the previous lesson, we will download the data using the requests package and the data’s URL obtained from the Arctic Data Center.\n\nimport os              \nimport requests \n\nimport dask.dataframe as dd\n\n\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A27e4043d-75eb-4c4f-9427-0d442526c154'\n\nresponse = requests.get(url)\nopen(\"dg_soil_moisture.csv\", \"wb\").write(response.content)\n\nIn the Arctic Data Center metadata we can see this file is 115 MB. To import this file as a dask.dataframe with more than one partition, we need to specify the size of each partition with the blocksize parameter. In this example, we will split the data frame into six partitions, meaning a block size of approximately 20 MB.\n\nfp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')\ndf = dd.read_csv(fp, blocksize = '20MB' , encoding='ISO-8859-1')\ndf\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nAbout the encoding parameter: If we try to import the file directly, we will receive an UnicodeDecodeError. We can run the following code to find the file’s encoding and add the appropriate encoding to dask.dataframe.read_csv.\n\nimport chardet\nfp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')\nwith open(fp, 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(100000))\nresult\n\n\n\n\nNotice that we cannot see any values in the data frame. This is because Dask has not really loaded the data. It will wait until we explicitly ask it to print or compute something to do so. However, we can still do df.head(). It’s not costly for memory to access a few data frame rows.\n\ndf.head(2)\n\n\n\n9.4.2 Lazy Computations\nThe application programming interface (API) of a dask.dataframe is a subset of the pandas.DataFrame API. So if you are familiar with pandas, many of the core pandas.DataFrame methods directly translate to dask.dataframes.\n\naverages = df.groupby('year').mean()\naverages\n\nNotice that we cannot see any values in the resulting data frame. A major difference between pandas.DataFrames and dask.dataframes is that dask.dataframes are “lazy”. This means an object will queue transformations and calculations without executing them until we explicitly ask for the result of that chain of computations using the compute method. Once we run compute, the scheduler can allocate memory and workers to execute the computations in parallel. This kind of lazy evaluation (or delayed computation) is how most Dask workloads work. This varies from eager evaluation methods and functions, which start computing results right when they are executed.\nBefore calling compute on an object, open the Dask dashboard to see how the parallel computation is happening.\n\naverages.compute()"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#dask.arrays",
    "href": "sections/09-parallel-with-dask.html#dask.arrays",
    "title": "9  Parallelization with Dask",
    "section": "9.5 dask.arrays",
    "text": "9.5 dask.arrays\nAnother common object we might want to parallelize is a NumPy array. The equivalent Dask object is the dask.array, which coordinates many NumPy arrays that may live on disk or other machines. Each of these NumPy arrays within the dask.array is called a chunk. Choosing how these chunks are arranged within the dask.array and their size can significantly affect the performance of our code. Here you can find more information about chunks.\n\n\n\nDask Array design (dask documentation)\n\n\nIn this short example we will create a 200x500 dask.array by specifying chunk sizes of 100x100.\n\nimport numpy as np\n\nimport dask.array as da\n\n\ndata = np.arange(100_000).reshape(200, 500)\na = da.from_array(data, chunks=(100, 100))\n\nComputations for dask.arrays also work lazily. We need to call compute to trigger computations and bring the result to memory.\n\na.mean()\n\n\na.mean().compute()"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#dask-and-xarray",
    "href": "sections/09-parallel-with-dask.html#dask-and-xarray",
    "title": "9  Parallelization with Dask",
    "section": "9.6 Dask and xarray",
    "text": "9.6 Dask and xarray\nIn the future, it might be more common having to read some big array-like dataset (like a high-resolution multiband raster) than creating one from scratch using NumPy. In this case, it can be useful to use the xarray module and its extender rioxarray together with Dask. In the previous lesson, Data Structures and Formats for Large Data, we explore how to use the xarray package to work with labelled arrays. rioxarray extends xarray with the rio accessor, which stands for “raster input and output”.\nIt is simple to wrap Dask around xarray objects. We only need to specify the number of chunks as an argument when we are reading in a dataset (see also [1]).\n\n9.6.1 Open .tif file\nAs an example, let’s do a Normalized Difference Vegetation Index (NDVI) calculation using remote sensing imagery collected by aerial vehicles over northeastern Siberia (Loranty, Forbath, Talucci, Alexander, DeMarco, et al. 2020. doi:10.18739/A2ZC7RV6H.). The NDVI is an index commonly used to check if an area has live green vegetation or not. It can also show the difference between water, plants, bare soil, and human-made structures, among other things.\nThe NDVI is calculated using the near-infrared and red bands of the satellite image. The formula is\n\\[NDVI = \\frac{NIR - Red}{NIR + Red}.\\]\nFirst, we download the data for the near-infrared (NIR) and red bands from the Arctic Data Center:\n\n# download red band\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3Aac25a399-b174-41c1-b6d3-09974b161e5a'\nresponse = requests.get(url)\nopen(\"RU_ANS_TR2_FL005M_red.tif\", \"wb\").write(response.content)\n\n# download nir band\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A1762205e-c505-450d-90ed-d4f3e4c302a7'\nresponse = requests.get(url)\nopen(\"RU_ANS_TR2_FL005M_nir.tif\", \"wb\").write(response.content)\n\n79182870\n\n\nBecause these are .tif files and have geospatial metadata, we will use rioxarray to read them. You can find more information about rioxarray here.\nTo indicate we will open these .tif files with dask.arrays as the underlying object to the xarray.DataArray (instead of a numpy.array), we need to specify either a shape or the size in bytes for each chunk. Both files are 76 MB, so let’s have chunks of 15 MB to have roughly six chunks.\n\nimport rioxarray as rioxr\n\n\n# read in the file\nfp_red = os.path.join(os.getcwd(),\"RU_ANS_TR2_FL005M_red.tif\")\nred = rioxr.open_rasterio(fp_red, chunks = '15MB')\n\nWe can see a lot of useful information here:\n\nThere are eight chunks in the array. We were aiming for six, but this often happens with how Dask distributes the memory (76MB is not divisible by 6).\nThere is geospatial information (transformation, CRS, resolution) and no-data values.\nThere is an unnecessary dimension: a constant value for the band. So our next step is to squeeze the array to flatten it.\n\n\n# getting rid of unnecessary dimension\nred = red.squeeze()\n\nNext, we read in the NIR band and do the same pre-processing:\n\n# open data\nfp_nir = os.path.join(os.getcwd(),\"RU_ANS_TR2_FL005M_nir.tif\")\nnir = rioxr.open_rasterio(fp_nir, chunks = '15MB')\n\n#squeeze\nnir = nir.squeeze()\n\n\n\n9.6.2 Calculating NDVI\nNow we set up the NDVI calculation. This step is easy because we can handle xarrays and Dask arrays as NumPy arrays for arithmetic operations. Also, both bands have values of type float32, so we won’t have trouble with the division.\n\nndvi = (nir - red) / (nir + red)\n\nWhen we look at the NDVI we can see the result is another dask.array, nothing has been computed yet. Remember, Dask computations are lazy, so we need to call compute() to bring the results to memory.\n\nndvi_values = ndvi.compute()\n\nAnd finally, we can see what these look like. Notice that xarray uses the value of the dimensions as labels along the x and y axes. We use robust=True to ignore the no-data values when plotting.\n\nndvi_values.plot(robust=True)"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#best-practices",
    "href": "sections/09-parallel-with-dask.html#best-practices",
    "title": "9  Parallelization with Dask",
    "section": "9.7 Best Practices",
    "text": "9.7 Best Practices\nDask is an exciting tool for parallel computing, but it may take a while to understand its nuances to make the most of it. There are many best practices and recommendations. These are some of the basic ones to take into consideration:\n\nFor data that fits into RAM, pandas, and NumPy can often be faster and easier to use than Dask workflows. The simplest solution can often be the best.\nWhile Dask may have similar APIs to pandas and NumPy, there are differences, and not all the methods for the pandas.DataFrames and numpy.arrays translate in the same way (or with the same efficiency) to Dask objects. When in doubt, always read the documentation.\nChoose appropriate chunk and partition sizes and layouts. This is crucial to best use how the scheduler distributes work. You can read here about best practices for chunking.\nAvoid calling compute repeatedly. It is best to group similar computations together and then compute once.\n\nFurther reading:\n\nA friendly article about common dask mistakes\nGeneral Dask best practices\ndask.dataframe best practices\ndask.array best practices"
  },
  {
    "objectID": "sections/10-geopandas.html",
    "href": "sections/10-geopandas.html",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "",
    "text": "Manipulating raster data with rasterio\nManipulating vector data with geopandas\nWorking with raster and vector data together"
  },
  {
    "objectID": "sections/10-geopandas.html#introduction",
    "href": "sections/10-geopandas.html#introduction",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "10.2 Introduction",
    "text": "10.2 Introduction\nIn this lesson, we’ll be working with geospatial raster and vector data to do an analysis on vessel traffic in south central Alaska. If you aren’t already familiar, geospatial vector data consists of points, lines, and/or polygons, which represent locations on the Earth. Geospatial vector data can have differing geometries, depending on what it is representing (eg: points for cities, lines for rivers, polygons for states.) Raster data uses a set of regularly gridded cells (or pixels) to represent geographic features.\nBoth geospatial vector and raster data have a coordinate reference system, which describes how the points in the dataset relate to the 3-dimensional sphereoid of Earth. A coordinate reference system contains both a datum and a projection. The datum is how you georeference your points (in 3 dimensions!) onto a spheroid. The projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. All coordinate reference systems require a datum. However, some coordinate reference systems are “unprojected” (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984.\nCoordinate reference systems are often referenced using a shorthand 4 digit code called an EPSG code. We’ll be working with two coordinate reference systems in this lesson with the following codes:\n\n3338: Alaska Albers\n4326: WGS84 (World Geodetic System 1984), used in GPS\n\nIn this lesson, we are going to take two datasets:\n\nAlaskan commercial salmon fishing statisical areas\nNorth Pacific and Arctic Marine Vessel Traffic Dataset\n\nand use them to calculate the total distance travelled by ships within each fishing area.\nThe high level steps will be\n\nread in the datasets\nreproject them so they are in the same projection\nextract a subset of the raster and vector data using a bounding box\nturn each polygon in the vector data into a raster mask\nuse the masks to calculate the total distance travelled (sum of pixels) for each fishing area"
  },
  {
    "objectID": "sections/10-geopandas.html#pre-processing-raster-data",
    "href": "sections/10-geopandas.html#pre-processing-raster-data",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "10.3 Pre-processing raster data",
    "text": "10.3 Pre-processing raster data\nFirst we need to load in our libraries. We’ll use geopandas for vector manipulation, rasterio for raster maniupulation.\nFirst, we’ll use requests to download the ship traffic raster from Kapsar et al.. We grab a one month slice from August, 2020 of a coastal subset of data with 1km resolution. To get the URL in the code chunk below, you can right click the download button for the file of interest and select “copy link address.”\n\nimport requests\n\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A6b847ab0-9a3d-4534-bf28-3a96c5fa8d72'\n\nresponse = requests.get(url)\nopen(\"Coastal_2020_08.tif\", \"wb\").write(response.content)\n\n1473505\n\n\nUsing rasterio, open the raster file, plot it, and look at the metadata. We use the with here as a context manager. This ensures that the connection to the raster file is closed and cleaned up when we are done with it.\n\nimport rasterio\nimport matplotlib.pyplot as plt\n\nwith rasterio.open(\"Coastal_2020_08.tif\") as ship_con:\n    # read in raster (1st band)\n    ships = ship_con.read(1)\n    ships_meta = ship_con.profile\n\nplt.imshow(ships)\nprint(ships_meta)\n\n{'driver': 'GTiff', 'dtype': 'float32', 'nodata': -3.3999999521443642e+38, 'width': 3087, 'height': 2308, 'count': 1, 'crs': CRS.from_epsg(3338), 'transform': Affine(999.7994153462766, 0.0, -2550153.29233849,\n       0.0, -999.9687691991521, 2711703.104608573), 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n\n\n\n\n\nYou’ll notice that we are saving two objects here, ships and ships_meta. Looking at the types of these two objects is useful to understand what rasterio is doing.\n\ntype(ships)\ntype(ships_meta)\n\nrasterio.profiles.Profile\n\n\nThe ships object is a numpy array, while the ships_meta is a special rasterio class called Profile. To understand why the raster data is represented as an array, and what that profile object is, let’s look into what raster data are, exactly.\n\nSource: Leah A. Wasser, Megan A. Jones, Zack Brym, Kristina Riemer, Jason Williams, Jeff Hollister, Mike Smorul. Raster 00: Intro to Raster Data in R. National Evologial Observatory Network (NEON).\nThe upper left panel of the figure above shows some satellite imagery data. These data are in raster format, which when you zoom in, really consist of regularly gridded pixels, each of which contails a value. When we plot these data, we can assign a color map to the pixel values, which generates the image we see. The data themselves, though, are just a two dimensional grid of numbers. Another way we might describe this is…an array! So, this is why raster data is represented in python using a numpy array.\nThis is all great, and the array of values is a lot of information, but there are some key items that are missing. This array isn’t imaginary, it represents a physical space on this earth, so where is all of that contextual information? The answer is in the rasterio profile object. This object contains all of the metadata needed to interpret the raster array. Here is what our ships_meta contains:\n'driver': 'GTiff',\n'dtype': 'float32',\n'nodata': -3.3999999521443642e+38,\n'width': 3087,\n'height': 2308,\n'count': 1,\n'crs': CRS.from_epsg(3338),\n'transform': Affine(999.7994153462766, 0.0, -2550153.29233849, 0.0, -999.9687691991521, 2711703.104608573),\n'tiled': False,\n'compress': 'lzw',\n'interleave': 'band'}\nThis object gives us critical information, like the CRS of the data, the no data value, and the transform. The transform is what allows us to move from image pixel (row, col) coordinates to and from geographic/projected (x, y) coordinates. The transform and the CRS are crititally important, and related. If the CRS are instructions for how the coordinates can be represented in space and on a flat surface (in the case of projected coordinate systems), then the transform describes how to locate the raster array positions in the correct coordinates given by the CRS.\nNote that since the array and the profile are in separate objects it is easy to lose track of one of them, accidentally overwrite it, etc. Try to adopt a naming convention that works for you because they usually need to work together in geospatial operations."
  },
  {
    "objectID": "sections/10-geopandas.html#pre-processing-vector-data",
    "href": "sections/10-geopandas.html#pre-processing-vector-data",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "10.4 Pre-processing vector data",
    "text": "10.4 Pre-processing vector data\nNow download a vector shapefile of commercial fishing districts in Alaska.\n\nurl = 'https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7c942c45-1539-4d47-b429-205499f0f3e4'\n\nresponse = requests.get(url)\nopen(\"Alaska_Commercial_Salmon_Boundaries.gpkg\", \"wb\").write(response.content)\n\n36544512\n\n\nRead in the data using geopandas.\n\nimport geopandas as gpd\n\ncomm = gpd.read_file(\"Alaska_Commercial_Salmon_Boundaries.gpkg\")\n\nNote the “pandas” in the library name “geopandas.” Our comm object is really just a special type of pandas data frame called a geodataframe. This means that in addition to any geospatial stuff we need to do, we can also just do regular pandas things on this data frame.\nFor example, we can get a list of column names (there are a lot!)\n\ncomm.columns.values\n\narray(['OBJECTID', 'GEOMETRY_START_DATE', 'GEOMETRY_END_DATE',\n       'STAT_AREA', 'STAT_AREA_NAME', 'FISHERY_GROUP_CODE',\n       'GIS_SERIES_NAME', 'GIS_SERIES_CODE', 'REGION_CODE',\n       'REGISTRATION_AREA_NAME', 'REGISTRATION_AREA_CODE',\n       'REGISTRATION_AREA_ID', 'REGISTRATION_LOCATION_ABBR',\n       'MANAGEMENT_AREA_NAME', 'MANAGEMENT_AREA_CODE', 'DISTRICT_NAME',\n       'DISTRICT_CODE', 'DISTRICT_ID', 'SUBDISTRICT_NAME',\n       'SUBDISTRICT_CODE', 'SUBDISTRICT_ID', 'SECTION_NAME',\n       'SECTION_CODE', 'SECTION_ID', 'SUBSECTION_NAME', 'SUBSECTION_CODE',\n       'SUBSECTION_ID', 'COAR_AREA_CODE', 'CREATOR', 'CREATE_DATE',\n       'EDITOR', 'EDIT_DATE', 'COMMENTS', 'STAT_AREA_VERSION_ID',\n       'Shape_Length', 'Shape_Area', 'geometry'], dtype=object)\n\n\nWe can also look at the head of the data frame:\n\ncomm.head\n\n<bound method NDFrame.head of      OBJECTID       GEOMETRY_START_DATE GEOMETRY_END_DATE  STAT_AREA  \\\n0          12 1975-01-01 00:00:00+00:00               NaT      33461   \n1          13 1975-01-01 00:00:00+00:00               NaT      33462   \n2          18 1978-01-01 00:00:00+00:00               NaT      33431   \n3          19 1980-01-01 00:00:00+00:00               NaT      33442   \n4          20 1980-01-01 00:00:00+00:00               NaT      33443   \n..        ...                       ...               ...        ...   \n860       959                       NaT               NaT      19241   \n861       960                       NaT               NaT      19242   \n862       961 1994-01-01 00:00:00+00:00               NaT      19245   \n863       962                       NaT               NaT      19250   \n864       963 1994-01-01 00:00:00+00:00               NaT      18252   \n\n                                 STAT_AREA_NAME FISHERY_GROUP_CODE  \\\n0        Tanana River mouth to Kantishna River                   B   \n1                 Kantishna River to Wood River                  B   \n2                    Toklik to Cottonwood Point                  B   \n3    Right Bank, Bishop Rock to Illinois Creek                   B   \n4      Left Bank, Cone Point to Illinois Creek                   B   \n..                                          ...                ...   \n860                               Kaliakh River                  B   \n861                                  Tsiu River                  B   \n862                             Midtimber River                  B   \n863                                  Seal River                  B   \n864                               Middle Italio                  B   \n\n    GIS_SERIES_NAME GIS_SERIES_CODE  REGION_CODE    REGISTRATION_AREA_NAME  \\\n0            Salmon               B            3                Yukon Area   \n1            Salmon               B            3                Yukon Area   \n2            Salmon               B            3                Yukon Area   \n3            Salmon               B            3                Yukon Area   \n4            Salmon               B            3                Yukon Area   \n..              ...             ...          ...                       ...   \n860          Salmon               B            1  Southeastern Alaska Area   \n861          Salmon               B            1  Southeastern Alaska Area   \n862          Salmon               B            1  Southeastern Alaska Area   \n863          Salmon               B            1  Southeastern Alaska Area   \n864          Salmon               B            1  Southeastern Alaska Area   \n\n     ... COAR_AREA_CODE         CREATOR               CREATE_DATE  \\\n0    ...             YU   Evelyn Russel 2006-03-26 00:00:00+00:00   \n1    ...             YU   Evelyn Russel 2006-03-26 00:00:00+00:00   \n2    ...             YL   Evelyn Russel 2006-03-26 00:00:00+00:00   \n3    ...             YU   Evelyn Russel 2006-03-26 00:00:00+00:00   \n4    ...             YU   Evelyn Russel 2006-03-26 00:00:00+00:00   \n..   ...            ...             ...                       ...   \n860  ...             A2   Evelyn Russel 2006-03-26 00:00:00+00:00   \n861  ...             A2   Evelyn Russel 2006-03-26 00:00:00+00:00   \n862  ...             A2   Evelyn Russel 2006-03-26 00:00:00+00:00   \n863  ...             A2   Evelyn Russel 2006-03-26 00:00:00+00:00   \n864  ...             A2  Sabrina Larsen 2017-05-05 00:00:00+00:00   \n\n             EDITOR                 EDIT_DATE  \\\n0    Sabrina Larsen 2017-02-02 00:00:00+00:00   \n1    Sabrina Larsen 2017-02-02 00:00:00+00:00   \n2    Sabrina Larsen 2017-02-02 00:00:00+00:00   \n3    Sabrina Larsen 2017-02-02 00:00:00+00:00   \n4    Sabrina Larsen 2017-02-02 00:00:00+00:00   \n..              ...                       ...   \n860  Sabrina Larsen                       NaT   \n861  Sabrina Larsen                       NaT   \n862  Sabrina Larsen                       NaT   \n863  Sabrina Larsen                       NaT   \n864            None                       NaT   \n\n                                              COMMENTS STAT_AREA_VERSION_ID  \\\n0    Yukon District, 6 Subdistrict and 6-A Section ...                 None   \n1    Yukon District, 6 Subdistrict and 6-B Section ...                 None   \n2      Yukon District and 3 Subdistrict until 1/1/1980                 None   \n3                                                 None                 None   \n4                                                 None                 None   \n..                                                 ...                  ...   \n860                                               None                 None   \n861                                               None                 None   \n862                                               None                 None   \n863                                               None                 None   \n864                                               None                 None   \n\n    Shape_Length Shape_Area                                           geometry  \n0       4.610183   0.381977  MULTIPOLYGON (((-151.32805 64.96913, -151.3150...  \n1       3.682421   0.321943  MULTIPOLYGON (((-149.96255 64.70518, -149.9666...  \n2       2.215641   0.198740  MULTIPOLYGON (((-161.39853 61.55463, -161.4171...  \n3       9.179852   0.382788  MULTIPOLYGON (((-153.15234 65.24944, -153.0761...  \n4       9.500826   0.378262  MULTIPOLYGON (((-152.99905 65.17027, -152.9897...  \n..           ...        ...                                                ...  \n860     0.223565   0.000408  MULTIPOLYGON (((-142.90787 60.09177, -142.9051...  \n861     0.030506   0.000006  MULTIPOLYGON (((-143.00416 60.07711, -143.0046...  \n862     0.019805   0.000012  MULTIPOLYGON (((-143.28504 60.05800, -143.2861...  \n863     0.096016   0.000238  MULTIPOLYGON (((-143.49701 60.04832, -143.5083...  \n864     0.016237   0.000006  MULTIPOLYGON (((-139.15063 59.30748, -139.1489...  \n\n[865 rows x 37 columns]>\n\n\nNote the existence of the geometry column. This is where the actual geospatial points that comprise the vector data are stored, and this brings up the important difference between raster and vector data - while raster data is regularly gridded at a specific resolution, vector data are just points in space.\n\nSource: Leah A. Wasser, Megan A. Jones, Zack Brym, Kristina Riemer, Jason Williams, Jeff Hollister, Mike Smorul. Raster 00: Intro to Raster Data in R. National Evologial Observatory Network (NEON).\nThe diagram above shows the three different types of geometries that geospatial vector data can take, points, lines or polygons. Whatever the geometry type, the geometry information (the x,y points) is stored in the column named geometry in the geopandas data frame. In this example, we have a dataset containing polygons of fishing districts. Each row in the dataset corresonds to a district, with unique attributes (the other columns in the dataset), and it’s own set of points defining the boundaries of the district, contained in the geometry column.\n\ncomm['geometry'][:5]\n\n0    MULTIPOLYGON (((-151.32805 64.96913, -151.3150...\n1    MULTIPOLYGON (((-149.96255 64.70518, -149.9666...\n2    MULTIPOLYGON (((-161.39853 61.55463, -161.4171...\n3    MULTIPOLYGON (((-153.15234 65.24944, -153.0761...\n4    MULTIPOLYGON (((-152.99905 65.17027, -152.9897...\nName: geometry, dtype: geometry\n\n\nSo, now we know where our x,y points are, where is all of the other information like the crs? With vector data, all of this information is contained within the geodataframe. We can access the crs attribute on the data frame and print it like so:\n\ncomm.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nNow that we know a little about what we are working with, let’s get this data ready to work with. First, we can make a plot of it just to see what we have.\n\ncomm.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nThis plot doesn’t look so good. Turns out, these data are in WGS 84 (EPSG 4326), as opposed to Alaska Albers (EPSG 3338), which is what our raster data is in. To make pretty plots, and allow our raster data and vector data to be analyzed together, we’ll need to reproject the vector data into 3338. To to this, we’ll use the to_crs method on our comm object, and specify as an argument the projection we want to transform to.\n\ncomm_3338 = comm.to_crs(\"EPSG:3338\")\n\ncomm_3338.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nMuch better!"
  },
  {
    "objectID": "sections/10-geopandas.html#crop-data-to-area-of-interest",
    "href": "sections/10-geopandas.html#crop-data-to-area-of-interest",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "10.5 Crop data to area of interest",
    "text": "10.5 Crop data to area of interest\nFor this example, we are only interested in south central Alaska, encompassing Prince William Sound, Cook Inlet, and Kodiak. Our raster data is significantly larger than that, and the vector data is statewide. So, as a first step we might want to crop our data to the area of interest.\nFirst, we’ll need to create a bounding box. We use the box function from shapely to create the bounding box, then create a geoDataFrame from the points, and convert the WGS 84 coordinates to the Alaska Albers projection.\n\nfrom shapely.geometry import box\n\ncoord_box = box(-159.5, 55, -144.5, 62)\n\ncoord_box_df = gpd.GeoDataFrame(\n    crs = 'EPSG:4326',\n    geometry = [coord_box]).to_crs(\"EPSG:3338\")\n\nNow, we can read in raster again cropped to bounding box. We use the mask function from rasterio.mask. Note that we apply this to the connection to the raster file (with rasterio.open(...)), then update the metadata associated with the raster.\n\nimport rasterio.mask\nimport numpy as np\n\nwith rasterio.open(\"Coastal_2020_08.tif\") as ship_con:\n    shipc_arr, shipc_transform = rasterio.mask.mask(ship_con, coord_box_df[\"geometry\"], crop=True)\n    shipc_meta = ship_con.meta\n    # select just the 2-D array (by default a 3-D array with 1 band is returned)\n    shipc_arr = shipc_arr[0,:,:]\n    # turn the no-data values into NaNs.\n    shipc_arr[shipc_arr == ship_con.nodata] = np.nan\n\n\nshipc_meta.update({\"driver\": \"GTiff\",\n                 \"height\": shipc_arr.shape[0],\n                 \"width\": shipc_arr.shape[1],\n                 \"transform\": shipc_transform,\n                 \"compress\": \"lzw\"})\n\nNext, we’ll use a spatial inner join for the vector data to select polygons that are within the bounding box.\n\ncomm_clip = gpd.sjoin(comm_3338, coord_box_df, how='inner', predicate='within')\n\n\n10.5.1 Check extents\nNow let’s look at the two cropped datasets overlayed on each other to ensure that the extents look right.\n\nimport rasterio.plot\n\n# set up plot\nfig, ax = plt.subplots(figsize=(15, 15))\n# plot the raster\nrasterio.plot.show(shipc_arr,\n                   ax=ax,\n                   vmin = 0,\n                   vmax = 50000,\n                   transform = shipc_transform)\n# plot the vector\ncomm_clip.plot(ax=ax, facecolor='none', edgecolor='white')\n\n<AxesSubplot:>"
  },
  {
    "objectID": "sections/10-geopandas.html#calculate-total-distance-per-fishing-area",
    "href": "sections/10-geopandas.html#calculate-total-distance-per-fishing-area",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "10.6 Calculate total distance per fishing area",
    "text": "10.6 Calculate total distance per fishing area\nIn this step, we rasterize each polygon in the shapefile, such that pixels in or touching the polygon get a value of 1, and pixels not touching it get a value of 0. Then, for each polygon, we extract the indices of the raster array that are equal to 1. We then extract the values of these indicies from the original ship traffic raster data, and calculate the sum of the values over all of those pixels.\nHere is a simplified diagram of the process:\n\n\n10.6.0.1 Zonal statistics over one polygon\nLet’s look at how this works over just one fishing area first. We use the rasterize method from the features module in rasterio. This takes as arguments the data to rasterize (in this case the 40th row of our dataset), the shape and transform the output raster will take (these were extracted from our raster data when we read it in). We alo set the all_touched argument to true, which means any pixel that touches a boundary of our vector will be burned into the mask.\n\nfrom rasterio import features\n\nr40 = features.rasterize(comm_clip['geometry'][40].geoms,\n                                    out_shape=shipc_arr.shape,\n                                    transform=shipc_meta['transform'],\n                                    all_touched=True)\n\nIf we have a look at a plot of our rasterized version of the single fishing district, we can see that instead of a vector, we now have a raster with the shape of the district.\n\n# set up plot\nfig, ax = plt.subplots(figsize=(15, 15))\n# plot the raster\nrasterio.plot.show(r40,\n                   ax=ax,\n                   vmin = 0,\n                   vmax = 1,\n                   transform = shipc_meta['transform'])\n# plot the vector\ncomm_clip.plot(ax=ax, facecolor='none', edgecolor='white')\n\n<AxesSubplot:>\n\n\n\n\n\nA quick call to np.unique shows our unique values are 0 or 1, which is what we expect.\n\nnp.unique(r40)\n\narray([0, 1], dtype=uint8)\n\n\nFinally, we need to know is the indices of the original raster where the fishing district is. We can use np.where to extract this information\n\nr40_index = np.where(r40 == 1)\nprint(r40_index)\n\n(array([108, 108, 108, 108, 108, 109, 109, 109, 109, 109, 109, 109, 109,\n       109, 109, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110,\n       110, 110, 110, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111,\n       111, 111, 111, 111, 112, 112, 112, 112, 112, 112, 112, 112, 112,\n       112, 112, 112, 112, 112, 112, 113, 113, 113, 113, 113, 113, 113,\n       113, 113, 113, 113, 113, 113, 113, 113, 114, 114, 114, 114, 114,\n       114, 114, 114, 114, 114, 114, 115, 115, 115, 115, 115, 115, 116]), array([759, 760, 762, 763, 764, 755, 756, 757, 758, 759, 761, 762, 763,\n       764, 765, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763,\n       764, 765, 766, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762,\n       763, 764, 765, 766, 753, 754, 755, 756, 757, 758, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 753, 754, 755, 756, 757, 758, 759,\n       760, 761, 762, 763, 764, 765, 766, 767, 753, 754, 755, 756, 757,\n       758, 759, 760, 761, 762, 763, 753, 754, 755, 756, 757, 758, 754]))\n\n\nIn the last step, we’ll using these indices to extract the values of the data from the fishing raster, and sum them to get a total distance travelled.\n\nnp.nansum(shipc_arr[r40_index])\n\n14369028.0\n\n\nNow that we know the individual steps, let’s run this over all of the districts. First we’ll create an id column in the vector data frame. This will help us track unique fishing districts later.\n\ncomm_clip['id'] = range(0,len(comm_clip))\n\nFor each district (with geometry and id), we run the features.rasterize function. If any values equal 1 (some of the districts are outside the bounds of the raster), we calculate the sum of the values of the shipping raster r_array based on the indicies in the raster where the district is located.\n\ndistance_dict = {}\nfor geom, idx in zip(comm_clip['geometry'], comm_clip['id']):\n    rasterized = features.rasterize(geom.geoms,\n                                    out_shape=shipc_arr.shape,\n                                    transform=shipc_meta['transform'],\n                                    all_touched=True)\n    # only save polygons that have a non-zero value\n    if any(np.unique(rasterized)) == 1:\n        r_index = np.where(rasterized == 1)\n        distance_dict[idx] = np.nansum(shipc_arr[r_index])\n\nNow we just create a data frame from that dictionary, and join it to the vector data using pandas operations.\n\nimport pandas as pd\n\n# create a data frame from the result\ndistance_df = pd.DataFrame.from_dict(distance_dict,\n    orient='index',\n    columns=['distance'])\n\n# extract the index of the data frame as a column to use in a join\ndistance_df['id'] = distance_df.index\ndistance_df['distance'] = distance_df['distance']/1000\n\nNow we join the result to the original geodataframe.\n\n# join the sums to the original data frame\nres_full = comm_clip.merge(distance_df, on = \"id\", how = 'inner')\n\nFinally, we can plot our result!\n\nimport matplotlib.ticker\nfig, ax = plt.subplots(figsize=(7, 7))\n\nax = res_full.plot(column = \"distance\", legend = True, ax = ax)\nfig = ax.figure\nlabel_format = '{:,.0f}'\ncb_ax = fig.axes[1]\nticks_loc = cb_ax.get_yticks().tolist()\ncb_ax.yaxis.set_major_locator(matplotlib.ticker.FixedLocator(ticks_loc))\ncb_ax.set_yticklabels([label_format.format(x) for x in ticks_loc])\nax.set_axis_off()\nax.set_title(\"Distance Traveled by Ships in Kilometers\")\nplt.show()\n\n\n\n\nFrom here we can do any additional geopandas operations we might be interested in. For example, what if we want to calculate the total distance by registration area (a superset of fishing district). We can do that using dissolve from geopandas.\n\nreg_area = res_full.dissolve(by = \"REGISTRATION_AREA_NAME\", aggfunc = 'sum')\n\nLet’s have a look at the same plot as before, but this time over our aggregated data.\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\nax = reg_area.plot(column = \"distance\", legend = True, ax = ax)\nfig = ax.figure\nlabel_format = '{:,.0f}'\ncb_ax = fig.axes[1]\nticks_loc = cb_ax.get_yticks().tolist()\ncb_ax.yaxis.set_major_locator(matplotlib.ticker.FixedLocator(ticks_loc))\ncb_ax.set_yticklabels([label_format.format(x) for x in ticks_loc])\nax.set_axis_off()\nax.set_title(\"Distance Traveled by Ships in Kilometers\")\nplt.show()"
  },
  {
    "objectID": "sections/10-geopandas.html#summary",
    "href": "sections/10-geopandas.html#summary",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "10.7 Summary",
    "text": "10.7 Summary\nWe covered a lot of ground here, so let’s recap some of the high level points:\n\nRaster data consists of regularly gridded values, and can be represented in python as an array\nVector data consists of any number of points, that might be connected, and is represented in python as a geodataframe\nWe can do geospatial operations like changing the projection or cropping the data to a particular extent on both raster and vector data\nYou can use vector data to help analyze raster data (and vice versa!) by rasterizing the vector data and using numpy operations on the resulting array."
  },
  {
    "objectID": "sections/11-parquet-arrow.html",
    "href": "sections/11-parquet-arrow.html",
    "title": "11  Parquet and Arrow",
    "section": "",
    "text": "The difference between column major and row major data\nSpeed advantages to columnnar data storage\nHow arrow enables faster processing"
  },
  {
    "objectID": "sections/11-parquet-arrow.html#introduction",
    "href": "sections/11-parquet-arrow.html#introduction",
    "title": "11  Parquet and Arrow",
    "section": "11.2 Introduction",
    "text": "11.2 Introduction\nParalleization is great, and can greatly help you in working with large data. However, it might not help you with every processing problem. Like we talked about with Dask, sometimes your data are too large to be read into memory, or you have I/O limitations. Parquet and pyarrow were designed to help overcome some of these problems.\nBefore jumping into those tools, however, first let’s discuss system calls. These are calls that are run by the operating system within their own process. There are several that are relevant to reading and writing data: open, read, write, seek, and close. Open establishes a connection with a file for reading, writing, or both. On open, a file offset points to the beginning of the file. After reading or writing n bytes, the offset will move n bytes forward to prepare for the next opration. Close closes the connection to the file. Read will read data from the file into a memory buffer, and write will write data from a memory buffer to a file. Seek is used to change the location of the offset pointer, for either reading or writing purposes.\nIf you’ve worked with even moderately sized datasets, you may have encounted an “out of memory” error. Memory is where a computer stores the information needed immediately for processes. This is in contrast to storage, which is typically slower to access than memory, but has a much larger capacity. When you open a file, you are establishing a connection between your processor and the information in storage. On read, the data is read into memory that is then available to your python process, for example.\nSo what happens if the data you need to read in are larger than your memory? 32GB is a common memory size, but this would be considered a modestly sized dataset by this courses’s standards. There are a number of solutions to this problem, which don’t involve just buying a computer with more memory. In this lesson we’ll discuss the difference between row major and column major file formats, and how leveraging column major formats can increase memory efficiency. We’ll also learn about another python library called pyarrow, which has a memory format that allows for “zero copy” read. As a note, pyarrow and Parquet are newer technologies, and definitely are on the bleeding edge, so you may run into some bumps incorporating them into workflows."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#row-major-vs-column-major",
    "href": "sections/11-parquet-arrow.html#row-major-vs-column-major",
    "title": "11  Parquet and Arrow",
    "section": "11.3 Row major vs column major",
    "text": "11.3 Row major vs column major\nThe difference between row major and column major is in the ordering of items in the array when they are read into memory.\nTake the array:\na11 a12 a13\n\na21 a22 a23\nThis array in a row-major order would be read in as:\na11, a12, a13, a21, a22, a23\nYou could also read it in column-major order as:\na11, a21, a12, a22, a13, a33\nBy default, C and SAS use row major order for arrays, and column major is used by Fortran, MATLAB, R, and Julia.\nPython uses neither, instead representing arrays as lists of lists, though numpy uses row-major order.\n\n11.3.1 Row major versus column major files\nThe same concept can be applied to file formats as the example with arrays above. In row-major file formats, the values (bytes) of each record are read sequentially.\n\n\n\nName\nLocation\nAge\n\n\n\n\nJohn\nWashington\n40\n\n\nMariah\nTexas\n21\n\n\nAllison\nOregon\n57\n\n\n\nIn the above row major example, data are read in the order: John, Washingon, 40 \\n Mariah, Texas, 21.\nThis means that getting a subset of rows with all the columns would be easy; you can specify to read in only the first X rows (utilizing the seek system call). However, if we are only interested in Name and Location, we would still have to read in all of the rows before discarding the Age column.\nIf these data were organized in a column major format, they might look like this:\nName: John, Mariah, Allison\nLocation: Washington, Texas, Oregon\nAge: 40, 21, 57\nAnd the read order would first be the names, then the locations, then the age. This means that selecting all values from a set of columns is quite easy (all of the Names and Ages, or all Names and Locations), but reading in only the first few records from each column would require reading in the entire dataset. Another advantage to column major formats is that compression is more efficient since compression can be done across each column, where the data type is uniform, as opposed to across rows with many data types."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#parquet",
    "href": "sections/11-parquet-arrow.html#parquet",
    "title": "11  Parquet and Arrow",
    "section": "11.4 Parquet",
    "text": "11.4 Parquet\nParquet is an open-source file format that stores data in a column-major format. The format contains several key components:\n\nrow group\ncolumn\npage\nfooter\n\n\nRow groups are blocks of data over a set number of rows that contain data from the same columns. Within each row group, data are organized in column-major format, and within each column are pages that are typically 1MB. The footer of the file contains metadata like the schema, encodings, unique values in each column, etc.\nThe parquet format has many tricks to to increase storage efficiency, and is increasingly being used to handle large datasets."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#arrow",
    "href": "sections/11-parquet-arrow.html#arrow",
    "title": "11  Parquet and Arrow",
    "section": "11.5 Arrow",
    "text": "11.5 Arrow\nSo far, we have discussed the difference between organizing information in row-major and column-major format, how that applies to arrays, and how it applies to data storage on disk using Parquet.\nArrow is a language-agnostic specification that enables representation of column-major information in memory without having to serialize data from disk. The Arrow project provides implementation of this specification in a number of languages, including Python.\nLet’s say that you have utilized the Parquet data format for more efficient storage of your data on disk. At some point, you’ll need to read that data into memory in order to do analysis on it. Arrow enables data transfer between the on disk Parquet files and in-memory Python computations, via the pyarrow library.\npyarrow is great, but relatively low level. It supports basic group by and aggregate functions, as well as table and dataset joins, but it does not support the full operations that pandas does."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#example",
    "href": "sections/11-parquet-arrow.html#example",
    "title": "11  Parquet and Arrow",
    "section": "11.6 Example",
    "text": "11.6 Example\nIn this example, we’ll read in a dataset of fish abundance in the San Francisco Estuary, which is published in csv format on the Environmental Data Initiative. This dataset isn’t huge, but it is big enough (3 GB) that working with it locally can be fairly taxing on memory. Motivated by user difficulties in actually working with the data, the deltafish R package was written using the R implementation of arrow. It works by downloading the EDI repository data, writing it to a local cache in parquet format, and using arrow to query it. In this example, I’ve put the Parquet files in a sharable location so we can explore it using pyarrow.\nFirst, we’ll load the modules we need.\n\nimport pyarrow.dataset as ds\nimport numpy as np\nimport pandas as pd\n\nNext we can read in the data using ds.dataset(), passing it the path to the parquet directory and how the data are partitioned.\n\ndeltafish = ds.dataset(\"/home/shares/deltafish/fish\", format=\"parquet\", partitioning='hive')\n\nYou can check out a file listing using the files method. Another great feature of parquet files is that they allow you to partition the data accross variables of the dataset. These partitions mean that, in this case, data from each species of fish is written to it’s own file. This allows for even faster operations down the road, since we know that users will commonly need to filter on the species variable. Even though the data are partitioned into different files, pyarrow knows that this is a single dataset, and you still work with it by referencing just the directory in which all of the partitioned files live.\n\ndeltafish.files\n\n['/home/shares/deltafish/fish/Taxa=Acanthogobius flavimanus/part-0.parquet',\n '/home/shares/deltafish/fish/Taxa=Acipenser medirostris/part-0.parquet',\n '/home/shares/deltafish/fish/Taxa=Acipenser transmontanus/part-0.parquet',\n '/home/shares/deltafish/fish/Taxa=Acipenser/part-0.parquet'...\nYou can view the columns of a dataset using schema.to_string()\n\ndeltafish.schema.to_string()\n\nSampleID: string\nLength: double\nCount: double\nNotes_catch: string\nSpecies: string\nIf we are only interested in a few species, we can do a filter:\n\nexpr = ((ds.field(\"Taxa\")==\"Dorosoma petenense\")| \n        (ds.field(\"Taxa\")==\"Morone saxatilis\") |\n        (ds.field(\"Taxa\")== \"Spirinchus thaleichthys\"))\n\nfishf = deltafish.to_table(filter = expr, columns =['SampleID', 'Length', 'Count', 'Taxa'])\n\nThere is another dataset included, the survey information. To do a join, we can just use the join method on the arrow dataset.\nFirst read in the survey dataset.\n\nsurvey = ds.dataset(\"/home/jclark/deltafish/survey\", format=\"parquet\", partitioning='hive')\n\nTake a look at the columns again:\n\nsurvey.schema.to_string()\n\nLet’s pick out only the ones we are interested in.\n\nsurvey_s = survey.to_table(columns=['SampleID','Datetime', 'Station', 'Longitude', 'Latitude'])\n\nThen do the join, and convert to a pandas data.frame.\n\nfish_j = fishf.join(survey_s, \"SampleID\").to_pandas()\n\nNote that when we did our first manipulation of this dataset, we went from working with a FileSystemDataset, which is a representation of a dataset on disk without reading it into memory, to a Table, which is read into memory. pyarrow has a number of functions that do computations on datasets without reading them into memory. However these are evaluated “eagerly,” as opposed to “lazily.” These are useful in some cases, like above, where we want to take a larger than memory dataset and generate a smaller dataset (via filter, or group by/summarize), but are not as useful if we need to do a join before our summarization/filter.\nMore functionality for lazy evaluation is on the horizon for pyarrow though, by leveraging Ibis."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#synopsis",
    "href": "sections/11-parquet-arrow.html#synopsis",
    "title": "11  Parquet and Arrow",
    "section": "11.7 Synopsis",
    "text": "11.7 Synopsis\nIn this lesson we learned:\n\nthe difference between row major and column major formats\nunder what circumstances a column major format can improve memory efficiency\nwhat Parquet is\nhow pyarrow can interact with Parquet files to analyze data"
  },
  {
    "objectID": "sections/13-group-project-2.html",
    "href": "sections/13-group-project-2.html",
    "title": "13  Group Project: Data Processing",
    "section": "",
    "text": "In your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-13.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills."
  },
  {
    "objectID": "sections/14-data-ethics.html",
    "href": "sections/14-data-ethics.html",
    "title": "14  Data Ethics for Scalable Computing",
    "section": "",
    "text": "Review FAIR and CARE Principles, and their relevance to data ethics\nExamine how ethical considerations are shared and considered at the Arctic Data Center\nDiscuss ethical considerations in machine learning"
  },
  {
    "objectID": "sections/14-data-ethics.html#intro-to-data-ethics",
    "href": "sections/14-data-ethics.html#intro-to-data-ethics",
    "title": "14  Data Ethics for Scalable Computing",
    "section": "14.2 Intro to Data Ethics",
    "text": "14.2 Intro to Data Ethics\n\nTo recap, the Arctic Data Center is an openly-accessible data repository and the data published through the repository is open for anyone to reuse, subject to conditions of the license (at the Arctic Data Center, data is released under one of two licenses: CC-0 Public Domain and CC-By Attribution 4.0). In facilitating use of data resources, the data stewardship community has converged on principles surrounding best practices for open data management.\nTwo principles that the Arctic Data Center explicitly adopts are FAIR Principles (Findable, Accessible, Interoperable, and Reproducible) and CARE Principles for Indigenous Governance (Collective Benefit, Authority to Control, Responsibility, Ethics).\nFAIR and CARE principles are relevant in the context of data ethics for multiple reasons. FAIR speaks to how metadata is managed, stored, and shared.\n\nFAIR principles and open science are overlapping concepts, but are distinctive from one another. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. The FAIR principles place emphasis on machine readability, “distinct from peer initiatives that focus on the human scholar” (Wilkinson et al 2016) and as such, do not fully engage with sensitive data considerations and with Indigenous rights and interests (Research Data Alliance International Indigenous Data Sovereignty Interest Group, 2019). Metadata can be FAIR but not open. For example, sensitive data (data that contains personal information) may not be appropriate to share, however sharing the anonymized metadata that is easily understandable will reduce research redundancy.\n\nResearch has historically perpetuated colonialism and represented extractive practices, meaning that the research results were not mutually beneficial. These issues also related to how data was owned, shared, and used. To address issues like these, the Global Indigenous Data Alliance (GIDA) introduced CARE Principles for Indigenous Data Governance to support Indigenous data sovereignty. CARE Principles speak directly to how the data is stored and shared in the context of Indigenous data sovereignty. CARE Principles stand for:\n\nCollective Benefit - Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data\nAuthority to Control - Indigenous Peoples’ rights and interests in Indigenous data must be recognized and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data.\nResponsibility - Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples.\nEthics - Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem. To many, the FAIR and CARE principles are viewed by many as complementary: CARE aligns with FAIR by outlining guidelines for publishing data that contributes to open-science and at the same time, accounts for Indigenous’ Peoples rights and interests."
  },
  {
    "objectID": "sections/14-data-ethics.html#ethics-at-the-arctic-data-center",
    "href": "sections/14-data-ethics.html#ethics-at-the-arctic-data-center",
    "title": "14  Data Ethics for Scalable Computing",
    "section": "14.3 Ethics at the Arctic Data Center",
    "text": "14.3 Ethics at the Arctic Data Center\nTransparency in data ethics is a vital part of open science. Regardless of discipline, various ethical concerns are always present, including professional ethics such as plagiarism, false authorship, or falsification of data, to ethics regarding the handling of animals, to concerns relevant to human subjects research. As the primary repository for the Arctic program of the National Science Foundation, the Arctic Data Center accepts Arctic data from all disciplines. Recently, a new submission feature was released which asks researchers to describe the ethical considerations that are apparent in their research. This question is asked to all researchers, regardless of disciplines.\nSharing ethical practices openly, similar in the way that data is shared, enables deeper discussion about data management practices, data reuse, sensitivity, sovereignty and other considerations. Further, such transparency promotes awareness and adoption of ethical practices.\nInspired by CARE Principles for Indigenous Data Governance (Collective Benefit, Authority to Control, Responsibility, Ethics) and FAIR Principles (Findable, Accessible, Interoperable, Reproducible), we include a space in the data submission process for researchers to describe their ethical research practices. These statements are published with each dataset, and the purpose of these statements is to promote greater transparency in data collection and to guide other researchers. For more information about the ethical research practices statement, check out this blog.\nTo help guide researchers as they write their ethical research statements, we have listed the following ethical considerations that are available on our website. The concerns are organized first by concerns that should be addressed by all researchers, and then by discipline.\nConsider the following ethical considerations that are relevant for your field of research.\n\n14.3.1 Ethical Considerations for all Arctic Researchers\n\nResearch Planning\n\nWere any permits required for your research?\nWas there a code of conduct for the research team decided upon prior to beginning data collection?\nWas institutional or local permission required for sampling?\nWhat impact will your research have on local communities or nearby communities (meaning the nearest community within a 100 mile radius)?\n\nData Collection\n\nWere any local community members involved at any point of the research process, including study site identification, sampling, camp setup, consultation or synthesis?\nWere the sample sites near or on Indigenous land or communities?\n\nData Sharing and Publication\n\nHow were the following concerns accounted for: misrepresentation of results, misrepresentation of experience, plagiarism, improper authorship, or the falsification or data?\nIf this data is intended for publication, are authorship expectations clear for everyone involved? Other professional ethics can be found here\n\n\n\n14.3.2 Archaeological and Paleontological Research\n\nResearch Planning\n\nWere there any cultural practices relevant to the study site? If yes, how were these practices accounted for by the research methodologies.\n\nData Collection\n\nDid your research include the removal or artifacts?\nWere there any contingencies made for the excavation and return of samples after cleaning, processing, and analysis?\n\nData Sharing and Publication 4. Were the samples deposited to a physical repository? 5. Were there any steps taken to account for looting threats? Please explain why or why not?\n\n\n14.3.3 Human Participation and Sensitive Data\n\nResearch Planning\n\nPlease describe the institutional IRB approval that was required for this research.\nWas any knowledge provided by community members?\n\nData Collection 3. Did participants receive compensation for their participation? 4. Were decolonization methods used?\nData Sharing and Publication\n\nHave you shared this data with the community or participants involved?\n\n\n\n14.3.4 Marines Sciences (e.g. Marine Biology Research)\n\nResearch Planning\n\nWere any of the study sites or species under federal or local protection?\n\nData Collection\n\nWere endangered, threatened, or otherwise special-status species collected?\nHow were samples collected? Please describe any handling practices used to collect data.\nWhat safety measures were in place to keep researchers, research assistants, technicians, etc., out of harms way during research?\nHow were animal care procedures evaluated, and do they follow community norms for organismal care?\n\nData Sharing and Publication 6. Did the species or study area represent any cultural importance to local communities, or include culturally sensitive information? Please explain how you came to this conclusion and how any cultural sensitivity was accounted for.\n\n\n14.3.5 Physical Sciences (e.g. Geology, Glaciology, and Ice Research)\n\nResearch Planning 1. Was any knowledge provided by community members, including information regarding the study site?\nData Collection\n\nWhat safety measures were in place to keep researchers, research assistants, technicians, etc., out of harms way during research?\nWere there any impacts to the environment/habitat before, during or after data collection?\n\nData Sharing and Publication\n\nIs there any sensitive information including information on sensitive sites, valuable samples, or culturally sensitive information?\n\n\n\n14.3.6 Plant and Soil Research\n\nResearch Planning\n\nWere any of the study sites protected under local or federal regulation?\nWas any knowledge provided by nearby community members, including information regarding the study site?\n\nData Collection\n\nDid sample collection result in erosion of soil or other physical damage? If so, how was this addressed?\nWhat safety measures were in place to keep researchers, research assistants, technicians, etc., out of harms way during research?\n\nData Sharing and Publication\n\nDo any of the study sites or specimens represent culturally sensitive areas or species? Please explain how you came to this conclusion, and if yes, how was this accounted for?\n\n\n\n14.3.7 Spatial Data\n\nResearch Planning\n\nWere any land permits required for this research?\n\nData Collection\n\nWere any data collected using citizen science or community participation?\nIf yes, were community members compensated for their time and made aware of their data being used and for what purpose?\n\nData Sharing and Publication\n\nIf data were ground-truthed, was institutional or local permissions required and/or obtained for land/property access?\nHave you shared this data with the community or participants involved?\nIf location sensitive data was obtained (endangered/threatened flora & fauna location, archaeological and historical sites, identifiable ships, sensitive spatial information), how were the data desensitized?\n\n\n\n14.3.8 Wildlife Sciences (e.g. Ecology and Biology Research)\n\nResearch Planning\n\nWere any permits required for data sampling?\n\nData Collection\n\nWere endangered, threatened, or otherwise special-status plants or animal species collected?\nHow were samples collected? Please describe any handling practices used to collect data.\nWhat safety measures were in place to keep researchers, research assistants, technicians, etc., out of harms way during research?\nHow were animal care procedures evaluated, and do they follow community norms for organism care?\n\nData Sharing and Publication\n\nDo any of the study sites or specimens represent culturally sensitive areas or species? Please explain how you came to this conclusion, and if yes, how was this accounted for?\n\nMenti question:\n\nHave you thought about any of the ethical considerations listed above before?\nWere any of the considerations new or surprising?\nAre there any for your relevant discipline that are missing?"
  },
  {
    "objectID": "sections/14-data-ethics.html#ethics-in-machine-learning",
    "href": "sections/14-data-ethics.html#ethics-in-machine-learning",
    "title": "14  Data Ethics for Scalable Computing",
    "section": "14.4 Ethics in Machine Learning",
    "text": "14.4 Ethics in Machine Learning\nMenti poll\n\nWhat is your level of familiarity with machine learning\nHave you thought about ethics in machine learning prior to this lesson?\nCan anyone list potential ethical considerations in machine learning?\n\nWhat comes to mind when considering ethics in machine learning?\nThe stories that hit the news are often of privacy breaches or biases seeping into the training data. Bias can enter at any point of the research project, from preparing the training data, designing the algorithms, to collecting and interpreting the data. When working with sensitive data, a question to also consider is how to deanonymize, anonymized data. A unique aspect to machine learning is how personal bias can influence the analysis and outcomes. A great example of this is the case of ImageNet.\n\n14.4.1 ImageNet: A case study of ethics and bias in machine learning\n Image source: Kate Crawford and Trevor Paglen, “Excavating AI: The Politics of Training Sets for Machine Learning” (September 19, 2019).\nImageNet is a great example of how personal bias can enter machine learning through the training data. ImageNet was a training data set of photos that was used to train image classifiers. The data set was initially created as a large collection of pictures, which were mainly used to identify objects, but some included images of people. The creators of the data set created labels to categorize the images, and through crowdsourcing, people from the internet labeled these images. (This example is from Kate Crawford and Trevor Paglen, “Excavating AI: The Politics of Training Sets for Machine Learning”, September 19, 2019).\n\n14.4.1.1 Discussion:\n\nWhere are the two areas bias could enter this scenario?\nAre there any ways that this bias could be avoided?\nWhile this example is specific to images, can you think of any room for bias in your research?"
  },
  {
    "objectID": "sections/14-data-ethics.html#references-and-further-reading",
    "href": "sections/14-data-ethics.html#references-and-further-reading",
    "title": "14  Data Ethics for Scalable Computing",
    "section": "14.5 References and Further Reading",
    "text": "14.5 References and Further Reading\nCarroll, S.R., Herczog, E., Hudson, M. et al. (2021) Operationalizing the CARE and FAIR Principles for Indigenous data futures. Sci Data 8, 108 https://doi.org/10.1038/s41597-021-00892-0\nChen, W., & Quan-Haase, A. (2020) Big Data Ethics and Politics: Towards New Understandings. Social Science Computer Review. https://journals.sagepub.com/doi/10.1177/0894439318810734\nCrawford, K., & Paglen, T. (2019) Excavating AI: The Politics of Training Sets for Machine Learning. https://excavating.ai/\nGray, J., & Witt, A. (2021) A feminist data ethics of care framework for machine learning: The what, why, who and how. First Monday, 26(12), Article number: 11833\nPuebla, I., & Lowenberg, D. (2021) Recommendations for the Handling for Ethical Concerns Relating to the Publication of Research Data. FORCE 11. https://force11.org/post/recommendations-for-the-handling-of-ethical-concerns-relating-to-the-publication-of-research-data/\nResearch Data Alliance International Indigenous Data Sovereignty Interest Group. (2019). “CARE Principles for Indigenous Data Governance.” The Global Indigenous Data Alliance. GIDA-global.org\nWilkinson, M., Dumontier, M., Aalbersberg, I. et al. (2016) The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018. https://doi.org/10.1038/sdata.2016.18\nZwitter, A., Big Data ethics. (2014) Big Data and Society. DOI: 10.1177/2053951714559253"
  },
  {
    "objectID": "sections/15-google-earth-engine.html",
    "href": "sections/15-google-earth-engine.html",
    "title": "15  Google Earth Engine",
    "section": "",
    "text": "TODO\n\n\n\n\ncreate jupyter notebook outline for scalable-computing-examples(?)\nincporporate Ingmar’s stuff (or keep it separate in scalable-computing-examples)?\nADD REF TO CHOOSING GOOD COLORS"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#learning-objectives",
    "href": "sections/15-google-earth-engine.html#learning-objectives",
    "title": "15  Google Earth Engine",
    "section": "15.1 Learning Objectives",
    "text": "15.1 Learning Objectives\n\nUnderstand what Google Earth Engine provides and its applications\nLearn how to search for, import, manipulate, and visualize Google Earth Engine Data\nLearn about some real-world applications of Google Earth Engine in Earth systems research"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#introduction",
    "href": "sections/15-google-earth-engine.html#introduction",
    "title": "15  Google Earth Engine",
    "section": "15.2 Introduction",
    "text": "15.2 Introduction\nGoogle Earth Engine (GEE) is a geospatial processing platform powered by Google Cloud Platform. It contains over 30 years (and multiple petabytes) of satellite imagery and geospatial datasets that are continually updated and available instantly. Users can process data using Google Cloud Platform and built-in algorithms or by using the Earth Engine API, which is available in Python (and JavaScript) for anyone with an account (Earth Engine is free to use for research, education, and nonprofit use). \n\n\n\n\nImage Source: Earth Engine Data Catalog\n\n So what’s so exciting about platforms like GEE? Ryan Abernathey frames this nicely in his blogpost Closed Platform vs. Open Architectures for Cloud-Native Earth System Analytics…\nTODO: GEE is one of many. Microsoft Planetary Computer, Pangeo, Amazon Web Services (Amazon Enviromental SOmething)\n\nas Earth System data have gotten larger, the typical download-data-work-locally workflow is no longer always feasible\nthose data are also produced and distributed by lots of different organizations (e.g. NASA, NOAA, Copernicus)\nresearchers often need to apply a wide range of analytical methods to those data, ranging from simple stats to machine learning approaches\n\nGEE offers web access (i.e. no need to download data to your computer) to an extensive catalog of analysis-ready geospatial data (from many different organizations) and scalable computing power via their cloud service, making global-scale analyses and visualizations possible for anyone with an account (sign up here!)\nExplore the public Earth Engine Data Catalog which includes a variety of standard Earth science raster datasets. Browse by dataset tags or by satellite (Landsat, MODIS, Sentinel).\nIn this lesson, we’ll get some hands-on practice connecting to and using Google Earth Engine to visualize global precicpation data. We’ll then __________Ingmar’s plan here__________."
  },
  {
    "objectID": "sections/15-google-earth-engine.html#exercise-1-an-introductory-lesson-on-using-google-earth-engine",
    "href": "sections/15-google-earth-engine.html#exercise-1-an-introductory-lesson-on-using-google-earth-engine",
    "title": "15  Google Earth Engine",
    "section": "15.3 Exercise 1: An introductory lesson on using Google Earth Engine",
    "text": "15.3 Exercise 1: An introductory lesson on using Google Earth Engine\n\n15.3.1 Part i. Setup\n\nCreate a Google Earth Engine account (if you haven’t already done so)\n\n\nPlease refer back to the Preface to find instructions on creating a GEE account.\n\n\nLoad libraries\n\n\nimport ee\nimport geemap\n\n\nAuthenticate your GEE account\n\n\nIn order to begin using GEE, you’ll need to connect your environment (scomp) to the authentication credentials associated with your Google account. This will need to be done each time you connect to GEE, (but only be done once per session).\n\n\nee.Authenticate() # triggers the authentication process\n\n\nThis should launch a browser window where you can login with your Google account to the Google Earth Engine Authenticator. Following the prompts will generate a code, which you’ll then need to copy and paste into the VS Code command palette (at the top of the IDE). This will be saved as an authentication token so you won’t need to go through this process again until the next time you start a new session. The browser-based authentication steps will look something like this:\n\n\nNotebook Authenticator: choose an active Google account and Cloud Project (you may have to create one if this is your first time authenticating) and click “Generate Token”\n\nChoose an account: if prompted, select the same Google account as above\nGoogle hasn’t verified this app: You may be temped to click the blue “Back to saftey” button, but don’t! Click “Continue”\n\nSelect what Earth Engine Notebook Client can access: click both check boxes, then “Continue”\n\nCopy your authentication code to your clipboard to paste into the VS Code command palette\n\n\nLastly, intialize. This verifies that valid credentials have been created and populates the Python client library with methods that the backend server supports.\n\n\nee.Initialize() \n\nIf successful, you’re now ready to begin working with Earth Engine data!\n\n\n15.3.2 Part ii. Explore the ERA5 Daily Aggregate Data\n\n\n\n\n\n\nTODO\n\n\n\n\nhttps://developers.google.com/earth-engine/datasets/catalog/ECMWF_ERA5_DAILY#description\nshow bands tab for ERA5 dataset\n\n\n\n\n\n15.3.3 Part iii. Visualize global precipitation data using ERA5 Daily Aggregate data\nContent for this section was adapted from Dr. Sam Stevenson’s Visualizing global precipitation using Google Earth Engine lesson, given in her EDS 220 course in Fall 2021.\n\nCreate an interactive basemap\n\n\nThe default basemap is (you guessed it) Google Maps. The following code displays an empty Google Map that you can manipulate just like you would in the typical Google Maps interface. Do this using the Map method from the geemap library. We’ll also center the map at a specified latitude and longitude (here, 40N, 100E), set a zoom level, and save our map as an object called myMap.\n\n\nmyMap = geemap.Map(center = [40, -100], zoom = 2)\nmyMap\n\n\nLoad ERA5 Image Collections from GEE\n\n\nNext, we need to tell GEE what data to layer on top of our basemap. We’ll be using the ERA5 daily aggregates reanalysis dataset, produced by the European Centre for Medium-Range Weather Forecasts (ECMWF), found here, which models atmospheric weather observations. We’ll load the total_precipitation field (check out the metadata on here).\n\n\n\nReanalysis combines observation data with model data to provide the most complete picture of past weather and climate. To read more about reanalyses, check out the EWCMWF website.\n\n\n\n\n\n\nTODO: GEE makes working with these very large data a breeze\n\n\n\nThe Arctic Data Center has worked with ERA5 data – took three weeks to download. The ee.ImageCollection() method is all you need to load and analyze large image collection data (more on that below!).\nADC is using ERA5 (hourly) data; reaggregating to weekly statistical summaries\n\n\n\nThe ImageCollection method extracts a set of individual images that satisfies some criterion that you pass to GEE through the ee package. This is stored as an ImageCollection object which can be filtered and processed in various ways. We can pass the ImageCollction method agruments to tell GEE which data we want to retrieve. Below, we retrieve all daily ERA5 data (so we can see individual rain events).\n\n\n\n\n\n\n\nEarth Engine Snippets make importing ImageCollections easy!\n\n\n\nTo import an ImageCollection, copy and paste the Earth Engine Snippet for your dataset of interest. For example, the Earth Enginge Snippet to import the ERA5 daily aggregates data can be found on the dataset page.\n\n\n\nweatherData = ee.ImageCollection('ECMWF/ERA5/DAILY')\n\n\nSelect an image to plot\n\n\nTo plot a map over our Google Maps basemap, we need an “Image” rather than an “ImageCollection.” ERA5 contains many different climate variables – explore which variables the dataset contains under the Bands tab. We’ll use the .select method to choose the parameter(s) (in this case, total_precipitation) we’re interested in from our weatherData object.\n\n\n# select desired bands (total_preciptation)\nprecip = weatherData.select(\"total_precipitation\")\n\n\nWe can look at our precip object using the print method to see that it’s still an “ImageCollection” which contains daily information from 1979 to 2020.\n\nTODO: this is a surrogate for the data (metadata)\n\nprint(precip)\n\n\n\n\n\n\n\nNote\n\n\n\nYou may see a message in the VS Code Interactive pane that says, “Output exceeds the size limit. Open the full output data in a text editor” when printing your image object. Click here to see the entire output, which includes date range information.\n\n\n\nLet’s say that we want to filter it down to a single field for a time of interest – e.g. January 1, 2019 - December 31, 2019. We can apply the .filterDate method to our selected precipitation parameter to filter for data from our chosen date range. We also apply the .mean method, which takes whatever precedes it and calculates the average.\n\n\n# initial date of interest (inclusive)\ni_date = '2019-01-01'\n\n# final data of interest (exclusive)\nf_date = '2020-01-01'\n\n# select apporpriate bands (total_preciptation) and dates\nprecip_filtered = weatherData.select(\"total_precipitation\").filterDate(i_date, f_date).mean()\n\n\nUse the print method again to check out your new precip_filtered object – notice that it’s now an ee.Image (rather than ee.ImageCollection) and the start and end date values are as we specified.\n\n\nprint(precip_filtered)\n\n\nAdd data to map\n\n\nWe can first use the setCenter method to tell the map where to center itself (here, we center it over Cook Inlet, Alaska). It takes the longitude and latitude as the first two coordinates, followed by the zoom level.\n\n\nmyMap.setCenter(lat = 60, lon = -151, zoom = 4)\nmyMap\n\n\nNext, set a color palette to use when plotting the data layer. The following is a palette specified for precipitation in the GEE description page for ERA5. GEE has lots of color tables like this that you can look up.\n\n\nprecip_palette = {\n    'min':0,\n    'max':0.01,\n    'palette': ['#FFFFFF', '#00FFFF', '#0080FF', '#DA00FF', '#FFA400', '#FF0000']\n}\n\n\nFinally, plot our filtered data, precip_filtered on top of our basemap using the .addLayer method. We’ll also pass it our visualization parameters (colors and ranges stored in precip_palette, the name of the data field total precipitation, and opacity so that we can see the basemap underneath)\n\n\nmyMap.addLayer(precip_filtered, precip_palette, 'total precipitation', opacity = 0.3)\nmyMap"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#exercise-2-ingmars-demo",
    "href": "sections/15-google-earth-engine.html#exercise-2-ingmars-demo",
    "title": "15  Google Earth Engine",
    "section": "15.4 Exercise 2: Ingmar’s Demo",
    "text": "15.4 Exercise 2: Ingmar’s Demo\n\n\n\n\n\n\nTODO\n\n\n\nAdd content here\n\n\n\nImport libraries\n\n\nimport ee \nimport geemap\nimport eemont\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#conclusionsummary",
    "href": "sections/15-google-earth-engine.html#conclusionsummary",
    "title": "15  Google Earth Engine",
    "section": "15.5 Conclusion/Summary",
    "text": "15.5 Conclusion/Summary\n\n\n\n\n\n\nTODO\n\n\n\n\nlessons learned\nutilities\netc."
  },
  {
    "objectID": "sections/15-google-earth-engine.html#other-resources",
    "href": "sections/15-google-earth-engine.html#other-resources",
    "title": "15  Google Earth Engine",
    "section": "15.6 Other Resources",
    "text": "15.6 Other Resources\n\nGEE Code Editor,a web-based IDE for using GEE (JavaScript)\nearthengine-api installation instructions\nCreating and managing Google Cloud projects\nTroubleshooting authentication issues\nAn Intro to the Earth Engine Python API"
  },
  {
    "objectID": "sections/17-group-project-3.html",
    "href": "sections/17-group-project-3.html",
    "title": "16  Group Project: Visualization",
    "section": "",
    "text": "In your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-17.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills."
  },
  {
    "objectID": "sections/18-arctic-data-staging.html",
    "href": "sections/18-arctic-data-staging.html",
    "title": "17  Workflows for data staging and publishing",
    "section": "",
    "text": "NSF archival policies for large datasets\nData transfer tools\nUploading large datasets to the Arctic Data Center\nWorkflow tools"
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#nsf-policy-for-large-datasets",
    "href": "sections/18-arctic-data-staging.html#nsf-policy-for-large-datasets",
    "title": "17  Workflows for data staging and publishing",
    "section": "17.2 NSF policy for large datasets",
    "text": "17.2 NSF policy for large datasets\nMany different research methods can generate large volumes of data. Numerical modeling (such as climate or ocean models) and anything generating high resolution imagery are two examples we see very commonly. The NSF requirements state:\n\nThe Office of Polar Programs policy requires that metadata files, full data sets, and derived data products, must be deposited in a long-lived and publicly accessible archive.\n\n\nMetadata for all Arctic supported data sets must be submitted to the NSF Arctic Data Center (https://arcticdata.io).\n\n\nExceptions to the above data reporting requirements may be granted for social science and indigenous knowledge data, where privacy or intellectual property rights might take precedence. Such requested exceptions must be documented in the Data Management Plan.\n\nThis means that datasets that are already published on a long lived archive do not need to be replicated to the Arctic Data Center, only a metadata record needs to be included. Often, the curation staff at the Arctic Data Center can replicate metadata programmatically such that the researcher in this case doesn’t have to publish their data twice. As an example of the myriad of scenarios that can arise in this realm, say a research project accesses many terabytes of VIIRS satellite data. In this case, the original satellite data does not need to be republished on the Arctic Data Center, since it is already available publicly, but the code that accessed it, and derived products, can be published, along with a citation to the satellite data indicating provenance.\nSimilarly, for some numerical models, if the model results can be faithfully reproduced from code, the code that generates the models can be a sufficient archival product, as opposed to the code and the model output. However, if the model is difficult to set up, or takes a very long time to run, we would probably reccommend publishing the output as well as code.\nThe Arctic Data Center is committed to archiving data of any volume, and our curation team is there to help researchers make decisions alongside NSF program officers, if necessary, to decide which portions of a large-data collection effort should be published in the archive."
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#data-transfer-tools",
    "href": "sections/18-arctic-data-staging.html#data-transfer-tools",
    "title": "17  Workflows for data staging and publishing",
    "section": "17.3 Data transfer tools",
    "text": "17.3 Data transfer tools\nNow that we’ve talked about what types of large datasets you might have that need to get published on the Arctic Data Center, let’s discuss how to actually get the data there. If you have even on the order of only 50GB, or more than 500 files, it will likely be more expedient for you to transfer your files via a command line tool than uploading them via our webform. So you know that you need to move a lot of data, how are you going to do it? More importantly, how can you do it in an efficient way?\nThere are three key elements to data transfer efficiency:\n\nendpoints\nnetwork\ntransfer tool\n\n\nEndpoints\nThe from and to locations of the transfer, an endpoint is a remote computing device that can communicate back and forth with the network to which it is connected. The speed with which an endpoint can communicate with the network varies depending on how it is configured. Performance depends on the CPU, RAM, OS, and disk configuration. One key factor that affects data transfer speed is how quickly that machine can write data to disk. Slow write speeds will throttle a data transfer on even the fastest internet connection with the most streamlined transfer tool. Examples of endpoints could be:\n\nNCEAS included-crab server\nYour standard laptop\nA cloud service like AWS\n\n\n\nNetwork speed\nNetwork speed determines how quickly information can be sent between endpoints. It is largely, but not enitrely, dependent on what you pay for. Importantly, not all networks are created equal, even if they nominally have the same speed capability. Wired networks get significantly more speed than wireless. Networks with lots of “stuff” along the pipe (like switches or firewalls) can perform worse than those that don’t. Even the length and type of network cabling used can matter.\n\n\nTransfer tools\nPoll: what data transfer tools do you use regularly?\nFinally, the tool or software that you use to transfer data can also significantly affect your transfer speed. There are a lot of tools out there that can move data around, both GUI driven and command line. We’ll discuss a few here, and their pros and cons.\n\nscp\nscp or secure copy uses ssh for authentication and transfer, and it is included with both unix and linux. It requires no setup (unless you are on a Windows machine and need to install), and if you can ssh to a server, you can probably use scp to move files without any other setup. scp copies all files linearly and simply. If a transfer fails in the middle, it is difficult to know exactly what files didn’t make it, so you might have to start the whole thing over and re-transfer all the files. This, obviously, would not be ideal for large data transfers. For a file or two, scp is a fine tool to use.\n\n\nrsync\nrsync is similar to scp, but syncs files/directories as opposed to copying. This means that rsync checks the destination to see if that file (with the same size and modified date) already exists. If it does, rsync will skip the file. This means that if an rsync transfer fails, it can be restarted again and will pick up where it left off, essentially. Neat!\n\n\nGlobus\nGlobus is a software that uses multiple network sockets simultaneously on endpoints, such that data transfers can run in parallel. As you can imagine, that parallelization can dramatically speed up data transfers. Globus, like rsync can also fail gracefully, and even restart itself. Globus does require that each endpoint be configured as a Globus node, which is more setup than is required of either scp or rsync. Many instituions computing resources may have endpoints already configured as Globus endpoints, so it is always worth checking in with any existing resources that might already be set up before setting up your own. Although Globus is a free software, there are paid options which provide support for configuring your local workstation as a Globus node. Globus is a fantastic tool, but remember the other two factors controlling data transfer, it can only help so much in overcoming slow network or write speeds.\n\n\n\n17.3.0.1 AWS sync\nAmazon Web Services (AWS) has a Command Line Interface (CLI) that includes a sync utility. This works much like rsync does in that it only copies new or updated files to the destination. The difference, of course, is that AWS sync is specifically built to work with interacting with the AWS cloud, and is compatible with S3 buckets.\n\n\n17.3.0.2 nc\nnc (or netcat) is a low level file transfer utility that is extremely efficient when moving files around on nodes in a cluster. It is not the easiest of these tools to use, however, in certain situations it might be the best option because it has the least overhead, and therefore can run extremely efficiently."
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#documenting-large-datasets",
    "href": "sections/18-arctic-data-staging.html#documenting-large-datasets",
    "title": "17  Workflows for data staging and publishing",
    "section": "17.4 Documenting large datasets",
    "text": "17.4 Documenting large datasets\nThe Arctic Data Center works hard to support datasets regardless of size, but we have performance considerations as well, and large datasets sometimes need special handling and require more processing time from our curation team. To help streamline a large dataset submission we have the following reccommendations:\n\nuse self documenting file formats, for metadata efficiency\n\nnetcdf\ngeotiff, geopackage\n\nregular, parseable filenames and consistent file formatting\ncommunicate early and often with the Arctic Data Center staff, preferably before you start a submission and well before your final report is due"
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#workflow-tools",
    "href": "sections/18-arctic-data-staging.html#workflow-tools",
    "title": "17  Workflows for data staging and publishing",
    "section": "17.5 Workflow tools",
    "text": "17.5 Workflow tools\nPreparing data for running analysis, models, and visualization processes can be complex, with many dependencies among datasets, as well as complex needs for data cleaning, munging, and integration that need to occur before “analysis” can begin.\nMany research projects would benefit from a structured approach to organizing these processes into workflows. A research workflow is an ordered sequence of steps in which the outputs of one process are connected to the inputs of the next in a formal way. Steps are then chained together to typically create a directed, acyclic graph that represents the entire data processing pipeline.\nThis hypothetical workflow shows three processing stages for downloading, integrating, and mapping the data, along with the outputs of each step. This is a simplified rendition of what is normally a much more complex process.\n\nWhether simple or complex, it is helpful to conceptualize your entire workflow as a directed graph, which helps to identify the explicit and implicit dependencies, and to plan work collaboratively.\n\n17.5.1 Workflow dependencies and encapsulation\nWhile there are many thousands of details in any given analysis, the reason to create a workflow is to structure all of those details so that they are understandable and traceable. Being explicit about dependencies and building a hierarchical workflow that encapsulates the steps of the work as independent modules. So the idea is to focus the workflow on the major steps in the pipeline, and to articulate each of their dependencies.\nWorkflows can be implemented in many ways, with various benefits:\n\nas a conceptual diagram\nas a series of functions that perform each step through a controlling script\nas a series functions managed by a workflow tool like parsl, snakemake, or ray\nmany others…\n\n\n\n17.5.2 DAGs\nWhile managing workflows solely as linked functions works, the presence of side-effects in a workflow can make it more difficult to efficiently run only the parts of the workflow where items have changed. Many workflow systems have been created to provide a structured way to specify, analyze, and track dependencies, and to execute only the parts of the workflow that are needed.\nA Directed Acyclic Graph (DAG) is a diagram that shows the dependencies of a workflow, whether they are data dependencies or process dependencies. Below is an example of a simplified DAG of the first step of the group project:\n\nThis graph only shows data dependencies, but process dependencies can also exist.\nIn your groups, draw out a simplified version of the rest of the workflow. What dependencies, both data and process based, exist?\nA more realistic workflow: https://github.com/NCEAS/scalable-computing-examples/tree/main/workflows"
  },
  {
    "objectID": "sections/20-reproducibility-containers.html",
    "href": "sections/20-reproducibility-containers.html",
    "title": "19  Reproducibility and Containers",
    "section": "",
    "text": "TODO: Decide about if/how to talk about WholeTale\nTODO: This lesson should be have a wow-factor and emphasize why we’re focusing all of this\nTODO: This lesson should be more about wrapping up and tying everything together than showing off new tech"
  },
  {
    "objectID": "sections/20-reproducibility-containers.html#learning-objectives",
    "href": "sections/20-reproducibility-containers.html#learning-objectives",
    "title": "19  Reproducibility and Containers",
    "section": "19.1 Learning Objectives",
    "text": "19.1 Learning Objectives\n\nLearn about software versioning\nBecome familiar with Docker as a tool to improve computational reproducibility"
  },
  {
    "objectID": "sections/20-reproducibility-containers.html#outline",
    "href": "sections/20-reproducibility-containers.html#outline",
    "title": "19  Reproducibility and Containers",
    "section": "19.2 Outline",
    "text": "19.2 Outline\n\nIntroduce software reproducibility\n\nMotivate the idea with examples and data\nTalk about software collapse\n\nhttp://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/\nhttps://xkcd.com/2347/\n\n\nSemantic versioning and the reality of it e.g., https://pandas.pydata.org/docs/development/policies.html#version-policy\nMyBinder\nWholeTale?\n\nExamples to look at including:\n\nhttps://numpy.org/neps/nep-0023-backwards-compatibility.html#example-cases\nhttps://github.com/scipy/scipy/issues/16418 > https://pandas.pydata.org/docs/whatsnew/v1.4.0.html#deprecations: DataFrame.append() and Series.append() have been deprecated and will be removed in a future version. Use pandas.concat() instead (GH35407).\n\nPrinciples to get across:\n\nYou probably should be thinking about software versioning\n\nKnow which version of versions of Python your code was written/tested under and keep track of that in a machine-readable way\nKnow the specific versions, of at least the specific MAJOR.MINOR of the packages your code was written+tested under and keep track of them in a machine-readable way (ie requirements.txt)"
  },
  {
    "objectID": "sections/20-reproducibility-containers.html#hands-off-demo",
    "href": "sections/20-reproducibility-containers.html#hands-off-demo",
    "title": "19  Reproducibility and Containers",
    "section": "19.3 Hands-off Demo",
    "text": "19.3 Hands-off Demo\nShow students an example of containerizing a workflow so it runs using a past version of Python and pinned versions of packages. Ideally find an example where behavior changes based on the Python or one or more package versions."
  }
]