[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "",
    "text": "This 5-day in-person workshop will provide researchers with an introduction to advanced topics in computationally reproducible research in python and R, including software and techniques for working with very large datasets. This includes working in cloud computing environments, docker containers, and parallel processing using tools like parsl and dask. The workshop will also cover concrete methods for documenting and uploading data to the Arctic Data Center, advanced approaches to tracking data provenance, responsible research and data management practices including data sovereignty and the CARE principles, and ethical concerns with data-intensive modeling and analysis.\n\n\n\n\n\n\n\n\n\n\nPlease note that by participating in this activity you agree to abide by the NCEAS Code of Conduct.\n\n\n\n\nIn this course, we will be using Python (> 3.0) as our primary language, and VS Code as our IDE. Below are instructions on how to get VS Code set up to work for the course. If you are already a regular Python user, you may already have another IDE set up. We strongly encourage you to set up VS Code with us, because we will use your local VS Code instance to write and execute code on one of the NCEAS servers.\n\n\nFirst, download VS Code if you do not already have it installed.\nYou’ll also need to download the Remote - SSH extension.\n\n\n\nTo connect to the server using VS Code follow these steps, from the VS Code window:\n\nopen the command pallette (Cmd + Shift + P)\nenter “Remote SSH: Connect to Host”\n\n\n\nselect “Add New SSH Host”\nenter the ssh command to connect to the host as if in a terminal (ssh username@included-crab.nceas.ucsb.edu)\n\nNote: you will only need to do this step once\n\n\n\n\nselect the SSH config file to update with the name of the host. You should select the one in your user directory (eg: /Users/jclark/.ssh/config)\nclick “Connect” in the popup in the lower right hand corner\n\nNote: If the dialog box does not appear, reopen the command palette (Cmd + Shift + P), type in “Remote-SSH: Connect to Host…”, choose included-crab.nceas.ucsb.edu from the options of configured SSH hosts, then enter your password into the dialog box that appears\n\nenter your password in the dialog box that pops up\n\nWhen you are connected, you will see in the lower left hand corner of the window a green bar that says “SSH: included-crab.nceas.ucsb.edu.”\n\n\n\n\nAfter connecting to the server, in the extensions pane (View > Extensions) search for, and install, the following extensions:\n- Python\n- Jupyter\n- Jupyter Keymap\n- Pylance\nNote that these extensions will be installed on the server, and not locally.\n\n\n\nWe are going to be working on the server exclusively, but if you are interested in setting up VS Code to work for you locally with Python, you can follow these instructions. This local setup section summarizes the official VS Code tutorial. For more detailed instructions and screenshots, see the source material. If you already use VS Code for Python you can skip this.\nLocally (not connected to the server), check to make sure you have Python installed if you aren’t sure you do. File > New Window will open up a new VS Code window locally.\nTo check your python, from the terminal run:\npython3 --version\nIf you get an error, it means you need to install Python. Here are instructions for getting installed, depending on your operating system. Note: There are many ways to install and manage your Python installations, and advantages and drawbacks to each. If you are unsure about how to proceed, feel free to reach out to the instructor team for guidance.\n\nWindows: Download and run an installer from Python.org.\nMac: Install using homebrew. If you don’t have homebrew installed, follow the instructions from their webpage.\n\nbrew install python3\n\n\nAfter you run your install, make sure you check that the install is on your system PATH by running python3 --version again.\nNext, install the Python extension for VS Code.\nOpen a terminal window in VS Code from the Terminal drop down in the main window. Run the following commands to initialize a project workspace in a directory called training. This example will show you how to do this locally. Later, we will show you how to set it up on the remote server with only one additional step.\nmkdir training\ncd training\ncode .\nNext, select the Python interpreter for the project. Open the Command Palette using Command + Shift + P (Control + Shift + P for windows). The Command Palette is a handy tool in VS Code that allows you to quickly find commands to VS Code, like editor commands, file edit and open commands, settings, etc. In the Command Palette, type “Python: Select Interpreter.” Push return to select the command, and then select the interpreter you want to use (your Python 3.X installation).\nTo make sure you can write and execute code in your project, create a Hello World test file.\n\nFrom the File Explorer toolbar, or using the terminal, create a file called hello.py\nAdd some test code to the file, and save\n\nmsg = \"Hello World\"\nprint(msg)\n\nExecute the script using either the Play button in the upper-right hand side of your window, or by running python3 hello.py in the terminal.\n\nFor more ways to run code in VS Code, see the tutorial\n\n\nFinally, to test Jupyter, download the Jupyter extension. You’ll also need to install ipykernel. From the terminal, run pip install ipykernel.\nYou can create a test Jupyter Notebook document from the command pallete by typing “Create: New Jupyter Notebook” and selecting the command. This will open up a code editor pane with a notebook that you can test.\n\n\n\n\nThese written materials reflect the continuous development of learning materials at the Arctic Data Center and NCEAS to support individuals to understand, adopt, and apply ethical open science practices. In bringing these materials together we recognize that many individuals have contributed to their development. The primary authors are listed alphabetically in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Matthew B. Jones, Bryce Mecum, S. Jeanette Clark, Samantha Csik. 2022. Scalable and Computationally Reproducible Approaches to Arctic Research.\nAdditional contributors: Amber E. Budden, Natasha Haycock-Chavez, Noor Johnson, Stephanie Hampton, Jim Regetz, Bryce Mecum, Julien Brun, Julie Lowndes, Erin McLean, Andrew Barrett, David LeBauer, Jessica Guo.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "sections/01-adc-intro.html",
    "href": "sections/01-adc-intro.html",
    "title": "1  Welcome and Introductions",
    "section": "",
    "text": "This course is one of three that we are currently offering, covering fundamentals of open data sharing, reproducible research, ethical data use and reuse, and scalable computing for reusing large data sets."
  },
  {
    "objectID": "sections/02-remote-computing.html",
    "href": "sections/02-remote-computing.html",
    "title": "2  Remote Computing",
    "section": "",
    "text": "Understand the basic architecture of computer networks\nLearn how to connect to a remote computer via a shell\nBecome familiarized with Bash Shell programming to navigate your computer’s file system, manipulate files and directories, and automate processes"
  },
  {
    "objectID": "sections/02-remote-computing.html#introduction",
    "href": "sections/02-remote-computing.html#introduction",
    "title": "2  Remote Computing",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nScientific synthesis and our ability to effectively and efficiently work with big data depends on the use of computers and the internet. Working on a personal computer may be sufficient for many tasks, but as data get larger and analyses more computationally intensive, scientists often find themselves needing more computing resources than they have available locally. Remote computing, or the process of connecting to a computer(s) in another location via a network link is becoming more and more common in overcoming big data challenges.\nIn this lesson, we’ll learn about the architecture of computer networks and explore some of the different remote computing configurations that you may encounter, we’ll learn how to securely connect to a remote computer via a shell, and we’ll become familiarized with using Bash Shell to efficiently manipulate files and directories. We will begin working in the VS Code IDE (integrated development environment), which is a versatile code editor that supports many different languages."
  },
  {
    "objectID": "sections/02-remote-computing.html#servers-networking",
    "href": "sections/02-remote-computing.html#servers-networking",
    "title": "2  Remote Computing",
    "section": "2.3 Servers & Networking",
    "text": "2.3 Servers & Networking\nRemote computing typically involves communication between two or more “host” computers. Host computers connect via networking equipment and can send messages to each other over communication protocols (aka an Internet Protocol, or IP). Host computers can take the role of client or server, where servers share their resources with the client. Importantly, these client and server roles are not inherent properties of a host (i.e. the same machine can play either role).\n\nClient: the host computer intiating a request\nServer: the host computer responding to a request\n\n\n\n\n\n\n\nNote\n\n\n\nHosts typically have one network address but can have many different ones (for example, adding multiple network cards to a single server increases bandwith).\n\n\n\nFig 1. Examples of different remote computing configurations. (a) A client uses secure shell protocol (SSH) to login/connect to a server over the internet. The client and the server exist in the physical world, but in different locations. (b) A client uses SSH to login/connect to a computing cluster (i.e. a set of computers (nodes) that work together so that they can be viewed as a single system) over the internet. The connection is first made through a gateway node (i.e. a computer that routes traffic from one network to another). The client and the cluster (server) exist in the physical world, but in different locations. (c) A client uses SSH to login/connect to a computing cluser where each node is a virtual machine (VM) hosted by a cloud computing service (e.g. Amazon Web Services, Google Cloud, Microsoft Azure, etc.). The connection is first made through a gateway node. The client and the gateway are located in the physical world, while the VM nodes are hosted in the cloud."
  },
  {
    "objectID": "sections/02-remote-computing.html#ip-addressing",
    "href": "sections/02-remote-computing.html#ip-addressing",
    "title": "2  Remote Computing",
    "section": "2.4 IP addressing",
    "text": "2.4 IP addressing\nHosts are assigned a unique numerical address used for all communication and routing called an Internet Protocol Address (IP Address). They look something like this: 128.111.220.7. Each IP Address can be used to communicate over various “ports”, which allows multiple applications to communicate with a host without mixing up traffic.\nBecause IP addresses can be difficult to remember, they are also assigned hostnames, which are handled through the global Domain Name System (DNS). Clients first look up a hostname in the DNS to find the IP address, then open a connection to the IP address.\n\n\n\n\n\n\nNote\n\n\n\nThroughout this course, we’ll be working on a server with the hostname, included-crab and an IP address, 128.111.85.1."
  },
  {
    "objectID": "sections/02-remote-computing.html#bash-shell-programming",
    "href": "sections/02-remote-computing.html#bash-shell-programming",
    "title": "2  Remote Computing",
    "section": "2.5 Bash Shell Programming",
    "text": "2.5 Bash Shell Programming\nWhat is a shell? From Wikipedia:\n\n“a computer program which exposes an operating system’s services to a human user or other programs. In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), depending on a computer’s role and particular operation.”\n\nWhat is Bash? Bash, or Bourne-again Shell, is a command line tool (language) commonly used to manipulate files and directories. Accessing and using bash is slightly different depending on what type of machine you work on:\n\nMac: bash via the Terminal, which comes ready-to-use with all Macs\nWindows: bash via Git Bash, which needs to be installed\n\n\n\n\n\n\n\nNote\n\n\n\nMac users may have to switch from Z Shell, or zsh, to bash. Use the command exec bash to switch your default shell to bash (or exec zsh to switch back).\n\n\n\n2.5.1 Some commonly used (and very helpful) bash commands:\nBelow are just a few bash commands that you’re likely to use. Some may be extended with options (more on that in the next section) or even piped together (i.e. where the output of one command gets sent to the next command, using the | operator).\n\n\n\n\n\n\n\nbash command\nwhat it does\n\n\n\n\npwd\nprint your current working directory\n\n\ncd\nchange directory\n\n\nls\nlist contents of a directory\n\n\ntree\ndisplay the contents of a directory in the form of a tree structure (not installed by default)\n\n\necho\nprint text that is passed in as an argument\n\n\nmv\nmove or rename a file\n\n\ncp\ncopy a file(s) or directory(ies)\n\n\ntouch\ncreate a new empty file\n\n\nmkdir\ncreate a new directory\n\n\nrm/rmdir\nremove a file/ empty directory (be careful – there is not “trash” folder!)\n\n\ngrep\nsearches a given file(s) for lines containing a match to a given pattern list\n\n\nsed\nstands for Stream Editor; a versatile command for editing files\n\n\ncut\nextract a specific portion of text in a file\n\n\njoin\njoin two files based on a key field present in both\n\n\ntop, htop\nview running processes in a Linux system (press Q to quit)\n\n\n\n\n\n2.5.2 General command syntax\nBash commands are typically are written as: command [options] [arguments] where the command must be an executable on your PATH and where options (settings that change the shell and/or script behavior) take one of two forms: short form (e.g. command -option-abbrev) or long form (e.g. command --option-name or command -o option-name). An example:\n# the `ls` command lists the files in a directory\nls file/path/to/directory\n\n# adding on the `-a` or `--all` option lists all files (including hidden files) in a directory\nls -a file/path/to/directory # short form\nls --all file/path/to/directory # long form\nls -o all file/path/to/directory # long form"
  },
  {
    "objectID": "sections/02-remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "href": "sections/02-remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "title": "2  Remote Computing",
    "section": "2.6 Connecting to a remote computer via a shell",
    "text": "2.6 Connecting to a remote computer via a shell\nIn addition to navigating your computer/manipulating your files, you can also use a shell to gain accesss to and remotely control other computers. To do so, you’ll need the following:\n\na remote computer (e.g. server) which is turned on\nthe IP address or name of the remote computer\nthe necessary permissions to access the remote computer\n\nSecure Shell, or SSH, is a network communication protocol that is often used for securely connecting to and running shell commands on a remote host. SSH temendously simplifies remote computing because ______, and it is supported out-of-the-box on Linux and Macs. If working on a Windows machine, you’ll need ____."
  },
  {
    "objectID": "sections/02-remote-computing.html#lets-practice",
    "href": "sections/02-remote-computing.html#lets-practice",
    "title": "2  Remote Computing",
    "section": "2.7 Let’s practice!",
    "text": "2.7 Let’s practice!\nWe’ll now use bash commands to do the following:\n\nconnect to the server, included-crab.nceas.ucsb.edu, that we’ll be working on for the remainder of this course\nnavigate through directories on the server and add/change/manipulate files\nautomate some of the above processes by writing a bash script\n\n\n2.7.1 Exercise 1: Connect to a server using the ssh command (or using VS Code’s command palette)\nLet’s connect to a remote computer (included-crab) and practice using some of above commands.\n\nLaunch your Terminal Program\n\n\nMacOS: navigate to Applications > Utilities and open Terminal\nWindows: Navigate to Windows Start > Git and open Git Bash\nALTERNATIVELY, from VS Code: Two options to open a terminal program, if a terminal isn’t already an open pane at the bottom of VS Code\n\nClick on Terminal > New Terminal in top menu bar\nClick on the + (dropdown menu) > bash in the bottom right corner\n\n\n\nConnect to a remote server\n\n\nYou can choose to SSH into the server (included-crab.nceas.ucsb.edu) through (a) the command line by using the ssh command, or (b) through VS Code’s command palette. If you prefer the latter, please refer back to the Log in to the server section. Doing so via the command line using the ssh command should look something like this:\n\nyourusername:~$ ssh yourusername@included-crab.nceas.ucsb.edu \nyourusername@included-crab.nceas.ucsb.edu's password: \nyourusername@included-crab:~$ \n\n\n\n\n\n\nImportant\n\n\n\nYou won’t see anything appear as you type or paste your password – this is a security feature! Type or paste your password and press enter/return when done to finish connecting to the server.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDO WE NEED THIS SECTION?\n\nChange your password\n\nyourusername@included-crab:~$ passwd\nChanging password for yourusername.\n(current) UNIX password: \nEnter new UNIX password: \nRetype new UNIX password: \n\n\n\n\n\n\n\n\nNote\n\n\n\nTo log out of the server, type exit – it should look something like this:\nyourusername@included-crab.nceas.ucsb.edu:$ exit\nlogout\nConnection to included-crab.nceas.ucsb.edu closed.\n(base) .....\n\n\n\n\n2.7.2 Exercise 2: Practice using some common bash commands\n\nUse the pwd command to print your current location, or working directory. You should be in your home directory on the server (e.g. /home/yourusername).\nUse the ls command to list the contents (any files or subdirectories) of your home directory\nUse the mkdir command to create a new directory named bash_practice:\n\nmkdir bash_practice\n\nUse the cd command to move into your new bash_practice directory:\n\n# move from /home/yourusername to home/yourusername/bash_practice\ncd bash_practice\n\nTo move up a directory level, use two dots, .. :\n\n# move from /home/yourusername/bash_practice back to /home/yourusername\n$ cd ..\n\n\n\n\n\n\nNote\n\n\n\nTo quickly navigate back to your home directory from wherever you may be on your computer, use a tilde, ~ :\n# e.g. to move from from some subdirectory, /home/yourusername/Projects/project1/data, back to your home directory, home/yourusername\n$ cd ~\n\n# or use .. to back out three subdirectories\n$ cd ../../..\n\n\n\nAdd some .txt files (file1.txt, file2.txt, file3.txt) to your bash_practice subdirectory using the touch command (Note: be sure to cd into bash_practice if you’re not already there):\n\n# add one file at a time\ntouch file1.txt\ntouch file2.txt\ntouch file3.txt\n\n# or add all files simultanously like this:\ntouch file{1..3}.txt\n\n# or like this:\ntouch file1.txt file2.txt file3.txt\n\nAdd other file types (e.g. .py, .csv, etc.)\n\ntouch mypython.py mycsv.csv\n\nCount the number of .txt files in bash_practice by combining the ls and wc (word count) funtions using the pipe, |, operator:\n\n# `ls *.txt` lists all files with a .txt extension\n# `wc` returns a word count (lines, words, chrs) and the `-l` option only returns the number of lines\n# use a pipe, `|`, to execute both commands at once\nls *.txt | wc -l\n\nDelete mypython.py and mycsv.csv using the rm command:\n\nrm mypython.py mycsv.csv\n\n\n2.7.3 Bonus Exercise: Automate processes with a Bash script\nAs we just demonstrated, we can use bash commands in the terminal to accomplish a variety of tasks like navigating our computer’s directories, manipulating/creating/adding files, and much more. However, writing a bash script allows us to gather and save our code for automated execusion.\nEarlier, we created a collection of .txt files and saved them to a new directory called bash_practice. Here, we’ll write a bash script to iterate over all those files and update ___.\nLet’s begin by creating a simple bash script that when executed, will print out the message, “Hello, World!” This simple script will help us determine whether or not things are working as expected before writing some more complex (and interesting) code.\n\nOpen a terminal window and determine where you are by using the pwd command. Navigate to where you’d like to save your bash script (your home directory on the server is fine) by using the cd command.\nNext, we’ll create a shell script called hello_world.sh using the touch command:\n\n$ touch hello_world.sh\n\nThere are a number of ways to edit a file or script – here, we’ll use Nano, a terminal-based text editor. Open your hello_world.sh with nano by running the following in your terminal:\n\n$ nano hello_world.sh\n\n\n\n\n\n\nTip\n\n\n\nYou can create and open a file in nano in just one line of code. For example, running nano hello_world.sh is the same as creating the file first using touch hello_world.sh, then opening it with nano using nano hello_world.sh\n\n\n\nWe can now start to write our script. Some important considerations:\n\n\nAnything following a # will not be executed as code – these are useful for adding comments to your scripts\nThe first line of a Bash script starts with a shebang, #!, followed by a path to the Bash interpreter – this is used to tell the operating system which interpreter to use to parse the rest of the file. There are two ways to use the shebang to set your interpreter (read up on the pros & cons of both methods on this Stack Overflow post):\n\n# (option a): use the absolute path to the bash binary\n#!/bin/bash\n\n# (option b): use the env untility to search for the bash executable in the user's $PATH environmental variable\n#!/usr/bin/env bash\n\nWe’ll first specify our bash interpreter using the shebang, which indicates the start of our script. Then, we’ll use the echo command, which when executed, will print whatever text is passed as an argument. Type the following into your script (which should be opened with nano), then save (Use the keyboard shortcut control + X to exit, then type Y when it asks if you’d like to save your work. Press enter/return to exit nano).\n\n# specify bash as the interpreter\n#!/bin/bash\n\n# print \"Hello, World!\"\n$ echo \"Hello, World!\"\n\nTo execute your script, run the following in your terminal (be sure that you’re in the same working directory as your hello_world.sh file or specify the file path to it):\n\nbash hello_world.sh\nIf successful, “Hello, World!” should be printed in your terminal window.\n\n\n\nUPDATE: write a simple shell script that does something – e.g. renaming files with bash loop (e.g. change extension, add date, move them around)\nUPDATE: nohup, screen, tmux for starting remote job that you can come back to later; look for tmux lesson in oss training"
  },
  {
    "objectID": "sections/03-python-intro.html",
    "href": "sections/03-python-intro.html",
    "title": "3  Python Programming on Clusters",
    "section": "",
    "text": "Basic Python review\nUsing virtual environments\nWriting in Jupyter notebooks\nWriting functions in Python"
  },
  {
    "objectID": "sections/03-python-intro.html#introduction",
    "href": "sections/03-python-intro.html#introduction",
    "title": "3  Python Programming on Clusters",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nWe’ve chosen to use VS Code in this training, in part, because it has great support for developing on remote machines. Hopefully, your VS Code setup went easily, and you were able to connect to our server included-crab. Once connected, the VS Code interface looks just like you were working locally, and connection to the server is seamless.\nOther aspects of VS Code that we like: it supports all languages thanks to the extensive free extension library, it has built in version control integration, and it is highly flexible/configurable.\nWe will also be working quite a bit in Jupyter notebooks in this course. Notebooks are great ways to interleave rich text (markdown formatted text, equations, images, links) and code in a way that a ‘literate analysis’ is generated. Although Jupyter notebooks are not subsitutes for python scripts, they can be great communication tools, and can also be convenient for code development."
  },
  {
    "objectID": "sections/03-python-intro.html#starting-a-project",
    "href": "sections/03-python-intro.html#starting-a-project",
    "title": "3  Python Programming on Clusters",
    "section": "3.3 Starting a project",
    "text": "3.3 Starting a project\nTo get set up for the course, let’s connect to the server again. If you were able to work through the setup for the lesson without difficulty, follow these steps to connect:\n\nopen the command pallette (Cmd + Shift + P)\nenter “Remote SSH: Connect to Host”\nselect included-crab\nenter your password in the dialog box that pops up\n\nNow we can get set up with a project to work in for the course. Head over to the scalable-computing-examples github repository and fork it to your account.\nBack in VS Code, in the terminal clone your fork of the scalable-computing-examples repo (git clone <url-to-forked-repo-here>) into the top level of your user directory. Run cd ~/ if you are in some other directory.\nTo open the project, open the folder into your workspace\n\nFile > Open Folder\nEnter password again if prompted"
  },
  {
    "objectID": "sections/03-python-intro.html#virtual-environments",
    "href": "sections/03-python-intro.html#virtual-environments",
    "title": "3  Python Programming on Clusters",
    "section": "3.4 Virtual Environments",
    "text": "3.4 Virtual Environments\n\nWhen you install a python library, let’s say pandas, via pip, unless you specify otherwise, pip will go out and grab the most recent version of the library, and install it somewhere on your system path (where, exactly, depends highly on how you install python originally, and other factors). This is all great, untl you realize that as part of a new project, a new library you are starting to work with requires a older version of pandas, what do you do? You need both pandas versions for each of your projects. Virtual environments help to solve this issue without making the all to common situation in the comic above even more complicated.\nA virtual environment is a folder structure which creates a symlink (pointer) to all of the libraries that you need into the folder. The three main components will be: the python distribution itself, its configuration, and a site-packages directory (where your libraries like pandas live). So the folder is a self contained directory of all the version-specific python software you need for your project.\nVirtual environments are very helpful to create reproducible workflows, and we’ll talk more about this concept of reproducible environements later in the course. Perhaps most importantly though, virtual environments also help you maintain your sanity when python programming. Because they are just folders, you can create and delete new ones at will, without worrying about bungling your underlying python setup.\nIn this course, we are going to use virtualenv as our tool to create and manage virtual environments. Other virtual environment tools used commonly are conda and pipenv. One reason we like using virtualenv is there is an extension to it called virtualenvwrapper, which provides easy to remember wrappers around common virtualenv operations that make creating, activating, and deactivating a virtual environment very easy.\nFirst we will create a .bash_profile file to create variables that point to the install locations of python and virtualenvwrapper. .bash_profile is just a text file that contains bash commands that are run every time you start up a new terminal. Although setting up this file is not required to use virtualenvwrapper, it is convenient because it allows you to set up some reasonable defaults to the commands (meaning less typing, overall), and it makes sure that the package is available every time you start a new terminal.\n\n3.4.0.1 Setup\n\nIn VS Code, select ‘File > New Text File’\nPaste this text into the file:\n\nexport VIRTUALENVWRAPPER_VIRTUAWORKON_HOME=$HOME/.virtualenvs\nsource /usr/share/virtualenvwrapper/virtualenvwrapper.sh\nThe first line points virtualenvwrapper to the directory where your virtual environments will be stored. We point it to a hidden directory (.virtualenvs) in your home directory. The last line sources a bash script that ships with virtualenvwrapper, which makes all of virtualenvwrapper commands available in your terminal session.\n\nSave the file in the top of your home directory as .bash_profile.\nRestart your terminal (Terminal > New Terminal)\nCheck to make sure it was installed and configured correctly by running this in the terminal:\n\nmkvirtualenv --version\nIt should return some content that looks like this (with more output, potentially).\nvirtualenv 20.13.0+ds from /usr/lib/python3/dist-packages/virtualenv/__init__.py\n\n\n3.4.0.2 Course environment\nNow we can create the virtual environment we will use for the course. In the terminal run:\nmkvirtualenv -p python3.9 scomp\nHere, we’ve specified explicitly which python version to use by using the -p flag, and the path to the python 3.9 installation on the server. After making a virtual environment, it will automatically be activated. You’ll see the name of the env you are working in on the left side of your terminal prompt in parentheses. To deactivate your environment (like if you want to work on a different project), just run deactivate. To activate it again, run:\nworkon scomp\nYou can get a list of all available environments by just running:\nworkon\nNow let’s install the dependencies for this course into that environment. To install our libraries we’ll use pip. As of Python 3.4, pip is automatically included with your python installation. pip is a package manager for python, and you might have used it already to install common python libraries like pandas or numpy. pip goes out to PyPI, the Python Package Index, to download the code and put it in your site-packages directory. Note that on this shared server, your user directory will ahve a site-packages directory, in addition to one that our systems administrator manages as the root of the system.\npip install -r requirements.txt\n\n\n3.4.0.3 Installing locally (optional)\nvirtualenvwrapper was already installed on the server we are working on. To install on your local computer, run:\npip3 install virtualenvwrapper\nAnd then follow the instructions as described above, making sure that you have the correct paths set when you edit your .bash_profile."
  },
  {
    "objectID": "sections/03-python-intro.html#brief-overview-of-python-syntax",
    "href": "sections/03-python-intro.html#brief-overview-of-python-syntax",
    "title": "3  Python Programming on Clusters",
    "section": "3.5 Brief overview of python syntax",
    "text": "3.5 Brief overview of python syntax\nWe’ll very briefly go over some basic python syntax and the base variable types. First, open a python script. From the File menu, select New File, type “python”, then save it as ‘python-intro.py’ in the top level of your directory.\nIn your file, assign a value to a variable using = and print the result.\n\nx = 4\nprint(x)\n\n4\n\n\nTo run this code in python we can:\n\nexecute python python-intro.py in the terminal\nclick the Play button in the upper right hand corner of the file editor\nright click any line and select: “Run to line in interactive terminal”\n\nIn that interactive window you can then run python code interactively, which is what we’ll use for the next bit of exploring data types.\nThere are 5 standard data types in python\n\nNumber (int, long, float, complex)\nString\nList\nTuple\nDictionary\n\nWe already saw a number type, here is a string:\n\nstr = 'Hello World!'\nprint(str)\n\nHello World!\n\n\nLists in python are very versatile, and are created using square brackets []. Items in a list can be of different data types.\n\nlist = [100, 50, -20, 'text']\nprint(list)\n\n[100, 50, -20, 'text']\n\n\nYou can access items in a list by index using the square brackets. Note indexing starts with 0 in python. The slice operator enables you to easily access a portion of the list without needing to specify every index.\n\nlist[0] # print first element\nlist[1:3] # print 2nd until 4th elements\nlist[:2] # print first until the 3rd\nlist[2:] # print last elements from 3rd\n\n100\n\n\n[50, -20]\n\n\n[100, 50]\n\n\n[-20, 'text']\n\n\nThe + and * operators work on lists by creating a new list using either concatenation (+) or repetition (*).\n\nlist2 = ['more', 'things']\n\nlist + list2\nlist * 3\n\n[100, 50, -20, 'text', 'more', 'things']\n\n\n[100, 50, -20, 'text', 100, 50, -20, 'text', 100, 50, -20, 'text']\n\n\nTuples are similar to lists, except the values cannot be changed in place. They are constructed with parentheses.\n\ntuple = ('a', 'b', 'c', 'd')\ntuple[0]\ntuple * 3\ntuple + tuple\n\n'a'\n\n\n('a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'a', 'b', 'c', 'd')\n\n\n('a', 'b', 'c', 'd', 'a', 'b', 'c', 'd')\n\n\nObserve the difference when we try to change the first value. It works for a list:\n\nlist[0] = 'new value'\nlist\n\n['new value', 50, -20, 'text']\n\n\n…and errors for a tuple.\n\ntuple[0] = 'new value'\n\nTypeError: 'tuple' object does not support item assignment\nDictionaries consist of key-value pairs, and are created using the syntax {key: value}. Keys are usually numbers or strings, and values can be any data type.\n\ndict = {'name': ['Jeanette', 'Matt'],\n        'location': ['Tucson', 'Juneau']}\n\ndict['name']\ndict.keys()\n\n['Jeanette', 'Matt']\n\n\ndict_keys(['name', 'location'])\n\n\nTo determine the type of an object, you can use the type() method.\n\ntype(list)\ntype(tuple)\ntype(dict)\n\nlist\n\n\ntuple\n\n\ndict"
  },
  {
    "objectID": "sections/03-python-intro.html#jupyter-notebooks",
    "href": "sections/03-python-intro.html#jupyter-notebooks",
    "title": "3  Python Programming on Clusters",
    "section": "3.6 Jupyter notebooks",
    "text": "3.6 Jupyter notebooks\nTo create a new notebook, from the file menu select File > New File > Jupyter Notebook\nAt the top of your notebook, add a first level header using a single hash. Practice some markdown text by creating:\n\na list\nbold text\na link\n\nUse the Markdown cheat sheet if needed.\nYou can click the plus button below any chunk to add a chunk of either markdown or python.\n\n3.6.1 Load libraries\nIn your first code chunk, lets load in some modules. We’ll use pandas, numpy, matplotlib.pyplot, requests, skimpy, and exists from os.path.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport requests\nimport skimpy\nimport os\n\nA note on style: There are a few ways to construct import statements. The above code uses three of the most common:\nimport module\nimport module as m\nfrom module import function\nThe first way of importing will make the module a function comes from more explicitly clear, and is the simplest. However for very long module names, or ones that are used very frequently (like pandas, numpy, and matplotlib.plot), the code in the notebook will be more cluttered with constant calls to longer module names. So module.function() instead is written as m.function()\nThe second way of importing a module is a good style to use in cases where modules are used frequently, or have extremely long names. If you import every single module with a short name, however, you might have a hard time remembering which modules are named what, and it might be more confusing for others trying to read your code. Many of the most commonly used libraries for python data science have community-driven styling for how they are abbreviated in import statements, and these community norms are generally best followed.\nFinally, the last way to import a single object from a module can be helpful if you only need that one piece from a larger module, but again, like the first case, results in less explicit code and therefore runs the risk of your or someone else misremembering the usage and source.\n\n\n3.6.2 Read in a csv\nCreate a new code chunk that will download the csv that we are going to use for this tutorial.\n\nNavigate to Rohi Muthyala, Åsa Rennermalm, Sasha Leidman, Matthew Cooper, Sarah Cooley, et al. 2022. 62 days of Supraglacial streamflow from June-August, 2016 over southwest Greenland. Arctic Data Center. doi:10.18739/A2XW47X5F.\nRight click the download button for ‘Discharge_timeseries.csv’\nClick ‘copy link address’\n\nCreate a variable called URL and assign it the link copied to your clipboard. Then use requests.get to download the file, and open to write it to disk, to a directory called data/. We’ll write this bundled in an if statement so that we only download the file if it doesn’t yet exist. First, we create the directory if it doesn’t exist:\n\nif not os.path.exists ('data/'):\n        os.mkdir('data/')\n\n\nif not os.path.exists('data/discharge_timeseries.csv'):\n\n        url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3Ae248467d-e1f9-4a32-9e38-a9b4fb17cefb'\n\n        data = requests.get(url)\n        a = open('data/discharge_timeseries.csv', 'wb').write(data.content)\n\nNow we can read in the data from the file.\n\ndf = pd.read_csv('data/discharge_timeseries.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Total Pressure [m]\n      Air Pressure [m]\n      Stage [m]\n      Discharge [m3/s]\n      temperature [degrees C]\n    \n  \n  \n    \n      0\n      6/13/2016 0:00\n      9.816\n      9.609775\n      0.206225\n      0.083531\n      -0.1\n    \n    \n      1\n      6/13/2016 0:05\n      9.810\n      9.609715\n      0.200285\n      0.077785\n      -0.1\n    \n    \n      2\n      6/13/2016 0:10\n      9.804\n      9.609656\n      0.194344\n      0.072278\n      -0.1\n    \n    \n      3\n      6/13/2016 0:15\n      9.800\n      9.609596\n      0.190404\n      0.068756\n      -0.1\n    \n    \n      4\n      6/13/2016 0:20\n      9.793\n      9.609537\n      0.183463\n      0.062804\n      -0.1\n    \n  \n\n\n\n\nThe column names are a bit messy so we can use clean_columns from skimpy to make them cleaner for programming very quickly. We can also use the skim function to get a quick summary of the data.\n\n\nclean_df = skimpy.clean_columns(df)\nskimpy.skim(clean_df)\n\n6 column names have been cleaned\n\n\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 17856  │ │ float64     │ 5     │                                                          │\n│ │ Number of columns │ 6      │ │ string      │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name              ┃ NA  ┃ NA %    ┃ mean     ┃ sd     ┃ p0        ┃ p25    ┃ p75   ┃ p100  ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━┩  │\n│ │ total_pressure_m         │   0 │       0 │      9.9 │   0.12 │       9.6 │    9.8 │    10 │    10 │ ▁▅█▇▅▂ │  │\n│ │ air_pressure_m           │   0 │       0 │      9.6 │   0.06 │       9.5 │    9.6 │   9.7 │   9.7 │ ▂▅▄▆█▃ │  │\n│ │ stage_m                  │   0 │       0 │     0.28 │   0.12 │   0.00056 │   0.17 │  0.37 │  0.56 │ ▁█▇▇▆▁ │  │\n│ │ discharge_m_3_s          │   0 │       0 │     0.22 │   0.19 │   4.7e-08 │  0.055 │  0.35 │  0.96 │  █▄▃▁  │  │\n│ │ temperature_degrees_     │   8 │   0.045 │   -0.034 │  0.053 │      -0.1 │   -0.1 │     0 │   0.2 │   ▅█   │  │\n│ └──────────────────────────┴─────┴─────────┴──────────┴────────┴───────────┴────────┴───────┴───────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name               ┃ NA      ┃ NA %       ┃ words per row                ┃ total words              ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ date                      │       0 │          0 │                            2 │                    36000 │  │\n│ └───────────────────────────┴─────────┴────────────┴──────────────────────────────┴──────────────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nWe can see that the date column is classed as a string, and not a date, so let’s fix that.\n\n\nclean_df['date'] = pd.to_datetime(clean_df['date'])\nskimpy.skim(clean_df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 17856  │ │ float64     │ 5     │                                                          │\n│ │ Number of columns │ 6      │ │ datetime64  │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name              ┃ NA  ┃ NA %    ┃ mean     ┃ sd     ┃ p0        ┃ p25    ┃ p75   ┃ p100  ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━┩  │\n│ │ total_pressure_m         │   0 │       0 │      9.9 │   0.12 │       9.6 │    9.8 │    10 │    10 │ ▁▅█▇▅▂ │  │\n│ │ air_pressure_m           │   0 │       0 │      9.6 │   0.06 │       9.5 │    9.6 │   9.7 │   9.7 │ ▂▅▄▆█▃ │  │\n│ │ stage_m                  │   0 │       0 │     0.28 │   0.12 │   0.00056 │   0.17 │  0.37 │  0.56 │ ▁█▇▇▆▁ │  │\n│ │ discharge_m_3_s          │   0 │       0 │     0.22 │   0.19 │   4.7e-08 │  0.055 │  0.35 │  0.96 │  █▄▃▁  │  │\n│ │ temperature_degrees_     │   8 │   0.045 │   -0.034 │  0.053 │      -0.1 │   -0.1 │     0 │   0.2 │   ▅█   │  │\n│ └──────────────────────────┴─────┴─────────┴──────────┴────────┴───────────┴────────┴───────┴───────┴────────┘  │\n│                                                    datetime                                                     │\n│ ┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name        ┃ NA    ┃ NA %     ┃ first            ┃ last                           ┃ frequency      ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩  │\n│ │ date               │     0 │        0 │    2016-06-13    │      2016-08-13 23:55:00       │ 5T             │  │\n│ └────────────────────┴───────┴──────────┴──────────────────┴────────────────────────────────┴────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nIf we wanted to calculate the daily mean flow (as opposed to the flow every 5 minutes), we need to:\n\ncreate a new column with only the date\ngroup by that variable\nsummarize over it by taking the mean\n\nFirst we should probably rename our existing date/time column to prevent from getting confused.\n\nclean_df = clean_df.rename(columns = {'date': 'datetime'})\n\nNow create the new date column\n\nclean_df['date'] = clean_df['datetime'].dt.date\n\nFinally, we use group by to split the data into groups according to the date. We can then apply the mean method to calculate the mean value across all of the columns. Note that there are other methods you can use to calculate different statistics across different columns (eg: clean_df.groupby('date').agg({'discharge_m_3_s': 'max'})).\n\ndaily_flow = clean_df.groupby('date', as_index = False).mean()\n\n\ncreate a simple plot\n\n\nvar = 'discharge_m_3_s'\nvar_labs = {'discharge_m_3_s': 'Total Discharge'}\n\nfig, ax = plt.subplots(figsize=(7, 7))\nplt.plot(daily_flow['date'], daily_flow[var])\nplt.xticks(rotation = 45)\nax.set_ylabel(var_labs.get('discharge_m_3_s'))\n\n(array([16967., 16974., 16983., 16990., 16997., 17004., 17014., 17021.,\n        17028.]),\n [Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, '')])\n\n\nText(0, 0.5, 'Total Discharge')"
  },
  {
    "objectID": "sections/03-python-intro.html#functions",
    "href": "sections/03-python-intro.html#functions",
    "title": "3  Python Programming on Clusters",
    "section": "3.7 Functions",
    "text": "3.7 Functions\nThe plot we made above is great, but what if we wanted to make it for each variable? We could copy paste it and replace some things, but this violates a core tenet of programming: Don’t Repeat Yourself! Instead, we’ll create a function called myplot that accepts the data frame and variable as arguments.\n\ncreate myplot.py\n\n\ndef myplot(df, var):\n\n        var_labs = {'discharge_m_3_s': 'Total Discharge (m^3/s)',\n                    'total_pressure_m': 'Total Pressure (m)',\n                    'air_pressure_m': 'Air Pressure (m)',\n                    'stage_m': 'Stage (m)',\n                    'temperature_degrees_c': 'Temperature (C)'}\n\n        fig, ax = plt.subplots(figsize=(7, 7))\n        plt.plot(df['date'], df[var])\n        plt.xticks(rotation = 45)\n        ax.set_ylabel(var_labs.get(var))\n\n\nload myplot into jupyter notebook (from myplot.py import myplot)\nreplace old plot method with new function\n\n\nmyplot(daily_flow, 'temperature_degrees_c')\n\n\n\n\nWe’ll have more on functions in the software design sections."
  },
  {
    "objectID": "sections/04-parallel-programming.html",
    "href": "sections/04-parallel-programming.html",
    "title": "4  Pleasingly Parallel Programming",
    "section": "",
    "text": "Understand what parallel computing is and when it may be useful\nUnderstand how parallelism can work\nReview sequential loops and map functions\nBuild a parallel program using concurrent.futures\nBuild a parallel program using parsl\nUnderstand Thread Pools and Process pools"
  },
  {
    "objectID": "sections/04-parallel-programming.html#introduction",
    "href": "sections/04-parallel-programming.html#introduction",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nProcessing large amounts of data with complex models can be time consuming. New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America:\n\n\n\nLevy et al. 2016. doi:10.5063/F1Z899CZ\n\n\nThere are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization.\nAlternatively, think of remote sensing data. Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years.\n\n\n\nNEON Data Cube"
  },
  {
    "objectID": "sections/04-parallel-programming.html#why-parallelism",
    "href": "sections/04-parallel-programming.html#why-parallelism",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.3 Why parallelism?",
    "text": "4.3 Why parallelism?\nMuch R code runs fast and fine on a single processor. But at times, computations can be:\n\ncpu-bound: Take too much cpu time\nmemory-bound: Take too much memory\nI/O-bound: Take too much time to read/write from disk\nnetwork-bound: Take too much time to transfer\n\nTo help with cpu-bound computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire cluster of those computers. Plus, these machines also have large amounts of memory to avoid memory-bound computing jobs."
  },
  {
    "objectID": "sections/04-parallel-programming.html#processors-cpus-and-cores",
    "href": "sections/04-parallel-programming.html#processors-cpus-and-cores",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.4 Processors (CPUs) and Cores",
    "text": "4.4 Processors (CPUs) and Cores\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with mutliple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations.\nA computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time.\n\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here we show four quad-core processors for a total of 16 cores in this machine.\n\nYou can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later).\nHistorically, R has only utilized one processor, which makes it single-threaded. Which is a shame, because the 2017 MacBook Pro that I am writing this on is much more powerful than that:\n{bash eval=FALSE} jones@powder:~$ sysctl hw.ncpu hw.physicalcpu hw.ncpu: 8 hw.physicalcpu: 4\nTo interpret that output, this machine powder has 4 physical CPUs, each of which has two processing cores, for a total of 8 cores for computation. I’d sure like my R computations to use all of that processing power. Because its all on one machine, we can easily use multicore processing tools to make use of those cores. Now let’s look at the computational server aurora at NCEAS:\n{bash eval=FALSE} jones@included-crab:~$ lscpu | egrep 'CPU\\(s\\)|per core|per socket' CPU(s):                88 On-line CPU(s) list:   0-87 Thread(s) per core:    2 Core(s) per socket:    22 NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86 NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87\nNow that’s more compute power! included-crab has 384 GB of RAM, and ample storage. All still under the control of a single operating system.\nHowever, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now:\n\nJetStream\n\n640 nodes, 15,360 cores, 80TB RAM\n\nStampede2 at TACC is coming online in 2017\n\n4200 nodes, 285,600 cores\n\nTODO: update with modern cluster sizes\n\nNote that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster. One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine. But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster."
  },
  {
    "objectID": "sections/04-parallel-programming.html#modes-of-parallelization",
    "href": "sections/04-parallel-programming.html#modes-of-parallelization",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.5 Modes of parallelization",
    "text": "4.5 Modes of parallelization\n\nTODO: develop diagram(s) showing\n\nSingle memory image task parallelization\n\n\nSerial Launch tasks --> Task 1 --> Task 2 --> Task 3 --> Task 4 --> Task 5 --> Finish\nParallel Launch tasks -->                     Task 1 --\\                    Task 2 ---\\                    Task 3 -----> Finish                      Task 4 ---/                     Task 5 --/\n\nCluster task parallelization\n\nCluster parallel Show dispatch to cluster nodes and reassembly of data   Launch tasks -->                     Marshal --> Task 1 --> Unmarshal --\\                    Marshal --> Task 2 --> Unmarshal ---\\                    Marshal --> Task 3 --> Unmarshal -----> Finish                      Marshal --> Task 4 --> Unmarshal ---/                     Marshal --> Task 5 --> Unmarshal --/\n\nTODO: Should we also include figure with data or functional dependencies?"
  },
  {
    "objectID": "sections/04-parallel-programming.html#task-parallelism-with-concurrent.futures",
    "href": "sections/04-parallel-programming.html#task-parallelism-with-concurrent.futures",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.6 Task parallelism with concurrent.futures",
    "text": "4.6 Task parallelism with concurrent.futures\nWhen you have a list of repetitive tasks, you may be able to speed it up by adding more computing power. If each task is completely independent of the others, then it is a prime candidate for executing those tasks in parallel, each on its own core. For example, let’s build a simple loop that downloads the data files that we need for an analysis. First, we start with the serial implementation.\n\n# Use loop for serial execution of tasks\n\n# Tasks are to download data from a dataset\n\nThe issue with this loop is that we execute each trial sequentially, which means that only one of our 8 processors on this machine are in use. In order to exploit parallelism, we need to be able to dispatch our tasks as functions, with one task going to each processor. To do that, we need to convert our task to a function, and then use the map() function to apply that function to all of the members of a set. Here’s the same code rewritten to use map(), which applies a function to each of the members of a list (in this case the files we want to download):\n\n# Use `map` for serial execution of tasks\n\n# Tasks are to download data from a dataset"
  },
  {
    "objectID": "sections/04-parallel-programming.html#approaches-to-parallelization",
    "href": "sections/04-parallel-programming.html#approaches-to-parallelization",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.7 Approaches to parallelization",
    "text": "4.7 Approaches to parallelization\nWhen parallelizing jobs, one can:\n\nUse the multiple cores on a local computer through mclapply\nUse multiple processors on local (and remote) machines using makeCluster and clusterApply\n\nIn this approach, one has to manually copy data and code to each cluster member using clusterExport\nThis is extra work, but sometimes gaining access to a large cluster is worth it"
  },
  {
    "objectID": "sections/04-parallel-programming.html#concurrent.futures",
    "href": "sections/04-parallel-programming.html#concurrent.futures",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.8 concurrent.futures",
    "text": "4.8 concurrent.futures\n\n# Loop versus map for parallel execution of tasks\n\n# Using concurrent.futures and ThreadPool\n\n# Tasks are to download data from a dataset"
  },
  {
    "objectID": "sections/04-parallel-programming.html#parsl",
    "href": "sections/04-parallel-programming.html#parsl",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.9 parsl",
    "text": "4.9 parsl\n\nOverview of parsl and it’s use of python decorators.\n\n\n# Loop versus map for parallel execution of tasks\n\n# Using parsl decorators and ThreadPool\n\n# Tasks are to download data from a dataset\n\n\nConfigurable Executors in parsl\n\nHightThroughputExecutor for cluster jobs\n\n\n\n# Loop versus map for parallel execution of tasks\n\n# Using parsl decorators and ThreadPool\n\n# Tasks are to download data from a dataset"
  },
  {
    "objectID": "sections/04-parallel-programming.html#when-to-parallelize",
    "href": "sections/04-parallel-programming.html#when-to-parallelize",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.10 When to parallelize",
    "text": "4.10 When to parallelize\nIt’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!\nIn addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced. Some propose that this may follow Amdahl’s Law, where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines):\n#| eval: false\nlibrary(ggplot2)\nlibrary(tidyr)\namdahl <- function(p, s) {\n  return(1 / ( (1-p) + p/s  ))\n}\ndoubles <- 2^(seq(0,16))\ncpu_perf <- cbind(cpus = doubles, p50 = amdahl(.5, doubles))\ncpu_perf <- cbind(cpu_perf, p75 = amdahl(.75, doubles))\ncpu_perf <- cbind(cpu_perf, p85 = amdahl(.85, doubles))\ncpu_perf <- cbind(cpu_perf, p90 = amdahl(.90, doubles))\ncpu_perf <- cbind(cpu_perf, p95 = amdahl(.95, doubles))\n#cpu_perf <- cbind(cpu_perf, p99 = amdahl(.99, doubles))\ncpu_perf <- as.data.frame(cpu_perf)\ncpu_perf <- cpu_perf %>% gather(prop, speedup, -cpus)\nggplot(cpu_perf, aes(cpus, speedup, color=prop)) + \n  geom_line() +\n  scale_x_continuous(trans='log2') +\n  theme_bw() +\n  labs(title = \"Amdahl's Law\")\nSo, its important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done. With that, let’s do some parallel computing…"
  },
  {
    "objectID": "sections/04-parallel-programming.html#summary",
    "href": "sections/04-parallel-programming.html#summary",
    "title": "4  Pleasingly Parallel Programming",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nIn this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores. Next, we reviewed the way in which traditional for loops in R can be rewritten as functions that are applied to a list serially using lapply, and then how the parallel package mclapply function can be substituted in order to utilize multiple cores on the local computer to speed up computations. Finally, we installed and reviewed the use of the foreach package with the %dopar operator to accomplish a similar parallelization using multiple cores."
  },
  {
    "objectID": "sections/04-parallel-programming.html#further-reading",
    "href": "sections/04-parallel-programming.html#further-reading",
    "title": "4  Pleasingly Parallel Programming",
    "section": "5.2 Further Reading",
    "text": "5.2 Further Reading\nRyan Abernathey & Joe Hamman. 2020. Closed Platforms vs. Open Architectures for Cloud-Native Earth System Analytics. Medium."
  },
  {
    "objectID": "sections/05-adc-data-publishing.html#introduction",
    "href": "sections/05-adc-data-publishing.html#introduction",
    "title": "5  Documenting and Publishing Data",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction"
  },
  {
    "objectID": "sections/06-group-project-1.html",
    "href": "sections/06-group-project-1.html",
    "title": "6  Group Project: Staging and Preprocessing",
    "section": "",
    "text": "Get familiarized with the overall group project workflow\nWrite a parsl app that will stage and tile the IWP example data in parallel"
  },
  {
    "objectID": "sections/06-group-project-1.html#introduction",
    "href": "sections/06-group-project-1.html#introduction",
    "title": "6  Group Project: Staging and Preprocessing",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nThe Permafrost Discovery Gateway is an online platform for archiving, processing, analysis, and visualization of permafrost big imagery products to enable discovery and knowledge-generation. The PDG utilizes and makes available products derived from high resolution satellite imagrey from the Polar Geospatial Center, Planet (3m), Sentinel (10 m), Landsat (30 m), and MODIS (250 m). One of these products is a dataset showing Ice Wedge Polygons (IWP) that form in melting permafrost.\nIce wedges form as a result of thermal contraction during melt/freeze cycles of permafrost. They can form very distinctive geometries clearly visible in satellite images. The PDG is using advanced analysis and computational tools to take high resolution satellite imagery and automatically dectect where ice wedge polygons form. Below is an example of a satellite image (left) and the detected ice wedge polygons in geospatial vector format (right) of that same image.\n\nIn the group project, we are going to use a subset of the high resolution dataset of these detected ice wedge polygons in order to learn some of the reproducible, scalable techniques that will allow us to process it. Our workflow will start with a set of large geopackage files that contain the detected ice wedge polygons. These files all have irregular extents due to the variation in satellite coverage, clouds, etc. Our first processing step will take these files and “tile” them into smaller files which have regular extents.\n\nIn step two of the workflow, we will take those regularly tiled geopackage files and rasterize them. The files will be regularly gridded, and a summary statistic will be calculated for each grid cell (such as the proportion of pixel area covered by polygons).\n\nIn the final step of the workflow, we will take the raster files and resample them to create a set of raster tiles at different resolutions. This last step is what will enable us to visualize our raster data dynamically, such that we look at lower resolutions when very zoomed out (and high resolution data would take too long to load), and higher resolution data when zoomed in and the extent is smaller."
  },
  {
    "objectID": "sections/06-group-project-1.html#staging-and-tiling",
    "href": "sections/06-group-project-1.html#staging-and-tiling",
    "title": "6  Group Project: Staging and Preprocessing",
    "section": "6.3 Staging and Tiling",
    "text": "6.3 Staging and Tiling\nToday we will undertake the first step of the workflow, staging and tiling the data.\n\nIn your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-06.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills."
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html",
    "href": "sections/08-data-structures-netcdf.html",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "",
    "text": "Efficient and reproducible data analysis begins with choosing a proper format to store our data, particularly when working with large, complex, multi-dimensional datasets. Consider, for example, the following Earth System Data Cube from Mahecha et al. 2020, which measures nine environmental variables at high resolution across space and time. We can consider this dataset large (high-resolution means we have a big file), complex (multiple variables), and multi-dimensional (each variable is measured along three dimensions: latitude, longitude, and time). Additionally, necessary metadata must accompany the dataset to make it functional, such as units of measurement for variables, information about the authors, and processing software used.\n\n\n\nMahecha et al. 2020 . Visualization of the implemented Earth system data cube. The figure shows from the top left to bottom right the variables sensible heat (H), latent heat (LE), gross primary production (GPP), surface moisture (SM), land surface temperature (LST), air temperature (Tair), cloudiness (C), precipitation (P), and water vapour (V). The resolution in space is 0.25° and 8 d in time, and we are inspecting the time from May 2008 to May 2010; the spatial range is from 15° S to 60° N, and 10° E to 65° W.\n\n\nKeeping complex datasets in a format that facilitates access, processing, sharing, and archiving can be at least as important as how we parallelize the code we use to analyze them. In practice, it is common to convert our data from less efficient formats into more efficient ones before we parallelize any processing. In this lesson, we will\n\nfamiliarize ourselves with the NetCDF data format, which enables us to store large, complex, multi-dimensional data efficiently, and\nlearn to use the xarray Python package to read, process, and create NetCDF files."
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#objectives",
    "href": "sections/08-data-structures-netcdf.html#objectives",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.2 Objectives",
    "text": "8.2 Objectives\n\nLearn about the NetCDF data format:\n\nCharacterstics: self-describing, scalable, portable, appendable, shareble, and archivable\nUnderstand the NetCDF data model: what are dimensions, variables and attributes\nAdvantages of random/arbitrary access and how that applies to parallel computing\n\nLearn how to use the xarray Python package to work with NetCDF files:\n\nDescribe the core xarray data structures, the xarray.DataArray and the xarray.Dataset, and their components, including: data variables, dimensions, coordinates, and attributes\nCreate xarray.DataArrays and xarra.DataSets out of raw numpy arrays and save them as netCDF files\nLoad xarray datasets from netCDF and understand the attributes view\nPerform basic indexing, processing and reduction of xarray.DataArrays"
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#netcdf-data-format",
    "href": "sections/08-data-structures-netcdf.html#netcdf-data-format",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.3 NetCDF data format",
    "text": "8.3 NetCDF data format\nNetCDF (network Common Data Form) is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. NetCDF was originaly developed at the Unidata Program Center and is supported on almost all platforms, and parsers exist for the vast majority of scientific programming languages.\n\n8.3.1 Characteristics\n[REF: https://docs.unidata.ucar.edu/netcdf-c/current/faq.html#ncFAQGeneral] TO DO: maybe add a 5 word discussion of each point\nNetCDF files are designed to be:\n\nSelf-describing: Information describing the data contents of the file are embedded within the data file itself. This means that there is a header which describes the layout of the rest of the file as well as arbitrary file metadata.\nScalable: Small subsets of large datasets may be accessed efficiently through netCDF interfaces, even from remote servers.\nPortable: A NetCDF file is machine-independent i.e. it can be accessed by computers with different ways of storing integers, characters, and floating-point numbers.\nAppendable: Data may be appended to a properly structured NetCDF file without copying the dataset or redefining its structure.\nSharable: One writer and multiple readers may simultaneously access the same NetCDF file.\nArchivable: Access to all earlier forms of NetCDF data will be supported by current and future versions of the software.\n\n\n\n8.3.2 Data Model\nThe NetCDF data model is the way that NetCDF organizes data. The Classic NetCDF Data Model consists of variables, dimensions, and attributes. This way of thinking about data was introduced with the very first NetCDF release, and is still the core of all netCDF files. There exists a new Enhanced Data Model, but for maximum interoperability with existing code, new data should be created with the Classic Model.\n[REF https://docs.unidata.ucar.edu/netcdf-c/current/netcdf_data_model.html#classic_model]\n\n\n\nClassic NetCDF Data Model (NetCDF documentation)\n\n\nThe model consists of three key components:\nVariables are N-dimensional arrays of data. Variables in netCDF files can be one of six types (char, byte, short, int, float, double). We can think of these as the varying/measured/dependent quantities.\nDimensions describe the axes of the data arrays. A dimension has a name and a length. An unlimited dimension has a length that can be expanded at any time, as more data are written to it. NetCDF files can contain at most one unlimited dimension. We can think of these as the constant/fixed/independent quantities at which we measure the variables.\nAttributes are small notes or supplementary metadata to annotate either a variable or the file as a whole. Although there is no enforced limit, the user is expected to keep attributes small.\n\n\n\n\n\n\nNote\n\n\n\nThe most commonly used metadata standard for geospatial data is the Climate and Forecast metadata standard, also called the CF conventions. These standards are specifically designed to promote the processing and sharing of files created with the NetCDF API. Principles of CF include self-describing data (no external tables needed for understanding); metadata equally readable by humans and software; minimum redundancy and maximum simplicity.\n\n\n\n\n8.3.3 Exercise\n[TO DO: check exercise setup]\nImagine the following scenario: we have a network of 25 weather stations. They are located in a square grid: starting at 30°0′N 60°0′E, there is a station every 10° North and every 10° East. Each station measures the air temperature at a set time for three days, starting on September 1st, 2022. The first day all stations record a temperature of 0°C, the second day all temperatures are 1°C, and the third day all temperatures are 3°C. What are the variables, dimensions and attributes for this data?\nVariables: There is a single variable being measured: temperature. The variable values can be represented as a 3x5x5 array, with constant values for each day as seen in the diagram.\nDimensions: There are three dimensions in this dataset: date, latitude, and longitude. Time indicates when the measurement happened, we can encode it as the dates 2022-09-1, 2022-09-02, and 2022-09-03. The pairs of latitude and longitude values indicate the positions of the weather stations. Latitude has values 30, 40, 50, 60, and 70, measured in degrees North, while longitude has values 60, 70, 80, 90, and 100, measured in degrees East.\nAttributes: Let’s divide these in attributes for the variable, the dimensions, and the whole dataset:\n\nTemperature attributes:\n\nunits: degrees Celsius\n\nTime attributes:\n\ndescription: date of measurement\n\nLatitude attributes:\n\nunits: degrees North\n\nLongitude attributes:\n\nunits: degrees East\n\nDataset attributes:\n\ntitle: temperature at weather stations\nsummary: an example of NetCDF data format\n\n\nOur next step is to see how we can translate all this information into something we can store and handle in our computer."
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#xarray",
    "href": "sections/08-data-structures-netcdf.html#xarray",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.4 xarray",
    "text": "8.4 xarray\nhttps://docs.xarray.dev/en/stable/getting-started-guide/why-xarray.html https://docs.xarray.dev/en/stable/getting-started-guide/faq.html\nMulti-dimensional arrays or ND arrays are frequently encountered in geosciences. Consider, for example, how many variables can be measured with respect to space-time dimensions, making those datasets three or even four-dimensional (for instance, if we use latitude, longitude, height/depth, and time). In Python, the NumPy package provides the fundamental data structure and API for working with raw ND arrays. However, real-world datasets are usually more than just raw numbers; they have labels that encode information about how the array values correspond to locations in space, time, etc. xarray is an answer to this necessity: an xarray.DataArray has labeled dimensions (e.g. “time”, “latitude”) that can be directly referenced for processing. It is easier to keep track of a dimension labeled “time” than to remember that time is the n-th dimension of the array. Moreover, xarray is based on the netCDF data model, making it the appropriate tool to open, process and create datasets in netCDF format.\n\n8.4.1 Creating an xarray.DataArray\nAn xarray.DataArray is an N-dimensional array with labeled coordinates and dimensions. It is the primary data structure of the xarray package. We can think of it as representing a single variable in the NetCDF data format: it holds the variable’s values, dimensions, and attributes. Additionally, each dimension has at least one set of coordinates, which indicate the values the dimension takes. We can thin of the coordinate’s values as the tick labels along a dimension. [REF https://docs.xarray.dev/en/stable/user-guide/terminology.html]\nAs our first example, let’s suppose we want to make an xarray.DataArray that includes the information from our previous exercise about measuring temperature across three days. First, we import all our necessary libraries.\n\nimport os              \nimport requests \nimport pandas as pd\nimport numpy as np\n\nimport xarray as xr   # This is the package we'll explore\n\nThe underlying data in the xarray.DataArray is a numpy.ndarray that holds the variable values. So we can start by making a numpy.ndarray with our mock temperature data:\n\n# values of a single variable at each point of the coords \ntemp_data = np.array([np.zeros((5,5)), \n                      np.ones((5,5)), \n                      np.ones((5,5))*2]).astype(int)\ntemp_data\n\narray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])\n\n\nWe could think that this is “all” we need to represent our data. But if we stopped at this point, we would need to\n\nremember that the numbers in this array represent temperature in degrees Celsius (doesn’t seem too bad),\nremember that the first dimension of the array represents time, the second latitude and the third longitude (maybe ok), and\nkeep track of the range of values that time, latitude and longitude take (not so good).\n\nKeeping track of all this information separately could quickly get messy and could make it challenging to share our data and analyses with others . This is what the netCDF data model and xarray aim to simplify. We can get data and its descriptors together in an xarray.DataArray by adding the dimensions over which the variable is being measured and including attributes that appropriately describe dimensions and variables.\n\n# names of the dimensions\ndims = ('time', 'lat', 'lon')\n\n# coordinates (tick labels) to use for indexing along each dimension \ncoords = {'time' : pd.date_range(\"2022-09-01\", \"2022-09-03\"),\n          'lat' : np.arange(30,80,10),\n          'lon' : np.arange(60,110,10)}  \n\n# attributes (metadata) of the data array \nattrs = { 'title' : 'temperature across weather stations',\n          'units' : 'degrees_celsius'}\n\n# initialize xarray.DataArray\ntemp = xr.DataArray(data = temp_data, \n                    dims = dims,\n                    coords = coords,\n                    attrs = attrs)\ntemp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 3, lat: 5, lon: 5)>\narray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])\nCoordinates:\n  * time     (time) datetime64[ns] 2022-09-01 2022-09-02 2022-09-03\n  * lat      (lat) int64 30 40 50 60 70\n  * lon      (lon) int64 60 70 80 90 100\nAttributes:\n    title:    temperature across weather stations\n    units:    degrees_celsiusxarray.DataArraytime: 3lat: 5lon: 50 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2array([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Coordinates: (3)time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03array(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')lat(lat)int6430 40 50 60 70array([30, 40, 50, 60, 70])lon(lon)int6460 70 80 90 100array([ 60,  70,  80,  90, 100])Attributes: (2)title :temperature across weather stationsunits :degrees_celsius\n\n\nWe can also update the variable’s attributes after creating the object. Notice that each of the coordinates is also an xarray.DataArray, so we can add attributes to them.\n\n# update attributes\ntemp.attrs['description'] = 'simple example of an xarray.DataArray'\n\n# add attributes to coordinates \ntemp.time.attrs = {'standard_name':'date of collection'}\n\ntemp.lat.attrs['standard_name']= 'latitude'\ntemp.lat.attrs['units'] = 'degrees_north'\n\ntemp.lon.attrs['standard_name']= 'longitude'\ntemp.lon.attrs['units'] = 'degrees_east'\ntemp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 3, lat: 5, lon: 5)>\narray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])\nCoordinates:\n  * time     (time) datetime64[ns] 2022-09-01 2022-09-02 2022-09-03\n  * lat      (lat) int64 30 40 50 60 70\n  * lon      (lon) int64 60 70 80 90 100\nAttributes:\n    title:        temperature across weather stations\n    units:        degrees_celsius\n    description:  simple example of an xarray.DataArrayxarray.DataArraytime: 3lat: 5lon: 50 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2array([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Coordinates: (3)time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03standard_name :date of collectionarray(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')lat(lat)int6430 40 50 60 70standard_name :latitudeunits :degrees_northarray([30, 40, 50, 60, 70])lon(lon)int6460 70 80 90 100standard_name :longitudeunits :degrees_eastarray([ 60,  70,  80,  90, 100])Attributes: (3)title :temperature across weather stationsunits :degrees_celsiusdescription :simple example of an xarray.DataArray\n\n\nAt this point, since we have a single variable, the dataset attributes and the variable attributes are the same.\n\n\n8.4.2 Indexing\nAn xarray.DataArray allows both positional indexing (like numpy) and label-based indexing (like pandas). Positional indexing is the most basic and it’s done using Python’s [] syntax, as in array[i,j] with i and j both integers. Label-based indexing takes advantage of dimensions in the array having names and coordinate values that we can use to access data, instead of remembering the positional order of each dimension.\nAs an example, suppose we want to know what was the temperature recorded by the weather station located at 40°0′N 80°0′E on September 1st, 2022. By recalling all the information about how the array is setup with respect to the dimensions and coordiantes, we can access this data positionally:\n\ntemp[0,1,2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray ()>\narray(0)\nCoordinates:\n    time     datetime64[ns] 2022-09-01\n    lat      int64 40\n    lon      int64 80\nAttributes:\n    title:        temperature across weather stations\n    units:        degrees_celsius\n    description:  simple example of an xarray.DataArrayxarray.DataArray0array(0)Coordinates: (3)time()datetime64[ns]2022-09-01standard_name :date of collectionarray('2022-09-01T00:00:00.000000000', dtype='datetime64[ns]')lat()int6440standard_name :latitudeunits :degrees_northarray(40)lon()int6480standard_name :longitudeunits :degrees_eastarray(80)Attributes: (3)title :temperature across weather stationsunits :degrees_celsiusdescription :simple example of an xarray.DataArray\n\n\nOr, we can use the dimensions names and their coordinates to access the same value:\n\ntemp.sel(time='2022-09-01', lat=40, lon=80)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray ()>\narray(0)\nCoordinates:\n    time     datetime64[ns] 2022-09-01\n    lat      int64 40\n    lon      int64 80\nAttributes:\n    title:        temperature across weather stations\n    units:        degrees_celsius\n    description:  simple example of an xarray.DataArrayxarray.DataArray0array(0)Coordinates: (3)time()datetime64[ns]2022-09-01standard_name :date of collectionarray('2022-09-01T00:00:00.000000000', dtype='datetime64[ns]')lat()int6440standard_name :latitudeunits :degrees_northarray(40)lon()int6480standard_name :longitudeunits :degrees_eastarray(80)Attributes: (3)title :temperature across weather stationsunits :degrees_celsiusdescription :simple example of an xarray.DataArray\n\n\nNotice that the result of this indeixing is a 1x1 xarray.DataArray. This is because operations on a xarray.DataArrayn(resp. xarray.DataSet) always return anotherxarray.DataArray (resp. xarray.DataSet) object. In particular, operations returning scalar values will also return an xarray objects, so we need to cast as numbers them manualy. See xarray.DataArray.item.\nMore about xarray indexing.\n\n\n8.4.3 Reduction\nxarray has implemented a number of methods to reduce an xarray.DataArray along any number of dimensions. One of the advantages of xarray.DataArray is that, if we chose to, it can carry over attributes when doing calculations. For example, we can calculate the average temperature at each weather station over time and obtain a new xarray.DataArray.\n\navg_temp = temp.mean(dim = 'time') \n# to keep attributes add keep_attrs = True\n\navg_temp.attrs = {'title':'average temperature over three days'}\navg_temp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (lat: 5, lon: 5)>\narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])\nCoordinates:\n  * lat      (lat) int64 30 40 50 60 70\n  * lon      (lon) int64 60 70 80 90 100\nAttributes:\n    title:    average temperature over three daysxarray.DataArraylat: 5lon: 51.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0array([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])Coordinates: (2)lat(lat)int6430 40 50 60 70standard_name :latitudeunits :degrees_northarray([30, 40, 50, 60, 70])lon(lon)int6460 70 80 90 100standard_name :longitudeunits :degrees_eastarray([ 60,  70,  80,  90, 100])Attributes: (1)title :average temperature over three days\n\n\nMore about xarray computations.\n\n\n8.4.4 Creating an xarray.DataSet\nAn xarray.DataSet resembles an in-memory representation of a NetCDF file, and consists of multiple variables, with coordinates and attributes, which together form a self describing dataset. We can make create a xarray.DataSet by putting together the temperature data with the average temperature data. We also add some attributes that now describe the whole dataset, not only each variable. Take some time to click through the data viewer and notice how all the data and metadata is ordered within the dataset.\n\n# make dictionaries with variables and attributes\ndata_vars = {'avg_temp': avg_temp,\n            'temp': temp}\n\nattrs = {'creator_name':'CGG',\n        'title':'temperature data at weather stations: daily and and average',\n        'description':'simple example of an xarray.Dataset'}\n\n# create xarray.Dataset\ntemp_dataset = xr.Dataset( data_vars = data_vars,\n                        attrs = attrs)\ntemp_dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (lat: 5, lon: 5, time: 3)\nCoordinates:\n  * lat       (lat) int64 30 40 50 60 70\n  * lon       (lon) int64 60 70 80 90 100\n  * time      (time) datetime64[ns] 2022-09-01 2022-09-02 2022-09-03\nData variables:\n    avg_temp  (lat, lon) float64 1.0 1.0 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0 1.0\n    temp      (time, lat, lon) int64 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2\nAttributes:\n    creator_name:  CGG\n    title:         temperature data at weather stations: daily and and average\n    description:   simple example of an xarray.Datasetxarray.DatasetDimensions:lat: 5lon: 5time: 3Coordinates: (3)lat(lat)int6430 40 50 60 70standard_name :latitudeunits :degrees_northarray([30, 40, 50, 60, 70])lon(lon)int6460 70 80 90 100standard_name :longitudeunits :degrees_eastarray([ 60,  70,  80,  90, 100])time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03standard_name :date of collectionarray(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (2)avg_temp(lat, lon)float641.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0title :average temperature over three daysarray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])temp(time, lat, lon)int640 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2title :temperature across weather stationsunits :degrees_celsiusdescription :simple example of an xarray.DataArrayarray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Attributes: (3)creator_name :CGGtitle :temperature data at weather stations: daily and and averagedescription :simple example of an xarray.Dataset\n\n\n\n\n8.4.5 Save and reopen\nFinally, we want to save our dataset as a NetCDF file. To do this, first specify the file path and use the .nc extension for the file name. Then save the datset by using the to_netcdf method with your file path. Opening NetCDF is similarly straightforward using xarray.open_dataset().\n\n# specify file path: don't forget the .nc extension!\nfp = os.path.join(os.getcwd(),'temp_dataset.nc') \n# save file\ntemp_dataset.to_netcdf(fp)\n\n# open to check:\ncheck = xr.open_dataset(fp)\ncheck\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (lat: 5, lon: 5, time: 3)\nCoordinates:\n  * lat       (lat) int64 30 40 50 60 70\n  * lon       (lon) int64 60 70 80 90 100\n  * time      (time) datetime64[ns] 2022-09-01 2022-09-02 2022-09-03\nData variables:\n    avg_temp  (lat, lon) float64 1.0 1.0 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0 1.0\n    temp      (time, lat, lon) int64 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2\nAttributes:\n    creator_name:  CGG\n    title:         temperature data at weather stations: daily and and average\n    description:   simple example of an xarray.Datasetxarray.DatasetDimensions:lat: 5lon: 5time: 3Coordinates: (3)lat(lat)int6430 40 50 60 70standard_name :latitudeunits :degrees_northarray([30, 40, 50, 60, 70])lon(lon)int6460 70 80 90 100standard_name :longitudeunits :degrees_eastarray([ 60,  70,  80,  90, 100])time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03standard_name :date of collectionarray(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (2)avg_temp(lat, lon)float64...title :average temperature over three daysarray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])temp(time, lat, lon)int64...title :temperature across weather stationsunits :degrees_celsiusdescription :simple example of an xarray.DataArrayarray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Attributes: (3)creator_name :CGGtitle :temperature data at weather stations: daily and and averagedescription :simple example of an xarray.Dataset\n\n\n\n\n8.4.6 Exercise\nFor this exercise we will use a dataset including timeseries of annual Arctic freshwater fluxes and storage terms. The data was produced for the publication Jahn and Laiho, 2020 about changes in the Arctic freshwater budget and is archived at the Arctic Data Center doi:10.18739/A2280504J\n\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A792bfc37-416e-409e-80b1-fdef8ab60033'\n\nresponse = requests.get(url)\nopen(\"FW_data_CESM_LW_2006_2100.nc\", \"wb\").write(response.content)\n\n208086\n\n\n\nfp = os.path.join(os.getcwd(),'FW_data_CESM_LW_2006_2100.nc')\nfw_data = xr.open_dataset(fp)\nfw_data\n#netPrec_annual\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                          (time: 95, member: 11)\nCoordinates:\n  * time                             (time) float64 2.006e+03 ... 2.1e+03\n  * member                           (member) float64 1.0 2.0 3.0 ... 10.0 11.0\nData variables: (12/16)\n    FW_flux_Fram_annual_net          (time, member) float64 -1.26e+03 ... -2....\n    FW_flux_Barrow_annual_net        (time, member) float64 -600.7 ... -537.2\n    FW_flux_Nares_annual_net         (time, member) float64 -1.805e+03 ... -2...\n    FW_flux_Davis_annual_net         (time, member) float64 -2.313e+03 ... -3...\n    FW_flux_BSO_annual_net           (time, member) float64 -859.2 ... -993.2\n    FW_flux_Bering_annual_net        (time, member) float64 2.351e+03 ... 3.1...\n    ...                               ...\n    Solid_FW_flux_BSO_annual_net     (time, member) float64 -26.77 ... -35.43\n    Solid_FW_flux_Bering_annual_net  (time, member) float64 56.3 86.62 ... 22.87\n    runoff_annual                    (time, member) float64 3.39e+03 ... 3.97...\n    netPrec_annual                   (time, member) float64 2.019e+03 ... 2.1...\n    Liquid_FW_storage_Arctic_annual  (time, member) float64 8.125e+04 ... 9.7...\n    Solid_FW_storage_Arctic_annual   (time, member) float64 1.828e+04 ... 7.6...\nAttributes:\n    creation_date:   02-Jun-2020 15:38:31\n    author:          Alexandra Jahn, CU Boulder, alexandra.jahn@colorado.edu\n    title:           Annual timeseries of freshwater data from the CESM Low W...\n    description:     Annual mean Freshwater (FW) fluxes and storage relative ...\n    data_structure:  The data structure is |Ensemble member | Time (in years)...xarray.DatasetDimensions:time: 95member: 11Coordinates: (2)time(time)float642.006e+03 2.007e+03 ... 2.1e+03long_name :time in years, 1920-2100array([2006., 2007., 2008., 2009., 2010., 2011., 2012., 2013., 2014., 2015.,\n       2016., 2017., 2018., 2019., 2020., 2021., 2022., 2023., 2024., 2025.,\n       2026., 2027., 2028., 2029., 2030., 2031., 2032., 2033., 2034., 2035.,\n       2036., 2037., 2038., 2039., 2040., 2041., 2042., 2043., 2044., 2045.,\n       2046., 2047., 2048., 2049., 2050., 2051., 2052., 2053., 2054., 2055.,\n       2056., 2057., 2058., 2059., 2060., 2061., 2062., 2063., 2064., 2065.,\n       2066., 2067., 2068., 2069., 2070., 2071., 2072., 2073., 2074., 2075.,\n       2076., 2077., 2078., 2079., 2080., 2081., 2082., 2083., 2084., 2085.,\n       2086., 2087., 2088., 2089., 2090., 2091., 2092., 2093., 2094., 2095.,\n       2096., 2097., 2098., 2099., 2100.])member(member)float641.0 2.0 3.0 4.0 ... 9.0 10.0 11.0long_name :Ensemble member, 1-11array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])Data variables: (16)FW_flux_Fram_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Fram Strait, relative to 34.8array([[-1259.970567, -1264.007662, -1043.912842, ..., -1127.209397,\n        -1094.937548, -1068.404606],\n       [-1211.286894, -1479.531102, -1018.166211, ..., -1243.711036,\n        -1267.207286, -1114.552581],\n       [-1280.228564, -1385.553358,  -880.902707, ..., -1289.849198,\n        -1050.289977, -1041.592238],\n       ...,\n       [-1994.723559, -2192.810731, -1867.436333, ..., -1801.993229,\n        -2151.202886, -2616.74365 ],\n       [-2312.975878, -1882.595866, -1626.129161, ..., -2050.419246,\n        -2007.994165, -2369.86382 ],\n       [-1869.903721, -1941.767155, -1944.454371, ..., -1797.444957,\n        -2291.658043, -2133.186992]])FW_flux_Barrow_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Barrow Strait, relative to 34.8array([[-600.656931, -591.794563, -623.46547 , ..., -530.954288, -557.466782,\n        -540.397683],\n       [-545.381073, -651.15272 , -647.759741, ..., -554.62912 , -614.254495,\n        -681.241813],\n       [-676.367103, -597.249584, -526.308636, ..., -458.975897, -539.056379,\n        -600.418649],\n       ...,\n       [-495.781816, -343.416049, -572.034808, ..., -547.660529, -503.001068,\n        -575.476408],\n       [-508.571693, -385.524045, -505.368339, ..., -560.393935, -432.867012,\n        -501.710328],\n       [-478.525844, -380.776262, -525.255738, ..., -418.526205, -514.73683 ,\n        -537.165685]])FW_flux_Nares_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Nares Strait, relative to 34.8array([[-1805.094645, -1750.974399, -1736.428853, ..., -1790.293088,\n        -1630.279788, -1698.111863],\n       [-1661.738712, -1906.896808, -1756.856956, ..., -1846.616134,\n        -1754.973196, -1978.856594],\n       [-1956.029601, -1729.47722 , -1506.906015, ..., -1742.673838,\n        -1613.488436, -1857.85344 ],\n       ...,\n       [-2835.663916, -2685.863919, -3123.0961  , ..., -2930.78795 ,\n        -3075.907956, -2954.079147],\n       [-2767.255034, -2435.987932, -2852.725589, ..., -3132.779162,\n        -2879.811506, -2758.960293],\n       [-2801.006622, -2418.965807, -2907.765449, ..., -2699.143264,\n        -3153.186248, -2825.880317]])FW_flux_Davis_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Davis Strait, relative to 34.8array([[-2312.903562, -2206.193327, -2513.938315, ..., -2452.741479,\n        -2178.565682, -2496.419307],\n       [-2442.875007, -2302.612039, -2550.956543, ..., -2594.189567,\n        -2411.537129, -2491.906155],\n       [-2380.667757, -2460.765939, -2355.580677, ..., -2566.93496 ,\n        -2292.468786, -2443.485248],\n       ...,\n       [-3222.803208, -3277.824803, -3791.472809, ..., -3864.113295,\n        -3704.395263, -3775.769167],\n       [-3091.625499, -3293.624915, -3857.535966, ..., -3733.405415,\n        -3404.497911, -3538.640386],\n       [-3657.58114 , -3137.665787, -3346.385689, ..., -3778.231506,\n        -3182.004745, -3294.19718 ]])FW_flux_BSO_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through the Barents Sea Opening, relative to 34.8array([[ -859.243059,  -942.211781,  -723.703298, ..., -1036.046746,\n        -1055.237416,  -736.846205],\n       [ -878.919322,  -862.165165,  -760.842132, ...,  -981.459231,\n        -1017.759096,  -719.537064],\n       [ -867.125354,  -771.0794  ,  -883.947051, ...,  -823.744405,\n        -1084.024653,  -832.384503],\n       ...,\n       [-1201.528063,  -956.321947,  -989.990348, ...,  -928.170124,\n        -1522.813339, -1333.944058],\n       [-1175.496328,  -992.809814, -1042.238636, ..., -1027.30427 ,\n        -1012.655464, -1161.633369],\n       [-1108.045221, -1149.213528,  -974.059705, ...,  -944.651881,\n        -1351.228427,  -993.196776]])FW_flux_Bering_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Bering Strait, relative to 34.8array([[2351.369668, 2345.677436, 2046.439566, ..., 2298.623674, 2036.719667,\n        1907.280919],\n       [2481.513182, 2178.085803, 2490.061059, ..., 2760.976043, 2139.010713,\n        2320.373448],\n       [2514.857782, 2488.589714, 2864.883346, ..., 2666.687224, 2805.529561,\n        1831.068287],\n       ...,\n       [2212.93228 , 2305.747855, 3165.795471, ..., 2682.178162, 2137.858094,\n        2769.764568],\n       [2402.817047, 2871.272846, 2241.49669 , ..., 2899.779166, 2921.583788,\n        3124.232717],\n       [2553.009217, 2738.646198, 2402.378728, ..., 2832.505199, 2564.175895,\n        3110.720966]])Solid_FW_flux_Fram_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Fram Strait, relative to 34.8array([[-2355.201527, -3079.918585, -1900.235976, ..., -2379.143692,\n        -2263.860412, -1740.013499],\n       [-2072.011081, -2483.281102, -2397.287222, ..., -2044.468084,\n        -2757.085258, -2187.036132],\n       [-2495.74312 , -1468.060247, -2391.734318, ..., -2991.861612,\n        -2881.262317, -2262.942851],\n       ...,\n       [ -989.394195, -1285.473222,  -763.780509, ...,  -657.173252,\n         -940.635132, -2216.186576],\n       [-1176.377381,  -715.543546,  -763.626743, ...,  -876.188872,\n         -550.202924, -1673.306079],\n       [ -842.80755 , -1236.592837,  -685.109519, ...,  -585.170298,\n        -1213.069558, -1393.501459]])Solid_FW_flux_Barrow_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Barrow Strait, relative to 34.8array([[ -5.907839, -11.016486,   2.898278, ...,  -6.772   ,  -2.346885,\n         -4.972206],\n       [ -5.405102, -12.325443,  -2.837828, ...,  -1.54938 ,  -1.524433,\n         -3.239178],\n       [-13.615343,  -3.321843,   3.297263, ...,  17.846225,  -8.849867,\n         -1.40733 ],\n       ...,\n       [ -4.252253,  -4.74888 ,  -2.318929, ...,  -0.692743,  -3.516814,\n         -7.24111 ],\n       [ -2.473455,  -2.43267 ,  -4.119768, ...,  -3.041222,  -0.642326,\n         -3.151623],\n       [ -4.023888,  -4.630743,  -6.034809, ...,  -3.946639,  -2.759937,\n         -3.67783 ]])Solid_FW_flux_Nares_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Nares Strait, relative to 34.8array([[-443.523805, -369.69004 , -319.816739, ..., -359.434137, -331.597514,\n        -351.161639],\n       [-402.060722, -365.66951 , -455.727733, ..., -359.163976, -398.079664,\n        -312.769165],\n       [-309.118123, -407.329009, -518.392379, ..., -380.334965, -392.379044,\n        -384.005149],\n       ...,\n       [-322.681791, -285.266534, -360.379804, ..., -249.28949 , -291.504876,\n        -301.008009],\n       [-214.401449, -252.253729, -303.416623, ..., -295.322261, -282.239789,\n        -357.222407],\n       [-361.437905, -301.694338, -259.185245, ..., -248.62952 , -255.963657,\n        -396.696598]])Solid_FW_flux_Davis_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Davis Strait, relative to 34.8array([[-647.548166, -690.769077, -803.010762, ..., -637.388108, -669.678922,\n        -636.923716],\n       [-738.654959, -712.727442, -787.469841, ..., -741.271782, -734.0404  ,\n        -652.683568],\n       [-607.804625, -582.211375, -735.954593, ..., -633.625357, -651.638696,\n        -628.742061],\n       ...,\n       [-547.210914, -467.312263, -533.798556, ..., -565.575028, -573.092124,\n        -579.833192],\n       [-381.382969, -369.539971, -560.781192, ..., -423.884627, -600.9458  ,\n        -596.006766],\n       [-544.597115, -384.419366, -492.503528, ..., -482.520345, -479.724188,\n        -568.860609]])Solid_FW_flux_BSO_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through the Barents Sea Opening, relative to 34.8array([[-2.677222e+01, -2.198672e+02, -7.108708e+01, ..., -8.265375e+00,\n        -7.027574e+01, -2.478501e-01],\n       [-1.119710e+00, -1.611613e+01, -9.113674e+01, ..., -7.502441e+01,\n        -4.567288e+01, -8.217717e+01],\n       [-1.862617e+01, -4.320776e+00, -1.400749e+02, ..., -1.114686e+01,\n        -2.599572e+02, -2.824174e+01],\n       ...,\n       [-2.107002e+01, -1.972472e+01, -1.322291e+00, ..., -1.250803e+01,\n         0.000000e+00, -8.715960e+00],\n       [-1.129319e-01, -3.020100e+00, -9.972180e+00, ..., -6.321711e+00,\n        -1.357067e-01, -5.525681e-01],\n       [-1.324610e-02, -1.583726e+01, -2.125017e-01, ..., -2.465729e+00,\n        -3.163840e-01, -3.543094e+01]])Solid_FW_flux_Bering_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Bering Strait, relative to 34.8array([[ 56.300783,  86.619422,  21.629406, ...,  45.369409, 120.205643,\n          6.023335],\n       [ 52.854622,  68.339477, 139.181478, ...,  79.513795,  82.273825,\n        216.946026],\n       [ 64.470058,  16.432358,  33.79051 , ...,  36.5111  ,  51.494807,\n        -83.580959],\n       ...,\n       [ 58.127421,  22.637266,  79.579919, ...,  48.221805,  29.201449,\n        146.166502],\n       [ 46.223662,  44.331598,   9.747562, ...,  44.638335,  63.846993,\n         49.628882],\n       [  4.125923,  26.376146,  44.08245 , ...,  35.386517, 144.93525 ,\n         22.865989]])runoff_annual(time, member)float64...units :km3/yrlong_name :FW flux from river runoff into the Arctic Ocean domain, relative to 34.8array([[3389.793118, 3653.250059, 3255.904314, ..., 3484.324351, 3762.057224,\n        3233.230548],\n       [2991.152992, 3493.827835, 3384.707049, ..., 3571.643922, 3489.125416,\n        3641.709685],\n       [3534.397219, 3128.514068, 3354.798516, ..., 3304.537755, 3539.266607,\n        3084.447612],\n       ...,\n       [3665.258107, 3551.863606, 3809.485292, ..., 3493.525529, 3663.667382,\n        4321.328755],\n       [3743.83261 , 3378.368684, 3672.490382, ..., 3739.293615, 3739.029349,\n        3973.012085],\n       [3552.160713, 3480.78009 , 3710.884772, ..., 3580.675064, 4001.506981,\n        3973.911416]])netPrec_annual(time, member)float64...units :km3/yrlong_name :Net FW flux from precipitation minus evaporation over the Arctic Ocean domain, relative to 34.8array([[2019.436111, 2208.638737, 1936.136054, ..., 2049.989982, 1959.873063,\n        1750.749748],\n       [1945.582104, 2039.416637, 2085.04094 , ..., 1870.845855, 1965.883031,\n        2057.552438],\n       [2032.998228, 2047.057141, 1904.70621 , ..., 1894.083722, 1825.828287,\n        2026.627389],\n       ...,\n       [1858.080937, 1946.276296, 2100.604079, ..., 1994.116693, 2114.804976,\n        2423.484939],\n       [1915.662721, 1961.74668 , 1920.628415, ..., 1993.625762, 1744.708491,\n        2245.915281],\n       [1767.588737, 1904.765559, 1953.973994, ..., 1760.192971, 2384.175174,\n        2118.712754]])Liquid_FW_storage_Arctic_annual(time, member)float64...units :km3long_name :FW storage in the Arctic Ocean domain, , relative to 34.8. Ignoring any negative FW (water above reference salinity)array([[ 81250.414556,  85443.803218,  80863.782256, ...,  86120.03671 ,\n         81999.322618,  87066.701325],\n       [ 82567.489008,  85437.534745,  81016.233259, ...,  86801.295397,\n         81720.408045,  85405.906043],\n       [ 83435.689171,  84300.641386,  81701.536079, ...,  88202.733212,\n         82363.689791,  84802.874562],\n       ...,\n       [ 99179.037787, 100587.947588,  99713.659894, ..., 105329.776774,\n        101183.194676,  96731.099064],\n       [ 99022.701221, 100071.990872, 100428.151173, ..., 106023.385394,\n        101243.934883,  97353.008726],\n       [ 99884.660244, 101526.449779, 100904.197851, ..., 106283.670855,\n        101106.948969,  97309.828159]])Solid_FW_storage_Arctic_annual(time, member)float64...units :km3long_name :FW storage in sea ice and snow in the Arctic Ocean domain, relative to 34.8array([[18283.302344, 16436.957815, 18942.480543, ..., 19105.369531,\n        17193.142364, 16026.957959],\n       [17228.15868 , 16184.248407, 19493.271233, ..., 18736.727461,\n        17731.125586, 18033.048197],\n       [16582.390706, 17113.875851, 19081.541068, ..., 18073.074623,\n        16677.630513, 18713.913839],\n       ...,\n       [ 8286.152773,  7618.147979,  7159.460183, ...,  7615.242652,\n         7314.728945,  8332.238818],\n       [ 7925.266   ,  8366.732533,  7583.306716, ...,  7247.856691,\n         6969.390301,  7445.873087],\n       [ 6850.743786,  7492.329279,  7275.546906, ...,  7656.442802,\n         7476.367423,  7656.280716]])Attributes: (5)creation_date :02-Jun-2020 15:38:31author :Alexandra Jahn, CU Boulder, alexandra.jahn@colorado.edutitle :Annual timeseries of freshwater data from the CESM Low Warming Ensembledescription :Annual mean Freshwater (FW) fluxes and storage relative to 34.8 shown in Jahn and Laiho, GRL, 2020, calculated from the 11-member Community Earth System Model (CESM) Low Warming Ensemble output (Sanderson et al., 2017, Earth Syst. Dynam., 8, 827-847. doi: 10.5194/esd-8-827-2017). These 11 ensemble members were branched from the first 11 ensemble members of the CESM Large Ensemble (companion data file) at the end of 2005. Convention for the fluxes is that positive fluxes signify a source of FW to the Arctic and negative fluxes are a sink/export of FW for the Arctic. FW fluxes are the net fluxes through a strait over the full ocean depth, adding up any positive and negative fluxes. Liquid FW storage is calculated over the full depth of the ocean but ignoring any negative FW (resulting from salinties over 34.8). Solid FW storage includes FW stored in sea ice and FW stored in snow on sea ice. Surface fluxes and FW storage is calculated over the Arctic domain bounded by Barrow Strait, Nares Strait, Bering Strait, Fram Strait, and the Barents Sea Opeing (BSO). Davis Strait fluxes are included for reference only and are outside of the Arctic domain. A map showing the domain and the location of the straits can be found in Jahn and Laiho, GRL, 2020.data_structure :The data structure is |Ensemble member | Time (in years)|. All data are annual means. The data covers 2006-2100. There are 11 ensemble members.\n\n\n\n\n8.4.7 Exercise 3\nhttps://arcticdata.io/catalog/view/doi:10.18739/A26T0GX63"
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#include-this---bryces-notes",
    "href": "sections/08-data-structures-netcdf.html#include-this---bryces-notes",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.5 INCLUDE THIS? - Bryce’s notes",
    "text": "8.5 INCLUDE THIS? - Bryce’s notes\n\nLook into parallel access and NetCDF and whether all wheels can take advantage of the built-in parallel access\nData Organization / File Naming?\nUsing hierarchical folder structures\n\nEncoding hierarchy into filenames\nFile naming for natural ordering (principle of most sig. fist)\nSetting things up for Dask multi-file support\n\nUnderstand the advantages of random and parallel formats such as NetCDF\nTalk about NetCDF and its role in data archival\nIs it a good archival format: Yes! Better than CSV in many (not all) ways. The format is open and well-documented, support for the reading the format is ubiquitous, it’s efficient w/ disk space (compared to CSV), it supports remote querying (unlike CSV)."
  },
  {
    "objectID": "sections/09-parallel-with-dask.html",
    "href": "sections/09-parallel-with-dask.html",
    "title": "9  Parallelization with Dask",
    "section": "",
    "text": "Dask is a library for parallel computing in Python. It can scale up code to use the full capacity of your personal computer or to distribute work in a cloud cluster. By mirroring APIs of other commonly used Python libraries such as Pandas, NumPy, and Scikit-learn, Dask provides a familiar interface that makes it easier to parallelize your code. In this lesson, we will get acquainted with some of Dask’s most commonly used objects and Dask’s way of distributing and evaluating computations."
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#objectives",
    "href": "sections/09-parallel-with-dask.html#objectives",
    "title": "9  Parallelization with Dask",
    "section": "9.2 Objectives",
    "text": "9.2 Objectives\n\nBecome familiar with Dask processing workflow:\n\nWhat are the client, scheduler, workers, and cluster\nhttps://www.datarevenue.com/en-blog/understanding-dask-architecture-client-scheduler-workers\nUnderstand delayed computations and “lazy” evaluation\nObtain information about computations via the Dask dashboard\nhttps://www.youtube.com/watch?v=N_GqzcuGLCY\n\nLearn basics of dask.arrays and dask.dataframes:\n\nLoad data and specifying chunk sizes\nhttps://saturncloud.io/blog/a-data-scientist-s-guide-to-lazy-evaluation-with-dask/\nInterpret a task graph\n\nIntegrate xarray with Dask:\n\nhttps://docs.xarray.dev/en/stable/user-guide/dask.html\n\nShare best practices and resources for a deeper dive\n\nhttps://docs.dask.org/en/stable/best-practices.html#load-data-with-dask\nhttps://coiled.io/blog/common-dask-mistakes/"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#possible-computing-examples",
    "href": "sections/09-parallel-with-dask.html#possible-computing-examples",
    "title": "9  Parallelization with Dask",
    "section": "9.3 Possible computing examples:",
    "text": "9.3 Possible computing examples:\nhttps://docs.dask.org/en/latest/10-minutes-to-dask.html\nhttps://tutorial.dask.org/00_overview.html"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#dask-cluster",
    "href": "sections/09-parallel-with-dask.html#dask-cluster",
    "title": "9  Parallelization with Dask",
    "section": "9.4 Dask Cluster",
    "text": "9.4 Dask Cluster\nDask clusters have three main components for processing computations in parallel. These are the client, the scheduler and the workers.\n\nWhen we code, we communicate directly with the client, which is responsible for submitting tasks to be executed to the scheduler.\nAfter receiving the tasks from the client, the scheduler determines how tasks will be distributed among the workers and coordinates them to process tasks in parallel.\nFinally, the workers are threads, processes, or separate machines in a cluster. They compute tasks and store and return computations results.\n\nIn order to interact with the client and generate tasks that can be processed in parallel we need to use Dasks’ objects to read our data. Here we will see examples of how to use dask.dataframes and dask.arrays.\n\n\n\nFrom: https://www.datarevenue.com/en-blog/understanding-dask-architecture-client-scheduler-workers Make this into a collored graph, add cluster envelope\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe can deploy a Dask cluster on a single machine or an actual cluster with multiple machines. Here we will use the Dask cluster instead of the default Dask scheduler to take advantage of the cluster Dashboard, which keeps track of the performance and progress of our computations. Read more about Dask Clusters"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#dask-data-frames",
    "href": "sections/09-parallel-with-dask.html#dask-data-frames",
    "title": "9  Parallelization with Dask",
    "section": "9.5 Dask Data Frames",
    "text": "9.5 Dask Data Frames\nWhen we analyze tabular data, we usually start our analysis by loading it into memory as a Pandas DataFrame. But, what if this data does not fit in memory? Or maybe our analyzes crash because we run out of memory. These scenarios are typical entry points into parallel computing. In such cases, Dask’s scalable alternative to a Pandas DataFrame is the dask.dataframe. A dask.DataFrame is made up of many pd.DataFrames, each containing a subset of rows of the original dataset. We call each of these pandas pieces a partition of the dask.DataFrame.\n\n\n\nDask Array design (dask documentation)\n\n\nFor this example we will use a dataset including weather and soil condition measurements at six forest stands in northeastern Siberia. The data has been collected since 2014 and is archived at the Arctic Data Center Loranty & Alexander, doi:10.18739/A24B2X59C. Today we will use the soil mositure measurements from this dataset. Just as we did on the previous lesson, we download the data using the requests package and the link to the data obtained from the Arctic Data Center website.\n\nimport os              \nimport requests \n\n\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A27e4043d-75eb-4c4f-9427-0d442526c154'\n\nresponse = requests.get(url)\nopen(\"dg_soil_moisture.csv\", \"wb\").write(response.content)\n\n121002029\n\n\nWe can see the file is 115MB. Next we import the data as dask.dataframe.\n\nimport dask.dataframe as dd\n\nfp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')\ndf = dd.read_csv(fp, blocksize = '20MB' , encoding='ISO-8859-1')\ndf\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      timestamp\n      year\n      doy\n      hour\n      minute\n      site\n      logger\n      port\n      sensor\n      sensorZ\n      m_soil\n      unit\n    \n    \n      npartitions=6\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      \n      object\n      int64\n      int64\n      int64\n      int64\n      object\n      object\n      object\n      object\n      int64\n      float64\n      object\n    \n    \n      \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n  \n\n\nDask Name: read-csv, 1 graph layer\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      timestamp\n      year\n      doy\n      hour\n      minute\n      site\n      logger\n      port\n      sensor\n      sensorZ\n      m_soil\n      unit\n    \n  \n  \n    \n      0\n      2014-07-07 16:30:00\n      2014\n      188\n      16\n      30\n      MDF1\n      MDF1met\n      Port 3\n      5TM Moisture/Temp\n      -6\n      0.273\n      m³/m³ VWC\n    \n    \n      1\n      2014-07-07 16:30:00\n      2014\n      188\n      16\n      30\n      MDF1\n      MDF1met\n      Port 4\n      5TM Moisture/Temp\n      -11\n      0.345\n      m³/m³ VWC\n    \n    \n      2\n      2014-07-07 17:00:00\n      2014\n      188\n      17\n      0\n      LDF2\n      LDF2met\n      Port 3\n      5TM Moisture/Temp\n      -8\n      0.308\n      m³/m³ VWC\n    \n    \n      3\n      2014-07-07 17:00:00\n      2014\n      188\n      17\n      0\n      LDF2\n      LDF2met\n      Port 4\n      5TM Moisture/Temp\n      -13\n      0.325\n      m³/m³ VWC\n    \n    \n      4\n      2014-07-07 17:00:00\n      2014\n      188\n      17\n      0\n      MDF1\n      MDF1met\n      Port 3\n      5TM Moisture/Temp\n      -6\n      0.283\n      m³/m³ VWC\n    \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nUnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xb3 in position 192: invalid start byte\n\n# import chardet\n# with open(fp, 'rb') as rawdata:\n#     result = chardet.detect(rawdata.read(100000))\n# result\n\n\n\n\nThe application programming interface (API) of a dask.DataFrame is a subset of the pd.DataFrame API. So if you are familiar with pandas, many of the core pd.DataFrame methods directly translate to dask.DataFrames.\nA major difference between both objects is that dask.DataFrames are lazy, meaning that they do not evaluate until we explicitly ask for a result using the compute method. This is the case with most Dask workloads.\n\n## THIS IS AN EXAMPLE OF LAZY COMPUTATION\n\nThis means that your framework will queue up sets of transformations or calculations so that they are ready to run later, in parallel. This is a concept you’ll find in lots of frameworks for parallel computing, including Dask. Your framework won’t evaluate the requested computations until explicitly told to. This differs from “eager” evaluation functions, which compute instantly upon being called.\nhttps://saturncloud.io/blog/a-data-scientist-s-guide-to-lazy-evaluation-with-dask/ Lead into task graphs.\n\n### THIS IS AN EXAMPLE OF TASK GRAPH"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#dask-arrays",
    "href": "sections/09-parallel-with-dask.html#dask-arrays",
    "title": "9  Parallelization with Dask",
    "section": "9.6 Dask Arrays",
    "text": "9.6 Dask Arrays\nIf you want scalable NumPy arrays, then start with Dask array\n\n\n\nDask Array design (dask documentation)"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#best-practices",
    "href": "sections/09-parallel-with-dask.html#best-practices",
    "title": "9  Parallelization with Dask",
    "section": "9.7 Best Practices",
    "text": "9.7 Best Practices\nIt is important to remember that, while APIs may be similar, some differences do exist. Additionally, the performance of some algorithms may differ from their in-memory counterparts due to the advantages and disadvantages of parallel programming. Some thought and attention is still required when using Dask. https://docs.dask.org/en/latest/user-interfaces.html\n\n\n\n\n\n\nWarning\n\n\n\nFor data that fits into RAM, Pandas can often be faster and easier to use than Dask DataFrame. While “Big Data” tools can be exciting, they are almost always worse than normal data tools while those remain appropriate. https://docs.dask.org/en/stable/dataframe-best-practices.html"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#todo",
    "href": "sections/09-parallel-with-dask.html#todo",
    "title": "9  Parallelization with Dask",
    "section": "9.8 TODO",
    "text": "9.8 TODO\n\nFind dataset(s) to use:\n\nhttps://arcticdata.io/catalog/view/doi%3A10.18739%2FA24B2X59C “Understory micrometorology across a larch forest density gradient in northeastern Siberia 2014-2020” ~50MB, has a few different axes to filter on\nhttps://arcticdata.io/catalog/view/doi%3A10.18739%2FA28W38388 “River and lake ice phenology data for Alaska and Northwest Canada from 1882 to 2021”\n\n\n\n9.8.1 Bryce’s Notes\n\nhttps://www.dask.org/\nhttps://docs.xarray.dev/en/stable/user-guide/dask.html#dask\nhttps://stephanhoyer.com/2015/06/11/xray-dask-out-of-core-labeled-arrays/\nhttps://examples.dask.org/xarray.html\nGood example to base exercise on: https://examples.dask.org/applications/image-processing.html\nSplit-apply-combine\nDask stuff\n\nLazy eval (compute())\n\nGrouping compute() calls versus calling compute() multiple times\n\nDask Array\nDask DataFrame\nSkip or just mention Bag, Delayed, Futures? Not sure yet.\nvisualize()\nChoosing how many chunks to divide work into\nTask overhead\nDistributed dask?\n\nPersist > Dask is convenient on a laptop. It installs trivially with conda or pip and extends the size of convenient datasets from “fits in memory” to “fits on disk”.\n\n\n\n– From https://docs.dask.org/en/stable/"
  },
  {
    "objectID": "sections/10-geopandas.html",
    "href": "sections/10-geopandas.html",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "",
    "text": "Manipulating raster data with rasterio\nManipulating vector data with geopandas\nWorking with raster and vector data together"
  },
  {
    "objectID": "sections/10-geopandas.html#introduction",
    "href": "sections/10-geopandas.html#introduction",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "10.2 Introduction",
    "text": "10.2 Introduction\nIn this lesson, we’ll be working with geospatial raster and vector data to do an analysis on vessel traffic in south central Alaska. If you aren’t already familiar, geospatial vector data consists of points, lines, and/or polygons, which represent locations on the Earth. Geospatial vector data can have differing geometries, depending on what it is representing (eg: points for cities, lines for rivers, polygons for states.) Raster data uses a set of regularly gridded cells (or pixels) to represent geographic features.\nBoth geospatial vector and raster data have a coordinate reference system, which describes how the points in the dataset relate to the 3-dimensional sphereoid of Earth. A coordinate reference system contains both a datum and a projection. The datum is how you georeference your points (in 3 dimensions!) onto a spheroid. The projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. All coordinate reference systems require a datum. However, some coordinate reference systems are “unprojected” (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984.\nCoordinate reference systems are often referenced using a shorthand 4 digit code called an EPSG code. We’ll be working with two coordinate reference systems in this lesson with the following codes:\n\n3338: Alaska Albers\n4326: WGS84 (World Geodetic System 1984), used in GPS\n\nIn this lesson, we are going to take two datasets:\n\nAlaskan commercial salmon fishing statisical areas\nNorth Pacific and Arctic Marine Vessel Traffic Dataset\n\nand use them to calculate the total distance travelled by ships within each fishing area.\nThe high level steps will be\n\nread in the datasets\nreproject them so they are in the same projection\nextract a subset of the raster and vector data using a bounding box\nturn each polygon in the vector data into a raster mask\nuse the masks to calculate the total distance travelled (sum of pixels) for each fishing area"
  },
  {
    "objectID": "sections/10-geopandas.html#pre-processing-raster-data",
    "href": "sections/10-geopandas.html#pre-processing-raster-data",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "10.3 Pre-processing raster data",
    "text": "10.3 Pre-processing raster data\nFirst we need to load in our libraries. We’ll use geopandas for vector manipulation, rasterio for raster maniupulation, and shapely for manipulating geospatial data generally.\n\nimport geopandas as gpd\n\nimport rasterio\nimport rasterio.mask\nimport rasterio.warp\nimport rasterio.plot\nfrom rasterio import features\n\nfrom shapely.geometry import box\n\nimport requests\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker\n\nimport pandas as pd\nimport numpy as np\n\nFirst, we’ll use requests to download the ship traffic raster from Kapsar et al.. We grab a one month slice from August, 2020 of a coastal subset of data with 1km resolution. To get the URL in the code chunk below, you can right click the download button for the file of interest and select “copy link address.”\n\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A6b847ab0-9a3d-4534-bf28-3a96c5fa8d72'\n\nresponse = requests.get(url)\nopen(\"Coastal_2020_08.tif\", \"wb\").write(response.content)\n\n1473505\n\n\nOpen the raster file, plot it, and look at the metadata. We use the with here as a context manager. This ensures that the connection to the raster file is closed and cleaned up when we are done with it.\n\nwith rasterio.open(\"Coastal_2020_08.tif\") as ship_con:\n    # read in raster (1st band)\n    ships = ship_con.read(1)\n    ships_meta = ship_con.profile\n\nplt.imshow(ships)\nprint(ships_meta)\n\n{'driver': 'GTiff', 'dtype': 'float32', 'nodata': -3.3999999521443642e+38, 'width': 3087, 'height': 2308, 'count': 1, 'crs': CRS.from_epsg(3338), 'transform': Affine(999.7994153462766, 0.0, -2550153.29233849,\n       0.0, -999.9687691991521, 2711703.104608573), 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n\n\n\n\n\nNow download a vector shapefile of commercial fishing districts in Alaska.\n\nurl = 'https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7c942c45-1539-4d47-b429-205499f0f3e4'\n\nresponse = requests.get(url)\nopen(\"Alaska_Commercial_Salmon_Boundaries.gpkg\", \"wb\").write(response.content)\n\n36544512\n\n\nRead in the data\n\ncomm = gpd.read_file(\"Alaska_Commercial_Salmon_Boundaries.gpkg\")\n\ncomm.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nThe raster data is in 3338, so we need to reproject this. We use the to_crs method on the comm object to transform it from 4326 (WGS 84) to 3338 (Alaska Albers).\n\ncomm.crs\ncomm_3338 = comm.to_crs(\"EPSG:3338\")\n\ncomm_3338.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nWe can create a bounding box for the area of interest, and use that to clip the original raster data to just the extent we need. We use the box function from shapely to create the bounding box, then create a geoDataFrame from the points, and convert the WGS 84 coordinates to the Alaska Albers projection.\n\ncoord_box = box(-159.5, 55, -144.5, 62)\n\ncoord_box_df = gpd.GeoDataFrame(\n    crs = 'EPSG:4326',\n    geometry = [coord_box]).to_crs(\"EPSG:3338\")\n\nNow, we can read in raster again cropped to bounding box. We use the mask function from rasterio.mask. Note that we apply this to the connection to the raster file (with rasterio.open(...)), update the metadata associated with the raster, and then write it back out again.\n\nwith rasterio.open(\"Coastal_2020_08.tif\") as ship_con:\n    out_image, out_transform = rasterio.mask.mask(ship_con, coord_box_df[\"geometry\"], crop=True)\n    out_meta = ship_con.meta\n\n\nout_meta.update({\"driver\": \"GTiff\",\n                 \"height\": out_image.shape[1],\n                 \"width\": out_image.shape[2],\n                 \"transform\": out_transform,\n                 \"compress\": \"lzw\"})\n\nwith rasterio.open(\"Coastal_2020_08_masked.tif\", \"w\", **out_meta) as dest:\n    dest.write(out_image)\n\n\n10.3.1 Check extents\nLet’s read in the clipped raster data, and make a quick plot to ensure they line up the way they should.\nFirst we read in the cropped data again, since we’ll need it later. We also save the shape and transform attributes of the raster into their own attributes.\n\nwith rasterio.open('Coastal_2020_08_masked.tif') as ship_con:\n    shape = ship_con.shape\n    transform = ship_con.transform\n    # read in the cropped raster (1st band only)\n    ship_arr = ship_con.read(1)\n    # turn no data values into actual NaNs\n    ship_arr[ship_arr == ship_con.nodata] = np.nan\n\n\n# set up plot\nfig, ax = plt.subplots(figsize=(15, 15))\n# plot the raster\nrasterio.plot.show(ship_arr,\n                   ax=ax,\n                   vmin = 0,\n                   vmax = 50000,\n                   transform = transform)\n# plot the vector\ncomm_3338.plot(ax=ax, facecolor='none', edgecolor='red')\n\n<AxesSubplot:>"
  },
  {
    "objectID": "sections/10-geopandas.html#calculate-total-distance-per-fishing-area",
    "href": "sections/10-geopandas.html#calculate-total-distance-per-fishing-area",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "10.4 Calculate total distance per fishing area",
    "text": "10.4 Calculate total distance per fishing area\nIn this step, we rasterize each polygon in the shapefile, such that pixels in or touching the polygon get a value of 1, and pixels not touching it get a value of 0. Then, for each polygon, we extract the indices of the raster array that are equal to 1. We then extract the values of these indicies from the original ship traffic raster data, and calculate the sum of the values over all of those pixels.\nHere is a simplified diagram of the process:\n\n\n10.4.0.1 Zonal statistics over one polygon\nLet’s look at how this works over just one fishing area first. We use the rasterize method from the features module in rasterio. This takes as arguments the data to rasterize (in this case the 40th row of our dataset), the shape and transform the output raster will take (these were extracted from our raster data when we read it in). We alo set the all_touched argument to true, which means any pixel that touches a boundary of our vector will be burned into the mask.\n\nr40 = features.rasterize(comm_3338['geometry'][40].geoms,\n                                    out_shape=shape,\n                                    transform=transform,\n                                    all_touched=True)\n\nIf we have a look at a plot of our rasterized version of the single fishing district, we can see that instead of a vector, we now have a raster with the shape of the district.\n\n# set up plot\nfig, ax = plt.subplots(figsize=(15, 15))\n# plot the raster\nrasterio.plot.show(r40,\n                   ax=ax,\n                   vmin = 0,\n                   vmax = 1,\n                   transform = transform)\n# plot the vector\ncomm_3338.plot(ax=ax, facecolor='none', edgecolor='red')\n\n<AxesSubplot:>\n\n\n\n\n\nA quick call to np.unique shows our unique values are 0 or 1, which is what we expect.\n\nnp.unique(r40)\n\narray([0, 1], dtype=uint8)\n\n\nFinally, we need to know is the indices of the original raster where the fishing district is. We can use np.where to extract this information\n\nr40_index = np.where(r40 == 1)\nprint(r40_index)\n\n(array([108, 108, 108, 108, 108, 109, 109, 109, 109, 109, 109, 109, 109,\n       109, 109, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110,\n       110, 110, 110, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111,\n       111, 111, 111, 111, 112, 112, 112, 112, 112, 112, 112, 112, 112,\n       112, 112, 112, 112, 112, 112, 113, 113, 113, 113, 113, 113, 113,\n       113, 113, 113, 113, 113, 113, 113, 113, 114, 114, 114, 114, 114,\n       114, 114, 114, 114, 114, 114, 115, 115, 115, 115, 115, 115, 116]), array([759, 760, 762, 763, 764, 755, 756, 757, 758, 759, 761, 762, 763,\n       764, 765, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763,\n       764, 765, 766, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762,\n       763, 764, 765, 766, 753, 754, 755, 756, 757, 758, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 753, 754, 755, 756, 757, 758, 759,\n       760, 761, 762, 763, 764, 765, 766, 767, 753, 754, 755, 756, 757,\n       758, 759, 760, 761, 762, 763, 753, 754, 755, 756, 757, 758, 754]))\n\n\nIn the last step, we’ll using these indices to extract the values of the data from the fishing raster, and sum them to get a total distance travelled.\n\nnp.nansum(ship_arr[r40_index])\n\n14369028.0\n\n\nNow that we know the individual steps, let’s run this over all of the districts. First we’ll create an id column in the vector data frame. This will help us track unique fishing districts later.\n\ncomm_3338['id'] = range(0,len(comm_3338))\n\nFor each district (with geometry and id), we run the features.rasterize function. If any values equal 1 (some of the districts are outside the bounds of the raster), we calculate the sum of the values of the shipping raster r_array based on the indicies in the raster where the district is located.\n\ndistance_dict = {}\nfor geom, idx in zip(comm_3338['geometry'], comm_3338['id']):\n    rasterized = features.rasterize(geom.geoms,\n                                    out_shape=shape,\n                                    transform=transform,\n                                    all_touched=True)\n    # only save polygons that have a non-zero value\n    if any(np.unique(rasterized)) == 1:\n        r_index = np.where(rasterized == 1)\n        distance_dict[idx] = np.nansum(ship_arr[r_index])\n\nNow we just create a data frame from that dictionary, and join it to the vector data using pandas operations.\n\n# create a data frame from the result\ndistance_df = pd.DataFrame.from_dict(distance_dict,\n    orient='index',\n    columns=['distance'])\n\n# extract the index of the data frame as a column to use in a join\ndistance_df['id'] = distance_df.index\ndistance_df['distance'] = distance_df['distance']/1000\n\nNow we join the result to the original geodataframe.\n\n# join the sums to the original data frame\nres_full = comm_3338.merge(distance_df, on = \"id\", how = 'inner')\n\nFinally, we can plot our result!\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\nax = res_full.plot(column = \"distance\", legend = True, ax = ax)\nfig = ax.figure\nlabel_format = '{:,.0f}'\ncb_ax = fig.axes[1]\nticks_loc = cb_ax.get_yticks().tolist()\ncb_ax.yaxis.set_major_locator(matplotlib.ticker.FixedLocator(ticks_loc))\ncb_ax.set_yticklabels([label_format.format(x) for x in ticks_loc])\nax.set_axis_off()\nax.set_title(\"Distance Traveled by Ships in Kilometers\")\nplt.show()\n\n\n\n\nFrom here we can do any additional geopandas operations we might be interested in. For example, what if we want to calculate the total distance by registration area (a superset of fishing district). We can do that using dissolve from geopandas.\n\nreg_area = res_full.dissolve(by = \"REGISTRATION_AREA_NAME\", aggfunc = 'sum')\n\nLet’s have a look at the same plot as before, but this time over our aggregated data.\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\nax = reg_area.plot(column = \"distance\", legend = True, ax = ax)\nfig = ax.figure\nlabel_format = '{:,.0f}'\ncb_ax = fig.axes[1]\nticks_loc = cb_ax.get_yticks().tolist()\ncb_ax.yaxis.set_major_locator(matplotlib.ticker.FixedLocator(ticks_loc))\ncb_ax.set_yticklabels([label_format.format(x) for x in ticks_loc])\nax.set_axis_off()\nax.set_title(\"Distance Traveled by Ships in Kilometers\")\nplt.show()"
  },
  {
    "objectID": "sections/11-parquet-arrow.html",
    "href": "sections/11-parquet-arrow.html",
    "title": "11  Data Futures: Parquet and Arrow",
    "section": "",
    "text": "The difference between column major and row major data\nSpeed advantages to columnnar data storage\nHow arrow enables faster processing"
  },
  {
    "objectID": "sections/11-parquet-arrow.html#introduction",
    "href": "sections/11-parquet-arrow.html#introduction",
    "title": "11  Data Futures: Parquet and Arrow",
    "section": "11.2 Introduction",
    "text": "11.2 Introduction\nSystem calls are calls that are run by the operating system within their own process. There are several that are relevant to reading and writing data: open, read, write, seek, and close. Open establishes a connection with a file for reading, writing, or both. On open, a file offset points to the beginning of the file. After reading or writing n bytes, the offset will move n bytes forward to prepare for the next opration. Close closes the connection to the file. Read will read data from the file into a memory buffer, and write will write data from a memory buffer to a file. Seek is used to change the location of the offset pointer, for either reading or writing purposes.\nIf you’ve worked with even moderately sized datasets, you may have encounted an “out of memory” error. Memory is where a computer stores the information needed immediately for processes. This is in contrast to storage, which is typically slower to access than memory, but has a much larger capacity. When you open a file, you are establishing a connection between your processor and the information in storage. On read, the data is read into memory that is then available to your python process, for example.\nSo what happens if the data you need to read in are larger than your memory? 32GB is a common memory size, but this would be considered a modestly sized dataset by this courses’s standards. There are a number of solutions to this problem, which don’t involve just buying a computer with more memory. In this lesson we’ll discuss the difference between row major and column major file formats, and how leveraging column major formats can increase memory efficiency. We’ll also learn about another python library called pyarrow, which has a memory format that allows for “zero copy” read."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#row-major-vs-column-major",
    "href": "sections/11-parquet-arrow.html#row-major-vs-column-major",
    "title": "11  Data Futures: Parquet and Arrow",
    "section": "11.3 Row major vs column major",
    "text": "11.3 Row major vs column major\nThe difference between row major and column major is in the ordering of items in the array.\nTake the array:\na11 a12 a13\n\na21 a22 a23\nThis array in a row-major order would be read in as:\na11, a12, a13, a21, a22, a23\nYou could also read it in column-major order as:\na11, a21, a12, a22, a13, a33\nBy default, C and SAS use row major order for arrays, and column major is used by Fortran, MATLAB, R, and Julia.\nPython uses neither, instead representing arrays as lists of lists, though numpy uses row-major order.\n\n11.3.1 Row major versus column major files\nThe same concept can be applied to file formats as the example with arrays above. In row-major file formats, the values (bytes) of each record are read sequentially.\n\n\n\nName\nLocation\nAge\n\n\n\n\nJohn\nWashington\n40\n\n\nMariah\nTexas\n21\n\n\nAllison\nOregon\n57\n\n\n\nIn the above row major example, data are read in the order: John, Washingon, 40, [new line], Mariah, Texas, 21.\nThis means that getting a subset of all columns would be easy; you can specify to read in only the first X rows. However, if we are only interested in Name and Location, we would still have to read in all of the rows before discarding the Age column.\nIf these data were organized in a column major format, they might look like this:\nName: John, Mariah, Allison\nLocation: Washington, Texas, Oregon\nAge: 40, 21, 57\nAnd the read order would first be the names, then the locations, then the age. This means that selecting all values from a set of columns is quite easy (all of the Names and Ages, or all Names and Locations), but reading in only the first few records from each column would require reading in the entire dataset. Another advantage to column major formats is that compression is more efficient since compression can be done across each column, where the data type is uniform, as opposed to across rows with many data types."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#parquet",
    "href": "sections/11-parquet-arrow.html#parquet",
    "title": "11  Data Futures: Parquet and Arrow",
    "section": "11.4 Parquet",
    "text": "11.4 Parquet\nParquet is an open-source file format that stores data in a column-major format. The format contains several key components:\n\nrow group\ncolumn\npage\nfooter\n\n\nRow groups are blocks of data over a set number of rows that contain data from the same columns. Within each row group, data are organized in column-major format, and within each column are pages that are typically 1MB. The footer of the file contains metaata like the schema, encodings, unique values in each column, etc.\nThe parquet format has many tricks to to increase storage efficiency, and is increasingly being used to handle large datasets."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#arrow",
    "href": "sections/11-parquet-arrow.html#arrow",
    "title": "11  Data Futures: Parquet and Arrow",
    "section": "11.5 Arrow",
    "text": "11.5 Arrow\nSo far, we have discussed the difference between organizing information in row-major and column-major format, how that applies to arrays, and how it applies to data storage on disk using Parquet.\nArrow is a language-agnostic specification that enables representation of column-major information in memory without having to serialize data from disk. The Arrow project provides implementation of this specification in a number of languages, including Python.\nLet’s say that you have utilized the Parquet data format for more efficient storage of your data on disk. At some point, you’ll need to read that data into memory in order to do analysis on it. Arrow enables data transfer between the on disk Parquet files and in-memory Python computations, via the pyarrow library.\npyarrow is great, but relatively low level. It supports basic group by and aggregate functions, as well as table and dataset joins, but it does not support the full operations that pandas does."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#example",
    "href": "sections/11-parquet-arrow.html#example",
    "title": "11  Data Futures: Parquet and Arrow",
    "section": "11.6 Example",
    "text": "11.6 Example\nIn this example, we’ll read in a dataset of fish abundance in the San Francisco Estuary, which is published in csv format on the Environmental Data Initiative. This dataset isn’t huge, but it is big enough (3 GB) that working with it locally can be fairly taxing on memory. Motivated by user difficulties in actually working with the data, the deltafish R package was written using the R implementation of arrow. It works by downloading the EDI repository data, writing it to a local cache in parquet format, and using arrow to query it. In this example, I’ve put the Parquet files in a sharable location so we can explore it using pyarrow.\nFirst, we’ll load the modules we need.\n\nimport pyarrow.dataset as ds\nimport numpy as np\nimport pandas as pd\n\nNext we can read in the data using ds.dataset(), passing it the path to the parquet directory and how the data are partitioned.\n\ndeltafish = ds.dataset(\"/home/shares/deltafish/fish\", format=\"parquet\", partitioning=[\"Species\"])\n\nYou can check out a file listing using the files method. Another great feature of parquet files is that they allow you to partition the data accross variables of the dataset. These partitions mean that, in this case, data from each species of fish is written to it’s own file. This allows for even faster operations down the road, since we know that users will commonly need to filter on the species variable. Even though the data are partitioned into different files, pyarrow knows that this is a single dataset, and you still work with it by referencing just the directory in which all of the partitioned files live.\n\ndeltafish.files\n\n['/home/shares/deltafish/fish/Taxa=Acanthogobius flavimanus/part-0.parquet',\n '/home/shares/deltafish/fish/Taxa=Acipenser medirostris/part-0.parquet',\n '/home/shares/deltafish/fish/Taxa=Acipenser transmontanus/part-0.parquet',\n '/home/shares/deltafish/fish/Taxa=Acipenser/part-0.parquet'...\nYou can view the columns of a dataset using schema.to_string()\n\ndeltafish.schema.to_string()\n\nSampleID: string\nLength: double\nCount: double\nNotes_catch: string\nSpecies: string\nIf we are only interested in a few species, we can do a filter:\n\nexpr = ((ds.field(\"Species\")==\"Dorosoma petenense\")| \n        (ds.field(\"Species\")==\"Morone saxatilis\") |\n        (ds.field(\"Species\")== \"Spirinchus thaleichthys\"))\n\nfishf = deltafish.to_table(filter = expr)\n\nThere is another dataset included, the survey information. To do a join, we can just use the join method on the arrow dataset.\nFirst read in the survey dataset.\n\nsurvey = ds.dataset(\"/home/jclark/deltafish/survey\", format=\"parquet\", partitioning=[\"Source\"])\n\n\nsurvey.schema.to_string()\n\nThen do the join, and convert to a pandas data.frame.\n\nfish_j = fishf.join(survey, \"SampleID\").to_pandas()\n\nNote that when we did our first manipulation of this dataset, we went from working with a FileSystemDataset, which is a representation of a dataset on disk without reading it into memory, to a Table, which is read into memory. pyarrow has a number of functions that do computations on datasets without reading them into memory. However these are evaluated “eagerly,” as opposed to “lazily.” These are useful in some cases, like above, where we want to take a larger than memory dataset and generate a smaller dataset (via filter, or group by/summarize), but are not as useful if we need to do a join before our summarization/filter.\nMore functionality for lazy evaluation is on the horizon for pyarrow though, by leveraging Ibis."
  },
  {
    "objectID": "sections/13-group-project-2.html",
    "href": "sections/13-group-project-2.html",
    "title": "13  Group Project: Data Processing",
    "section": "",
    "text": "In your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-13.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills."
  },
  {
    "objectID": "sections/15-google-earth-engine.html",
    "href": "sections/15-google-earth-engine.html",
    "title": "15  Google Earth Engine",
    "section": "",
    "text": "SAM NOTES, DELETE LATER - need to make sure that .ipynb that student’s will work out of are running in same virutal enviroment as everything else - embed .ipynb into quarto notebook (get book to build from those examples) - use Ryan Abernathey’s post to help frame introduction"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#learning-objectives",
    "href": "sections/15-google-earth-engine.html#learning-objectives",
    "title": "15  Google Earth Engine",
    "section": "15.1 Learning Objectives",
    "text": "15.1 Learning Objectives\n\nUnderstand what Google Earth Engine provides and its applications\nLearn about some real-world applications of Google Earth Engine\nLearn how to get started using Google Earth Engine on your own computer\nLearn how to find and access Google Earth Engine Data"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#introduction-15-20min",
    "href": "sections/15-google-earth-engine.html#introduction-15-20min",
    "title": "15  Google Earth Engine",
    "section": "15.2 Introduction (15-20min)",
    "text": "15.2 Introduction (15-20min)\nSAM NOTES, DELETE LATER - have Ingmar help frame utility of GEE in intro\nGoogle Earth Engine is a geospatial processing platform powered by Google Cloud Platform. It contains over 30 years of satellite imagery and geospatial datasets that are continually updated and available instantly. The Earth Engine API is available in Python (and JavaScript) for anyone with an account to access and analyze data.\nADD SOME COOL IMAGERY HERE\nExplore the public Earth Engine Data Catalog which includes a variety of standard Earth science raster datasets. Browse by dataset tags or by satellites (Landsat, MODIS, Sentinel).\nSAM NOTES, DELETE LATER - typical: download data and work locally - new way:“Moving compute to data” – GEE is a great example of this"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#getting-started-with-google-earth-engine-gee-on-your-own-machine-40-min",
    "href": "sections/15-google-earth-engine.html#getting-started-with-google-earth-engine-gee-on-your-own-machine-40-min",
    "title": "15  Google Earth Engine",
    "section": "15.3 Getting started with Google Earth Engine (GEE) on your own machine (40 min)",
    "text": "15.3 Getting started with Google Earth Engine (GEE) on your own machine (40 min)\nSAM NOTES, DELETE LATER\n\nremove #| eval: false once code actually runs to embed outputs\ndon’t really have time to actually do this in class? include instructions for those who want to try it out on their own later?\nContent borrowed from Dr. Samantha Stevenson’s guide to installling Jupyter/Google Earth Engine onyour personal laptop.\nPREREQUISITE: need conda (see Earth Engine API installion instructions here) – haven’t added these instructions below yet\nalso, not working on my machine at the moment. issue with library installs? starts with earthengine authenticate\n\n\nInstall the Google Earth Engine API\n\nSAM NOTES, DELETE LATER - MOVE MOST OF THIS TO COURSE SETUP (not using conda) BUT KEEP GEE ACTIVATION STEP HERE\n\nCreate an environment where the Google Earth Engine API will live. This ensures that it and it’s dependent packages will not cause versioning issues with your base environment (or other environments). We’ll call our environment gee_env.\n\n#| eval: false\nconda create --name gee_env\n\nActivate your environment so your machine knows where to store subsequent installs.\n\n#| eval: false\nconda activate gee_env\nYou’ll know your environment is activated successfully when (gee_env) appears before the prompt in your terminal window (as opposed to (base), for example).\n\nInstall the Google Earth Engine API in your gee_env\n\n#| eval: false\nconda install -c conda-forge earthengine-api\n\nSign up for a GEE Account\n\nGEE is currently free for educational use. Sign up for an account at https://signup.earthengine.google.com (you’ll need this to authenticate in the next step).\n\nSet up GEE Authentication\n\nIn order to begin using GEE, you’ll need to connect your GEE envionment (gee_env) to the authentication credentials associated with your Google account. This will need to be done each time you connect to GEE, but should only be done once per session.\n\nOn the command line, type:\n\n#| eval: false\nearthengine authenticate\nThis should launch a browser window where you can login with your Google account to the Google Earth Engine Authenticator. Following the prompts will generate a code, which you’ll then need to copy and paste back onto the command line. This will be saved as an authentication token so you won’t need to go through this process again until the next time you start a new session.\n\nInstall necessary packages (if you don’t already have them)\n\n#| eval: false\npip install ee # Earth Engine API package\npip install geemap # package for interactive maping with GEE \npip install pandas # contains useful tools for data manipulation (may not need this)"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#visualize-global-precipitation-data-using-google-earth-engine",
    "href": "sections/15-google-earth-engine.html#visualize-global-precipitation-data-using-google-earth-engine",
    "title": "15  Google Earth Engine",
    "section": "15.4 Visualize global precipitation data using Google Earth Engine",
    "text": "15.4 Visualize global precipitation data using Google Earth Engine\nContent for this section was adapted from Dr. Sam Stevenson’s Visualizing global precipitation using Google Earth Engine lesson, given in her EDS 220 course in Fall 2021.\n\nImport necessary packages\n\n\nimport ee # MODULENOTFOUNDERROR\nimport geemap\nimport pandas as pd\n\n\nCreate an interactive basemap\n\nThe default basemap is (you guessed it) Google Maps. The following code displays an empty Google Map that you can manipulate just like you would in the typical Google Maps interface. Do this using the Map method from the geemap library. We’ll also center the map at a specified latitude and longitude (here, 40N, 100E), set a zoom level, and save our map as an object called myMap.\n\nmyMap = geemap.Map(center = [40, -100], zoom = 2)\nmyMap\n\n\nLoad ERA5 Image Collections from GEE\n\n\nNOTE: ADC has worked with these data – took 3 weeks to download\nEE colleciton is all you need to load and analyze imgage collection\nprecursor to Ingmar’s stuff\n\nWe’ll be using the ERA5 daily aggregates reanalysis dataset, produced by the European Centre for Medium-Range Weather Forecasts (ECMWF), found here, which models atmospheric weather observations. We’ll load the total_precipitation field (check out the dataset metadata on here).\nThe ImageCollection method extracts a set of individual images that satisfies some criterion that you pass to GEE through the ee package. This is stored as an ImageCollection object which can be filtered and processed in various ways. We can pass the ImageCollction method agruments to tell GEE which data we want to retrieve. Below, we retrieve all daily ERA5 data (so we can see individual rain events).\n\nweatherData = ee.ImageCollection('ECMWF/ERA5/DAILY')\n\n\nSelect an image to plot\n\nTo plot a map over our Google Maps basemap, we need an “Image” rather than an “ImageCollection.” ERA5 contains many different climate variables, so we need to pick what we’d like to plot. We’ll use the .select method to choose the parameter(s) we’re interested in from our weatherData object.\n\nprecip = weatherData.select(\"total_precipitation\")\n\nWe can look at our precip object using the print method to see that it’s still an “ImageCollection” which contains daily infomration from 1979 to 2020.\n\nprint(precip)\n\nWe want to filter it down to a single field for a time of interest – let’s say December 1-2, 2019. We apply the .filter method to our precip object and apply the ee.Filter.date method (from the ee package) to filter for data from our chosen date range. We also apply the .mean method, which takes whatever precedes it and calculates the average.\n\nprecip_filtered = precip.filter(ee.Filter.date('2019-12-01', '2019-12-02')).mean()\n\n\nAdd data to map\n\nWe can fist use the setCenter method to tell the map where to center itself. It takes the longitude and latitude as the first two coordinates, followed by the zoom level.\n\nMap.setCenter(-152.505706, 59.432367, 2) # Cook Inlet, Alaska (WE CAN CHANGE THIS LOCATION)\n\nNext, set a color palette to use when plotting the data layer. The following is a palette specified for precipitation in the GEE description page for ERA5. GEE has lots of color tables like this that you can look up.\n\nprecip_palette = {\n    'min':0,\n    'max':0.1,\n    'palette': ['#FFFFFF', '#00FFFF', '#0080FF', '#DA00FF', '#FFA400', '#FF0000']\n}\n\nFinally, plot our filtered data, precip_filtered on top of our basemap using the .addLayer method. We’ll aslo pass it our visualization parameters (colors and ranges stored in precip_palette, the name of the data field total precipitation, and opacity so that we can see the basemap underneath)\n\nMap.addLayer(precip_filtered, precip_palette, 'total precipitation', opacity = 0.3)"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#ingmars-demonstration-here30-40-min",
    "href": "sections/15-google-earth-engine.html#ingmars-demonstration-here30-40-min",
    "title": "15  Google Earth Engine",
    "section": "15.5 INGMAR’S DEMONSTRATION HERE?(30-40 min)",
    "text": "15.5 INGMAR’S DEMONSTRATION HERE?(30-40 min)"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#conclusionsummary",
    "href": "sections/15-google-earth-engine.html#conclusionsummary",
    "title": "15  Google Earth Engine",
    "section": "15.6 Conclusion/Summary",
    "text": "15.6 Conclusion/Summary\n\nlessons learned\nutilities\netc."
  },
  {
    "objectID": "sections/15-google-earth-engine.html#other-resources",
    "href": "sections/15-google-earth-engine.html#other-resources",
    "title": "15  Google Earth Engine",
    "section": "15.7 Other Resources",
    "text": "15.7 Other Resources\n\nGEE Code Editor is a web-based IDE for using GEE (JavaScript)"
  },
  {
    "objectID": "sections/17-group-project-3.html",
    "href": "sections/17-group-project-3.html",
    "title": "16  Group Project: Visualization",
    "section": "",
    "text": "In your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-17.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills."
  },
  {
    "objectID": "sections/18-arctic-data-staging.html",
    "href": "sections/18-arctic-data-staging.html",
    "title": "17  Workflows for data staging and publishing",
    "section": "",
    "text": "NSF archival policies for large datasets\nData transfer tools\nHow to manage co-locating data and code\n\neg: where model runs only has 1 TB of storage but model outputs 10 TB of data\nworkflow tools (pegasus, condor, slurm, snakemake)\n\nUploading large datasets to the Arctic Data Center"
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#nsf-policy-for-large-datasets",
    "href": "sections/18-arctic-data-staging.html#nsf-policy-for-large-datasets",
    "title": "17  Workflows for data staging and publishing",
    "section": "17.2 NSF policy for large datasets",
    "text": "17.2 NSF policy for large datasets\n\nthere are many different research methods that can generate large volumes of data. Numerical modeling (such as climate or ocean models) and anything generating high resolution imagery are two examples we see very commonly.\n\n\nThe Office of Polar Programs policy requires that metadata files, full data sets, and derived data products, must be deposited in a long-lived and publicly accessible archive.\n\n\nMetadata for all Arctic supported data sets must be submitted to the NSF Arctic Data Center (https://arcticdata.io).\n\n\nExceptions to the above data reporting requirements may be granted for social science and indigenous knowledge data, where privacy or intellectual property rights might take precedence. Such requested exceptions must be documented in the Data Management Plan.\n\n\ndatasets that are already published on a long lived archive do not need to be replicated to the Arctic Data Center\n\nexample: a research project accesses many terabytes of VIIRS satellite data. The original satellite data does not need to be published on the Arctic Data Center, but the code that accessed it, and derived products, can be published\n\nfor some numerical models, if the model results can be faithfully reproduced from code, the code that generates the models can be a sufficient archival product, as opposed to the code and the model output\n\nif the model is difficult to set up, or takes a very long time to run, we would probably reccommend publishing the output as well as code\n\nthe Arctic Data Center is committed to archiving data of any volume"
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#data-transfer-tools",
    "href": "sections/18-arctic-data-staging.html#data-transfer-tools",
    "title": "17  Workflows for data staging and publishing",
    "section": "17.3 Data transfer tools",
    "text": "17.3 Data transfer tools\n\nscenario: you need to send a bunch of data to the Arctic Data Center. after getting the credentials, you use scp to start the transfer. You know this typically takes around 12 hours so you start it at 5pm right when you leave the office expecting it to be done when you get back. When you arrive, you see there was a short network outage in the middle of the night. The whole job failed so you have to start it again…\n\nThere is a better way!\nThree key elements to data transfer\n- endpoints\n- network\n- transfer tool\n\n17.3.0.1 Endpoints\nThe from and to locations of the transfer, an endpoint is a remote computing device that can communicate back and forth with the network to which it is connected. The speed with which an endpoint can communicate with the network varies depending on how it is configured. Performance depends on the CPU, RAM, OS, and disk configuration. Examples:\n\nNCEAS datateam server:\nStandard laptop\n\n\n\n17.3.0.2 Network speed\nDetermines how quickly information can be sent between endpoints, largely dependent on what you pay for. Wired networks get significantly more speed than wireless.\n\nnot all networks are created equal\nserver to server (north hall to san diego) versus server to your house\n\n\n\n17.3.0.3 Transfer tools\n\nscp\n\nuses ssh for authentication and transfer\nif you can ssh to a server, you can probably use scp to move files without any other setup\ncopies all files linearly and simply. if a transfer fails in the middle, difficult to know exactly what files didn’t make it, so you have to start the whole thing over and re-transfer all the files\n\nrsync\n\nsimilar to scp but syncs files/directories as opposed to copying\nif the file already exists on the other side, it is skipped\n\nglobus\n\nparellelizes transfers by utilizing multiple network sockets simultaneously\nis able to fail and restart itself efficiently\nrequires more setup, endpoints need to be configured as globus nodes\n\n\n\n\n17.3.0.4 Globus\n\neasy to use, as long as your data are accessible via an endpoint configured as a Globus node\nleverage your institutions computing resources! they may be able to help get you access to a data transfer node already configured correctly\nthere are paid options to set up a node from your own workstation (Globus Connect Personal - check the naming here, and feature list)\n\nremember the other factors though! Globus won’t help you overcome a 1 Gb/s laptop connection speed, or a 50 Mb/s network speed"
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#documenting-large-datasets",
    "href": "sections/18-arctic-data-staging.html#documenting-large-datasets",
    "title": "17  Workflows for data staging and publishing",
    "section": "17.4 Documenting large datasets",
    "text": "17.4 Documenting large datasets\n\nthe Arctic Data Center is working to support large datasets, but we have performance considerations as well\nself documenting file formats are preferred, to prevent us from needing to document thousands-millions of files in a single metadata document\n\nnetcdf\ngeotiff, geopackage\n\nregular, parseable filenames and consistent file formatting is key\ncommunicate early and often with the Arctic Data Center staff"
  },
  {
    "objectID": "sections/20-reproducibility-containers.html",
    "href": "sections/20-reproducibility-containers.html",
    "title": "19  Reproducibility and Containers",
    "section": "",
    "text": "TODO: Decide about if/how to talk about WholeTale\nTODO: This lesson should be have a wow-factor and emphasize why we’re focusing all of this\nTODO: This lesson should be more about wrapping up and tying everything together than showing off new tech"
  },
  {
    "objectID": "sections/20-reproducibility-containers.html#learning-objectives",
    "href": "sections/20-reproducibility-containers.html#learning-objectives",
    "title": "19  Reproducibility and Containers",
    "section": "19.1 Learning Objectives",
    "text": "19.1 Learning Objectives\n\nLearn about software versioning\nBecome familiar with Docker as a tool to improve computational reproducibility"
  },
  {
    "objectID": "sections/20-reproducibility-containers.html#outline",
    "href": "sections/20-reproducibility-containers.html#outline",
    "title": "19  Reproducibility and Containers",
    "section": "19.2 Outline",
    "text": "19.2 Outline\n\nIntroduce software reproducibility\n\nMotivate the idea with examples and data\nTalk about software collapse\n\nhttp://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/\nhttps://xkcd.com/2347/\n\n\nSemantic versioning and the reality of it e.g., https://pandas.pydata.org/docs/development/policies.html#version-policy\nMyBinder\nWholeTale?\n\nExamples to look at including:\n\nhttps://numpy.org/neps/nep-0023-backwards-compatibility.html#example-cases\nhttps://github.com/scipy/scipy/issues/16418 > https://pandas.pydata.org/docs/whatsnew/v1.4.0.html#deprecations: DataFrame.append() and Series.append() have been deprecated and will be removed in a future version. Use pandas.concat() instead (GH35407).\n\nPrinciples to get across:\n\nYou probably should be thinking about software versioning\n\nKnow which version of versions of Python your code was written/tested under and keep track of that in a machine-readable way\nKnow the specific versions, of at least the specific MAJOR.MINOR of the packages your code was written+tested under and keep track of them in a machine-readable way (ie requirements.txt)"
  },
  {
    "objectID": "sections/20-reproducibility-containers.html#hands-off-demo",
    "href": "sections/20-reproducibility-containers.html#hands-off-demo",
    "title": "19  Reproducibility and Containers",
    "section": "19.3 Hands-off Demo",
    "text": "19.3 Hands-off Demo\nShow students an example of containerizing a workflow so it runs using a past version of Python and pinned versions of packages. Ideally find an example where behavior changes based on the Python or one or more package versions."
  }
]