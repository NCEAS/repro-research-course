[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "",
    "text": "This 5-day in-person workshop will provide researchers with an introduction to advanced topics in computationally reproducible research in python and R, including software and techniques for working with very large datasets. This includes working in cloud computing environments, docker containers, and parallel processing using tools like parsl and dask. The workshop will also cover concrete methods for documenting and uploading data to the Arctic Data Center, advanced approaches to tracking data provenance, responsible research and data management practices including data sovereignty and the CARE principles, and ethical concerns with data-intensive modeling and analysis.\n\n\n\n\n\n\n\n\n\n\nPlease note that by participating in this activity you agree to abide by the NCEAS Code of Conduct.\n\n\n\n\nIn this course, we will be using Python (> 3.0) as our primary language, and VS Code as our IDE. Below are instructions on how to get VS Code set up to work for the course. If you are already a regular Python user, you may already have another IDE set up. We strongly encourage you to set up VS Code with us, because we will use your local VS Code instance to write and execute code on one of the NCEAS servers.\n\n\nFirst, download VS Code if you do not already have it installed.\nCheck to make sure you have Python installed if you aren’t sure you do. To do this, from the terminal run:\npython3 --version\nIf you get an error, it means you need to install Python. Here are instructions for getting installed, depending on your operating system. Note: There are many ways to install and manage your Python installations, and advantages and drawbacks to each. If you are unsure about how to proceed, feel free to reach out to the instructor team for guidance.\n\nWindows: Download and run an installer from Python.org.\nMac: Install using homebrew. If you don’t have homebrew installed, follow the instructions from their webpage.\n\nbrew install python3\n\n\nAfter you run your install, make sure you check that the install is on your system PATH by running python3 --version again.\n\n\n\nThis section summarizes the official VS Code tutorial. For more detailed instructions and screenshots, see the source material\nFirst, install the Python extension for VS Code.\nOpen a terminal window in VS Code from the Terminal drop down in the main window. Run the following commands to initialize a project workspace in a directory called training. This example will show you how to do this locally. Later, we will show you how to set it up on the remote server with only one additional step.\nmkdir training\ncd training\ncode .\nNext, we will select the Python interpreter for the project. Open the Command Palette using Command + Shift + P (Control + Shift + P for windows). The Command Palette is a handy tool in VS Code that allows you to quickly find commands to VS Code, like editor commands, file edit and open commands, settings, etc. In the Command Palette, type “Python: Select Interpreter.” Push return to select the command, and then select the interpreter you want to use (your Python 3.X installation).\nFinally, download the Jupyter extension. You can create a test Jupyter Notebook document from the command pallete by typing “Create: New Jupyter Notebook” and selecting the command. This will open up a code editor pane with a notebook that you can test.\n\n\n\nTo make sure you can write and execute code in your project, create a Hello World test file.\n\nFrom the File Explorer toolbar, or using the terminal, create a file called hello.py\nAdd some test code to the file, and save\n\nmsg = \"Hello World\"\nprint(msg)\n\nExecute the script using either the Play button in the upper-right hand side of your window, or by running python3 hello.py in the terminal.\n\nFor more ways to run code in VS Code, see the tutorial\n\n\n\n\n\n\nThese written materials reflect the continuous development of learning materials at the Arctic Data Center and NCEAS to support individuals to understand, adopt, and apply ethical open science practices. In bringing these materials together we recognize that many individuals have contributed to their development. The primary authors are listed alphabetically in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Matthew B. Jones, Bryce Mecum, S. Jeanette Clark, Samantha Csik. 2022. Scalable and Computationally Reproducible Approaches to Arctic Research.\nAdditional contributors: Amber E. Budden, Natasha Haycock-Chavez, Noor Johnson, Stephanie Hampton, Jim Regetz, Bryce Mecum, Julien Brun, Julie Lowndes, Erin McLean, Andrew Barrett, David LeBauer, Jessica Guo.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "sections/01-adc-intro.html",
    "href": "sections/01-adc-intro.html",
    "title": "1  Welcome and Introductions",
    "section": "",
    "text": "This course is one of three that we are currently offering, covering fundamentals of open data sharing, reproducible research, ethical data use and reuse, and scalable computing for reusing large data sets."
  },
  {
    "objectID": "sections/02-remote-computing.html",
    "href": "sections/02-remote-computing.html",
    "title": "2  Remote Computing",
    "section": "",
    "text": "Notes from Google Sheet (DELETE LATER)\n- Servers & Networking\n- IP addressing\n- Bash shell programming\n- SSH\n- Remote session in VS Code"
  },
  {
    "objectID": "sections/02-remote-computing.html#learning-objectives",
    "href": "sections/02-remote-computing.html#learning-objectives",
    "title": "2  Remote Computing",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\n\nUnderstand the basic architecture of computer networks\nBecome familiarized with Bash Shell programming to navigate your computer’s file system (??)\nLearn how to connect to a remote computer via a shell"
  },
  {
    "objectID": "sections/02-remote-computing.html#introduction",
    "href": "sections/02-remote-computing.html#introduction",
    "title": "2  Remote Computing",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\n\nScientific synthesis and our ability to effectively and efficiently work with big data depends on the use of computers & the internet\nVS Code + remote development on a cluster is easy and way faster than your local machine"
  },
  {
    "objectID": "sections/02-remote-computing.html#servers-networking",
    "href": "sections/02-remote-computing.html#servers-networking",
    "title": "2  Remote Computing",
    "section": "2.3 Servers & Networking",
    "text": "2.3 Servers & Networking\n\nHost computers connect via networking equipment and can send messages to each other over communication protocols (aka internet protocols)\n\nClient: the host initiating the request\nServer: the host responding to a request"
  },
  {
    "objectID": "sections/02-remote-computing.html#ip-addressing",
    "href": "sections/02-remote-computing.html#ip-addressing",
    "title": "2  Remote Computing",
    "section": "2.4 IP addressing",
    "text": "2.4 IP addressing\n\nHosts are assigned a unique numerical address used for all communication and routing called an Internet Protocol Address (IP Address). They look something like this: 128.111.220.7\nEach IP Address can be used to communicate over various “ports”, which allows multiple applications to communicate with a host without mixing up traffic\nIP addresses can be difficult to remember, so they are also assigned hostnames\n\nHostnames are handled through the global Domain Name System (DNS)\nClients first look up a hostname in DNS to find the IP address, then they open a connection the the IP address\n\naurora.nceas.ucsb.edu == 128.111.220.46 (UPDATE THIS WITH SERVER USED FOR COURSE?)"
  },
  {
    "objectID": "sections/02-remote-computing.html#bash-shell-programming",
    "href": "sections/02-remote-computing.html#bash-shell-programming",
    "title": "2  Remote Computing",
    "section": "2.5 Bash Shell Programming",
    "text": "2.5 Bash Shell Programming\n\nWhat is a shell? From Wikipedia\n\n\n“a computer program which exposes an operating system’s services to a human user or other programs. In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), depending on a computer’s role and particular operation.”\n\n\nWhat is Bash Shell? A command line tool (language) commonly used to manipulate files and directories\n\nMac: bash via the Terminal (QUESTION: Mac users may have to switch from zsh to bash? exec bash? or exec zsh to switch back)\nWindows: bash via Git Bash"
  },
  {
    "objectID": "sections/02-remote-computing.html#some-group-exercise",
    "href": "sections/02-remote-computing.html#some-group-exercise",
    "title": "2  Remote Computing",
    "section": "2.6 Some group exercise:",
    "text": "2.6 Some group exercise:\n\nNavigate file system (show that this is equivalent to using Finder/Windows version), create a file, edit file, etc.\n\npwd\ncd\nls\ntouch\nmkdir\n(Queston: Do we want/need to show all of these? Missing any important ones?)"
  },
  {
    "objectID": "sections/02-remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "href": "sections/02-remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "title": "2  Remote Computing",
    "section": "2.7 Connecting to a remote computer via a shell",
    "text": "2.7 Connecting to a remote computer via a shell\n\nYou can use a shell to gain accesss to and remotely control (manage/transfer files/etc) other computers. To do so, you’ll need the following:\n\nremote computer (e.g. server) turned on\nIP address or name of remote computer\nnecessary permissions to access the remote computer\n\nSecure Shell, or SSH, is often used for securely connecting to and running shell commands on a remote host\n\nTremendously simplifies remote computing\nSupported out-of-the-box on Linux and Macs"
  },
  {
    "objectID": "sections/02-remote-computing.html#exercise",
    "href": "sections/02-remote-computing.html#exercise",
    "title": "2  Remote Computing",
    "section": "2.8 Exercise:",
    "text": "2.8 Exercise:\n\nLaunch your Terimal program:\n\nMacOS: navigate to Applications | Utilities and open Terminal\nWindows: Navigate to Windows Start | Git and open Git Bash\nALTERNATIVELY, from VS Code: Two options to open a terminal program\n\nClick on Terminal | New Terminal in top menu bar\nClick on the + (dropdown menu) | bash in the bottom right corner (QUESTION: Not sure that is always open/available depending on user configurations??)\n\n\nConnect to a remote server (UPDATE THIS SECTION)\n\njones@powder:~$ ssh jones@aurora.nceas.ucsb.edu\njones@aurora.nceas.ucsb.edu's password: \njones@aurora:~$ \n\nChange your password (UPDATE THIS SECTION)\n\njones@aurora:~$ passwd\nChanging password for jones.\n(current) UNIX password: \nEnter new UNIX password: \nRetype new UNIX password: \n\ncreate python script on server | write/execute some code | etc"
  },
  {
    "objectID": "sections/03-python-intro.html",
    "href": "sections/03-python-intro.html",
    "title": "3  Python Programming on Clusters",
    "section": "",
    "text": "Basic Python review\nUsing virtual environments\nWriting in Jupyter notebooks\nWriting functions in Python"
  },
  {
    "objectID": "sections/03-python-intro.html#introduction",
    "href": "sections/03-python-intro.html#introduction",
    "title": "3  Python Programming on Clusters",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\n\nVS Code + remote development on a cluster is easy and way faster than your local machine\nJupyter is a great way to do literate analysis\nFunctions provide ways to reuse your code across notebooks/projects"
  },
  {
    "objectID": "sections/03-python-intro.html#python-on-the-cluster",
    "href": "sections/03-python-intro.html#python-on-the-cluster",
    "title": "3  Python Programming on Clusters",
    "section": "3.3 Python on the cluster",
    "text": "3.3 Python on the cluster\n\nConnect to the server\nStart a training project and pick interpreter (this could also go in Sam’s session)\nCreate and execute hello.py\n\nfrom the IDE as a whole\nfrom IDE line by line\nfrom the terminal"
  },
  {
    "objectID": "sections/03-python-intro.html#virtual-environments",
    "href": "sections/03-python-intro.html#virtual-environments",
    "title": "3  Python Programming on Clusters",
    "section": "3.4 Virtual Environments",
    "text": "3.4 Virtual Environments\nWhy virtual environments? We’ll answer this.\nFirst we will create .bash_profile file to create variables that point to the install locations of python and virtualenvwrapper. .bash_profile is just a text file that contains bash commands that are run every time you start up a new terminal. Although setting up this file is not required to use virtualenvwrapper, it is convenient because it allows you to set up some reasonable defaults to the commands (meaning less typing, overall), and it makes sure that the package is available every time you start a new terminal.\nTo set up the .bash_profile. In VS Code, select “File > New Text File” then paste this into the file:\nexport VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3\nexport VIRTUALENVWRAPPER_VIRTUAWORKON_HOME=$HOME/.virtualenvs\nsource /usr/share/virtualenvwrapper/virtualenvwrapper.sh\nThe first line points virtualenvwrapper to the default python installation to use. In this case, we point it to the system wide install of python on the server. The next line sets the directory where your virtual environments will be stored. We point it to a hidden directory (.virtualenvs) in your home directory. Finally, the last line sources a bash script that ships with virtualenvwrapper, which makes all of virtualenvwrapper commands available in your terminal session.\nSave the file in the top of your home directory as .bash_profile.\nRestart your terminal, then check to make sure it was installed and configured correctly\n{bash, eval = FALSE} mkvirtualenv --version\nNow we can create the virtual environment we will use for the course\nmkvirtualenv scomp\nBy default, this will point to our Python 3.9 installation on the server, because of the settings in .bash_profile. If you want to point to a different version of python, or be more explicit about the version, you can use the -p flag and pass a path to the python install, like so:\n mkvirtualenv -p /usr/bin/python3 test\nAfter making a virtual environment, it will automatically be activated. You’ll see the name of the env you are working in on the left side of your terminal prompt in parentheses. To deactivate your environment (like if you want to work on a different project), just run deactivate. To activate it again, run\nworkon scomp\nYou can get a list of all available environments by just running:\nworkon\nNow let’s install the dependencies for this course into that environment. (Note: need to figure out how to get them this file)\npython3 -m pip install -r requirements.txt\n\n3.4.0.1 Installing locally (optional)\nvirtualenvwrapper was already installed on the server we are working on. To install on your local computer, run:\npip3 install virtualenvwrapper\nAnd then follow the instructions as described above, making sure that you have the correct paths set when you edit your .bash_profile."
  },
  {
    "objectID": "sections/03-python-intro.html#brief-overview-of-python-syntax",
    "href": "sections/03-python-intro.html#brief-overview-of-python-syntax",
    "title": "3  Python Programming on Clusters",
    "section": "3.5 Brief overview of python syntax",
    "text": "3.5 Brief overview of python syntax\n\nlists, arrays, dictionaries, associative arrays"
  },
  {
    "objectID": "sections/03-python-intro.html#jupyter-notebooks",
    "href": "sections/03-python-intro.html#jupyter-notebooks",
    "title": "3  Python Programming on Clusters",
    "section": "3.6 Jupyter notebooks",
    "text": "3.6 Jupyter notebooks\n\nCreate a notebook\nLoad in some libraries (pandas, numpy, scipy, matplotlib)\nRead in a csv\ngroup and summarize by a variable\ncreate a simple plot"
  },
  {
    "objectID": "sections/03-python-intro.html#functions",
    "href": "sections/03-python-intro.html#functions",
    "title": "3  Python Programming on Clusters",
    "section": "3.7 Functions",
    "text": "3.7 Functions\n\ncreate myplot.py\nwrite myplot() function to create the same plot we did in section above\nload myplot into jupyter notebook (from myplot.py import myplot)\nreplace old plot method with new function\nmore to come in Bryce’s section"
  },
  {
    "objectID": "sections/03-python-intro.html#resources",
    "href": "sections/03-python-intro.html#resources",
    "title": "3  Python Programming on Clusters",
    "section": "3.8 Resources",
    "text": "3.8 Resources"
  },
  {
    "objectID": "sections/04-parallel-programming.html",
    "href": "sections/04-parallel-programming.html",
    "title": "4  Pleasingly Parallel Programming",
    "section": "",
    "text": "Understand what parallel computing is and when it may be useful\nUnderstand how parallelism can work\nReview sequential loops and map functions\nBuild a parallel program using concurrent.futures\nBuild a parallel program using parsl\nUnderstand Thread Pools and Process pools"
  },
  {
    "objectID": "sections/04-parallel-programming.html#introduction",
    "href": "sections/04-parallel-programming.html#introduction",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nProcessing large amounts of data with complex models can be time consuming. New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America:\n\n\n\nLevy et al. 2016. doi:10.5063/F1Z899CZ\n\n\nThere are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization.\nAlternatively, think of remote sensing data. Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years.\n\n\n\nNEON Data Cube"
  },
  {
    "objectID": "sections/04-parallel-programming.html#why-parallelism",
    "href": "sections/04-parallel-programming.html#why-parallelism",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.3 Why parallelism?",
    "text": "4.3 Why parallelism?\nMuch R code runs fast and fine on a single processor. But at times, computations can be:\n\ncpu-bound: Take too much cpu time\nmemory-bound: Take too much memory\nI/O-bound: Take too much time to read/write from disk\nnetwork-bound: Take too much time to transfer\n\nTo help with cpu-bound computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire cluster of those computers. Plus, these machines also have large amounts of memory to avoid memory-bound computing jobs."
  },
  {
    "objectID": "sections/04-parallel-programming.html#processors-cpus-and-cores",
    "href": "sections/04-parallel-programming.html#processors-cpus-and-cores",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.4 Processors (CPUs) and Cores",
    "text": "4.4 Processors (CPUs) and Cores\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with mutliple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations.\nA computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time.\n\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here we show four quad-core processors for a total of 16 cores in this machine.\n\nYou can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later).\nHistorically, R has only utilized one processor, which makes it single-threaded. Which is a shame, because the 2017 MacBook Pro that I am writing this on is much more powerful than that:\n{bash eval=FALSE} jones@powder:~$ sysctl hw.ncpu hw.physicalcpu hw.ncpu: 8 hw.physicalcpu: 4\nTo interpret that output, this machine powder has 4 physical CPUs, each of which has two processing cores, for a total of 8 cores for computation. I’d sure like my R computations to use all of that processing power. Because its all on one machine, we can easily use multicore processing tools to make use of those cores. Now let’s look at the computational server aurora at NCEAS:\n{bash eval=FALSE} jones@included-crab:~$ lscpu | egrep 'CPU\\(s\\)|per core|per socket' CPU(s):                88 On-line CPU(s) list:   0-87 Thread(s) per core:    2 Core(s) per socket:    22 NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86 NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87\nNow that’s more compute power! included-crab has 384 GB of RAM, and ample storage. All still under the control of a single operating system.\nHowever, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now:\n\nJetStream\n\n640 nodes, 15,360 cores, 80TB RAM\n\nStampede2 at TACC is coming online in 2017\n\n4200 nodes, 285,600 cores\n\nTODO: update with modern cluster sizes\n\nNote that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster. One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine. But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster."
  },
  {
    "objectID": "sections/04-parallel-programming.html#modes-of-parallelization",
    "href": "sections/04-parallel-programming.html#modes-of-parallelization",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.5 Modes of parallelization",
    "text": "4.5 Modes of parallelization\n\nTODO: develop diagram(s) showing\n\nSingle memory image task parallelization\n\n\nSerial Launch tasks --> Task 1 --> Task 2 --> Task 3 --> Task 4 --> Task 5 --> Finish\nParallel Launch tasks -->                     Task 1 --\\                    Task 2 ---\\                    Task 3 -----> Finish                      Task 4 ---/                     Task 5 --/\n\nCluster task parallelization\n\nCluster parallel Show dispatch to cluster nodes and reassembly of data   Launch tasks -->                     Marshal --> Task 1 --> Unmarshal --\\                    Marshal --> Task 2 --> Unmarshal ---\\                    Marshal --> Task 3 --> Unmarshal -----> Finish                      Marshal --> Task 4 --> Unmarshal ---/                     Marshal --> Task 5 --> Unmarshal --/\n\nTODO: Should we also include figure with data or functional dependencies?"
  },
  {
    "objectID": "sections/04-parallel-programming.html#task-parallelism-with-concurrent.futures",
    "href": "sections/04-parallel-programming.html#task-parallelism-with-concurrent.futures",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.6 Task parallelism with concurrent.futures",
    "text": "4.6 Task parallelism with concurrent.futures\nWhen you have a list of repetitive tasks, you may be able to speed it up by adding more computing power. If each task is completely independent of the others, then it is a prime candidate for executing those tasks in parallel, each on its own core. For example, let’s build a simple loop that downloads the data files that we need for an analysis. First, we start with the serial implementation.\n# Use loop for serial execution of tasks\n\n# Tasks are to download data from a dataset\nThe issue with this loop is that we execute each trial sequentially, which means that only one of our 8 processors on this machine are in use. In order to exploit parallelism, we need to be able to dispatch our tasks as functions, with one task going to each processor. To do that, we need to convert our task to a function, and then use the map() function to apply that function to all of the members of a set. Here’s the same code rewritten to use map(), which applies a function to each of the members of a list (in this case the files we want to download):\n# Use `map` for serial execution of tasks\n\n# Tasks are to download data from a dataset"
  },
  {
    "objectID": "sections/04-parallel-programming.html#approaches-to-parallelization",
    "href": "sections/04-parallel-programming.html#approaches-to-parallelization",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.7 Approaches to parallelization",
    "text": "4.7 Approaches to parallelization\nWhen parallelizing jobs, one can:\n\nUse the multiple cores on a local computer through mclapply\nUse multiple processors on local (and remote) machines using makeCluster and clusterApply\n\nIn this approach, one has to manually copy data and code to each cluster member using clusterExport\nThis is extra work, but sometimes gaining access to a large cluster is worth it"
  },
  {
    "objectID": "sections/04-parallel-programming.html#concurrent.futures",
    "href": "sections/04-parallel-programming.html#concurrent.futures",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.8 concurrent.futures",
    "text": "4.8 concurrent.futures\n# Loop versus map for parallel execution of tasks\n\n# Using concurrent.futures and ThreadPool\n\n# Tasks are to download data from a dataset"
  },
  {
    "objectID": "sections/04-parallel-programming.html#parsl",
    "href": "sections/04-parallel-programming.html#parsl",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.9 parsl",
    "text": "4.9 parsl\n\nOverview of parsl and it’s use of python decorators.\n\n# Loop versus map for parallel execution of tasks\n\n# Using parsl decorators and ThreadPool\n\n# Tasks are to download data from a dataset\n\nConfigurable Executors in parsl\n\nHightThroughputExecutor for cluster jobs\n\n\n# Loop versus map for parallel execution of tasks\n\n# Using parsl decorators and ThreadPool\n\n# Tasks are to download data from a dataset"
  },
  {
    "objectID": "sections/04-parallel-programming.html#when-to-parallelize",
    "href": "sections/04-parallel-programming.html#when-to-parallelize",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.10 When to parallelize",
    "text": "4.10 When to parallelize\nIt’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!\nIn addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced. Some propose that this may follow Amdahl’s Law, where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines):\n#| eval: false\nlibrary(ggplot2)\nlibrary(tidyr)\namdahl <- function(p, s) {\n  return(1 / ( (1-p) + p/s  ))\n}\ndoubles <- 2^(seq(0,16))\ncpu_perf <- cbind(cpus = doubles, p50 = amdahl(.5, doubles))\ncpu_perf <- cbind(cpu_perf, p75 = amdahl(.75, doubles))\ncpu_perf <- cbind(cpu_perf, p85 = amdahl(.85, doubles))\ncpu_perf <- cbind(cpu_perf, p90 = amdahl(.90, doubles))\ncpu_perf <- cbind(cpu_perf, p95 = amdahl(.95, doubles))\n#cpu_perf <- cbind(cpu_perf, p99 = amdahl(.99, doubles))\ncpu_perf <- as.data.frame(cpu_perf)\ncpu_perf <- cpu_perf %>% gather(prop, speedup, -cpus)\nggplot(cpu_perf, aes(cpus, speedup, color=prop)) + \n  geom_line() +\n  scale_x_continuous(trans='log2') +\n  theme_bw() +\n  labs(title = \"Amdahl's Law\")\nSo, its important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done. With that, let’s do some parallel computing…"
  },
  {
    "objectID": "sections/04-parallel-programming.html#summary",
    "href": "sections/04-parallel-programming.html#summary",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.11 Summary",
    "text": "4.11 Summary\nIn this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores. Next, we reviewed the way in which traditional for loops in R can be rewritten as functions that are applied to a list serially using lapply, and then how the parallel package mclapply function can be substituted in order to utilize multiple cores on the local computer to speed up computations. Finally, we installed and reviewed the use of the foreach package with the %dopar operator to accomplish a similar parallelization using multiple cores."
  },
  {
    "objectID": "sections/04-parallel-programming.html#further-reading",
    "href": "sections/04-parallel-programming.html#further-reading",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.12 Further Reading",
    "text": "4.12 Further Reading"
  },
  {
    "objectID": "sections/05-adc-data-publishing.html#introduction",
    "href": "sections/05-adc-data-publishing.html#introduction",
    "title": "5  Documenting and Publishing Data",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction"
  },
  {
    "objectID": "sections/10-geopandas.html",
    "href": "sections/10-geopandas.html",
    "title": "6  Spatial and Image Data Using GeoPandas",
    "section": "",
    "text": "Reading raster data with rasterasterio\nUsing geopandas and rasterasterio to process raster data\nWorking with raster and vector data together"
  },
  {
    "objectID": "sections/10-geopandas.html#introduction",
    "href": "sections/10-geopandas.html#introduction",
    "title": "6  Spatial and Image Data Using GeoPandas",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\n\nRaster vs vector data\nWhat is a projection\nProcessing overview\n\ngoal is to calculate vessel distance per commercial fishing area"
  },
  {
    "objectID": "sections/10-geopandas.html#pre-processing-raster-data",
    "href": "sections/10-geopandas.html#pre-processing-raster-data",
    "title": "6  Spatial and Image Data Using GeoPandas",
    "section": "6.3 Pre-processing raster data",
    "text": "6.3 Pre-processing raster data\nThis is a test to make sure we can run some code in this notebook.\nimport geopandas as gpd\nimport rasterio\nimport rasterio.mask\nimport rasterio.warp\nimport rasterio.plot\nfrom rasterio import features\nfrom shapely.geometry import box\nfrom shapely.geometry import Polygon\nimport requests\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport pandas as pd\nimport numpy as np\nDownload the ship traffic raster from Kapsar et al.. We grab a one month slice from December, 2020 of a coastal subset of data with 1km resolution.\n\nurl_sf = 'https://cn.dataone.org/cn/v2/resolve/urn:uuid:dd61089d-f50e-4d87-9b75-6b4e2bd24776'\n\nresponse_sf = requests.get(url_sf)\nopen(\"Coastal_2020_12.tif\", \"wb\").write(response_sf.content)\n\n1132748\n\n\nOpen the raster file, plot it, and look at the metadata.\n\nwith rasterio.open(\"Coastal_2020_12.tif\") as dem_src:\n    ships = dem_src.read(1)\n    ships_meta = dem_src.profile\n\nplt.imshow(ships)\nprint(ships_meta)\n\n{'driver': 'GTiff', 'dtype': 'float32', 'nodata': -3.3999999521443642e+38, 'width': 3087, 'height': 2308, 'count': 1, 'crs': CRS.from_epsg(3338), 'transform': Affine(999.7994153462766, 0.0, -2550153.29233849,\n       0.0, -999.9687691991521, 2711703.104608573), 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n\n\n\n\n\nNow download a vector shapefile of commercial fishing districts in Alaska.\n\nurl = 'https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7c942c45-1539-4d47-b429-205499f0f3e4'\n\nresponse = requests.get(url)\nopen(\"Alaska_Commercial_Salmon_Boundaries.gpkg\", \"wb\").write(response.content)\n\n36544512\n\n\nRead in the data\ncomm = gpd.read_file(\"Alaska_Commercial_Salmon_Boundaries.gpkg\")\nThe raster data is in 3338, so we need to reproject this.\n\ncomm.crs\ncomm_3338 = comm.to_crs(\"EPSG:3338\")\n\ncomm_3338.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nWe can extract the bounding box for the area of interest, and use that to clip the original raster data to just the extent we need. We use the box function from shapely to create the bounding box, then create a geoDataFrame from them and convert the WGS84 coordinates to the Alaska Albers projection.\ntodo: explain the warp transform thing here\ncoords = rasterio.warp.transform_bounds('EPSG:4326',\n                                        'EPSG:3338',\n                                         -159.5,\n                                         55,\n                                         -144.5,\n                                         62)\ncoord_list = list(coords)\n\ncoord_box = box(coord_list[0],coord_list[1], coord_list[2], coord_list[3])\n\nbbox_crop = gpd.GeoDataFrame(\n    crs = 'EPSG:3338',\n    geometry = [coord_box])\nRead in raster again cropped to bounding box.\nwith rasterio.open(\"Coastal_2020_12.tif\") as src:\n    out_image, out_transform = rasterio.mask.mask(src, bbox_crop[\"geometry\"], crop=True)\n    out_meta = src.meta\n\nout_meta.update({\"driver\": \"GTiff\",\n                 \"height\": out_image.shape[1],\n                 \"width\": out_image.shape[2],\n                 \"transform\": out_transform,\n                 \"compress\": \"lzw\"})\n\nwith rasterio.open(\"Coastal_2020_12_masked.tif\", \"w\", **out_meta) as dest:\n    dest.write(out_image)\nWe can also clip the shapefile data to the same bounding box\ncomm_clip = comm_3338.clip(bbox_crop['geometry'])\n\n6.3.1 Check extents\nQuick plot to ensure they are in the same extent, and look as expected.\n\nwith rasterio.open('Coastal_2020_12_masked.tif') as src:\n    r = src.read(1)\n\nr[r == src.nodata] = np.nan\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nrasterio.plot.show(r,\n                   ax=ax,\n                   vmin = 0,\n                   vmax = 6000,\n                   transform = src.transform)\n\ncomm_clip.plot(ax=ax, facecolor='none', edgecolor='red')\n\n<AxesSubplot:>"
  },
  {
    "objectID": "sections/10-geopandas.html#calculate-total-distance-per-fishing-area",
    "href": "sections/10-geopandas.html#calculate-total-distance-per-fishing-area",
    "title": "6  Spatial and Image Data Using GeoPandas",
    "section": "6.4 Calculate total distance per fishing area",
    "text": "6.4 Calculate total distance per fishing area\nRasterize each polygon in the shapefile that falls within the bounds of the raster data we are calculating statistics for.\nWe return a dictionary of indexed arrays, where each item corresponds to one polygon (fishing area). The array contains the indices of the original raster that fall within that fishing area.\nwith rasterio.open('Coastal_2020_12_masked.tif') as src:\n    shape = src.shape\n    transform = src.transform\n    # read in the cropped raster\n    r_array = src.read(1)\n    # turn no data values into actual NaNs\n    r_array[r_array == src.nodata] = np.nan\n\ncomm_3338['id'] = range(0,len(comm_3338))\n\ncrosswalk_dict = {}\nfor geom, idx in zip(comm_3338.geometry, comm_3338['id']):\n    rasterized = features.rasterize(geom,\n                                    out_shape=shape,\n                                    transform=transform,\n                                    all_touched=True,\n                                    fill=0,\n                                    dtype='uint8')\n    # only save polygons that have a non-zero value\n    if any(np.unique(rasterized)) == 1:\n        crosswalk_dict[idx] = np.where(rasterized == 1)\n\n/usr/local/lib/python3.9/site-packages/rasterio/features.py:284: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n  for index, item in enumerate(shapes):\n\n\nNow we use the dictionary to calculate the sum of all of the pixels in the original raster that fall within each fishing area.\nmean_dict = {}\n# for each item in the dictionary\nfor key, value in crosswalk_dict.items():\n    # save the sum of the indices of the raster to a new dictionary\n    mean_dict[key] = np.nansum(r_array[value])\n# create a data frame from the result\ndf = pd.DataFrame.from_dict(mean_dict,\n    orient='index',\n    columns=['distance'])\n# extract the index of the data frame as a column to use in a join\ndf['id'] = df.index\nNow we join the result to the original geodataframe.\n# join the sums to the original data frame\nres_full = comm_3338.merge(df, on = \"id\", how = 'inner')\ntodo: Group by/summarize across another variable\n\nfig, ax = plt.subplots(figsize=(7, 7))\nplt.style.use(\"seaborn-talk\")\nax = res_full.plot(column = \"distance\", legend = True, ax = ax)\nfig = ax.figure\ncb_ax = fig.axes[1]\ncb_ax.set_yticklabels([\"0\", \"2,000\", \"4,000\", \"6,000\", \"8,000\", \"10,000\", \"12,000\", \"14,000\", \"16,000\"])\nax.set_axis_off()\nax.set_title(\"Distance Traveled by Ships in Kilometers\")\nplt.show()\n\n/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/ipykernel_3015/3303410610.py:6: UserWarning: FixedFormatter should only be used together with FixedLocator\n  cb_ax.set_yticklabels([\"0\", \"2,000\", \"4,000\", \"6,000\", \"8,000\", \"10,000\", \"12,000\", \"14,000\", \"16,000\"])"
  },
  {
    "objectID": "sections/11-parquet-arrow.html",
    "href": "sections/11-parquet-arrow.html",
    "title": "7  Data Futures: Parquet and Arrow",
    "section": "",
    "text": "The difference between column major and row major data\nSpeed advantages to columnnar data storage\nHow arrow enables faster processing"
  },
  {
    "objectID": "sections/11-parquet-arrow.html#introduction",
    "href": "sections/11-parquet-arrow.html#introduction",
    "title": "7  Data Futures: Parquet and Arrow",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\n\nopen, seek, read, write, close - ways to access data\ndifference between parquet and arrow\n\nhow paging and memory management works, blocks are organized by pages\non disk and in memory representation are the same\n\ncolumn (parquet) vs row (csv) data example\nwhy column can give faster read speeds\nhow arrow interacts with columnar data formats (like parquet)"
  },
  {
    "objectID": "sections/11-parquet-arrow.html#example",
    "href": "sections/11-parquet-arrow.html#example",
    "title": "7  Data Futures: Parquet and Arrow",
    "section": "7.3 Example",
    "text": "7.3 Example\n\nshow a read write example and benchmark maybe"
  }
]