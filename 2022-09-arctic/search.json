[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "",
    "text": "This 5-day in-person workshop will provide researchers with an introduction to advanced topics in computationally reproducible research in python and R, including software and techniques for working with very large datasets. This includes working in cloud computing environments, docker containers, and parallel processing using tools like parsl and dask. The workshop will also cover concrete methods for documenting and uploading data to the Arctic Data Center, advanced approaches to tracking data provenance, responsible research and data management practices including data sovereignty and the CARE principles, and ethical concerns with data-intensive modeling and analysis.\n\n\n\n\n\n\n\n\n\n\nPlease note that by participating in this activity you agree to abide by the NCEAS Code of Conduct.\n\n\n\n\nIn this course, we will be using Python (> 3.0) as our primary language, and VS Code as our IDE. Below are instructions on how to get VS Code set up to work for the course. If you are already a regular Python user, you may already have another IDE set up. We strongly encourage you to set up VS Code with us, because we will use your local VS Code instance to write and execute code on one of the NCEAS servers.\n\n\nFirst, download VS Code if you do not already have it installed.\nCheck to make sure you have Python installed if you aren’t sure you do. To do this, from the terminal run:\npython3 --version\nIf you get an error, it means you need to install Python. Here are instructions for getting installed, depending on your operating system. Note: There are many ways to install and manage your Python installations, and advantages and drawbacks to each. If you are unsure about how to proceed, feel free to reach out to the instructor team for guidance.\n\nWindows: Download and run an installer from Python.org.\nMac: Install using homebrew. If you don’t have homebrew installed, follow the instructions from their webpage.\n\nbrew install python3\n\n\nAfter you run your install, make sure you check that the install is on your system PATH by running python3 --version again.\n\n\n\nThis section summarizes the official VS Code tutorial. For more detailed instructions and screenshots, see the source material\nFirst, install the Python extension for VS Code.\nOpen a terminal window in VS Code from the Terminal drop down in the main window. Run the following commands to initialize a project workspace in a directory called training. This example will show you how to do this locally. Later, we will show you how to set it up on the remote server with only one additional step.\nmkdir training\ncd training\ncode .\nNext, we will select the Python interpreter for the project. Open the Command Palette using Command + Shift + P (Control + Shift + P for windows). The Command Palette is a handy tool in VS Code that allows you to quickly find commands to VS Code, like editor commands, file edit and open commands, settings, etc. In the Command Palette, type “Python: Select Interpreter.” Push return to select the command, and then select the interpreter you want to use (your Python 3.X installation).\nFinally, download the Jupyter extension. You can create a test Jupyter Notebook document from the command pallete by typing “Create: New Jupyter Notebook” and selecting the command. This will open up a code editor pane with a notebook that you can test.\n\n\n\nTo make sure you can write and execute code in your project, create a Hello World test file.\n\nFrom the File Explorer toolbar, or using the terminal, create a file called hello.py\nAdd some test code to the file, and save\n\nmsg = \"Hello World\"\nprint(msg)\n\nExecute the script using either the Play button in the upper-right hand side of your window, or by running python3 hello.py in the terminal.\n\nFor more ways to run code in VS Code, see the tutorial\n\n\n\n\n\n\nThese written materials reflect the continuous development of learning materials at the Arctic Data Center and NCEAS to support individuals to understand, adopt, and apply ethical open science practices. In bringing these materials together we recognize that many individuals have contributed to their development. The primary authors are listed alphabetically in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Matthew B. Jones, Bryce Mecum, S. Jeanette Clark, Samantha Csik. 2022. Scalable and Computationally Reproducible Approaches to Arctic Research.\nAdditional contributors: Amber E. Budden, Natasha Haycock-Chavez, Noor Johnson, Stephanie Hampton, Jim Regetz, Bryce Mecum, Julien Brun, Julie Lowndes, Erin McLean, Andrew Barrett, David LeBauer, Jessica Guo.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "sections/01-adc-intro.html",
    "href": "sections/01-adc-intro.html",
    "title": "1  Welcome and Introductions",
    "section": "",
    "text": "This course is one of three that we are currently offering, covering fundamentals of open data sharing, reproducible research, ethical data use and reuse, and scalable computing for reusing large data sets."
  },
  {
    "objectID": "sections/02-remote-computing.html",
    "href": "sections/02-remote-computing.html",
    "title": "2  Remote Computing",
    "section": "",
    "text": "Notes from Google Sheet (DELETE LATER)\n- Servers & Networking\n- IP addressing\n- Bash shell programming\n- SSH\n- Remote session in VS Code"
  },
  {
    "objectID": "sections/02-remote-computing.html#learning-objectives",
    "href": "sections/02-remote-computing.html#learning-objectives",
    "title": "2  Remote Computing",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\n\nUnderstand the basic architecture of computer networks\nBecome familiarized with Bash Shell programming to navigate your computer’s file system (??)\nLearn how to connect to a remote computer via a shell"
  },
  {
    "objectID": "sections/02-remote-computing.html#introduction",
    "href": "sections/02-remote-computing.html#introduction",
    "title": "2  Remote Computing",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\n\nScientific synthesis and our ability to effectively and efficiently work with big data depends on the use of computers & the internet\nVS Code + remote development on a cluster is easy and way faster than your local machine"
  },
  {
    "objectID": "sections/02-remote-computing.html#servers-networking",
    "href": "sections/02-remote-computing.html#servers-networking",
    "title": "2  Remote Computing",
    "section": "2.3 Servers & Networking",
    "text": "2.3 Servers & Networking\n\nHost computers connect via networking equipment and can send messages to each other over communication protocols (aka internet protocols)\n\nClient: the host initiating the request\nServer: the host responding to a request"
  },
  {
    "objectID": "sections/02-remote-computing.html#ip-addressing",
    "href": "sections/02-remote-computing.html#ip-addressing",
    "title": "2  Remote Computing",
    "section": "2.4 IP addressing",
    "text": "2.4 IP addressing\n\nHosts are assigned a unique numerical address used for all communication and routing called an Internet Protocol Address (IP Address). They look something like this: 128.111.220.7\nEach IP Address can be used to communicate over various “ports”, which allows multiple applications to communicate with a host without mixing up traffic\nIP addresses can be difficult to remember, so they are also assigned hostnames\n\nHostnames are handled through the global Domain Name System (DNS)\nClients first look up a hostname in DNS to find the IP address, then they open a connection the the IP address\n\naurora.nceas.ucsb.edu == 128.111.220.46 (UPDATE THIS WITH SERVER USED FOR COURSE?)"
  },
  {
    "objectID": "sections/02-remote-computing.html#bash-shell-programming",
    "href": "sections/02-remote-computing.html#bash-shell-programming",
    "title": "2  Remote Computing",
    "section": "2.5 Bash Shell Programming",
    "text": "2.5 Bash Shell Programming\n\nWhat is a shell? From Wikipedia\n\n\n“a computer program which exposes an operating system’s services to a human user or other programs. In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), depending on a computer’s role and particular operation.”\n\n\nWhat is Bash Shell? A command line tool (language) commonly used to manipulate files and directories\n\nMac: bash via the Terminal (QUESTION: Mac users may have to switch from zsh to bash? exec bash? or exec zsh to switch back)\nWindows: bash via Git Bash"
  },
  {
    "objectID": "sections/02-remote-computing.html#some-group-exercise",
    "href": "sections/02-remote-computing.html#some-group-exercise",
    "title": "2  Remote Computing",
    "section": "2.6 Some group exercise:",
    "text": "2.6 Some group exercise:\n\nNavigate file system (show that this is equivalent to using Finder/Windows version), create a file, edit file, etc.\n\npwd\ncd\nls\ntouch\nmkdir\n(Queston: Do we want/need to show all of these? Missing any important ones?)"
  },
  {
    "objectID": "sections/02-remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "href": "sections/02-remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "title": "2  Remote Computing",
    "section": "2.7 Connecting to a remote computer via a shell",
    "text": "2.7 Connecting to a remote computer via a shell\n\nYou can use a shell to gain accesss to and remotely control (manage/transfer files/etc) other computers. To do so, you’ll need the following:\n\nremote computer (e.g. server) turned on\nIP address or name of remote computer\nnecessary permissions to access the remote computer\n\nSecure Shell, or SSH, is often used for securely connecting to and running shell commands on a remote host\n\nTremendously simplifies remote computing\nSupported out-of-the-box on Linux and Macs"
  },
  {
    "objectID": "sections/02-remote-computing.html#exercise",
    "href": "sections/02-remote-computing.html#exercise",
    "title": "2  Remote Computing",
    "section": "2.8 Exercise:",
    "text": "2.8 Exercise:\n\nLaunch your Terimal program:\n\nMacOS: navigate to Applications | Utilities and open Terminal\nWindows: Navigate to Windows Start | Git and open Git Bash\nALTERNATIVELY, from VS Code: Two options to open a terminal program\n\nClick on Terminal | New Terminal in top menu bar\nClick on the + (dropdown menu) | bash in the bottom right corner (QUESTION: Not sure that is always open/available depending on user configurations??)\n\n\nConnect to a remote server (UPDATE THIS SECTION)\n\njones@powder:~$ ssh jones@aurora.nceas.ucsb.edu\njones@aurora.nceas.ucsb.edu's password: \njones@aurora:~$ \n\nChange your password (UPDATE THIS SECTION)\n\njones@aurora:~$ passwd\nChanging password for jones.\n(current) UNIX password: \nEnter new UNIX password: \nRetype new UNIX password: \n\ncreate python script on server | write/execute some code | etc"
  },
  {
    "objectID": "sections/03-python-intro.html",
    "href": "sections/03-python-intro.html",
    "title": "3  Python Programming on Clusters",
    "section": "",
    "text": "Basic Python review\nUsing virtual environments\nWriting in Jupyter notebooks\nWriting functions in Python"
  },
  {
    "objectID": "sections/03-python-intro.html#introduction",
    "href": "sections/03-python-intro.html#introduction",
    "title": "3  Python Programming on Clusters",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\n\nVS Code + remote development on a cluster is easy and way faster than your local machine\nJupyter is a great way to do literate analysis\nFunctions provide ways to reuse your code across notebooks/projects"
  },
  {
    "objectID": "sections/03-python-intro.html#starting-a-project",
    "href": "sections/03-python-intro.html#starting-a-project",
    "title": "3  Python Programming on Clusters",
    "section": "3.3 Starting a project",
    "text": "3.3 Starting a project\n\nConnect to the server\nStart a training workspace"
  },
  {
    "objectID": "sections/03-python-intro.html#virtual-environments",
    "href": "sections/03-python-intro.html#virtual-environments",
    "title": "3  Python Programming on Clusters",
    "section": "3.4 Virtual Environments",
    "text": "3.4 Virtual Environments\nWhy virtual environments? We’ll answer this.\nFirst we will create .bash_profile file to create variables that point to the install locations of python and virtualenvwrapper. .bash_profile is just a text file that contains bash commands that are run every time you start up a new terminal. Although setting up this file is not required to use virtualenvwrapper, it is convenient because it allows you to set up some reasonable defaults to the commands (meaning less typing, overall), and it makes sure that the package is available every time you start a new terminal.\nTo set up the .bash_profile. In VS Code, select ‘File > New Text File’ then paste this into the file:\nexport VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3\nexport VIRTUALENVWRAPPER_VIRTUAWORKON_HOME=$HOME/.virtualenvs\nsource /usr/share/virtualenvwrapper/virtualenvwrapper.sh\nThe first line points virtualenvwrapper to the default python installation to use. In this case, we point it to the system wide install of python on the server. The next line sets the directory where your virtual environments will be stored. We point it to a hidden directory (.virtualenvs) in your home directory. Finally, the last line sources a bash script that ships with virtualenvwrapper, which makes all of virtualenvwrapper commands available in your terminal session.\nSave the file in the top of your home directory as .bash_profile.\nRestart your terminal, then check to make sure it was installed and configured correctly\nmkvirtualenv --version\nNow we can create the virtual environment we will use for the course\nmkvirtualenv scomp\nBy default, this will point to our Python 3.9 installation on the server, because of the settings in .bash_profile. If you want to point to a different version of python, or be more explicit about the version, you can use the -p flag and pass a path to the python install, like so:\n mkvirtualenv -p /usr/bin/python3 test\nAfter making a virtual environment, it will automatically be activated. You’ll see the name of the env you are working in on the left side of your terminal prompt in parentheses. To deactivate your environment (like if you want to work on a different project), just run deactivate. To activate it again, run\nworkon scomp\nYou can get a list of all available environments by just running:\nworkon\nNow let’s install the dependencies for this course into that environment. (Note: need to figure out how to get them this file)\npython3 -m pip install -r requirements.txt\n\n3.4.0.1 Installing locally (optional)\nvirtualenvwrapper was already installed on the server we are working on. To install on your local computer, run:\npip3 install virtualenvwrapper\nAnd then follow the instructions as described above, making sure that you have the correct paths set when you edit your .bash_profile."
  },
  {
    "objectID": "sections/03-python-intro.html#brief-overview-of-python-syntax",
    "href": "sections/03-python-intro.html#brief-overview-of-python-syntax",
    "title": "3  Python Programming on Clusters",
    "section": "3.5 Brief overview of python syntax",
    "text": "3.5 Brief overview of python syntax\nAssign values to variables using =\n\nx = 4\nprint(x)\n\n4\n\n\nThere are 5 standard data types in python\n\nNumber (int, long, float, complex)\nString\nList\nTuple\nDictionary\n\nWe already saw a number type, here is a string:\n\nstr = 'Hello World!'\nprint(str)\n\nHello World!\n\n\nLists in python are very versatile, and are created using square brackets []. Items in a list can be of different data types.\n\nlist = [100, 50, -20, 'text']\nprint(list)\n\n[100, 50, -20, 'text']\n\n\nYou can access items in a list by index using the square brackets. Note indexing starts with 0 in python. The slice operator enables you to easily access a portion of the list without needing to specify every index.\n\nlist[0] # print first element\nlist[1:3] # print 2nd until 4th elements\nlist[:2] # print first until the 3rd\nlist[2:] # print last elements from 3rd\n\n100\n\n\n[50, -20]\n\n\n[100, 50]\n\n\n[-20, 'text']\n\n\nThe + and * operators work on lists by creating a new list using either concatenation (+) or repetition (*).\n\nlist2 = ['more', 'things']\n\nlist + list2\nlist * 3\n\n[100, 50, -20, 'text', 'more', 'things']\n\n\n[100, 50, -20, 'text', 100, 50, -20, 'text', 100, 50, -20, 'text']\n\n\nTuples are similar to lists, except the values cannot be changed in place. They are constructed with parentheses.\n\ntuple = ('a', 'b', 'c', 'd')\ntuple[0]\ntuple * 3\ntuple + tuple\n\n'a'\n\n\n('a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'a', 'b', 'c', 'd')\n\n\n('a', 'b', 'c', 'd', 'a', 'b', 'c', 'd')\n\n\nObserve the difference when we try to change the first value. It works for a list:\n\nlist[0] = 'new value'\nlist\n\n['new value', 50, -20, 'text']\n\n\n…and errors for a tuple.\ntuple[0] = 'new value'\nTypeError: 'tuple' object does not support item assignment\nDictionaries consist of key-value pairs, and are created using the syntax {key: value}. Keys are usually numbers or strings, and values can be any data type.\n\ndict = {'name': ['Jeanette', 'Matt'],\n        'location': ['Tucson', 'Juneau']}\n\ndict['name']\ndict.keys()\n\n['Jeanette', 'Matt']\n\n\ndict_keys(['name', 'location'])\n\n\nTo determine the type of an object, you can use the type() method.\n\ntype(list)\ntype(tuple)\ntype(dict)\n\nlist\n\n\ntuple\n\n\ndict"
  },
  {
    "objectID": "sections/03-python-intro.html#jupyter-notebooks",
    "href": "sections/03-python-intro.html#jupyter-notebooks",
    "title": "3  Python Programming on Clusters",
    "section": "3.6 Jupyter notebooks",
    "text": "3.6 Jupyter notebooks\nTo create a new notebook, from the file menu select File > New File > Jupyter Notebook\nAt the top of your notebook, add a first level header using a single hash. Practice some markdown text by creating:\n\na list\nbold text\na link\n\nUse the Markdown cheat sheet if needed.\nTo open a chunk of code, type three backticks (`), curly braces, and then the word python. Close the code chunk using three more backticks.\n\n3.6.1 Load libraries\nIn your first code chunk, lets load in some modules. We’ll use pandas, numpy, matplotlib.pyplot, requests, skimpy, and exists from os.path.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport requests\nimport skimpy\nfrom os.path import exists\nA note on style: There are a few ways to construct import statements. The above code uses three of the most common:\nimport module\nimport module as m\nfrom module import function\nThe first way of importing will make the module a function comes from more explicitly clear, and is the simplest. However for very long module names, or ones that are used very frequently (like pandas, numpy, and matplotlib.plot), the code in the notebook will be more cluttered with constant calls to longer module names. So module.function() instead is written as m.function()\nThe second way of importing a module is a good style to use in cases where modules are used frequently, or have extremely long names. If you import every single module with a short name, however, you might have a hard time remembering which modules are named what, and it might be more confusing for others trying to read your code. Many of the most commonly used libraries for python data science have community-driven styling for how they are abbreviated in import statements, and these community norms are generally best followed.\nFinally, the last way to import a single object from a module can be helpful if you only need that one piece from a larger module, but again, like the first case, results in less explicit code and therefore runs the risk of your or someone else misremembering the usage and source.\n\n\n3.6.2 Read in a csv\nCreate a new code chunk that will download the csv that we are going to use for this tutorial.\n\nNavigate to Rohi Muthyala, Åsa Rennermalm, Sasha Leidman, Matthew Cooper, Sarah Cooley, et al. 2022. 62 days of Supraglacial streamflow from June-August, 2016 over southwest Greenland. Arctic Data Center. doi:10.18739/A2XW47X5F.\nRight click the download button for ‘Discharge_timeseries.csv’\nClick ‘copy link address’\n\nCreate a variable called URL and assign it the link copied to your clipboard. Then use requests.get to download the file, and open to write it to disk, to a directory called data/. We’ll write this bundled in an if statement so that we only download the file if it doesn’t yet exist.\nif not exists('data/discharge_timeseries.csv'):\n\n        url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3Ae248467d-e1f9-4a32-9e38-a9b4fb17cefb'\n\n        data = requests.get(url)\n        a = open('data/discharge_timeseries.csv', 'wb').write(data.content)\nNow we can read in the data from the file.\n\ndf = pd.read_csv('data/discharge_timeseries.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Total Pressure [m]\n      Air Pressure [m]\n      Stage [m]\n      Discharge [m3/s]\n      temperature [degrees C]\n    \n  \n  \n    \n      0\n      6/13/2016 0:00\n      9.816\n      9.609775\n      0.206225\n      0.083531\n      -0.1\n    \n    \n      1\n      6/13/2016 0:05\n      9.810\n      9.609715\n      0.200285\n      0.077785\n      -0.1\n    \n    \n      2\n      6/13/2016 0:10\n      9.804\n      9.609656\n      0.194344\n      0.072278\n      -0.1\n    \n    \n      3\n      6/13/2016 0:15\n      9.800\n      9.609596\n      0.190404\n      0.068756\n      -0.1\n    \n    \n      4\n      6/13/2016 0:20\n      9.793\n      9.609537\n      0.183463\n      0.062804\n      -0.1\n    \n  \n\n\n\n\nThe column names are a bit messy so we can use clean_columns from skimpy to make them cleaner for programming very quickly. We can also use the skim function to get a quick summary of the data. :::{.column-page}\n\nclean_df = skimpy.clean_columns(df)\nskimpy.skim(clean_df)\n\n6 column names have been cleaned\n\n\n\n╭───────────────────────────────────── skimpy summary ──────────────────────────────────────╮\n│          Data Summary                Data Types                                           │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                    │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                    │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                    │\n│ │ Number of rows    │ 17856  │ │ float64     │ 5     │                                    │\n│ │ Number of columns │ 6      │ │ string      │ 1     │                                    │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                    │\n│                                          number                                           │\n│ ┏━━━━━━━━━━━━━━━┳━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name   ┃ NA ┃ NA %  ┃ mean   ┃ sd    ┃ p0      ┃ p25   ┃ p75  ┃ p100 ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━╇━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n│ │ total_pressur │  0 │     0 │    9.9 │  0.12 │     9.6 │   9.8 │   10 │   10 │ ▁▅█▇▅▂ │  │\n│ │ e_m           │    │       │        │       │         │       │      │      │        │  │\n│ │ air_pressure_ │  0 │     0 │    9.6 │  0.06 │     9.5 │   9.6 │  9.7 │  9.7 │ ▂▅▄▆█▃ │  │\n│ │ m             │    │       │        │       │         │       │      │      │        │  │\n│ │ stage_m       │  0 │     0 │   0.28 │  0.12 │ 0.00056 │  0.17 │ 0.37 │ 0.56 │ ▁█▇▇▆▁ │  │\n│ │ discharge_m_3 │  0 │     0 │   0.22 │  0.19 │ 4.7e-08 │ 0.055 │ 0.35 │ 0.96 │  █▄▃▁  │  │\n│ │ _s            │    │       │        │       │         │       │      │      │        │  │\n│ │ temperature_d │  8 │ 0.045 │ -0.034 │ 0.053 │    -0.1 │  -0.1 │    0 │  0.2 │   ▅█   │  │\n│ │ egrees_       │    │       │        │       │         │       │      │      │        │  │\n│ └───────────────┴────┴───────┴────────┴───────┴─────────┴───────┴──────┴──────┴────────┘  │\n│                                          string                                           │\n│ ┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name         ┃ NA    ┃ NA %     ┃ words per row          ┃ total words        ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ date                │     0 │        0 │                      2 │              36000 │  │\n│ └─────────────────────┴───────┴──────────┴────────────────────────┴────────────────────┘  │\n╰─────────────────────────────────────────── End ───────────────────────────────────────────╯\n\n\n\n:::\nWe can see that the date column is classed as a string, and not a date, so let’s fix that.\n\n\nclean_df['date'] = pd.to_datetime(clean_df['date'])\nskimpy.skim(clean_df)\n\n╭───────────────────────────────────── skimpy summary ──────────────────────────────────────╮\n│          Data Summary                Data Types                                           │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                    │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                    │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                    │\n│ │ Number of rows    │ 17856  │ │ float64     │ 5     │                                    │\n│ │ Number of columns │ 6      │ │ datetime64  │ 1     │                                    │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                    │\n│                                          number                                           │\n│ ┏━━━━━━━━━━━━━━━┳━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name   ┃ NA ┃ NA %  ┃ mean   ┃ sd    ┃ p0      ┃ p25   ┃ p75  ┃ p100 ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━╇━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n│ │ total_pressur │  0 │     0 │    9.9 │  0.12 │     9.6 │   9.8 │   10 │   10 │ ▁▅█▇▅▂ │  │\n│ │ e_m           │    │       │        │       │         │       │      │      │        │  │\n│ │ air_pressure_ │  0 │     0 │    9.6 │  0.06 │     9.5 │   9.6 │  9.7 │  9.7 │ ▂▅▄▆█▃ │  │\n│ │ m             │    │       │        │       │         │       │      │      │        │  │\n│ │ stage_m       │  0 │     0 │   0.28 │  0.12 │ 0.00056 │  0.17 │ 0.37 │ 0.56 │ ▁█▇▇▆▁ │  │\n│ │ discharge_m_3 │  0 │     0 │   0.22 │  0.19 │ 4.7e-08 │ 0.055 │ 0.35 │ 0.96 │  █▄▃▁  │  │\n│ │ _s            │    │       │        │       │         │       │      │      │        │  │\n│ │ temperature_d │  8 │ 0.045 │ -0.034 │ 0.053 │    -0.1 │  -0.1 │    0 │  0.2 │   ▅█   │  │\n│ │ egrees_       │    │       │        │       │         │       │      │      │        │  │\n│ └───────────────┴────┴───────┴────────┴───────┴─────────┴───────┴──────┴──────┴────────┘  │\n│                                         datetime                                          │\n│ ┏━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓  │\n│ ┃ column_name    ┃ NA  ┃ NA %   ┃ first         ┃ last                    ┃ frequency  ┃  │\n│ ┡━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩  │\n│ │ date           │   0 │      0 │  2016-06-13   │   2016-08-13 23:55:00   │ 5T         │  │\n│ └────────────────┴─────┴────────┴───────────────┴─────────────────────────┴────────────┘  │\n╰─────────────────────────────────────────── End ───────────────────────────────────────────╯\n\n\n\n\nIf we wanted to calculate the daily mean flow (as opposed to the flow every 5 minutes), we need to:\n\ncreate a new column with only the date\ngroup by that variable\nsummarize over it by taking the mean of the discharge variable\n\nFirst we should probably rename our existing date/time column to prevent from getting confused.\nclean_df = clean_df.rename(columns = {'date': 'datetime'})\nNow create the new date column\nclean_df['date'] = clean_df['datetime'].dt.date\nFinally, we use group by to split the data into groups according to the date, apply a function (mean) to each group, and then combine the results in a single data table.\ndaily_flow = clean_df.groupby('date', as_index = False).mean()\n\ncreate a simple plot\n\n\nvar = 'discharge_m_3_s'\nvar_labs = {'discharge_m_3_s': 'Total Discharge'}\n\nfig, ax = plt.subplots(figsize=(7, 7))\nplt.style.use(\"seaborn-talk\")\nplt.plot(daily_flow['date'], daily_flow[var]);\nplt.xticks(rotation = 45);\nax.set_ylabel(var_labs.get('discharge_m_3_s'));"
  },
  {
    "objectID": "sections/03-python-intro.html#functions",
    "href": "sections/03-python-intro.html#functions",
    "title": "3  Python Programming on Clusters",
    "section": "3.7 Functions",
    "text": "3.7 Functions\nThe plot we made above is great, but what if we wanted to make it for each variable? We could copy paste it and replace some things, but this violates a core tenet of programming: Don’t Repeat Yourself! Instead, we’ll create a function called myplot that accepts the data frame and variable as arguments.\n\ncreate myplot.py\n\ndef myplot(df, var):\n\n        var_labs = {'discharge_m_3_s': 'Total Discharge (m^3/s)',\n                    'total_pressure_m': 'Total Pressure (m)',\n                    'air_pressure_m': 'Air Pressure (m)',\n                    'stage_m': 'Stage (m)',\n                    'temperature_degrees_c': 'Temperature (C)'}\n\n        fig, ax = plt.subplots(figsize=(7, 7))\n        plt.style.use(\"seaborn-talk\")\n        plt.plot(df['date'], df[var]);\n        plt.xticks(rotation = 45);\n        ax.set_ylabel(var_labs.get(var));\n\nload myplot into jupyter notebook (from myplot.py import myplot)\nreplace old plot method with new function\n\n\nmyplot(daily_flow, 'temperature_degrees_c')\n\n\n\n\n\nmore to come in Bryce’s section"
  },
  {
    "objectID": "sections/03-python-intro.html#resources",
    "href": "sections/03-python-intro.html#resources",
    "title": "3  Python Programming on Clusters",
    "section": "3.8 Resources",
    "text": "3.8 Resources"
  },
  {
    "objectID": "sections/04-parallel-programming.html",
    "href": "sections/04-parallel-programming.html",
    "title": "4  Pleasingly Parallel Programming",
    "section": "",
    "text": "Understand what parallel computing is and when it may be useful\nUnderstand how parallelism can work\nReview sequential loops and map functions\nBuild a parallel program using concurrent.futures\nBuild a parallel program using parsl\nUnderstand Thread Pools and Process pools"
  },
  {
    "objectID": "sections/04-parallel-programming.html#introduction",
    "href": "sections/04-parallel-programming.html#introduction",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nProcessing large amounts of data with complex models can be time consuming. New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America:\n\n\n\nLevy et al. 2016. doi:10.5063/F1Z899CZ\n\n\nThere are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization.\nAlternatively, think of remote sensing data. Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years.\n\n\n\nNEON Data Cube"
  },
  {
    "objectID": "sections/04-parallel-programming.html#why-parallelism",
    "href": "sections/04-parallel-programming.html#why-parallelism",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.3 Why parallelism?",
    "text": "4.3 Why parallelism?\nMuch R code runs fast and fine on a single processor. But at times, computations can be:\n\ncpu-bound: Take too much cpu time\nmemory-bound: Take too much memory\nI/O-bound: Take too much time to read/write from disk\nnetwork-bound: Take too much time to transfer\n\nTo help with cpu-bound computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire cluster of those computers. Plus, these machines also have large amounts of memory to avoid memory-bound computing jobs."
  },
  {
    "objectID": "sections/04-parallel-programming.html#processors-cpus-and-cores",
    "href": "sections/04-parallel-programming.html#processors-cpus-and-cores",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.4 Processors (CPUs) and Cores",
    "text": "4.4 Processors (CPUs) and Cores\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with mutliple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations.\nA computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time.\n\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here we show four quad-core processors for a total of 16 cores in this machine.\n\nYou can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later).\nHistorically, R has only utilized one processor, which makes it single-threaded. Which is a shame, because the 2017 MacBook Pro that I am writing this on is much more powerful than that:\n{bash eval=FALSE} jones@powder:~$ sysctl hw.ncpu hw.physicalcpu hw.ncpu: 8 hw.physicalcpu: 4\nTo interpret that output, this machine powder has 4 physical CPUs, each of which has two processing cores, for a total of 8 cores for computation. I’d sure like my R computations to use all of that processing power. Because its all on one machine, we can easily use multicore processing tools to make use of those cores. Now let’s look at the computational server aurora at NCEAS:\n{bash eval=FALSE} jones@included-crab:~$ lscpu | egrep 'CPU\\(s\\)|per core|per socket' CPU(s):                88 On-line CPU(s) list:   0-87 Thread(s) per core:    2 Core(s) per socket:    22 NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86 NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87\nNow that’s more compute power! included-crab has 384 GB of RAM, and ample storage. All still under the control of a single operating system.\nHowever, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now:\n\nJetStream\n\n640 nodes, 15,360 cores, 80TB RAM\n\nStampede2 at TACC is coming online in 2017\n\n4200 nodes, 285,600 cores\n\nTODO: update with modern cluster sizes\n\nNote that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster. One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine. But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster."
  },
  {
    "objectID": "sections/04-parallel-programming.html#modes-of-parallelization",
    "href": "sections/04-parallel-programming.html#modes-of-parallelization",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.5 Modes of parallelization",
    "text": "4.5 Modes of parallelization\n\nTODO: develop diagram(s) showing\n\nSingle memory image task parallelization\n\n\nSerial Launch tasks --> Task 1 --> Task 2 --> Task 3 --> Task 4 --> Task 5 --> Finish\nParallel Launch tasks -->                     Task 1 --\\                    Task 2 ---\\                    Task 3 -----> Finish                      Task 4 ---/                     Task 5 --/\n\nCluster task parallelization\n\nCluster parallel Show dispatch to cluster nodes and reassembly of data   Launch tasks -->                     Marshal --> Task 1 --> Unmarshal --\\                    Marshal --> Task 2 --> Unmarshal ---\\                    Marshal --> Task 3 --> Unmarshal -----> Finish                      Marshal --> Task 4 --> Unmarshal ---/                     Marshal --> Task 5 --> Unmarshal --/\n\nTODO: Should we also include figure with data or functional dependencies?"
  },
  {
    "objectID": "sections/04-parallel-programming.html#task-parallelism-with-concurrent.futures",
    "href": "sections/04-parallel-programming.html#task-parallelism-with-concurrent.futures",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.6 Task parallelism with concurrent.futures",
    "text": "4.6 Task parallelism with concurrent.futures\nWhen you have a list of repetitive tasks, you may be able to speed it up by adding more computing power. If each task is completely independent of the others, then it is a prime candidate for executing those tasks in parallel, each on its own core. For example, let’s build a simple loop that downloads the data files that we need for an analysis. First, we start with the serial implementation.\n# Use loop for serial execution of tasks\n\n# Tasks are to download data from a dataset\nThe issue with this loop is that we execute each trial sequentially, which means that only one of our 8 processors on this machine are in use. In order to exploit parallelism, we need to be able to dispatch our tasks as functions, with one task going to each processor. To do that, we need to convert our task to a function, and then use the map() function to apply that function to all of the members of a set. Here’s the same code rewritten to use map(), which applies a function to each of the members of a list (in this case the files we want to download):\n# Use `map` for serial execution of tasks\n\n# Tasks are to download data from a dataset"
  },
  {
    "objectID": "sections/04-parallel-programming.html#approaches-to-parallelization",
    "href": "sections/04-parallel-programming.html#approaches-to-parallelization",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.7 Approaches to parallelization",
    "text": "4.7 Approaches to parallelization\nWhen parallelizing jobs, one can:\n\nUse the multiple cores on a local computer through mclapply\nUse multiple processors on local (and remote) machines using makeCluster and clusterApply\n\nIn this approach, one has to manually copy data and code to each cluster member using clusterExport\nThis is extra work, but sometimes gaining access to a large cluster is worth it"
  },
  {
    "objectID": "sections/04-parallel-programming.html#concurrent.futures",
    "href": "sections/04-parallel-programming.html#concurrent.futures",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.8 concurrent.futures",
    "text": "4.8 concurrent.futures\n# Loop versus map for parallel execution of tasks\n\n# Using concurrent.futures and ThreadPool\n\n# Tasks are to download data from a dataset"
  },
  {
    "objectID": "sections/04-parallel-programming.html#parsl",
    "href": "sections/04-parallel-programming.html#parsl",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.9 parsl",
    "text": "4.9 parsl\n\nOverview of parsl and it’s use of python decorators.\n\n# Loop versus map for parallel execution of tasks\n\n# Using parsl decorators and ThreadPool\n\n# Tasks are to download data from a dataset\n\nConfigurable Executors in parsl\n\nHightThroughputExecutor for cluster jobs\n\n\n# Loop versus map for parallel execution of tasks\n\n# Using parsl decorators and ThreadPool\n\n# Tasks are to download data from a dataset"
  },
  {
    "objectID": "sections/04-parallel-programming.html#when-to-parallelize",
    "href": "sections/04-parallel-programming.html#when-to-parallelize",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.10 When to parallelize",
    "text": "4.10 When to parallelize\nIt’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!\nIn addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced. Some propose that this may follow Amdahl’s Law, where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines):\n#| eval: false\nlibrary(ggplot2)\nlibrary(tidyr)\namdahl <- function(p, s) {\n  return(1 / ( (1-p) + p/s  ))\n}\ndoubles <- 2^(seq(0,16))\ncpu_perf <- cbind(cpus = doubles, p50 = amdahl(.5, doubles))\ncpu_perf <- cbind(cpu_perf, p75 = amdahl(.75, doubles))\ncpu_perf <- cbind(cpu_perf, p85 = amdahl(.85, doubles))\ncpu_perf <- cbind(cpu_perf, p90 = amdahl(.90, doubles))\ncpu_perf <- cbind(cpu_perf, p95 = amdahl(.95, doubles))\n#cpu_perf <- cbind(cpu_perf, p99 = amdahl(.99, doubles))\ncpu_perf <- as.data.frame(cpu_perf)\ncpu_perf <- cpu_perf %>% gather(prop, speedup, -cpus)\nggplot(cpu_perf, aes(cpus, speedup, color=prop)) + \n  geom_line() +\n  scale_x_continuous(trans='log2') +\n  theme_bw() +\n  labs(title = \"Amdahl's Law\")\nSo, its important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done. With that, let’s do some parallel computing…"
  },
  {
    "objectID": "sections/04-parallel-programming.html#summary",
    "href": "sections/04-parallel-programming.html#summary",
    "title": "4  Pleasingly Parallel Programming",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nIn this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores. Next, we reviewed the way in which traditional for loops in R can be rewritten as functions that are applied to a list serially using lapply, and then how the parallel package mclapply function can be substituted in order to utilize multiple cores on the local computer to speed up computations. Finally, we installed and reviewed the use of the foreach package with the %dopar operator to accomplish a similar parallelization using multiple cores."
  },
  {
    "objectID": "sections/04-parallel-programming.html#further-reading",
    "href": "sections/04-parallel-programming.html#further-reading",
    "title": "4  Pleasingly Parallel Programming",
    "section": "5.2 Further Reading",
    "text": "5.2 Further Reading\nRyan Abernathey & Joe Hamman. 2020. Closed Platforms vs. Open Architectures for Cloud-Native Earth System Analytics. Medium."
  },
  {
    "objectID": "sections/05-adc-data-publishing.html#introduction",
    "href": "sections/05-adc-data-publishing.html#introduction",
    "title": "5  Documenting and Publishing Data",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction"
  },
  {
    "objectID": "sections/10-geopandas.html",
    "href": "sections/10-geopandas.html",
    "title": "6  Spatial and Image Data Using GeoPandas",
    "section": "",
    "text": "Reading raster data with rasterasterio\nUsing geopandas and rasterasterio to process raster data\nWorking with raster and vector data together"
  },
  {
    "objectID": "sections/10-geopandas.html#introduction",
    "href": "sections/10-geopandas.html#introduction",
    "title": "6  Spatial and Image Data Using GeoPandas",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\n\nRaster vs vector data\nWhat is a projection\nProcessing overview\n\ngoal is to calculate vessel distance per commercial fishing area"
  },
  {
    "objectID": "sections/10-geopandas.html#pre-processing-raster-data",
    "href": "sections/10-geopandas.html#pre-processing-raster-data",
    "title": "6  Spatial and Image Data Using GeoPandas",
    "section": "6.3 Pre-processing raster data",
    "text": "6.3 Pre-processing raster data\nThis is a test to make sure we can run some code in this notebook.\nimport geopandas as gpd\nimport rasterio\nimport rasterio.mask\nimport rasterio.warp\nimport rasterio.plot\nfrom rasterio import features\nfrom shapely.geometry import box\nfrom shapely.geometry import Polygon\nimport requests\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport pandas as pd\nimport numpy as np\nDownload the ship traffic raster from Kapsar et al.. We grab a one month slice from December, 2020 of a coastal subset of data with 1km resolution.\n\nurl_sf = 'https://cn.dataone.org/cn/v2/resolve/urn:uuid:dd61089d-f50e-4d87-9b75-6b4e2bd24776'\n\nresponse_sf = requests.get(url_sf)\nopen(\"Coastal_2020_12.tif\", \"wb\").write(response_sf.content)\n\n1132748\n\n\nOpen the raster file, plot it, and look at the metadata.\n\nwith rasterio.open(\"Coastal_2020_12.tif\") as dem_src:\n    ships = dem_src.read(1)\n    ships_meta = dem_src.profile\n\nplt.imshow(ships)\nprint(ships_meta)\n\n{'driver': 'GTiff', 'dtype': 'float32', 'nodata': -3.3999999521443642e+38, 'width': 3087, 'height': 2308, 'count': 1, 'crs': CRS.from_epsg(3338), 'transform': Affine(999.7994153462766, 0.0, -2550153.29233849,\n       0.0, -999.9687691991521, 2711703.104608573), 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n\n\n\n\n\nNow download a vector shapefile of commercial fishing districts in Alaska.\n\nurl = 'https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7c942c45-1539-4d47-b429-205499f0f3e4'\n\nresponse = requests.get(url)\nopen(\"Alaska_Commercial_Salmon_Boundaries.gpkg\", \"wb\").write(response.content)\n\n36544512\n\n\nRead in the data\ncomm = gpd.read_file(\"Alaska_Commercial_Salmon_Boundaries.gpkg\")\nThe raster data is in 3338, so we need to reproject this.\n\ncomm.crs\ncomm_3338 = comm.to_crs(\"EPSG:3338\")\n\ncomm_3338.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nWe can extract the bounding box for the area of interest, and use that to clip the original raster data to just the extent we need. We use the box function from shapely to create the bounding box, then create a geoDataFrame from them and convert the WGS84 coordinates to the Alaska Albers projection.\ntodo: explain the warp transform thing here\ncoords = rasterio.warp.transform_bounds('EPSG:4326',\n                                        'EPSG:3338',\n                                         -159.5,\n                                         55,\n                                         -144.5,\n                                         62)\ncoord_list = list(coords)\n\ncoord_box = box(coord_list[0],coord_list[1], coord_list[2], coord_list[3])\n\nbbox_crop = gpd.GeoDataFrame(\n    crs = 'EPSG:3338',\n    geometry = [coord_box])\nRead in raster again cropped to bounding box.\nwith rasterio.open(\"Coastal_2020_12.tif\") as src:\n    out_image, out_transform = rasterio.mask.mask(src, bbox_crop[\"geometry\"], crop=True)\n    out_meta = src.meta\n\nout_meta.update({\"driver\": \"GTiff\",\n                 \"height\": out_image.shape[1],\n                 \"width\": out_image.shape[2],\n                 \"transform\": out_transform,\n                 \"compress\": \"lzw\"})\n\nwith rasterio.open(\"Coastal_2020_12_masked.tif\", \"w\", **out_meta) as dest:\n    dest.write(out_image)\nWe can also clip the shapefile data to the same bounding box\ncomm_clip = comm_3338.clip(bbox_crop['geometry'])\n\n6.3.1 Check extents\nQuick plot to ensure they are in the same extent, and look as expected.\n\nwith rasterio.open('Coastal_2020_12_masked.tif') as src:\n    r = src.read(1)\n\nr[r == src.nodata] = np.nan\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nrasterio.plot.show(r,\n                   ax=ax,\n                   vmin = 0,\n                   vmax = 6000,\n                   transform = src.transform)\n\ncomm_clip.plot(ax=ax, facecolor='none', edgecolor='red')\n\n<AxesSubplot:>"
  },
  {
    "objectID": "sections/10-geopandas.html#calculate-total-distance-per-fishing-area",
    "href": "sections/10-geopandas.html#calculate-total-distance-per-fishing-area",
    "title": "6  Spatial and Image Data Using GeoPandas",
    "section": "6.4 Calculate total distance per fishing area",
    "text": "6.4 Calculate total distance per fishing area\nRasterize each polygon in the shapefile that falls within the bounds of the raster data we are calculating statistics for.\nWe return a dictionary of indexed arrays, where each item corresponds to one polygon (fishing area). The array contains the indices of the original raster that fall within that fishing area.\nwith rasterio.open('Coastal_2020_12_masked.tif') as src:\n    shape = src.shape\n    transform = src.transform\n    # read in the cropped raster\n    r_array = src.read(1)\n    # turn no data values into actual NaNs\n    r_array[r_array == src.nodata] = np.nan\n\ncomm_3338['id'] = range(0,len(comm_3338))\n\ncrosswalk_dict = {}\nfor geom, idx in zip(comm_3338.geometry, comm_3338['id']):\n    rasterized = features.rasterize(geom,\n                                    out_shape=shape,\n                                    transform=transform,\n                                    all_touched=True,\n                                    fill=0,\n                                    dtype='uint8')\n    # only save polygons that have a non-zero value\n    if any(np.unique(rasterized)) == 1:\n        crosswalk_dict[idx] = np.where(rasterized == 1)\n\n/home/runner/.local/lib/python3.8/site-packages/rasterio/features.py:284: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n  for index, item in enumerate(shapes):\n\n\nNow we use the dictionary to calculate the sum of all of the pixels in the original raster that fall within each fishing area.\nmean_dict = {}\n# for each item in the dictionary\nfor key, value in crosswalk_dict.items():\n    # save the sum of the indices of the raster to a new dictionary\n    mean_dict[key] = np.nansum(r_array[value])\n# create a data frame from the result\ndf = pd.DataFrame.from_dict(mean_dict,\n    orient='index',\n    columns=['distance'])\n# extract the index of the data frame as a column to use in a join\ndf['id'] = df.index\nNow we join the result to the original geodataframe.\n# join the sums to the original data frame\nres_full = comm_3338.merge(df, on = \"id\", how = 'inner')\ntodo: Group by/summarize across another variable\n\nfig, ax = plt.subplots(figsize=(7, 7))\nplt.style.use(\"seaborn-talk\")\nax = res_full.plot(column = \"distance\", legend = True, ax = ax)\nfig = ax.figure\ncb_ax = fig.axes[1]\ncb_ax.set_yticklabels([\"0\", \"2,000\", \"4,000\", \"6,000\", \"8,000\", \"10,000\", \"12,000\", \"14,000\", \"16,000\"])\nax.set_axis_off()\nax.set_title(\"Distance Traveled by Ships in Kilometers\")\nplt.show()\n\n/tmp/ipykernel_51268/3303410610.py:6: UserWarning: FixedFormatter should only be used together with FixedLocator\n  cb_ax.set_yticklabels([\"0\", \"2,000\", \"4,000\", \"6,000\", \"8,000\", \"10,000\", \"12,000\", \"14,000\", \"16,000\"])"
  },
  {
    "objectID": "sections/11-parquet-arrow.html",
    "href": "sections/11-parquet-arrow.html",
    "title": "7  Data Futures: Parquet and Arrow",
    "section": "",
    "text": "The difference between column major and row major data\nSpeed advantages to columnnar data storage\nHow arrow enables faster processing"
  },
  {
    "objectID": "sections/11-parquet-arrow.html#introduction",
    "href": "sections/11-parquet-arrow.html#introduction",
    "title": "7  Data Futures: Parquet and Arrow",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\n\nopen, seek, read, write, close - ways to access data\ndifference between parquet and arrow\n\nhow paging and memory management works, blocks are organized by pages\non disk and in memory representation are the same\n\ncolumn (parquet) vs row (csv) data example\nwhy column can give faster read speeds\nhow arrow interacts with columnar data formats (like parquet)"
  },
  {
    "objectID": "sections/11-parquet-arrow.html#example",
    "href": "sections/11-parquet-arrow.html#example",
    "title": "7  Data Futures: Parquet and Arrow",
    "section": "7.3 Example",
    "text": "7.3 Example\n\nshow a read write example and benchmark maybe"
  },
  {
    "objectID": "sections/18-arctic-data-staging.html",
    "href": "sections/18-arctic-data-staging.html",
    "title": "8  Workflows for data staging and publishing",
    "section": "",
    "text": "NSF archival policies for large datasets\nData transfer tools\nUploading large datasets to the Arctic Data Center"
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#introduction",
    "href": "sections/18-arctic-data-staging.html#introduction",
    "title": "8  Workflows for data staging and publishing",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\n\nthere are many different data collection mechanisms that can generate large volumes of data\n\ntwo major types: imagery and model output\n\n\n\nThe Office of Polar Programs policy requires that metadata files, full data sets, and derived data products, must be deposited in a long-lived and publicly accessible archive.\n\n\nMetadata for all Arctic supported data sets must be submitted to the NSF Arctic Data Center (https://arcticdata.io).\n\n\nExceptions to the above data reporting requirements may be granted for social science and indigenous knowledge data, where privacy or intellectual property rights might take precedence. Such requested exceptions must be documented in the Data Management Plan.\n\n\ndatasets that are already published on a long lived archive do not need to be replicated to the Arctic Data Center\n\nexample: a research project accesses many terabytes of VIIRS satellite data. The original satellite data does not need to be published on the Arctic Data Center, but the code that accessed it, and derived products, can be published\n\nfor some numerical models, if the model results can be faithfully reproduced from code, the code that generates the models can be a sufficient archival product, as opposed to the code and the model output\n\nif the model is difficult to set up, or takes a very long time to run, we would probably reccommend publishing the output as well as code"
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#data-transfer-tools",
    "href": "sections/18-arctic-data-staging.html#data-transfer-tools",
    "title": "8  Workflows for data staging and publishing",
    "section": "8.3 Data transfer tools",
    "text": "8.3 Data transfer tools\n\nscenario: you need to send a bunch of data to the Arctic Data Center. after getting the credentials, you use scp to start the transfer. You know this typically takes around 12 hours so you start it at 5pm right when you leave the office expecting it to be done when you get back. When you arrive, you see there was a short network outage in the middle of the night. The whole job failed so you have to start it again…\n\nThere is a better way!\nThree key elements to data transfer - endpoints - network - transfer tool\n\n8.3.0.1 Endpoints\nThe from and to locations of the transfer, an endpoint is a remote computing device that can communicate back and forth with the network to which it is connected. The speed with which an endpoint can communicate with the network varies depending on how it is configured. Performance depends on the CPU, RAM, OS, and disk configuration. Examples:\n\nNCEAS datateam server:\nStandard laptop\n\n\n\n8.3.0.2 Network speed\nDetermines how quickly information can be sent between endpoints, largely dependent on what you pay for. Wired networks get significantly more speed than wireless.\n\n\n8.3.0.3 Transfer tools\n\nscp\n\nuses ssh for authentication and transfer\nif you can ssh to a server, you can probably use scp to move files without any other setup\ncopies all files linearly and simply. if a transfer fails in the middle, difficult to know exactly what files didn’t make it, so you have to start the whole thing over and re-transfer all the files\n\nrsync\n\nsimilar to scp but syncs files/directories as opposed to copying\nif the file already exists on the other side, it is skipped\n\nglobus\n\nparellelizes transfers by utilizing multiple network sockets simultaneously\nis able to fail and restart itself efficiently\nrequires more setup, endpoints need to be configured as globus nodes\n\n\n\n\n8.3.0.4 Globus\n\neasy to use, as long as your data are accessible via an endpoint configured as a Globus node\nleverage your institutions computing resources! they may be able to help get you access to a data transfer node already configured correctly\nthere are paid options to set up a node from your own workstation (Globus Connect Personal)\n\nremember the other factors though! Globus won’t help you overcome a 1 Gb/s laptop connection speed, or a 50 Mb/s network speed"
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#documenting-large-datasets",
    "href": "sections/18-arctic-data-staging.html#documenting-large-datasets",
    "title": "8  Workflows for data staging and publishing",
    "section": "8.4 Documenting large datasets",
    "text": "8.4 Documenting large datasets\n\nthe Arctic Data Center is working to support large datasets, but we have performance considerations as well\nself documenting file formats are preferred, to prevent us from needing to document thousands-millions of files in a single metadata document\n\nnetcdf\nothers?\n\nregular, parseable filenames and consistent file formatting is key\ncommunicate early and often with the Arctic Data Center staff"
  }
]