[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "",
    "text": "This 5-day in-person workshop will provide researchers with an introduction to advanced topics in computationally reproducible research in python and R, including software and techniques for working with very large datasets. This includes working in cloud computing environments, docker containers, and parallel processing using tools like parsl and dask. The workshop will also cover concrete methods for documenting and uploading data to the Arctic Data Center, advanced approaches to tracking data provenance, responsible research and data management practices including data sovereignty and the CARE principles, and ethical concerns with data-intensive modeling and analysis.\n\n\n\n\n\n\n\n\n\n\nPlease note that by participating in this activity you agree to abide by the NCEAS Code of Conduct.\n\n\n\n\nIn this course, we will be using Python (> 3.0) as our primary language, and VS Code as our IDE. Below are instructions on how to get VS Code set up to work for the course. If you are already a regular Python user, you may already have another IDE set up. We strongly encourage you to set up VS Code with us, because we will use your local VS Code instance to write and execute code on one of the NCEAS servers.\n\n\nFirst, download VS Code if you do not already have it installed.\nYou’ll also need to download the Remote - SSH extension.\n\n\n\nTo connect to the server using VS Code follow these steps, from the VS Code window:\n\nopen the command pallette (Cmd + Shift + P)\nenter “Remote SSH: Connect to Host”\n\n\n\nselect “Add New SSH Host”\nenter the ssh command to connect to the host as if in a terminal (ssh username@included-crab.nceas.ucsb.edu)\n\nNote: you will only need to do this step once\n\n\n\n\nselect the SSH config file to update with the name of the host. You should select the one in your user directory (eg: /Users/jclark/.ssh/config)\nclick “Connect” in the popup in the lower right hand corner\n\nNote: If the dialog box does not appear, reopen the command palette (Cmd + Shift + P), type in “Remote-SSH: Connect to Host…”, choose included-crab.nceas.ucsb.edu from the options of configured SSH hosts, then enter your password into the dialog box that appears\n\nenter your password in the dialog box that pops up\n\nWhen you are connected, you will see in the lower left hand corner of the window a green bar that says “SSH: included-crab.nceas.ucsb.edu.”\n\n\n\n\nAfter connecting to the server, in the extensions pane (View > Extensions) search for, and install, the following extensions:\n- Python\n- Jupyter\n- Jupyter Keymap\n- Pylance\nNote that these extensions will be installed on the server, and not locally.\n\n\n\nWe are going to be working on the server exclusively, but if you are interested in setting up VS Code to work for you locally with Python, you can follow these instructions. This local setup section summarizes the official VS Code tutorial. For more detailed instructions and screenshots, see the source material. If you already use VS Code for Python you can skip this.\nLocally (not connected to the server), check to make sure you have Python installed if you aren’t sure you do. File > New Window will open up a new VS Code window locally.\nTo check your python, from the terminal run:\npython3 --version\nIf you get an error, it means you need to install Python. Here are instructions for getting installed, depending on your operating system. Note: There are many ways to install and manage your Python installations, and advantages and drawbacks to each. If you are unsure about how to proceed, feel free to reach out to the instructor team for guidance.\n\nWindows: Download and run an installer from Python.org.\nMac: Install using homebrew. If you don’t have homebrew installed, follow the instructions from their webpage.\n\nbrew install python3\n\n\nAfter you run your install, make sure you check that the install is on your system PATH by running python3 --version again.\nNext, install the Python extension for VS Code.\nOpen a terminal window in VS Code from the Terminal drop down in the main window. Run the following commands to initialize a project workspace in a directory called training. This example will show you how to do this locally. Later, we will show you how to set it up on the remote server with only one additional step.\nmkdir training\ncd training\ncode .\nNext, select the Python interpreter for the project. Open the Command Palette using Command + Shift + P (Control + Shift + P for windows). The Command Palette is a handy tool in VS Code that allows you to quickly find commands to VS Code, like editor commands, file edit and open commands, settings, etc. In the Command Palette, type “Python: Select Interpreter.” Push return to select the command, and then select the interpreter you want to use (your Python 3.X installation).\nTo make sure you can write and execute code in your project, create a Hello World test file.\n\nFrom the File Explorer toolbar, or using the terminal, create a file called hello.py\nAdd some test code to the file, and save\n\nmsg = \"Hello World\"\nprint(msg)\n\nExecute the script using either the Play button in the upper-right hand side of your window, or by running python3 hello.py in the terminal.\n\nFor more ways to run code in VS Code, see the tutorial\n\n\nFinally, to test Jupyter, download the Jupyter extension. You’ll also need to install ipykernel. From the terminal, run pip install ipykernel.\nYou can create a test Jupyter Notebook document from the command pallete by typing “Create: New Jupyter Notebook” and selecting the command. This will open up a code editor pane with a notebook that you can test.\n\n\n\n\nThese written materials reflect the continuous development of learning materials at the Arctic Data Center and NCEAS to support individuals to understand, adopt, and apply ethical open science practices. In bringing these materials together we recognize that many individuals have contributed to their development. The primary authors are listed alphabetically in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Matthew B. Jones, Bryce Mecum, S. Jeanette Clark, Samantha Csik. 2022. Scalable and Computationally Reproducible Approaches to Arctic Research.\nAdditional contributors: Amber E. Budden, Natasha Haycock-Chavez, Noor Johnson, Stephanie Hampton, Jim Regetz, Bryce Mecum, Julien Brun, Julie Lowndes, Erin McLean, Andrew Barrett, David LeBauer, Jessica Guo.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "sections/01-adc-intro.html",
    "href": "sections/01-adc-intro.html",
    "title": "1  Welcome and Introductions",
    "section": "",
    "text": "This course is one of three that we are currently offering, covering fundamentals of open data sharing, reproducible research, ethical data use and reuse, and scalable computing for reusing large data sets."
  },
  {
    "objectID": "sections/02-remote-computing.html",
    "href": "sections/02-remote-computing.html",
    "title": "2  Remote Computing",
    "section": "",
    "text": "Understand the basic architecture of computer networks\nLearn how to connect to a remote computer via a shell\nBecome familiarized with Bash Shell programming to navigate your computer’s file system, manipulate files and directories, and automate processes"
  },
  {
    "objectID": "sections/02-remote-computing.html#introduction",
    "href": "sections/02-remote-computing.html#introduction",
    "title": "2  Remote Computing",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nScientific synthesis and our ability to effectively and efficiently work with big data depends on the use of computers & the internet. Working on a personal computer may be sufficient for many tasks, but as data get larger and analyses more computationally intensive, scientists often find themselves needing more computing resources than they have available locally. Remote computing, or the process of connecting to a computer(s) in another location via a network link is becoming more and more common in overcoming big data challenges.\nIn this lesson, we’ll learn about the architecture of computer networks and explore some of the different remote computing configurations that you may encounter, we’ll learn how to securely connect to a remote computer via a shell, and we’ll become familiarized with using Bash Shell to efficiently manipulate files and directories. We will begin working in the VS Code IDE (integrated development environment), which is a versatile code editor that supports many different languages."
  },
  {
    "objectID": "sections/02-remote-computing.html#servers-networking",
    "href": "sections/02-remote-computing.html#servers-networking",
    "title": "2  Remote Computing",
    "section": "2.3 Servers & Networking",
    "text": "2.3 Servers & Networking\nHost computers connect via networking equipment and can send messages to each other over communication protocols (aka an Internet Protocol, or IP). Host computers can take the role of client or server, though these are not inherent properties of a host (i.e. the same machine can play either role).\n\nClient: the host intiating a request\nServer: the host responding to a request\n\n\n\n\n\n\n\nNote\n\n\n\nHosts typically have one network address but can have many different ones (for example, adding multiple network cards to a single server increases bandwith).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGET FEEDBACK ON VISUAL + ADD KUBERNETES CONFIGURATION VISUAL; MAKE LARGER? \n\n\nFig 1. Different remote computing configurations. (a) A client uses secure shell protocol (SSH) to login/connect to a server over the internet. The client and the server exist in the physical world, but in different locations. (b) A client uses SSH to login/connect to a computing cluster (i.e. a set of computers (nodes) that work together so that they can be viewed as a single system) over the internet. The connection is first made through a gateway node (i.e. a computer that routes traffic from one network to another). The client and the cluster (server) exist in the physical world, but in different locations. (c) A client uses SSH to login/connect to a computing cluser where each node is a virtual machine (VM) hosted by a cloud computing service (e.g. Amazong Web Services, Google Cloud, Microsoft Azure, etc.). The connection is first made through a gateway node. The client and the gateway are located in the physical world, while the VM nodes are hosted in the cloud."
  },
  {
    "objectID": "sections/02-remote-computing.html#ip-addressing",
    "href": "sections/02-remote-computing.html#ip-addressing",
    "title": "2  Remote Computing",
    "section": "2.4 IP addressing",
    "text": "2.4 IP addressing\nHosts are assigned a unique numerical address used for all communication and routing called an Internet Protocol Address (IP Address). They look something like this: 128.111.220.7. Each IP Address can be used to communicate over various “ports”, which allows multiple applications to communicate with a host without mixing up traffic.\nBecause IP addresses can be difficult to remember, they are also assigned hostnames, which are handled through the global Domain Name System (DNS). Clients first look up a hostname in the DNS to find the IP address, then open a connection to the IP address.\n\n\n\n\n\n\nNote\n\n\n\nThe IP address for included-crab.nceas.ucsb.edu is _______."
  },
  {
    "objectID": "sections/02-remote-computing.html#bash-shell-programming",
    "href": "sections/02-remote-computing.html#bash-shell-programming",
    "title": "2  Remote Computing",
    "section": "2.5 Bash Shell Programming",
    "text": "2.5 Bash Shell Programming\nWhat is a shell? From Wikipedia:\n\n“a computer program which exposes an operating system’s services to a human user or other programs. In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), depending on a computer’s role and particular operation.”\n\nWhat is Bash? Bash, or Bourne-again Shell, is a command line tool (language) commonly used to manipulate files and directories. Accessing and using bash is slightly different depending on what type of machine you work on:\n\nMac: bash via the Terminal, which comes ready-to-use with all Macs\nWindows: bash via Git Bash, which needs to be installed\n\n\n\n\n\n\n\nNote\n\n\n\nMac users may have to switch from Z Shell, or zsh, to bash. Use the command exec bash to switch your default shell to bash (or exec zsh to switch switch back)\n\n\nSome commonly used (and very helpful) bash commands:\n\n\n\n\n\n\n\nbash command\nwhat it does\n\n\n\n\npwd\nprint your current working directory\n\n\ncd\nchange directory\n\n\nls\nlist contents of a directory\n\n\ntree\ndisplay the contents of a directory in the form of a tree structure\n\n\nmv\nmove or rename a file\n\n\ntouch\ncreate a new empty file\n\n\nmkdir\ncreate a new directory\n\n\ngrep\nsearches a given file(s) for lines containing a match to a given pattern list\n\n\nawk\n\n\n\nsed\nstands for Stream Editor; a versatile command for editing files\n\n\ncut\nextract a specific portion of text in a file\n\n\njoin\njoin two files based on a key field present in both\n\n\ntop, htop\nview running processes in a Linux system"
  },
  {
    "objectID": "sections/02-remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "href": "sections/02-remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "title": "2  Remote Computing",
    "section": "2.6 Connecting to a remote computer via a shell",
    "text": "2.6 Connecting to a remote computer via a shell\nYou can also use a shell to gain accesss to and remotely control other computers (manage/transfer files/etc). To do so, you’ll need the following:\n\na remote computer (e.g. server) turned on\nthe IP address or name of remote computer\nthe necessary permissions to access the remote computer\n\nSecure Shell, or SSH, is a network communication protocol that is often used for securely connecting to and running shell commands on a remote host. SSH temendously simplifies remote computing because ______, and it is supported out-of-the-box on Linux and Macs. If working on a Windows machine, you’ll need ____."
  },
  {
    "objectID": "sections/02-remote-computing.html#exercise-1-connect-to-a-server-practice-bash-commands",
    "href": "sections/02-remote-computing.html#exercise-1-connect-to-a-server-practice-bash-commands",
    "title": "2  Remote Computing",
    "section": "2.7 Exercise 1: Connect to a server & practice bash commands",
    "text": "2.7 Exercise 1: Connect to a server & practice bash commands\nLet’s connect to a remote computer (a server named included-crab.nceas.ucsb.edu) and practice using some of above commands.\n\n2.7.1 Launch your Terminal Program\n\nMacOS: navigate to Applications > Utilities and open Terminal\nWindows: Navigate to Windows Start > Git and open Git Bash UPDATE: see if this still stands\nALTERNATIVELY, from VS Code: Two options to open a terminal program, if a terminal isn’t already an open pane at the bottom of VS Code\n\nClick on Terminal > New Terminal in top menu bar\nClick on the + (dropdown menu) > bash in the bottom right corner\n\n\n\n\n2.7.2 Connect to a remote server\nYou can choose to SSH into the server (included-crab.nceas.ucsb.edu) through the command line by using the ssh command, or through VS Code’s command palette. If you prefer the latter, please refer back to the Log in to the server section. Doing so via the command line should look something like this:\n#| eval: false\nsamanthacsik:~$ ssh scsik@included-crab.nceas.ucsb.edu \nscsik@included-crab.nceas.ucsb.edu's password: \nscsik@included-crab:~$ \n\n\n\n\n\n\nImportant\n\n\n\nYou won’t see anything appear as you type your password!\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDO WE NEED THIS SECTION?\n\nChange your password\n\n#| eval: false\nscsik@included-crab:~$ passwd\nChanging password for scsik.\n(current) UNIX password: \nEnter new UNIX password: \nRetype new UNIX password: \n\n\n\n\n2.7.3 Practice\n\nUse the pwd command to print your current location, or working directory. You’ll likely be in your home directory on the server (e.g. /home/yourusername).\nUse the ls command to list the contents (any files or subdirectories) of your home directory – you should see a directory named **___**\nUse the cd command to move the **___** directory:\n\n#| eval: false\n# move from /home/yourusername to home/yourusername/____\ncd ___\nTo move up a directory level, use two dots, ..:\n#| eval: false\n# move from /home/yourusername/___ back to /home/yourusername\n$ cd ..\nTo quickly navigate back to your home directory from wherever you may be on your computer, use a tilde, ~:\n#| eval: false\n# e.g. to move from from some subdirectory, /home/yourusername/Projects/project1/data, back to your home directory, home/yourusername\n$ cd ~\n\n# or use .. to back out three subdirectories\n$ cd ../../..\n\nCreate a new directory called, bash_practice within your home directory using mkdir.\n\n#| eval: false\n$ mkdir bash_practice\nRun ls to print out the contents of your home directory – you should now see your bash_practice directory added to the list.\n\nTO BE CONTINUED"
  },
  {
    "objectID": "sections/02-remote-computing.html#exercise-2-write-a-bash-shell-script",
    "href": "sections/02-remote-computing.html#exercise-2-write-a-bash-shell-script",
    "title": "2  Remote Computing",
    "section": "2.8 Exercise 2: Write a bash shell script",
    "text": "2.8 Exercise 2: Write a bash shell script\nAs we just demonstrated, we can use bash commands in the terminal to accomplish a variety of tasks like navigating our computer’s directories, manipulating/creating/adding files, and much more. However, writing a bash script allows us to gather and save our code for automated execusion.\nEarlier, we created a collection of .txt files and saved them to a new directory called bash_practice. Here, we’ll write a bash script to iterate over all those files and update ___.\nLet’s begin by creating a simple bash script that when executed, will print out the message, “Hello, World!” This simple script will help us determine whether or not things are working as expected before writing some more complex (and interesting) code.\n\nOpen a terminal window and determine where you are by using the pwd command. Navigate to where you’d like to save your bash script (your home directory on the server is fine) by using the cd command.\nNext, we’ll create a shell script called hello_world.sh using the touch command:\n\n#| eval: false\n$ touch hello_world.sh\n\nThere are a number of ways to edit a file or script – here, we’ll use Nano, a terminal-based text editor. Open your hello_world.sh with nano by running the following in your terminal:\n\n#| eval: false\n$ nano hello_world.sh\n\n\n\n\n\n\nTip\n\n\n\nYou can create and open a file in nano in just one line of code. For example, running nano hello_world.sh is the same as creating the file first using touch hello_world.sh, then opening it with nano using nano hello_world.sh\n\n\n\nWe can now start to write our script. Some important considerations:\n\n\nAnything following a # will not be executed as code – these are useful for adding comments to your scripts\nThe first line of a Bash script starts with a shebang, #!, followed by a path to the Bash interpreter – this is used to tell the operating system which interpreter to use to parse the rest of the file. There are two ways to use the shebang to set your interpreter (read up on the pros & cons of both methods on this Stack Overflow post):\n\n#| eval: false\n\n# (option a): use the absolute path to the bash binary\n#!/bin/bash\n\n# (option b): use the env untility to search for the bash executable in the user's $PATH environmental variable\n#!/usr/bin/env bash\n\nWe’ll first specify our bash interpreter using the shebang, which indicates the start of our script. Then, we’ll use the echo command, which when executed, will print whatever text is passed as an argument. Type the following into your script (which should be opened with nano), then save (Use the keyboard shortcut control + X to exit, then type Y when it asks if you’d like to save your work. Press enter/return to exit nano).\n\n#| eval: false\n# specify bash as the interpreter\n#!/bin/bash\n\n# print \"Hello, World!\"\n$ echo \"Hello, World!\"\n\nTo execute your script, then run the following in your terminal (be sure that you’re in the same working directory as your hello_world.sh file or specify the file path to it):\n\n#| eval: false\nbash hello_world.sh\nIf successful, “Hello, World!” should be printed in your terminal window.\n\n\n\nUPDATE: write a simple shell script that does something – e.g. renaming files with bash loop (e.g. change extension, add date, move them around)\nUPDATE: nohup, screen, tmux for starting remote job that you can come back to later; look for tmux lesson in oss training"
  },
  {
    "objectID": "sections/03-python-intro.html",
    "href": "sections/03-python-intro.html",
    "title": "3  Python Programming on Clusters",
    "section": "",
    "text": "Basic Python review\nUsing virtual environments\nWriting in Jupyter notebooks\nWriting functions in Python"
  },
  {
    "objectID": "sections/03-python-intro.html#introduction",
    "href": "sections/03-python-intro.html#introduction",
    "title": "3  Python Programming on Clusters",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nWe’ve chosen to use VS Code in this training, in part, because it has great support for developing on remote machines. Hopefully, your VS Code setup went easily, and you were able to connect to our server included-crab. Once connected, the VS Code interface looks just like you were working locally, and connection to the server is seamless.\nOther aspects of VS Code that we like: it supports all languages thanks to the extensive free extension library, it has built in version control integration, and it is highly flexible/configurable.\nWe will also be working quite a bit in Jupyter notebooks in this course. Notebooks are great ways to interleave rich text (markdown formatted text, equations, images, links) and code in a way that a ‘literate analysis’ is generated. Although Jupyter notebooks are not subsitutes for python scripts, they can be great communication tools, and can also be convenient for code development."
  },
  {
    "objectID": "sections/03-python-intro.html#starting-a-project",
    "href": "sections/03-python-intro.html#starting-a-project",
    "title": "3  Python Programming on Clusters",
    "section": "3.3 Starting a project",
    "text": "3.3 Starting a project\nTo get set up for the course, let’s connect to the server again. If you were able to work through the setup for the lesson without difficulty, follow these steps to connect:\n\nopen the command pallette (Cmd + Shift + P)\nenter “Remote SSH: Connect to Host”\nselect included-crab\nenter your password in the dialog box that pops up\n\nNow we can get set up with a project to work in for the course. Head over to the scalable-computing-examples github repository and fork it to your account.\nBack in VS Code, in the terminal clone your fork of the scalable-computing-examples repo (git clone <url-to-forked-repo-here>) into the top level of your user directory. Run cd ~/ if you are in some other directory.\nTo open the project, open the folder into your workspace\n\nFile > Open Folder\nEnter password again if prompted"
  },
  {
    "objectID": "sections/03-python-intro.html#virtual-environments",
    "href": "sections/03-python-intro.html#virtual-environments",
    "title": "3  Python Programming on Clusters",
    "section": "3.4 Virtual Environments",
    "text": "3.4 Virtual Environments\n\nWhen you install a python library, let’s say pandas, via pip, unless you specify otherwise, pip will go out and grab the most recent version of the library, and install it somewhere on your system path (where, exactly, depends highly on how you install python originally, and other factors). This is all great, untl you realize that as part of a new project, a new library you are starting to work with requires a older version of pandas, what do you do? You need both pandas versions for each of your projects. Virtual environments help to solve this issue without making the all to common situation in the comic above even more complicated.\nA virtual environment is a folder structure which creates a symlink (pointer) to all of the libraries that you need into the folder. The three main components will be: the python distribution itself, its configuration, and a site-packages directory (where your libraries like pandas live). So the folder is a self contained directory of all the version-specific python software you need for your project.\nVirtual environments are very helpful to create reproducible workflows, and we’ll talk more about this concept of reproducible environements later in the course. Perhaps most importantly though, virtual environments also help you maintain your sanity when python programming. Because they are just folders, you can create and delete new ones at will, without worrying about bungling your underlying python setup.\nIn this course, we are going to use virtualenv as our tool to create and manage virtual environments. Other virtual environment tools used commonly are conda and pipenv. One reason we like using virtualenv is there is an extension to it called virtualenvwrapper, which provides easy to remember wrappers around common virtualenv operations that make creating, activating, and deactivating a virtual environment very easy.\nFirst we will create a .bash_profile file to create variables that point to the install locations of python and virtualenvwrapper. .bash_profile is just a text file that contains bash commands that are run every time you start up a new terminal. Although setting up this file is not required to use virtualenvwrapper, it is convenient because it allows you to set up some reasonable defaults to the commands (meaning less typing, overall), and it makes sure that the package is available every time you start a new terminal.\n\n3.4.0.1 Setup\n\nIn VS Code, select ‘File > New Text File’\nPaste this text into the file:\n\nexport VIRTUALENVWRAPPER_VIRTUAWORKON_HOME=$HOME/.virtualenvs\nsource /usr/share/virtualenvwrapper/virtualenvwrapper.sh\nThe first line points virtualenvwrapper to the directory where your virtual environments will be stored. We point it to a hidden directory (.virtualenvs) in your home directory. The last line sources a bash script that ships with virtualenvwrapper, which makes all of virtualenvwrapper commands available in your terminal session.\n\nSave the file in the top of your home directory as .bash_profile.\nRestart your terminal (Terminal > New Terminal)\nCheck to make sure it was installed and configured correctly by running this in the terminal:\n\nmkvirtualenv --version\nIt should return some content that looks like this (with more output, potentially).\nvirtualenv 20.13.0+ds from /usr/lib/python3/dist-packages/virtualenv/__init__.py\n\n\n3.4.0.2 Course environment\nNow we can create the virtual environment we will use for the course. In the terminal run:\nmkvirtualenv -p python3.9 scomp\nHere, we’ve specified explicitly which python version to use by using the -p flag, and the path to the python 3.9 installation on the server. After making a virtual environment, it will automatically be activated. You’ll see the name of the env you are working in on the left side of your terminal prompt in parentheses. To deactivate your environment (like if you want to work on a different project), just run deactivate. To activate it again, run:\nworkon scomp\nYou can get a list of all available environments by just running:\nworkon\nNow let’s install the dependencies for this course into that environment. To install our libraries we’ll use pip. As of Python 3.4, pip is automatically included with your python installation. pip is a package manager for python, and you might have used it already to install common python libraries like pandas or numpy. pip goes out to PyPI, the Python Package Index, to download the code and put it in your site-packages directory. Note that on this shared server, your user directory will ahve a site-packages directory, in addition to one that our systems administrator manages as the root of the system.\npip install -r requirements.txt\n\n\n3.4.0.3 Installing locally (optional)\nvirtualenvwrapper was already installed on the server we are working on. To install on your local computer, run:\npip3 install virtualenvwrapper\nAnd then follow the instructions as described above, making sure that you have the correct paths set when you edit your .bash_profile."
  },
  {
    "objectID": "sections/03-python-intro.html#brief-overview-of-python-syntax",
    "href": "sections/03-python-intro.html#brief-overview-of-python-syntax",
    "title": "3  Python Programming on Clusters",
    "section": "3.5 Brief overview of python syntax",
    "text": "3.5 Brief overview of python syntax\nWe’ll very briefly go over some basic python syntax and the base variable types. First, open a python script. From the File menu, select New File, type “python”, then save it as ‘python-intro.py’ in the top level of your directory.\nIn your file, assign a value to a variable using = and print the result.\n\nx = 4\nprint(x)\n\n4\n\n\nTo run this code in python we can:\n\nexecute python python-intro.py in the terminal\nclick the Play button in the upper right hand corner of the file editor\nright click any line and select: “Run to line in interactive terminal”\n\nIn that interactive window you can then run python code interactively, which is what we’ll use for the next bit of exploring data types.\nThere are 5 standard data types in python\n\nNumber (int, long, float, complex)\nString\nList\nTuple\nDictionary\n\nWe already saw a number type, here is a string:\n\nstr = 'Hello World!'\nprint(str)\n\nHello World!\n\n\nLists in python are very versatile, and are created using square brackets []. Items in a list can be of different data types.\n\nlist = [100, 50, -20, 'text']\nprint(list)\n\n[100, 50, -20, 'text']\n\n\nYou can access items in a list by index using the square brackets. Note indexing starts with 0 in python. The slice operator enables you to easily access a portion of the list without needing to specify every index.\n\nlist[0] # print first element\nlist[1:3] # print 2nd until 4th elements\nlist[:2] # print first until the 3rd\nlist[2:] # print last elements from 3rd\n\n100\n\n\n[50, -20]\n\n\n[100, 50]\n\n\n[-20, 'text']\n\n\nThe + and * operators work on lists by creating a new list using either concatenation (+) or repetition (*).\n\nlist2 = ['more', 'things']\n\nlist + list2\nlist * 3\n\n[100, 50, -20, 'text', 'more', 'things']\n\n\n[100, 50, -20, 'text', 100, 50, -20, 'text', 100, 50, -20, 'text']\n\n\nTuples are similar to lists, except the values cannot be changed in place. They are constructed with parentheses.\n\ntuple = ('a', 'b', 'c', 'd')\ntuple[0]\ntuple * 3\ntuple + tuple\n\n'a'\n\n\n('a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'a', 'b', 'c', 'd')\n\n\n('a', 'b', 'c', 'd', 'a', 'b', 'c', 'd')\n\n\nObserve the difference when we try to change the first value. It works for a list:\n\nlist[0] = 'new value'\nlist\n\n['new value', 50, -20, 'text']\n\n\n…and errors for a tuple.\n\ntuple[0] = 'new value'\n\nTypeError: 'tuple' object does not support item assignment\nDictionaries consist of key-value pairs, and are created using the syntax {key: value}. Keys are usually numbers or strings, and values can be any data type.\n\ndict = {'name': ['Jeanette', 'Matt'],\n        'location': ['Tucson', 'Juneau']}\n\ndict['name']\ndict.keys()\n\n['Jeanette', 'Matt']\n\n\ndict_keys(['name', 'location'])\n\n\nTo determine the type of an object, you can use the type() method.\n\ntype(list)\ntype(tuple)\ntype(dict)\n\nlist\n\n\ntuple\n\n\ndict"
  },
  {
    "objectID": "sections/03-python-intro.html#jupyter-notebooks",
    "href": "sections/03-python-intro.html#jupyter-notebooks",
    "title": "3  Python Programming on Clusters",
    "section": "3.6 Jupyter notebooks",
    "text": "3.6 Jupyter notebooks\nTo create a new notebook, from the file menu select File > New File > Jupyter Notebook\nAt the top of your notebook, add a first level header using a single hash. Practice some markdown text by creating:\n\na list\nbold text\na link\n\nUse the Markdown cheat sheet if needed.\nYou can click the plus button below any chunk to add a chunk of either markdown or python.\n\n3.6.1 Load libraries\nIn your first code chunk, lets load in some modules. We’ll use pandas, numpy, matplotlib.pyplot, requests, skimpy, and exists from os.path.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport requests\nimport skimpy\nimport os\n\nA note on style: There are a few ways to construct import statements. The above code uses three of the most common:\nimport module\nimport module as m\nfrom module import function\nThe first way of importing will make the module a function comes from more explicitly clear, and is the simplest. However for very long module names, or ones that are used very frequently (like pandas, numpy, and matplotlib.plot), the code in the notebook will be more cluttered with constant calls to longer module names. So module.function() instead is written as m.function()\nThe second way of importing a module is a good style to use in cases where modules are used frequently, or have extremely long names. If you import every single module with a short name, however, you might have a hard time remembering which modules are named what, and it might be more confusing for others trying to read your code. Many of the most commonly used libraries for python data science have community-driven styling for how they are abbreviated in import statements, and these community norms are generally best followed.\nFinally, the last way to import a single object from a module can be helpful if you only need that one piece from a larger module, but again, like the first case, results in less explicit code and therefore runs the risk of your or someone else misremembering the usage and source.\n\n\n3.6.2 Read in a csv\nCreate a new code chunk that will download the csv that we are going to use for this tutorial.\n\nNavigate to Rohi Muthyala, Åsa Rennermalm, Sasha Leidman, Matthew Cooper, Sarah Cooley, et al. 2022. 62 days of Supraglacial streamflow from June-August, 2016 over southwest Greenland. Arctic Data Center. doi:10.18739/A2XW47X5F.\nRight click the download button for ‘Discharge_timeseries.csv’\nClick ‘copy link address’\n\nCreate a variable called URL and assign it the link copied to your clipboard. Then use requests.get to download the file, and open to write it to disk, to a directory called data/. We’ll write this bundled in an if statement so that we only download the file if it doesn’t yet exist. First, we create the directory if it doesn’t exist:\n\nif not os.path.exists ('data/'):\n        os.mkdir('data/')\n\n\nif not os.path.exists('data/discharge_timeseries.csv'):\n\n        url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3Ae248467d-e1f9-4a32-9e38-a9b4fb17cefb'\n\n        data = requests.get(url)\n        a = open('data/discharge_timeseries.csv', 'wb').write(data.content)\n\nNow we can read in the data from the file.\n\ndf = pd.read_csv('data/discharge_timeseries.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Total Pressure [m]\n      Air Pressure [m]\n      Stage [m]\n      Discharge [m3/s]\n      temperature [degrees C]\n    \n  \n  \n    \n      0\n      6/13/2016 0:00\n      9.816\n      9.609775\n      0.206225\n      0.083531\n      -0.1\n    \n    \n      1\n      6/13/2016 0:05\n      9.810\n      9.609715\n      0.200285\n      0.077785\n      -0.1\n    \n    \n      2\n      6/13/2016 0:10\n      9.804\n      9.609656\n      0.194344\n      0.072278\n      -0.1\n    \n    \n      3\n      6/13/2016 0:15\n      9.800\n      9.609596\n      0.190404\n      0.068756\n      -0.1\n    \n    \n      4\n      6/13/2016 0:20\n      9.793\n      9.609537\n      0.183463\n      0.062804\n      -0.1\n    \n  \n\n\n\n\nThe column names are a bit messy so we can use clean_columns from skimpy to make them cleaner for programming very quickly. We can also use the skim function to get a quick summary of the data.\n\n\nclean_df = skimpy.clean_columns(df)\nskimpy.skim(clean_df)\n\n6 column names have been cleaned\n\n\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 17856  │ │ float64     │ 5     │                                                          │\n│ │ Number of columns │ 6      │ │ string      │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name              ┃ NA  ┃ NA %    ┃ mean     ┃ sd     ┃ p0        ┃ p25    ┃ p75   ┃ p100  ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━┩  │\n│ │ total_pressure_m         │   0 │       0 │      9.9 │   0.12 │       9.6 │    9.8 │    10 │    10 │ ▁▅█▇▅▂ │  │\n│ │ air_pressure_m           │   0 │       0 │      9.6 │   0.06 │       9.5 │    9.6 │   9.7 │   9.7 │ ▂▅▄▆█▃ │  │\n│ │ stage_m                  │   0 │       0 │     0.28 │   0.12 │   0.00056 │   0.17 │  0.37 │  0.56 │ ▁█▇▇▆▁ │  │\n│ │ discharge_m_3_s          │   0 │       0 │     0.22 │   0.19 │   4.7e-08 │  0.055 │  0.35 │  0.96 │  █▄▃▁  │  │\n│ │ temperature_degrees_     │   8 │   0.045 │   -0.034 │  0.053 │      -0.1 │   -0.1 │     0 │   0.2 │   ▅█   │  │\n│ └──────────────────────────┴─────┴─────────┴──────────┴────────┴───────────┴────────┴───────┴───────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name               ┃ NA      ┃ NA %       ┃ words per row                ┃ total words              ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ date                      │       0 │          0 │                            2 │                    36000 │  │\n│ └───────────────────────────┴─────────┴────────────┴──────────────────────────────┴──────────────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nWe can see that the date column is classed as a string, and not a date, so let’s fix that.\n\n\nclean_df['date'] = pd.to_datetime(clean_df['date'])\nskimpy.skim(clean_df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 17856  │ │ float64     │ 5     │                                                          │\n│ │ Number of columns │ 6      │ │ datetime64  │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name              ┃ NA  ┃ NA %    ┃ mean     ┃ sd     ┃ p0        ┃ p25    ┃ p75   ┃ p100  ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━┩  │\n│ │ total_pressure_m         │   0 │       0 │      9.9 │   0.12 │       9.6 │    9.8 │    10 │    10 │ ▁▅█▇▅▂ │  │\n│ │ air_pressure_m           │   0 │       0 │      9.6 │   0.06 │       9.5 │    9.6 │   9.7 │   9.7 │ ▂▅▄▆█▃ │  │\n│ │ stage_m                  │   0 │       0 │     0.28 │   0.12 │   0.00056 │   0.17 │  0.37 │  0.56 │ ▁█▇▇▆▁ │  │\n│ │ discharge_m_3_s          │   0 │       0 │     0.22 │   0.19 │   4.7e-08 │  0.055 │  0.35 │  0.96 │  █▄▃▁  │  │\n│ │ temperature_degrees_     │   8 │   0.045 │   -0.034 │  0.053 │      -0.1 │   -0.1 │     0 │   0.2 │   ▅█   │  │\n│ └──────────────────────────┴─────┴─────────┴──────────┴────────┴───────────┴────────┴───────┴───────┴────────┘  │\n│                                                    datetime                                                     │\n│ ┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name        ┃ NA    ┃ NA %     ┃ first            ┃ last                           ┃ frequency      ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩  │\n│ │ date               │     0 │        0 │    2016-06-13    │      2016-08-13 23:55:00       │ 5T             │  │\n│ └────────────────────┴───────┴──────────┴──────────────────┴────────────────────────────────┴────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nIf we wanted to calculate the daily mean flow (as opposed to the flow every 5 minutes), we need to:\n\ncreate a new column with only the date\ngroup by that variable\nsummarize over it by taking the mean\n\nFirst we should probably rename our existing date/time column to prevent from getting confused.\n\nclean_df = clean_df.rename(columns = {'date': 'datetime'})\n\nNow create the new date column\n\nclean_df['date'] = clean_df['datetime'].dt.date\n\nFinally, we use group by to split the data into groups according to the date. We can then apply the mean method to calculate the mean value across all of the columns. Note that there are other methods you can use to calculate different statistics across different columns (eg: clean_df.groupby('date').agg({'discharge_m_3_s': 'max'})).\n\ndaily_flow = clean_df.groupby('date', as_index = False).mean()\n\n\ncreate a simple plot\n\n\nvar = 'discharge_m_3_s'\nvar_labs = {'discharge_m_3_s': 'Total Discharge'}\n\nfig, ax = plt.subplots(figsize=(7, 7))\nplt.plot(daily_flow['date'], daily_flow[var])\nplt.xticks(rotation = 45)\nax.set_ylabel(var_labs.get('discharge_m_3_s'))\n\n(array([16967., 16974., 16983., 16990., 16997., 17004., 17014., 17021.,\n        17028.]),\n [Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, '')])\n\n\nText(0, 0.5, 'Total Discharge')"
  },
  {
    "objectID": "sections/03-python-intro.html#functions",
    "href": "sections/03-python-intro.html#functions",
    "title": "3  Python Programming on Clusters",
    "section": "3.7 Functions",
    "text": "3.7 Functions\nThe plot we made above is great, but what if we wanted to make it for each variable? We could copy paste it and replace some things, but this violates a core tenet of programming: Don’t Repeat Yourself! Instead, we’ll create a function called myplot that accepts the data frame and variable as arguments.\n\ncreate myplot.py\n\n\ndef myplot(df, var):\n\n        var_labs = {'discharge_m_3_s': 'Total Discharge (m^3/s)',\n                    'total_pressure_m': 'Total Pressure (m)',\n                    'air_pressure_m': 'Air Pressure (m)',\n                    'stage_m': 'Stage (m)',\n                    'temperature_degrees_c': 'Temperature (C)'}\n\n        fig, ax = plt.subplots(figsize=(7, 7))\n        plt.plot(df['date'], df[var])\n        plt.xticks(rotation = 45)\n        ax.set_ylabel(var_labs.get(var))\n\n\nload myplot into jupyter notebook (from myplot.py import myplot)\nreplace old plot method with new function\n\n\nmyplot(daily_flow, 'temperature_degrees_c')\n\n\n\n\nWe’ll have more on functions in the software design sections."
  },
  {
    "objectID": "sections/04-parallel-programming.html",
    "href": "sections/04-parallel-programming.html",
    "title": "4  Pleasingly Parallel Programming",
    "section": "",
    "text": "Understand what parallel computing is and when it may be useful\nUnderstand how parallelism can work\nReview sequential loops and map functions\nBuild a parallel program using concurrent.futures\nBuild a parallel program using parsl\nUnderstand Thread Pools and Process pools"
  },
  {
    "objectID": "sections/04-parallel-programming.html#introduction",
    "href": "sections/04-parallel-programming.html#introduction",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nProcessing large amounts of data with complex models can be time consuming. New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America:\n\n\n\nLevy et al. 2016. doi:10.5063/F1Z899CZ\n\n\nThere are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization.\nAlternatively, think of remote sensing data. Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years.\n\n\n\nNEON Data Cube"
  },
  {
    "objectID": "sections/04-parallel-programming.html#why-parallelism",
    "href": "sections/04-parallel-programming.html#why-parallelism",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.3 Why parallelism?",
    "text": "4.3 Why parallelism?\nMuch R code runs fast and fine on a single processor. But at times, computations can be:\n\ncpu-bound: Take too much cpu time\nmemory-bound: Take too much memory\nI/O-bound: Take too much time to read/write from disk\nnetwork-bound: Take too much time to transfer\n\nTo help with cpu-bound computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire cluster of those computers. Plus, these machines also have large amounts of memory to avoid memory-bound computing jobs."
  },
  {
    "objectID": "sections/04-parallel-programming.html#processors-cpus-and-cores",
    "href": "sections/04-parallel-programming.html#processors-cpus-and-cores",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.4 Processors (CPUs) and Cores",
    "text": "4.4 Processors (CPUs) and Cores\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with mutliple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations.\nA computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time.\n\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here we show four quad-core processors for a total of 16 cores in this machine.\n\nYou can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later).\nHistorically, R has only utilized one processor, which makes it single-threaded. Which is a shame, because the 2017 MacBook Pro that I am writing this on is much more powerful than that:\n{bash eval=FALSE} jones@powder:~$ sysctl hw.ncpu hw.physicalcpu hw.ncpu: 8 hw.physicalcpu: 4\nTo interpret that output, this machine powder has 4 physical CPUs, each of which has two processing cores, for a total of 8 cores for computation. I’d sure like my R computations to use all of that processing power. Because its all on one machine, we can easily use multicore processing tools to make use of those cores. Now let’s look at the computational server aurora at NCEAS:\n{bash eval=FALSE} jones@included-crab:~$ lscpu | egrep 'CPU\\(s\\)|per core|per socket' CPU(s):                88 On-line CPU(s) list:   0-87 Thread(s) per core:    2 Core(s) per socket:    22 NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86 NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87\nNow that’s more compute power! included-crab has 384 GB of RAM, and ample storage. All still under the control of a single operating system.\nHowever, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now:\n\nJetStream\n\n640 nodes, 15,360 cores, 80TB RAM\n\nStampede2 at TACC is coming online in 2017\n\n4200 nodes, 285,600 cores\n\nTODO: update with modern cluster sizes\n\nNote that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster. One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine. But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster."
  },
  {
    "objectID": "sections/04-parallel-programming.html#modes-of-parallelization",
    "href": "sections/04-parallel-programming.html#modes-of-parallelization",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.5 Modes of parallelization",
    "text": "4.5 Modes of parallelization\n\nTODO: develop diagram(s) showing\n\nSingle memory image task parallelization\n\n\nSerial Launch tasks --> Task 1 --> Task 2 --> Task 3 --> Task 4 --> Task 5 --> Finish\nParallel Launch tasks -->                     Task 1 --\\                    Task 2 ---\\                    Task 3 -----> Finish                      Task 4 ---/                     Task 5 --/\n\nCluster task parallelization\n\nCluster parallel Show dispatch to cluster nodes and reassembly of data   Launch tasks -->                     Marshal --> Task 1 --> Unmarshal --\\                    Marshal --> Task 2 --> Unmarshal ---\\                    Marshal --> Task 3 --> Unmarshal -----> Finish                      Marshal --> Task 4 --> Unmarshal ---/                     Marshal --> Task 5 --> Unmarshal --/\n\nTODO: Should we also include figure with data or functional dependencies?"
  },
  {
    "objectID": "sections/04-parallel-programming.html#task-parallelism-with-concurrent.futures",
    "href": "sections/04-parallel-programming.html#task-parallelism-with-concurrent.futures",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.6 Task parallelism with concurrent.futures",
    "text": "4.6 Task parallelism with concurrent.futures\nWhen you have a list of repetitive tasks, you may be able to speed it up by adding more computing power. If each task is completely independent of the others, then it is a prime candidate for executing those tasks in parallel, each on its own core. For example, let’s build a simple loop that downloads the data files that we need for an analysis. First, we start with the serial implementation.\n\n# Use loop for serial execution of tasks\n\n# Tasks are to download data from a dataset\n\nThe issue with this loop is that we execute each trial sequentially, which means that only one of our 8 processors on this machine are in use. In order to exploit parallelism, we need to be able to dispatch our tasks as functions, with one task going to each processor. To do that, we need to convert our task to a function, and then use the map() function to apply that function to all of the members of a set. Here’s the same code rewritten to use map(), which applies a function to each of the members of a list (in this case the files we want to download):\n\n# Use `map` for serial execution of tasks\n\n# Tasks are to download data from a dataset"
  },
  {
    "objectID": "sections/04-parallel-programming.html#approaches-to-parallelization",
    "href": "sections/04-parallel-programming.html#approaches-to-parallelization",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.7 Approaches to parallelization",
    "text": "4.7 Approaches to parallelization\nWhen parallelizing jobs, one can:\n\nUse the multiple cores on a local computer through mclapply\nUse multiple processors on local (and remote) machines using makeCluster and clusterApply\n\nIn this approach, one has to manually copy data and code to each cluster member using clusterExport\nThis is extra work, but sometimes gaining access to a large cluster is worth it"
  },
  {
    "objectID": "sections/04-parallel-programming.html#concurrent.futures",
    "href": "sections/04-parallel-programming.html#concurrent.futures",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.8 concurrent.futures",
    "text": "4.8 concurrent.futures\n\n# Loop versus map for parallel execution of tasks\n\n# Using concurrent.futures and ThreadPool\n\n# Tasks are to download data from a dataset"
  },
  {
    "objectID": "sections/04-parallel-programming.html#parsl",
    "href": "sections/04-parallel-programming.html#parsl",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.9 parsl",
    "text": "4.9 parsl\n\nOverview of parsl and it’s use of python decorators.\n\n\n# Loop versus map for parallel execution of tasks\n\n# Using parsl decorators and ThreadPool\n\n# Tasks are to download data from a dataset\n\n\nConfigurable Executors in parsl\n\nHightThroughputExecutor for cluster jobs\n\n\n\n# Loop versus map for parallel execution of tasks\n\n# Using parsl decorators and ThreadPool\n\n# Tasks are to download data from a dataset"
  },
  {
    "objectID": "sections/04-parallel-programming.html#when-to-parallelize",
    "href": "sections/04-parallel-programming.html#when-to-parallelize",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.10 When to parallelize",
    "text": "4.10 When to parallelize\nIt’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!\nIn addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced. Some propose that this may follow Amdahl’s Law, where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines):\n#| eval: false\nlibrary(ggplot2)\nlibrary(tidyr)\namdahl <- function(p, s) {\n  return(1 / ( (1-p) + p/s  ))\n}\ndoubles <- 2^(seq(0,16))\ncpu_perf <- cbind(cpus = doubles, p50 = amdahl(.5, doubles))\ncpu_perf <- cbind(cpu_perf, p75 = amdahl(.75, doubles))\ncpu_perf <- cbind(cpu_perf, p85 = amdahl(.85, doubles))\ncpu_perf <- cbind(cpu_perf, p90 = amdahl(.90, doubles))\ncpu_perf <- cbind(cpu_perf, p95 = amdahl(.95, doubles))\n#cpu_perf <- cbind(cpu_perf, p99 = amdahl(.99, doubles))\ncpu_perf <- as.data.frame(cpu_perf)\ncpu_perf <- cpu_perf %>% gather(prop, speedup, -cpus)\nggplot(cpu_perf, aes(cpus, speedup, color=prop)) + \n  geom_line() +\n  scale_x_continuous(trans='log2') +\n  theme_bw() +\n  labs(title = \"Amdahl's Law\")\nSo, its important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done. With that, let’s do some parallel computing…"
  },
  {
    "objectID": "sections/04-parallel-programming.html#summary",
    "href": "sections/04-parallel-programming.html#summary",
    "title": "4  Pleasingly Parallel Programming",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nIn this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores. Next, we reviewed the way in which traditional for loops in R can be rewritten as functions that are applied to a list serially using lapply, and then how the parallel package mclapply function can be substituted in order to utilize multiple cores on the local computer to speed up computations. Finally, we installed and reviewed the use of the foreach package with the %dopar operator to accomplish a similar parallelization using multiple cores."
  },
  {
    "objectID": "sections/04-parallel-programming.html#further-reading",
    "href": "sections/04-parallel-programming.html#further-reading",
    "title": "4  Pleasingly Parallel Programming",
    "section": "5.2 Further Reading",
    "text": "5.2 Further Reading\nRyan Abernathey & Joe Hamman. 2020. Closed Platforms vs. Open Architectures for Cloud-Native Earth System Analytics. Medium."
  },
  {
    "objectID": "sections/05-adc-data-publishing.html#introduction",
    "href": "sections/05-adc-data-publishing.html#introduction",
    "title": "5  Documenting and Publishing Data",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction"
  },
  {
    "objectID": "sections/06-group-project-1.html",
    "href": "sections/06-group-project-1.html",
    "title": "6  Group Project: Staging and Preprocessing",
    "section": "",
    "text": "Get familiarized with the overall group project workflow\nWrite a parsl app that will stage and tile the IWP example data in parallel"
  },
  {
    "objectID": "sections/06-group-project-1.html#introduction",
    "href": "sections/06-group-project-1.html#introduction",
    "title": "6  Group Project: Staging and Preprocessing",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nThe Permafrost Discovery Gateway is an online platform for archiving, processing, analysis, and visualization of permafrost big imagery products to enable discovery and knowledge-generation. The PDG utilizes and makes available products derived from high resolution satellite imagrey from the Polar Geospatial Center, Planet (3m), Sentinel (10 m), Landsat (30 m), and MODIS (250 m). One of these products is a dataset showing Ice Wedge Polygons (IWP) that form in melting permafrost.\nIce wedges form as a result of thermal contraction during melt/freeze cycles of permafrost. They can form very distinctive geometries clearly visible in satellite images. The PDG is using advanced analysis and computational tools to take high resolution satellite imagery and automatically dectect where ice wedge polygons form. Below is an example of a satellite image (left) and the detected ice wedge polygons in geospatial vector format (right) of that same image.\n\nIn the group project, we are going to use a subset of the high resolution dataset of these detected ice wedge polygons in order to learn some of the reproducible, scalable techniques that will allow us to process it. Our workflow will start with a set of large geopackage files that contain the detected ice wedge polygons. These files all have irregular extents due to the variation in satellite coverage, clouds, etc. Our first processing step will take these files and “tile” them into smaller files which have regular extents.\n\nIn step two of the workflow, we will take those regularly tiled geopackage files and rasterize them. The files will be regularly gridded, and a summary statistic will be calculated for each grid cell (such as the proportion of pixel area covered by polygons).\n\nIn the final step of the workflow, we will take the raster files and resample them to create a set of raster tiles at different resolutions. This last step is what will enable us to visualize our raster data dynamically, such that we look at lower resolutions when very zoomed out (and high resolution data would take too long to load), and higher resolution data when zoomed in and the extent is smaller."
  },
  {
    "objectID": "sections/06-group-project-1.html#staging-and-tiling",
    "href": "sections/06-group-project-1.html#staging-and-tiling",
    "title": "6  Group Project: Staging and Preprocessing",
    "section": "6.3 Staging and Tiling",
    "text": "6.3 Staging and Tiling\nToday we will undertake the first step of the workflow, staging and tiling the data.\n\nIn your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-06.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills."
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html",
    "href": "sections/08-data-structures-netcdf.html",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "",
    "text": "Look into parallel access and NetCDF and whether all wheels can take advantage of the built-in parallel access. Looking at NetCDF’s Parallel I/O page you can see the low-level libraries support it.\nFind dataset(s) to use in the xarray hands on.\n\nJeanette says there on on the Arctic Data Center we store in /var/data. Ask her for more info.\nCandiate: https://www.ncei.noaa.gov/products/climate-data-records/snow-cover-extent (https://arcticdata.io/catalog/view/doi%3A10.7289%2FV5N014G9)\nCandiate: https://arcticdata.io/catalog/view/doi%3A10.18739%2FA24B2X59C “Understory micrometorology across a larch forest density gradient in northeastern Siberia 2014-2020” ~50MB, has a few different axes to filter on\nAlso check out candidats in the Dask lesson TODO section"
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#learning-objectives",
    "href": "sections/08-data-structures-netcdf.html#learning-objectives",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.2 Learning Objectives",
    "text": "8.2 Learning Objectives\n\nLearn about the NetCDF data format\nUnderstand how formats such as NetCDF differ from formats like CSV in terms of random/arbitrary access and how that applies to parallel computing\nLearn how to use the xarray package to work with N-Dimensional datasets in NetCDF files"
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#introduction",
    "href": "sections/08-data-structures-netcdf.html#introduction",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.3 Introduction",
    "text": "8.3 Introduction\nTODO\n\n[Insert amazing image representing “large scale” and “multidimensional”]\nTalk about how the choice of our data formats is at least as important as how we parallelize our code\nMaybe talk about how it’s very common to convert our data from a less efficient format (e.g., CSV) into a more efficient one (NetCDF, Parquet) before we parallelize"
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#working-with-large-data",
    "href": "sections/08-data-structures-netcdf.html#working-with-large-data",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.4 Working with Large Data",
    "text": "8.4 Working with Large Data\nTODO\n\nSize & Dimension, “N-D”: Talk about common dimensions (ie space-time, where space is xy, or xyz). This is a great time to ask students about their N-Dimensional experience\nData Organization / File Naming?\n\nUsing hierarchical folder structures\nEncoding hierarchy into filenames\nFile naming for natural ordering (principle of most sig. fist, ie YMD vs DMY)\nSetting things up for multi-file support in tools like Dask, XArray, Arrow"
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#netcdfhdf-overview",
    "href": "sections/08-data-structures-netcdf.html#netcdfhdf-overview",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.5 NetCDF/HDF Overview",
    "text": "8.5 NetCDF/HDF Overview\nTODO\nOverview major functionality of NetCDF. Students who aren’t already familiar with NetCDF should come away feeling more able to engage with it and maybe even excited about it.\n\nNetCDF is for Multidimensional / N-Dimensional data: Storing multidimensional data in common taular formats such as CSV can get cumbersome, though there are ways to do it by adding columns full of redundant data or splitting data into multiple files. But once you get into 4+ dimensions (e.g., x, y, z, time) formats like NetCDF start looking a lot better.\nNetCDF is “self-describing”, meaning we can encode metadata about every variable such as its units, definitions, and lots of other information\nCover the NetCDF data model so students have the lingo\nRandom access: While CSVs are ubiquitous and easy to work with, they can get really slow to read into our scripts and programs and can take up a lot of RAM because we have to read and parse the entire CSV into RAM before we can do anything with it. With NetCDF, on the other hand, we don’t need to read the entire file into RAM before we query it. Instead of reading the entire dataset into RAM, we only need to read the first part of a NetCDF to get enough information about the data in the rest of the file in order to query it. We call this random or arbitrary access meaning the file describes itself well enough for us to know where the subset of the file we care about is within the whole file. Random access file formats such as NetCDF are fantastic in scalable computing contexts because our parallel workers don’t need to read our data files completely into RAM before running our queries or other transformation.\nRemote access: See https://rabernat.github.io/research_computing_2018/xarray-tips-and-tricks.html. Because NetCDF supports random access (we don’t have to read the entire file to know where the subset of the dataa we want is), servers can host more NetCDF files on disk than they could store in RAM, throw something like a THREDDS server in front of them, and we can just query the files without making the server read each file into memory an the server can return just the data we asked for.\n\nOther topics:\n\nDescribe available tooling (python packages, Panoply, others?). This doesn’t necessarily need to be hands on:\n\nCommand line\nPython packages: xarray\nGUI applications: Panoply, others?\nSome students might have experience with this, what do they use and like/hate?\n\n? Talk about CF conventions (climate forecast conventions for metadata). This could be cut for time.\nTalk about NetCDF and its role in data archival\n\nIs it a good archival format: Yes! Better than CSV in many (not all) ways. The format is open and well-documented, support for the reading the format is ubiquitous, it’s efficient w/ disk space (compared to CSV), it supports remote querying (unlike CSV)."
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#introduction-to-xarray",
    "href": "sections/08-data-structures-netcdf.html#introduction-to-xarray",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.6 Introduction to Xarray",
    "text": "8.6 Introduction to Xarray\n[Time estimate ~20-30min, link to the dataset and work through dataset with students bit-by-bit]\nTODO: Base on https://docs.xarray.dev/en/stable/getting-started-guide/quick-overview.html but with a course-appropriate dataset.\nCourse appropriate datasets:\n\nProbably a space<->time one, maybe one with x, y, z and time\nJeanette says there’s a nice one One in /var/data on ADC, may have to ask her for more information\n\nThe focus here is for students to learn how to open up a NetCDF file, get info on it, read in the data and do some basic map-reduce type operations using xarray. Being able to write out a NetCDF file might be outside the scope here."
  },
  {
    "objectID": "sections/08-data-structures-netcdf.html#exercise",
    "href": "sections/08-data-structures-netcdf.html#exercise",
    "title": "8  Data Structures and Formats for Large Data",
    "section": "8.7 Exercise",
    "text": "8.7 Exercise\n(30-45min)\nTODO: Students work on their own or in pairs to write a script to analyze a NetCDF dataset"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html",
    "href": "sections/09-parallel-with-dask.html",
    "title": "9  Parallelization with Dask",
    "section": "",
    "text": "Find dataset(s) to use:\n\nhttps://arcticdata.io/catalog/view/doi%3A10.18739%2FA24B2X59C “Understory micrometorology across a larch forest density gradient in northeastern Siberia 2014-2020” ~50MB, has a few different axes to filter on\nhttps://arcticdata.io/catalog/view/doi%3A10.18739%2FA28W38388 “River and lake ice phenology data for Alaska and Northwest Canada from 1882 to 2021”\nAlso check out candidats in the xarray lesson TODO section"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#learning-objectives",
    "href": "sections/09-parallel-with-dask.html#learning-objectives",
    "title": "9  Parallelization with Dask",
    "section": "9.2 Learning Objectives",
    "text": "9.2 Learning Objectives\n\nLearn about the map-reduce\nLearn how to use Dask"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#introduction",
    "href": "sections/09-parallel-with-dask.html#introduction",
    "title": "9  Parallelization with Dask",
    "section": "9.3 Introduction",
    "text": "9.3 Introduction\nTODO\n\n9.3.1 Notes\n\nhttps://www.dask.org/\nhttps://docs.xarray.dev/en/stable/user-guide/dask.html#dask\nhttps://stephanhoyer.com/2015/06/11/xray-dask-out-of-core-labeled-arrays/\nhttps://examples.dask.org/xarray.html\nGood example to base exercise on: https://examples.dask.org/applications/image-processing.html\nSplit-apply-combine\nDask stuff\n\nLazy eval (compute())\n\nGrouping compute() calls versus calling compute() multiple times\n\nDask Array\nDask DataFrame\nSkip or just mention Bag, Delayed, Futures? Not sure yet.\nvisualize()\nChoosing how many chunks to divide work into\nTask overhead\nDistributed dask?\n\nPersist > Dask is convenient on a laptop. It installs trivially with conda or pip and extends the size of convenient datasets from “fits in memory” to “fits on disk”.\n\n\n\n– From https://docs.dask.org/en/stable/"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#dask-tutorial",
    "href": "sections/09-parallel-with-dask.html#dask-tutorial",
    "title": "9  Parallelization with Dask",
    "section": "9.4 Dask Tutorial",
    "text": "9.4 Dask Tutorial\nTODO ## Exercise\nTODO (50min)"
  },
  {
    "objectID": "sections/09-parallel-with-dask.html#conclusion",
    "href": "sections/09-parallel-with-dask.html#conclusion",
    "title": "9  Parallelization with Dask",
    "section": "9.5 Conclusion",
    "text": "9.5 Conclusion\n\nComparison with other libraries Thread pools, process pools, distributed dask clusters"
  },
  {
    "objectID": "sections/10-geopandas.html",
    "href": "sections/10-geopandas.html",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "",
    "text": "Reading raster data with rasterio\nUsing geopandas and rasterio to process raster data\nWorking with raster and vector data together"
  },
  {
    "objectID": "sections/10-geopandas.html#introduction",
    "href": "sections/10-geopandas.html#introduction",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "10.2 Introduction",
    "text": "10.2 Introduction\nIn this lesson, we’ll be working with geospatial raster and vector data to do an analysis on vessel traffic in south central Alaska. If you aren’t already familiar, geospatial vector data consists of points, lines, and/or polygons, which represent locations on the Earth. Geospatial vector data can have differing geometries, depending on what it is representing (eg: points for cities, lines for rivers, polygons for states.) Raster data uses a set of regularly gridded cells (or pixels) to represent geographic features.\nBoth geospatial vector and raster data have a coordinate reference system, which describes how the points in the dataset relate to the 3-dimensional sphereoid of Earth. A coordinate reference system contains both a datum and a projection. The datum is how you georeference your points (in 3 dimensions!) onto a spheroid. The projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. All coordinate reference systems require a datum. However, some coordinate reference systems are “unprojected” (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984.\nCoordinate reference systems are often referenced using a shorthand 4 digit code called an EPSG code. We’ll be working with two coordinate referece systems in this lesson with the following codes:\n\n3338: Alaska Albers\n4326: WGS84 (World Geodetic System 1984), used in GPS\n\nIn this lesson, we are going to take two datasets:\n\nAlaskan commercial salmon fishing statisical areas\nNorth Pacific and Arctic Marine Vessel Traffic Dataset\n\nand use them to calculate the total distance travelled by ships within each fishing area.\nThe high level steps will be\n\nread in the datasets\nreproject them so they are in the same projection\nextract a subset of the raster and vector data using a bounding box\nturn each polygon in the vector data into a raster mask\nuse the masks to calculate the total distance travelled (sum of pixels) for each fishing area"
  },
  {
    "objectID": "sections/10-geopandas.html#pre-processing-raster-data",
    "href": "sections/10-geopandas.html#pre-processing-raster-data",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "10.3 Pre-processing raster data",
    "text": "10.3 Pre-processing raster data\nFirst we need to load in our libraries. We’ll use geopandas for vector manipulation, rasterio for raster maniupulation, and shapely for manipulating geospatial data generally.\n\nimport geopandas as gpd\n\nimport rasterio\nimport rasterio.mask\nimport rasterio.warp\nimport rasterio.plot\nfrom rasterio import features\n\nfrom shapely.geometry import box\nfrom shapely.geometry import Polygon\n\nimport requests\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\n\nimport pandas as pd\nimport numpy as np\n\nFirst, we’ll use requests to download the ship traffic raster from Kapsar et al.. We grab a one month slice from August, 2020 of a coastal subset of data with 1km resolution. To get the URL in the code chunk below, you can right click the download button for the file of interest and select “copy link address.”\n\nurl_sf = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A2cc77a90-080b-4696-b5bc-3232523d5e3e'\n\nresponse_sf = requests.get(url_sf)\nopen(\"Coastal_2020_08.tif\", \"wb\").write(response_sf.content)\n\n1422727\n\n\nOpen the raster file, plot it, and look at the metadata. We use the with here as a context manager. This ensures that the connection to the raster file is closed and cleaned up when we are done with it.\n\nwith rasterio.open(\"Coastal_2020_08.tif\") as dem_src:\n    ships = dem_src.read(1)\n    ships_meta = dem_src.profile\n\nplt.imshow(ships)\nprint(ships_meta)\n\n{'driver': 'GTiff', 'dtype': 'float32', 'nodata': -3.3999999521443642e+38, 'width': 3087, 'height': 2308, 'count': 1, 'crs': CRS.from_epsg(3338), 'transform': Affine(999.7994153462766, 0.0, -2550153.29233849,\n       0.0, -999.9687691991521, 2711703.104608573), 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n\n\n\n\n\nNow download a vector shapefile of commercial fishing districts in Alaska.\n\nurl = 'https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7c942c45-1539-4d47-b429-205499f0f3e4'\n\nresponse = requests.get(url)\nopen(\"Alaska_Commercial_Salmon_Boundaries.gpkg\", \"wb\").write(response.content)\n\n36544512\n\n\nRead in the data\n\ncomm = gpd.read_file(\"Alaska_Commercial_Salmon_Boundaries.gpkg\")\n\ncomm.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nThe raster data is in 3338, so we need to reproject this. We use the to_crs method on the comm object to transform it from 4326 (WGS 84) to 3338 (Alaska Albers).\n\ncomm.crs\ncomm_3338 = comm.to_crs(\"EPSG:3338\")\n\ncomm_3338.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nWe can create a bounding box for the area of interest, and use that to clip the original raster data to just the extent we need. We use the box function from shapely to create the bounding box, then create a geoDataFrame from the points, and convert the WGS 84 coordinates to the Alaska Albers projection.\n\ncoord_box = box(-159.5, 55, -144.5, 62)\n\nbbox_crop = gpd.GeoDataFrame(\n    crs = 'EPSG:4326',\n    geometry = [coord_box]).to_crs(\"EPSG:3338\")\n\nNow, we can read in raster again cropped to bounding box. We use the mask function from rasterio.mask. Note that we apply this to the connection to the raster file (with rasterio.open(...)), update the metadata associated with the raster, and then write it back out again.\n\nwith rasterio.open(\"Coastal_2020_08.tif\") as src:\n    out_image, out_transform = rasterio.mask.mask(src, bbox_crop[\"geometry\"], crop=True)\n    out_meta = src.meta\n\nout_meta.update({\"driver\": \"GTiff\",\n                 \"height\": out_image.shape[1],\n                 \"width\": out_image.shape[2],\n                 \"transform\": out_transform,\n                 \"compress\": \"lzw\"})\n\nwith rasterio.open(\"Coastal_2020_08_masked.tif\", \"w\", **out_meta) as dest:\n    dest.write(out_image)\n\nWe can also clip the shapefile data to the same bounding box using the clip method, passing it as an argument the geometry column of our bbox_crop geo data frame.\n\ncomm_clip = comm_3338.clip(bbox_crop['geometry'])\n\n\n10.3.1 Check extents\nLet’s read in the clipped raster data, and make a quick plot to ensure they are in the same extent, and look as expected.\nFirst we read in the cropped data again, since we’ll need it later. We also save the shape and transform attributes of the raster into their own attributes.\n\nwith rasterio.open('Coastal_2020_08_masked.tif') as src:\n    shape = src.shape\n    transform = src.transform\n    # read in the cropped raster\n    ship_arr = src.read(1)\n    # turn no data values into actual NaNs\n    ship_arr[ship_arr == src.nodata] = np.nan\n\n\n# set up plot\nfig, ax = plt.subplots(figsize=(15, 15))\n# plot the raster\nrasterio.plot.show(ship_arr,\n                   ax=ax,\n                   vmin = 0,\n                   vmax = 6000,\n                   transform = transform)\n# plot the vector\ncomm_clip.plot(ax=ax, facecolor='none', edgecolor='red')\n\n<AxesSubplot:>"
  },
  {
    "objectID": "sections/10-geopandas.html#calculate-total-distance-per-fishing-area",
    "href": "sections/10-geopandas.html#calculate-total-distance-per-fishing-area",
    "title": "10  Spatial and Image Data Using GeoPandas",
    "section": "10.4 Calculate total distance per fishing area",
    "text": "10.4 Calculate total distance per fishing area\nIn this step, we rasterize each polygon in the shapefile, such that pixels in or touching the polygon get a value of 1, and pixels not touching it get a value of 0. Then, for each polygon, we extract the indices of the raster array that are equal to 1. We then extract the values of these indicies from the original ship traffic raster data, and calculate the sum of the values over all of those pixels.\nHere is a simplified diagram of the process:\n\n\n10.4.0.1 Zonal statistics over one polygon\nLet’s look at how this works over just one fishing area first. We use the rasterize method from the features module in rasterio. This takes as arguments the data to rasterize (in this case the 40th row of our dataset), the shape and transform the output raster will take (these were extracted from our raster data when we read it in). We alo set the all_touched argument to true, which means any pixel that touches a boundary of our vector will be burned into the mask.\n\nr40 = features.rasterize(comm_3338['geometry'][40].geoms,\n                                    out_shape=shape,\n                                    transform=transform,\n                                    all_touched=True)\n\nIf we have a look at a plot of our rasterized version of the single fishing district, we can see that instead of a vector, we now have a raster with the shape of the district.\n\n# set up plot\nfig, ax = plt.subplots(figsize=(15, 15))\n# plot the raster\nrasterio.plot.show(r40,\n                   ax=ax,\n                   vmin = 0,\n                   vmax = 1,\n                   transform = transform)\n# plot the vector\ncomm_clip.plot(ax=ax, facecolor='none', edgecolor='red')\n\n<AxesSubplot:>\n\n\n\n\n\nA quick call to np.unique shows our unique values are 0 or 1, which is what we expect.\n\nnp.unique(r40)\n\narray([0, 1], dtype=uint8)\n\n\nFinally, we need to know is the indices of the original raster where the fishing district is. We can use np.where to extract this information\n\nr40_index = np.where(r40 == 1)\nprint(r40_index)\n\n(array([108, 108, 108, 108, 108, 109, 109, 109, 109, 109, 109, 109, 109,\n       109, 109, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110,\n       110, 110, 110, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111,\n       111, 111, 111, 111, 112, 112, 112, 112, 112, 112, 112, 112, 112,\n       112, 112, 112, 112, 112, 112, 113, 113, 113, 113, 113, 113, 113,\n       113, 113, 113, 113, 113, 113, 113, 113, 114, 114, 114, 114, 114,\n       114, 114, 114, 114, 114, 114, 115, 115, 115, 115, 115, 115, 116]), array([759, 760, 762, 763, 764, 755, 756, 757, 758, 759, 761, 762, 763,\n       764, 765, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763,\n       764, 765, 766, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762,\n       763, 764, 765, 766, 753, 754, 755, 756, 757, 758, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 753, 754, 755, 756, 757, 758, 759,\n       760, 761, 762, 763, 764, 765, 766, 767, 753, 754, 755, 756, 757,\n       758, 759, 760, 761, 762, 763, 753, 754, 755, 756, 757, 758, 754]))\n\n\nIn the last step, we’ll using these indices to extract the values of the data from the fishing raster, and sum them to get a total distance travelled.\n\nnp.nansum(ship_arr[r40_index])\n\n8387952.0\n\n\nNow that we know the individual steps, let’s run this over all of the districts. First we’ll create an id column in the vector data frame. This will help us track unique fishing districts later.\n\ncomm_3338['id'] = range(0,len(comm_3338))\n\nFor each district (with geometry and id), we run the features.rasterize function. If any values equal 1 (some of the districts are outside the bounds of the raster), we calculate the sum of the values of the shipping raster r_array based on the indicies in the raster where the district is located.\n\ndistance_dict = {}\nfor geom, idx in zip(comm_3338['geometry'], comm_3338['id']):\n    rasterized = features.rasterize(geom.geoms,\n                                    out_shape=shape,\n                                    transform=transform,\n                                    all_touched=True)\n    # only save polygons that have a non-zero value\n    if any(np.unique(rasterized)) == 1:\n        r_index = np.where(rasterized == 1)\n        distance_dict[idx] = np.nansum(ship_arr[r_index])\n\nNow we just create a data frame from that dictionary, and join it to the vector data using pandas operations.\n\n# create a data frame from the result\ndf = pd.DataFrame.from_dict(distance_dict,\n    orient='index',\n    columns=['distance'])\n\n# extract the index of the data frame as a column to use in a join\ndf['id'] = df.index\n\nNow we join the result to the original geodataframe.\n\n# join the sums to the original data frame\nres_full = comm_3338.merge(df, on = \"id\", how = 'inner')\n\nFinally, we can plot our result!\n\nfig, ax = plt.subplots(figsize=(7, 7))\nplt.style.use(\"seaborn-talk\")\nax = res_full.plot(column = \"distance\", legend = True, ax = ax)\nfig = ax.figure\ncb_ax = fig.axes[1]\ncb_ax.set_yticklabels([\"0\", \"2,000\", \"4,000\", \"6,000\", \"8,000\", \"10,000\", \"12,000\", \"14,000\", \"16,000\"])\nax.set_axis_off()\nax.set_title(\"Distance Traveled by Ships in Kilometers\")\nplt.show()\n\n/tmp/ipykernel_52750/3303410610.py:6: UserWarning: FixedFormatter should only be used together with FixedLocator\n  cb_ax.set_yticklabels([\"0\", \"2,000\", \"4,000\", \"6,000\", \"8,000\", \"10,000\", \"12,000\", \"14,000\", \"16,000\"])"
  },
  {
    "objectID": "sections/11-parquet-arrow.html",
    "href": "sections/11-parquet-arrow.html",
    "title": "11  Data Futures: Parquet and Arrow",
    "section": "",
    "text": "The difference between column major and row major data\nSpeed advantages to columnnar data storage\nHow arrow enables faster processing"
  },
  {
    "objectID": "sections/11-parquet-arrow.html#introduction",
    "href": "sections/11-parquet-arrow.html#introduction",
    "title": "11  Data Futures: Parquet and Arrow",
    "section": "11.2 Introduction",
    "text": "11.2 Introduction\nSystem calls are calls that are run by the operating system within their own process. There are several that are relevant to reading and writing data: open, read, write, seek, and close. Open establishes a connection with a file for reading, writing, or both. On open, a file offset points to the beginning of the file. After reading or writing n bytes, the offset will move n bytes forward to prepare for the next opration. Close closes the connection to the file. Read will read data from the file into a memory buffer, and write will write data from a memory buffer to a file. Seek is used to change the location of the offset pointer, for either reading or writing purposes.\nIf you’ve worked with even moderately sized datasets, you may have encounted an “out of memory” error. Memory is where a computer stores the information needed immediately for processes. This is in contrast to storage, which is typically slower to access than memory, but has a much larger capacity. When you open a file, you are establishing a connection between your processor and the information in storage. On read, the data is read into memory that is then available to your python process, for example.\nSo what happens if the data you need to read in are larger than your memory? 32GB is a common memory size, but this would be considered a modestly sized dataset by this courses’s standards. There are a number of solutions to this problem, which don’t involve just buying a computer with more memory. In this lesson we’ll discuss the difference between row major and column major file formats, and how leveraging column major formats can increase memory efficiency. We’ll also learn about another python library called pyarrow, which has a memory format that allows for “zero copy” read."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#row-major-vs-column-major",
    "href": "sections/11-parquet-arrow.html#row-major-vs-column-major",
    "title": "11  Data Futures: Parquet and Arrow",
    "section": "11.3 Row major vs column major",
    "text": "11.3 Row major vs column major\nThe difference between row major and column major is in the ordering of items in the array.\nTake the array:\na11 a12 a13\n\na21 a22 a23\nThis array in a row-major order would be read in as:\na11, a12, a13, a21, a22, a23\nYou could also read it in column-major order as:\na11, a21, a12, a22, a13, a33\nBy default, C and SAS use row major order for arrays, and column major is used by Fortran, MATLAB, R, and Julia.\nPython uses neither, instead representing arrays as lists of lists, though numpy uses row-major order.\n\n11.3.1 Row major versus column major files\nThe same concept can be applied to file formats as the example with arrays above. In row-major file formats, the values (bytes) of each record are read sequentially.\n\n\n\nName\nLocation\nAge\n\n\n\n\nJohn\nWashington\n40\n\n\nMariah\nTexas\n21\n\n\nAllison\nOregon\n57\n\n\n\nIn the above row major example, data are read in the order: John, Washingon, 40, [new line], Mariah, Texas, 21.\nThis means that getting a subset of all columns would be easy; you can specify to read in only the first X rows. However, if we are only interested in Name and Location, we would still have to read in all of the rows before discarding the Age column.\nIf these data were organized in a column major format, they might look like this:\nName: John, Mariah, Allison\nLocation: Washington, Texas, Oregon\nAge: 40, 21, 57\nAnd the read order would first be the names, then the locations, then the age. This means that selecting all values from a set of columns is quite easy (all of the Names and Ages, or all Names and Locations), but reading in only the first few records from each column would require reading in the entire dataset. Another advantage to column major formats is that compression is more efficient since compression can be done across each column, where the data type is uniform, as opposed to across rows with many data types."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#parquet",
    "href": "sections/11-parquet-arrow.html#parquet",
    "title": "11  Data Futures: Parquet and Arrow",
    "section": "11.4 Parquet",
    "text": "11.4 Parquet\nParquet is an open-source file format that stores data in a column-major format. The format contains several key components:\n\nrow group\ncolumn\npage\nfooter\n\n\nRow groups are blocks of data over a set number of rows that contain data from the same columns. Within each row group, data are organized in column-major format, and within each column are pages that are typically 1MB. The footer of the file contains metaata like the schema, encodings, unique values in each column, etc.\nThe parquet format has many tricks to to increase storage efficiency, and is increasingly being used to handle large datasets."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#arrow",
    "href": "sections/11-parquet-arrow.html#arrow",
    "title": "11  Data Futures: Parquet and Arrow",
    "section": "11.5 Arrow",
    "text": "11.5 Arrow\nSo far, we have discussed the difference between organizing information in row-major and column-major format, how that applies to arrays, and how it applies to data storage on disk using Parquet.\nArrow is a language-agnostic specification that enables representation of column-major information in memory without having to serialize data from disk. The Arrow project provides implementation of this specification in a number of languages, including Python.\nLet’s say that you have utilized the Parquet data format for more efficient storage of your data on disk. At some point, you’ll need to read that data into memory in order to do analysis on it. Arrow enables data transfer between the on disk Parquet files and in-memory Python computations, via the PyArrow library.\nPyArrow is great, but relatively low level. It supports basic group by and aggregate functions, as well as table and dataset joins, but it does not support the full operations that pandas does."
  },
  {
    "objectID": "sections/11-parquet-arrow.html#example",
    "href": "sections/11-parquet-arrow.html#example",
    "title": "11  Data Futures: Parquet and Arrow",
    "section": "11.6 Example",
    "text": "11.6 Example\nIn this example, we’ll read in a dataset of fish abundance in the San Francisco Estuary, which is published in csv format on the Environmental Data Initiative. This dataset isn’t huge, but it is big enough (3 GB) that working with it locally can be fairly taxing on memory. Motivated by user difficulties in actually working with the data, the deltafish R package was written using the R implementation of arrow. It works by downloading the EDI repository data, writing it to a local cache in parquet format, and using arrow to query it. In this example, I’ve put the Parquet files in a sharable location so we can explore it using pyarrow.\nFirst, we’ll load the modules we need.\n\nimport pyarrow.dataset as ds\nimport numpy as np\nimport pandas as pd\n\nNext we can read in the data using ds.dataset(), passing it the path to the parquet directory and how the data are partitioned.\n\ndeltafish = ds.dataset(\"/home/jclark/deltafish/fish\", format=\"parquet\", partitioning=[\"Species\"])\n\nYou can check out a file listing using the files method. Another great feature of parquet files is that they allow you to partition the data accross variables of the dataset. These partitiions mean that, in this case, data from each species of fish is written to it’s own file. This allows for even faster operations down the road, since we know that users will commonly need to filter on the species variable. Even though the data are partitioned into different files, pyarrow knows that this is a single dataset, and you still work with it by referencing just the directory in which all of the partitioned files live.\n\ndeltafish.files\n\n['/home/jclark/deltafish/fish/Taxa=Acanthogobius flavimanus/part-0.parquet',\n '/home/jclark/deltafish/fish/Taxa=Acipenser medirostris/part-0.parquet',\n '/home/jclark/deltafish/fish/Taxa=Acipenser transmontanus/part-0.parquet',\n '/home/jclark/deltafish/fish/Taxa=Acipenser/part-0.parquet'...\nYou can view the columns of a dataset using schema.to_string()\n\nprint(deltafish.schema.to_string())\n\nSampleID: string\nLength: double\nCount: double\nNotes_catch: string\nSpecies: string\nIf we are only interested in a few species, we can do a filter:\n\nexpr = ((ds.field(\"Species\")==\"Dorosoma petenense\")| \n        (ds.field(\"Species\")==\"Morone saxatilis\") |\n        (ds.field(\"Species\")== \"Spirinchus thaleichthys\"))\n\nfishf = deltafish.to_table(filter = expr)\n\nThere is another dataset included, the survey information. To do a join, we can just use the join method on the arrow dataset.\nFirst read in the survey dataset.\n\nsurvey = ds.dataset(\"/home/jclark/deltafish/survey\", format=\"parquet\", partitioning=[\"Source\"])\n\n\nprint(survey.schema.to_string())\n\nThen do the join, and convert to a pandas data.frame.\n\nfish_j = fishf.join(survey, \"SampleID\").to_pandas()\n\nNote that when we did our first manipulation of this dataset, we went from working with a FileSystemDataset, which is a representation of a dataset on disk without reading it into memory, to a Table, which is read into memory. pyarrow has a number of functions that do computations on datasets without reading them into memory. However these are evaluated “eagerly,” as opposed to “lazily.” These are useful in some cases, like above, where we want to take a larger than memory dataset and generate a smaller dataset (via filter, or group by/summarize), but are not as useful if we need to do a join before our summarization/filter.\nMore functionality for lazy evaluation is on the horizon for pyarrow though, by leveraging Ibis."
  },
  {
    "objectID": "sections/13-group-project-2.html",
    "href": "sections/13-group-project-2.html",
    "title": "13  Group Project: Data Processing",
    "section": "",
    "text": "In your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-13.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills."
  },
  {
    "objectID": "sections/15-google-earth-engine.html",
    "href": "sections/15-google-earth-engine.html",
    "title": "15  Google Earth Engine",
    "section": "",
    "text": "SAM NOTES, DELETE LATER - need to make sure that .ipynb that student’s will work out of are running in same virutal enviroment as everything else - embed .ipynb into quarto notebook (get book to build from those examples) - use Ryan Abernathey’s post to help frame introduction"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#learning-objectives",
    "href": "sections/15-google-earth-engine.html#learning-objectives",
    "title": "15  Google Earth Engine",
    "section": "15.1 Learning Objectives",
    "text": "15.1 Learning Objectives\n\nUnderstand what Google Earth Engine provides and its applications\nLearn about some real-world applications of Google Earth Engine\nLearn how to get started using Google Earth Engine on your own computer\nLearn how to find and access Google Earth Engine Data"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#introduction-15-20min",
    "href": "sections/15-google-earth-engine.html#introduction-15-20min",
    "title": "15  Google Earth Engine",
    "section": "15.2 Introduction (15-20min)",
    "text": "15.2 Introduction (15-20min)\nSAM NOTES, DELETE LATER - have Ingmar help frame utility of GEE in intro\nGoogle Earth Engine is a geospatial processing platform powered by Google Cloud Platform. It contains over 30 years of satellite imagery and geospatial datasets that are continually updated and available instantly. The Earth Engine API is available in Python (and JavaScript) for anyone with an account to access and analyze data.\nADD SOME COOL IMAGERY HERE\nExplore the public Earth Engine Data Catalog which includes a variety of standard Earth science raster datasets. Browse by dataset tags or by satellites (Landsat, MODIS, Sentinel).\nSAM NOTES, DELETE LATER - typical: download data and work locally - new way:“Moving compute to data” – GEE is a great example of this"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#getting-started-with-google-earth-engine-gee-on-your-own-machine-40-min",
    "href": "sections/15-google-earth-engine.html#getting-started-with-google-earth-engine-gee-on-your-own-machine-40-min",
    "title": "15  Google Earth Engine",
    "section": "15.3 Getting started with Google Earth Engine (GEE) on your own machine (40 min)",
    "text": "15.3 Getting started with Google Earth Engine (GEE) on your own machine (40 min)\nSAM NOTES, DELETE LATER\n\nremove #| eval: false once code actually runs to embed outputs\ndon’t really have time to actually do this in class? include instructions for those who want to try it out on their own later?\nContent borrowed from Dr. Samantha Stevenson’s guide to installling Jupyter/Google Earth Engine onyour personal laptop.\nPREREQUISITE: need conda (see Earth Engine API installion instructions here) – haven’t added these instructions below yet\nalso, not working on my machine at the moment. issue with library installs? starts with earthengine authenticate\n\n\nInstall the Google Earth Engine API\n\nSAM NOTES, DELETE LATER - MOVE MOST OF THIS TO COURSE SETUP (not using conda) BUT KEEP GEE ACTIVATION STEP HERE\n\nCreate an environment where the Google Earth Engine API will live. This ensures that it and it’s dependent packages will not cause versioning issues with your base environment (or other environments). We’ll call our environment gee_env.\n\n#| eval: false\nconda create --name gee_env\n\nActivate your environment so your machine knows where to store subsequent installs.\n\n#| eval: false\nconda activate gee_env\nYou’ll know your environment is activated successfully when (gee_env) appears before the prompt in your terminal window (as opposed to (base), for example).\n\nInstall the Google Earth Engine API in your gee_env\n\n#| eval: false\nconda install -c conda-forge earthengine-api\n\nSign up for a GEE Account\n\nGEE is currently free for educational use. Sign up for an account at https://signup.earthengine.google.com (you’ll need this to authenticate in the next step).\n\nSet up GEE Authentication\n\nIn order to begin using GEE, you’ll need to connect your GEE envionment (gee_env) to the authentication credentials associated with your Google account. This will need to be done each time you connect to GEE, but should only be done once per session.\n\nOn the command line, type:\n\n#| eval: false\nearthengine authenticate\nThis should launch a browser window where you can login with your Google account to the Google Earth Engine Authenticator. Following the prompts will generate a code, which you’ll then need to copy and paste back onto the command line. This will be saved as an authentication token so you won’t need to go through this process again until the next time you start a new session.\n\nInstall necessary packages (if you don’t already have them)\n\n#| eval: false\npip install ee # Earth Engine API package\npip install geemap # package for interactive maping with GEE \npip install pandas # contains useful tools for data manipulation (may not need this)"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#visualize-global-precipitation-data-using-google-earth-engine",
    "href": "sections/15-google-earth-engine.html#visualize-global-precipitation-data-using-google-earth-engine",
    "title": "15  Google Earth Engine",
    "section": "15.4 Visualize global precipitation data using Google Earth Engine",
    "text": "15.4 Visualize global precipitation data using Google Earth Engine\nContent for this section was adapted from Dr. Sam Stevenson’s Visualizing global precipitation using Google Earth Engine lesson, given in her EDS 220 course in Fall 2021.\n\nImport necessary packages\n\n\nimport ee # MODULENOTFOUNDERROR\nimport geemap\nimport pandas as pd\n\n\nCreate an interactive basemap\n\nThe default basemap is (you guessed it) Google Maps. The following code displays an empty Google Map that you can manipulate just like you would in the typical Google Maps interface. Do this using the Map method from the geemap library. We’ll also center the map at a specified latitude and longitude (here, 40N, 100E), set a zoom level, and save our map as an object called myMap.\n\nmyMap = geemap.Map(center = [40, -100], zoom = 2)\nmyMap\n\n\nLoad ERA5 Image Collections from GEE\n\n\nNOTE: ADC has worked with these data – took 3 weeks to download\nEE colleciton is all you need to load and analyze imgage collection\nprecursor to Ingmar’s stuff\n\nWe’ll be using the ERA5 daily aggregates reanalysis dataset, produced by the European Centre for Medium-Range Weather Forecasts (ECMWF), found here, which models atmospheric weather observations. We’ll load the total_precipitation field (check out the dataset metadata on here).\nThe ImageCollection method extracts a set of individual images that satisfies some criterion that you pass to GEE through the ee package. This is stored as an ImageCollection object which can be filtered and processed in various ways. We can pass the ImageCollction method agruments to tell GEE which data we want to retrieve. Below, we retrieve all daily ERA5 data (so we can see individual rain events).\n\nweatherData = ee.ImageCollection('ECMWF/ERA5/DAILY')\n\n\nSelect an image to plot\n\nTo plot a map over our Google Maps basemap, we need an “Image” rather than an “ImageCollection.” ERA5 contains many different climate variables, so we need to pick what we’d like to plot. We’ll use the .select method to choose the parameter(s) we’re interested in from our weatherData object.\n\nprecip = weatherData.select(\"total_precipitation\")\n\nWe can look at our precip object using the print method to see that it’s still an “ImageCollection” which contains daily infomration from 1979 to 2020.\n\nprint(precip)\n\nWe want to filter it down to a single field for a time of interest – let’s say December 1-2, 2019. We apply the .filter method to our precip object and apply the ee.Filter.date method (from the ee package) to filter for data from our chosen date range. We also apply the .mean method, which takes whatever precedes it and calculates the average.\n\nprecip_filtered = precip.filter(ee.Filter.date('2019-12-01', '2019-12-02')).mean()\n\n\nAdd data to map\n\nWe can fist use the setCenter method to tell the map where to center itself. It takes the longitude and latitude as the first two coordinates, followed by the zoom level.\n\nMap.setCenter(-152.505706, 59.432367, 2) # Cook Inlet, Alaska (WE CAN CHANGE THIS LOCATION)\n\nNext, set a color palette to use when plotting the data layer. The following is a palette specified for precipitation in the GEE description page for ERA5. GEE has lots of color tables like this that you can look up.\n\nprecip_palette = {\n    'min':0,\n    'max':0.1,\n    'palette': ['#FFFFFF', '#00FFFF', '#0080FF', '#DA00FF', '#FFA400', '#FF0000']\n}\n\nFinally, plot our filtered data, precip_filtered on top of our basemap using the .addLayer method. We’ll aslo pass it our visualization parameters (colors and ranges stored in precip_palette, the name of the data field total precipitation, and opacity so that we can see the basemap underneath)\n\nMap.addLayer(precip_filtered, precip_palette, 'total precipitation', opacity = 0.3)"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#ingmars-demonstration-here30-40-min",
    "href": "sections/15-google-earth-engine.html#ingmars-demonstration-here30-40-min",
    "title": "15  Google Earth Engine",
    "section": "15.5 INGMAR’S DEMONSTRATION HERE?(30-40 min)",
    "text": "15.5 INGMAR’S DEMONSTRATION HERE?(30-40 min)"
  },
  {
    "objectID": "sections/15-google-earth-engine.html#conclusionsummary",
    "href": "sections/15-google-earth-engine.html#conclusionsummary",
    "title": "15  Google Earth Engine",
    "section": "15.6 Conclusion/Summary",
    "text": "15.6 Conclusion/Summary\n\nlessons learned\nutilities\netc."
  },
  {
    "objectID": "sections/15-google-earth-engine.html#other-resources",
    "href": "sections/15-google-earth-engine.html#other-resources",
    "title": "15  Google Earth Engine",
    "section": "15.7 Other Resources",
    "text": "15.7 Other Resources\n\nGEE Code Editor is a web-based IDE for using GEE (JavaScript)"
  },
  {
    "objectID": "sections/17-group-project-3.html",
    "href": "sections/17-group-project-3.html",
    "title": "16  Group Project: Visualization",
    "section": "",
    "text": "In your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-17.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills."
  },
  {
    "objectID": "sections/18-arctic-data-staging.html",
    "href": "sections/18-arctic-data-staging.html",
    "title": "17  Workflows for data staging and publishing",
    "section": "",
    "text": "NSF archival policies for large datasets\nData transfer tools\nHow to manage co-locating data and code\n\neg: where model runs only has 1 TB of storage but model outputs 10 TB of data\nworkflow tools (pegasus, condor, slurm, snakemake)\n\nUploading large datasets to the Arctic Data Center"
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#nsf-policy-for-large-datasets",
    "href": "sections/18-arctic-data-staging.html#nsf-policy-for-large-datasets",
    "title": "17  Workflows for data staging and publishing",
    "section": "17.2 NSF policy for large datasets",
    "text": "17.2 NSF policy for large datasets\n\nthere are many different research methods that can generate large volumes of data. Numerical modeling (such as climate or ocean models) and anything generating high resolution imagery are two examples we see very commonly.\n\n\nThe Office of Polar Programs policy requires that metadata files, full data sets, and derived data products, must be deposited in a long-lived and publicly accessible archive.\n\n\nMetadata for all Arctic supported data sets must be submitted to the NSF Arctic Data Center (https://arcticdata.io).\n\n\nExceptions to the above data reporting requirements may be granted for social science and indigenous knowledge data, where privacy or intellectual property rights might take precedence. Such requested exceptions must be documented in the Data Management Plan.\n\n\ndatasets that are already published on a long lived archive do not need to be replicated to the Arctic Data Center\n\nexample: a research project accesses many terabytes of VIIRS satellite data. The original satellite data does not need to be published on the Arctic Data Center, but the code that accessed it, and derived products, can be published\n\nfor some numerical models, if the model results can be faithfully reproduced from code, the code that generates the models can be a sufficient archival product, as opposed to the code and the model output\n\nif the model is difficult to set up, or takes a very long time to run, we would probably reccommend publishing the output as well as code\n\nthe Arctic Data Center is committed to archiving data of any volume"
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#data-transfer-tools",
    "href": "sections/18-arctic-data-staging.html#data-transfer-tools",
    "title": "17  Workflows for data staging and publishing",
    "section": "17.3 Data transfer tools",
    "text": "17.3 Data transfer tools\n\nscenario: you need to send a bunch of data to the Arctic Data Center. after getting the credentials, you use scp to start the transfer. You know this typically takes around 12 hours so you start it at 5pm right when you leave the office expecting it to be done when you get back. When you arrive, you see there was a short network outage in the middle of the night. The whole job failed so you have to start it again…\n\nThere is a better way!\nThree key elements to data transfer\n- endpoints\n- network\n- transfer tool\n\n17.3.0.1 Endpoints\nThe from and to locations of the transfer, an endpoint is a remote computing device that can communicate back and forth with the network to which it is connected. The speed with which an endpoint can communicate with the network varies depending on how it is configured. Performance depends on the CPU, RAM, OS, and disk configuration. Examples:\n\nNCEAS datateam server:\nStandard laptop\n\n\n\n17.3.0.2 Network speed\nDetermines how quickly information can be sent between endpoints, largely dependent on what you pay for. Wired networks get significantly more speed than wireless.\n\nnot all networks are created equal\nserver to server (north hall to san diego) versus server to your house\n\n\n\n17.3.0.3 Transfer tools\n\nscp\n\nuses ssh for authentication and transfer\nif you can ssh to a server, you can probably use scp to move files without any other setup\ncopies all files linearly and simply. if a transfer fails in the middle, difficult to know exactly what files didn’t make it, so you have to start the whole thing over and re-transfer all the files\n\nrsync\n\nsimilar to scp but syncs files/directories as opposed to copying\nif the file already exists on the other side, it is skipped\n\nglobus\n\nparellelizes transfers by utilizing multiple network sockets simultaneously\nis able to fail and restart itself efficiently\nrequires more setup, endpoints need to be configured as globus nodes\n\n\n\n\n17.3.0.4 Globus\n\neasy to use, as long as your data are accessible via an endpoint configured as a Globus node\nleverage your institutions computing resources! they may be able to help get you access to a data transfer node already configured correctly\nthere are paid options to set up a node from your own workstation (Globus Connect Personal - check the naming here, and feature list)\n\nremember the other factors though! Globus won’t help you overcome a 1 Gb/s laptop connection speed, or a 50 Mb/s network speed"
  },
  {
    "objectID": "sections/18-arctic-data-staging.html#documenting-large-datasets",
    "href": "sections/18-arctic-data-staging.html#documenting-large-datasets",
    "title": "17  Workflows for data staging and publishing",
    "section": "17.4 Documenting large datasets",
    "text": "17.4 Documenting large datasets\n\nthe Arctic Data Center is working to support large datasets, but we have performance considerations as well\nself documenting file formats are preferred, to prevent us from needing to document thousands-millions of files in a single metadata document\n\nnetcdf\ngeotiff, geopackage\n\nregular, parseable filenames and consistent file formatting is key\ncommunicate early and often with the Arctic Data Center staff"
  },
  {
    "objectID": "sections/20-reproducibility-containers.html",
    "href": "sections/20-reproducibility-containers.html",
    "title": "19  Reproducibility and Containers",
    "section": "",
    "text": "TODO: Decide about if/how to talk about WholeTale\nTODO: This lesson should be have a wow-factor and emphasize why we’re focusing all of this\nTODO: This lesson should be more about wrapping up and tying everything together than showing off new tech"
  },
  {
    "objectID": "sections/20-reproducibility-containers.html#learning-objectives",
    "href": "sections/20-reproducibility-containers.html#learning-objectives",
    "title": "19  Reproducibility and Containers",
    "section": "19.1 Learning Objectives",
    "text": "19.1 Learning Objectives\n\nLearn about software versioning\nBecome familiar with Docker as a tool to improve computational reproducibility"
  },
  {
    "objectID": "sections/20-reproducibility-containers.html#outline",
    "href": "sections/20-reproducibility-containers.html#outline",
    "title": "19  Reproducibility and Containers",
    "section": "19.2 Outline",
    "text": "19.2 Outline\n\nIntroduce software reproducibility\n\nMotivate the idea with examples and data\nTalk about software collapse\n\nhttp://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/\nhttps://xkcd.com/2347/\n\n\nSemantic versioning and the reality of it e.g., https://pandas.pydata.org/docs/development/policies.html#version-policy\nMyBinder\nWholeTale?\n\nExamples to look at including:\n\nhttps://numpy.org/neps/nep-0023-backwards-compatibility.html#example-cases\nhttps://github.com/scipy/scipy/issues/16418 > https://pandas.pydata.org/docs/whatsnew/v1.4.0.html#deprecations: DataFrame.append() and Series.append() have been deprecated and will be removed in a future version. Use pandas.concat() instead (GH35407).\n\nPrinciples to get across:\n\nYou probably should be thinking about software versioning\n\nKnow which version of versions of Python your code was written/tested under and keep track of that in a machine-readable way\nKnow the specific versions, of at least the specific MAJOR.MINOR of the packages your code was written+tested under and keep track of them in a machine-readable way (ie requirements.txt)"
  },
  {
    "objectID": "sections/20-reproducibility-containers.html#hands-off-demo",
    "href": "sections/20-reproducibility-containers.html#hands-off-demo",
    "title": "19  Reproducibility and Containers",
    "section": "19.3 Hands-off Demo",
    "text": "19.3 Hands-off Demo\nShow students an example of containerizing a workflow so it runs using a past version of Python and pinned versions of packages. Ideally find an example where behavior changes based on the Python or one or more package versions."
  }
]