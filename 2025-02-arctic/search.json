[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible Approaches to Arctic Research Using R",
    "section": "",
    "text": "About the course\nThis 5-day virtual workshop will provide researchers with an overview of reproducible and ethical research practices, steps and methods for more easily documenting and preserving their data at the Arctic Data Center, and an introduction to programming in R. Special attention will be paid to qualitative data management, including practices working with sensitive data. Example datasets will draw from natural and social sciences, and methods for conducting reproducible research will be discussed in the context of both qualitative and quantitative data. Responsible and reproducible data management practices will be discussed as they apply to all aspects of the data life cycle. This includes ethical data collection and data sharing, data sovereignty, and the CARE principles. The CARE principles are guidelines that help ensure open data practices (like the FAIR principles) appropriately engage with Indigenous Peoples’ rights and interests.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#weeks-schedule",
    "href": "index.html#weeks-schedule",
    "title": "Reproducible Approaches to Arctic Research Using R",
    "section": "Week’s Schedule",
    "text": "Week’s Schedule",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#logistics",
    "href": "index.html#logistics",
    "title": "Reproducible Approaches to Arctic Research Using R",
    "section": "Logistics",
    "text": "Logistics\n\nServer\nYou should receive a separate email from NCEAS Account Services prompting you to change your password using the NCEAS account service. Please change your password and then ensure that you can log in at https://included-crab.nceas.ucsb.edu/.\n\n\nMonitors\nIf you have a second monitor or second device, it would be useful for this training. You’ll need enough screen real estate to handle the primary Zoom window, the participant pane in Zoom, Slack, and a browser with tabs for RStudio and our training curriculum. We recommend either using two monitors or joining the Zoom room from a second device. Having two monitors could look like this:\n\nIf you must be on one machine for everything, here’s an example of what it could look like when you are following along with the class and how your screen will shift when you have a more detailed question that requires breakout assistance.\n\nWhen we’re in session, please turn your camera on and mute your microphone unless you would like to ask a question or contribute to a discussion.\n\n\nWorking from Home\nWe recognize that working from home may come with challenges. The appearance or sound of other adults, children, and pets in remote meetings such as this is completely normal and understandable. Having your video on and enabling the instructors and your fellow participants to see you brings some humanity to this physically distant workshop, and we believe that this is a crucial element of its success. If you would like to use the Zoom virtual background feature to hide your surroundings, please do so, making sure your background of choice fits within the code of conduct (here are some Arctic-themed backgrounds if you need inspiration).\n\n\nNon-Verbal Feedback\nWe’ll be using the Zoom “Non Verbal Feedback” buttons throughout this course. We will ask you to put a green check by your name when you’re all set and ready to move on with the lesson and a red x by your name if you’re stuck or need assistance. These buttons can be found in the participant’s panel of the Zoom room. When you’re asked to answer using these buttons, please ensure that you select one so that the instructor has the feedback that they need to either continue the lesson or pause until everyone gets back on the same page.\n\n\n\n\n\n\nQuestions and Getting Help\nWhen you need to ask a question, please do so in one of the following ways:\n\nTurn your mic on and ask. If you are uncomfortable interrupting the instructor, you may also raise your virtual hand (in the participant panel) and the session facilitator will ask the instructor to pause and call upon you.\nAsk your question in the slack channel.\n\nIf you have an issue/error and get stuck, you can ask for help in the following ways:\n\nTurn your mic on and ask for help. See also above regarding the use of a virtual raised hand.\nLet one of the instructors know in the slack channel.\nIf prompted to do so, put a red X next to your name as your status in the participant window.\n\nWhen you have detailed questions or need one on one coding assistance, we will have zoom breakout rooms available with helpers. The helper will try to help you in Slack first. If the issue requires more in-depth troubleshooting, the helper will invite you to join their named Breakout Room.\n\n\n\n\n\nOnce you have been assigned a breakout room, but you don’t see the pop-up shown in the image above, you can click on the “Join Breakout Room” button on the bottom menu and then click on “Join Breakout Room” in the pop-up message.\n\n\n\n\n\n\n\n\nThe Power of Open\nTo facilitate a lively and interactive learning environment, we’ll be calling on folks to share their code and to answer various questions posed by the instructor. It’s completely okay to say “Pass” or “I Don’t Know” - this is a supportive learning environment and we will all learn from each other. The instructors will be able to see your code as you go to help you if you get stuck, and the lead instructor may share participants’ code to show a successful example or illustrate a teaching moment.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Reproducible Approaches to Arctic Research Using R",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Reproducible Approaches to Arctic Research Using R",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Matthew B. Jones, S. Jeanette Clark, Nicole Greco, Maggie Klope, Jim Regetz (2025), Reproducible Practices for Arctic Research Using R. Arctic Data Center & NCEAS Learning Hub. doi:10.18739/A2057CV1S. URL https://learning.nceas.ucsb.edu/2025-01-arctic.\nAdditional contributors: Ben Bolker, Amber E. Budden, Julien Brun, Samantha Csik, Halina Do-Linh, Natasha Haycock-Chavez, S. Jeanette Clark, Julie Lowndes, Stephanie Hampton, Samanta Katz, Erin McLean, Bryce Mecum, Casey O’Hara, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Camila Vargas-Poulsen, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "session_01.html",
    "href": "session_01.html",
    "title": "1  Introduction to the Arctic Data Center and RStudio Server Setup",
    "section": "",
    "text": "1.1 Introduction to the Arctic Data Center and NSF Standards and Policies",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Arctic Data Center and RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_01.html#introduction-to-the-arctic-data-center-and-nsf-standards-and-policies",
    "href": "session_01.html#introduction-to-the-arctic-data-center-and-nsf-standards-and-policies",
    "title": "1  Introduction to the Arctic Data Center and RStudio Server Setup",
    "section": "",
    "text": "1.1.1 Learning Objectives\nIn this lesson, we will discuss:\n\nThe mission and structure of the Arctic Data Center\nHow the Arctic Data Center supports the research community\nAbout data policies from the NSF Arctic program\n\n\n\n1.1.2 Arctic Data Center - History and Introduction\nThe Arctic Data Center is the primary data and software repository for the Arctic section of National Science Foundation’s Office of Polar Programs (NSF OPP).\nWe’re best known in the research community as a data archive – researchers upload their data to preserve it for the future and make it available for re-use. This isn’t the end of that data’s life, though. These data can then be downloaded for different analyses or synthesis projects. In addition to being a data discovery portal, we also offer top-notch tools, support services, and training opportunities. We also provide data rescue services.\n\nNSF has long had a commitment to data reuse and sharing. Since our start in 2016, we’ve grown substantially – from that original 4 TB of data from ACADIS to now over 140 TB towards the end of 2024. In 2021 alone, we saw 16% growth in dataset count, and about 30% growth in data volume. This increase has come from advances in tools – both ours and of the scientific community, plus active community outreach and a strong culture of data preservation from NSF and from researchers. We plan to add more storage capacity in the coming months, as researchers are coming to us with datasets in the terabytes, and we’re excited to preserve these research products in our archive. We’re projecting our growth to be around several hundred TB this year, which has a big impact on processing time. Give us a heads up if you’re planning on having larger submissions so that we can work with you and be prepared for a large influx of data.\n\nThe data that we have in the Arctic Data Center comes from a wide variety of disciplines. These different programs within NSF all have different focuses – the Arctic Observing Network supports scientific and community-based observations of biodiversity, ecosystems, human societies, land, ice, marine and freshwater systems, and the atmosphere as well as their social, natural, and/or physical environments, so that encompasses a lot right there in just that one program. We’re also working on a way right now to classify the datasets by discipline, so keep an eye out for that coming soon.\n\nAlong with that diversity of disciplines comes a diversity of file types. The most common file type we have are image files in four different file types. Probably less than 200-300 of the datasets have the majority of those images – we have some large datasets that have image and/or audio files from drones. Most of those 6600+ datasets are tabular datasets. There’s a large diversity of data files, though, whether you want to look at remote sensing images, listen to passive acoustic audio files, or run applications – or something else entirely. We also cover a long period of time, at least by human standards. The data represented in our repository spans across centuries.\n\nWe also have data that spans the entire Arctic, as well as the sub-Arctic, regions.\n\n\n\n1.1.3 Data Discovery Portal\nTo browse the data catalog, navigate to arcticdata.io. Go to the top of the page and under data, go to search. Right now, you’re looking at the whole catalog. You can narrow your search down by the map area, a general search, or searching by an attribute.\n Clicking on a dataset brings you to this page. You have the option to download all the files by clicking the green “Download All” button, which will zip together all the files in the dataset to your Downloads folder. You can also pick and choose to download just specific files.\n\nAll the raw data is in open formats to make it easily accessible and compliant with FAIR principles – for example, tabular documents are in .csv (comma separated values) rather than Excel documents.\nThe metrics at the top give info about the number of citations with this data, the number of downloads, and the number of views. This is what it looks like when you click on the Downloads tab for more information.\n\nScroll down for more info about the dataset – abstract, keywords. Then you’ll see more info about the data itself. This shows the data with a description, as well as info about the attributes (or variables or parameters) that were measured. The green check mark indicates that those attributes have been annotated, which means the measurements have a precise definition to them. Scrolling further, we also see who collected the data, where they collected it, and when they collected it, as well as any funding information like a grant number. For biological data, there is the option to add taxa.\n\n\n1.1.4 Tools and Infrastructure\nAcross all our services and partnership, we are strongly aligned with the community principles of making data FAIR (Findable, Accesible, Interoperable and Reusable).\n\nWe have a number of tools available to submitters and researchers who are there to download data. We also partner with other organizations, like Make Data Count and DataONE, and leverage those partnerships to create a better data experience.\n\nOne of those tools is provenance tracking. With provenance tracking, users of the Arctic Data Center can see exactly what datasets led to what product, using the particular script that the researcher ran.\n\nAnother tool are our Metadata Quality Checks. We know that data quality is important for researchers to find datasets and to have trust in them to use them for another analysis. For every submitted dataset, the metadata is run through a quality check to increase the completeness of submitted metadata records. These checks are seen by the submitter as well as are available to those that view the data, which helps to increase knowledge of how complete their metadata is before submission. That way, the metadata that is uploaded to the Arctic Data Center is as complete as possible, and close to following the guideline of being understandable to any reasonable scientist.\n\n\n\n1.1.5 Support Services\nMetadata quality checks are the automatic way that we ensure quality of data in the repository, but the real quality and curation support is done by our curation team. The process by which data gets into the Arctic Data Center is iterative, meaning that our team works with the submitter to ensure good quality and completeness of data. When a submitter submits data, our team gets a notification and beings to evaluate the data for upload. They then go in and format it for input into the catalog, communicating back and forth with the researcher if anything is incomplete or not communicated well. This process can take anywhere from a few days or a few weeks, depending on the size of the dataset and how quickly the researcher gets back to us. Once that process has been completed, the dataset is published with a DOI (digital object identifier).\n\n\n\n1.1.6 Training and Outreach\nIn addition to the tools and support services, we also interact with the community via trainings like this one and outreach events. We run workshops at conferences like the American Geophysical Union, Arctic Science Summit Week and others. We also run an intern and fellows program, and webinars with different organizations. We’re invested in helping the Arctic science community learn reproducible techniques, since it facilitates a more open culture of data sharing and reuse.\n\nWe strive to keep our fingers on the pulse of what researchers like yourselves are looking for in terms of support. We’re active on Twitter and BlueSky to share Arctic updates, data science updates, and specifically Arctic Data Center updates, but we’re also happy to feature new papers or successes that you all have had with working with the data. We can also take data science questions if you’re running into those in the course of your research, or how to make a quality data management plan. Follow us on our media platforms and interact with us – we love to be involved in your research as it’s happening as well as after it’s completed.\n\n\n\n1.1.7 Data Rescue\nWe also run data rescue operations. We digitized Autin Post’s collection of glacier photos that were taken from 1964 to 1997. There were 100,000+ files and almost 5 TB of data to ingest, and we reconstructed flight paths, digitized the images of his notes, and documented image metadata, including the camera specifications.\n\n\n\n1.1.8 Who Must Submit\nProjects that have to submit their data include all Arctic Research Opportunities through the NSF Office of Polar Programs. That data has to be uploaded within two years of collection. The Arctic Observing Network has a shorter timeline – their data products must be uploaded within 6 months of collection. Additionally, we have social science data, though that data often has special exceptions due to sensitive human subjects data. At the very least, the metadata has to be deposited with us.\nArctic Research Opportunities (ARC)\n\nComplete metadata and all appropriate data and derived products\nWithin 2 years of collection or before the end of the award, whichever comes first\n\nARC Arctic Observation Network (AON)\n\nComplete metadata and all data\nReal-time data made public immediately\nWithin 6 months of collection\n\nArctic Social Sciences Program (ASSP)\n\nNSF policies include special exceptions for ASSP and other awards that contain sensitive data\nHuman subjects, governed by an Institutional Review Board, ethically or legally sensitive, at risk of decontextualization\nMetadata record that documents non-sensitive aspects of the project and data\n\nTitle, Contact information, Abstract, Methods\n\n\nFor more complete information see our “Who Must Submit” webpage\nRecognizing the importance of sensitive data handling and of ethical treatment of all data, the Arctic Data Center submission system provides the opportunity for researchers to document the ethical treatment of data and how collection is aligned with community principles (such as the CARE principles). Submitters may also tag the metadata according to community develop data sensitivity tags. We will go over these features in more detail shortly.\n\n\n\n1.1.9 Summary\nAll the above informtion can be found on our website or if you need help, ask our support team at support@arcticdata.io or tweet us @arcticdatactr!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Arctic Data Center and RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_01.html#learning-objectives-1",
    "href": "session_01.html#learning-objectives-1",
    "title": "1  Introduction to the Arctic Data Center and RStudio Server Setup",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPractice creating an R Project\nOrganize an R Project for effective project management\nUnderstand how to move in an R Project using paths and working directories",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Arctic Data Center and RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_01.html#logon-to-the-rstudio-server",
    "href": "session_01.html#logon-to-the-rstudio-server",
    "title": "1  Introduction to the Arctic Data Center and RStudio Server Setup",
    "section": "1.2 Logon to the RStudio Server",
    "text": "1.2 Logon to the RStudio Server\nTo prevent us from spending most of this lesson troubleshooting the myriad of issues that can arise when setting up the R, RStudio, and git environments, we have chosen to have everyone work on a remote server with all of the software you need installed. We will be using a special kind of RStudio just for servers called RStudio Server. If you have never worked on a remote server before, you can think of it like working on a different computer via the internet. Note that the server has no knowledge of the files on your local filesystem, but it is easy to transfer files from the server to your local computer, and vice-versa, using the RStudio server interface.\n\n\n\n\n\n\nServer Setup\n\n\n\nYou should have received an email prompting you to change your password for your server account. If you did not, please put up a post-it and someone will help you.\nAfter you have successfully changed your password log in at: https://included-crab.nceas.ucsb.edu/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Arctic Data Center and RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_01.html#create-an-r-project",
    "href": "session_01.html#create-an-r-project",
    "title": "1  Introduction to the Arctic Data Center and RStudio Server Setup",
    "section": "1.3 Create an R Project",
    "text": "1.3 Create an R Project\nIn this course, we are going to be using an R project to organize our work. An R project is tied to a directory on your local computer, and makes organizing your work and collaborating with others easier.\nThe Big Idea: using an R project is a reproducible research best practice because it bundles all your work within a working directory. Consider your current data analysis workflow. Where do you import you data? Where do you clean and wrangle it? Where do you create graphs, and ultimately, a final report? Are you going back and forth between multiple software tools like Microsoft Excel, JMP, and Google Docs? An R project and the tools in R that we will talk about today will consolidate this process because it can all be done (and updated) in using one software tool, RStudio, and within one R project.\n\n\n\n\n\n\nR Project Setup\n\n\n\n\nIn the “File” menu, select “New Project”\nClick “New Directory”\nClick “New Project”\nUnder “Directory name” type: training_{USERNAME} (i.e. training_vargas)\nLeave “Create Project as subdirectory of:” set to ~\nClick “Create Project”\n\nRStudio should open your new project automatically after creating it. One way to check this is by looking at the top right corner and checking for the project name.\n\n\n\n\n\n\n\n\nSharing your project\n\n\n\nTo share your project with the instructor team, locate the “project switcher” dropdown menu in the upper right of your RStudio window. This dropdown has the name of your project (eg: training_vargas), and a dropdown arrow. Click the dropdown menu, then “Share Project.” When the dialog box pops up, add the following usernames to your project:\n\njclark\nmmklope\nregetz\n\nOnce those names show up in the list, click “OK”.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Arctic Data Center and RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_01.html#organizing-an-r-project",
    "href": "session_01.html#organizing-an-r-project",
    "title": "1  Introduction to the Arctic Data Center and RStudio Server Setup",
    "section": "1.4 Organizing an R Project",
    "text": "1.4 Organizing an R Project\nWhen starting a new research project, one of the first things I do is create an R Project for it (just like we have here!). The next step is to then populate that project with relevant directories. There are many tools out there that can do this automatically. Some examples are rrtools or usethis::create_package(). The goal is to organize your project so that it is a compendium of your research. This means that the project has all of the digital parts needed to replicate your analysis, like code, figures, the manuscript, and data access.\nSome common directories are:\n\n\n\ndata: where we store our data (often contains subdirectories for raw, processed, and metadata data)\nR: contains scripts with your custom R functions, etc. (some find this name misleading if their work has other scripts beyond the R programming language, in which case they call this directory scripts)\nplots or figs: generated plots, graphs, and figures\ndocs: summaries or reports of analysis or other relevant project information\n\nDirectory organization will vary from project to project, but the ultimate goal is to create a well organized project for both reproducibility and collaboration.\n\n\n\n\n\n\n\n\n\n\nProject Sub-directories\n\n\n\nFor this week we are going to create 3 folder (directories) in our training_{USERNAME} Rproject.\n\nIn the files pane in RStudio (bottom right), click on Folder button (with a green circle and plus sign) and create 3 new folders: data, plots, scripts.\n\nThe idea here is treat this RProject as an example of how to organize our work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Arctic Data Center and RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_01.html#moving-in-an-r-project-using-paths-working-directories",
    "href": "session_01.html#moving-in-an-r-project-using-paths-working-directories",
    "title": "1  Introduction to the Arctic Data Center and RStudio Server Setup",
    "section": "1.5 Moving in an R Project using Paths & Working Directories",
    "text": "1.5 Moving in an R Project using Paths & Working Directories\n\nNow that we have your project created (and notice we know it’s an R Project because we see a .Rproj file in our Files pane), let’s learn how to move in a project. We do this using paths.\nThere are two types of paths in computing: absolute paths and relative paths.\n\nAn absolute path always starts with the root of your file system and locates files from there. The absolute path to my project directory is: /home/vargas-poulsen/training_vargas\nRelative paths start from some location in your file system that is below the root. Relative paths are combined with the path of that location to locate files on your system. R (and some other languages like MATLAB) refer to the location where the relative path starts as our working directory.\n\nRStudio projects automatically set the working directory to the directory of the project. This means that you can reference files from within the project without worrying about where the project directory itself is. If I want to read in a file from the data directory within my project, I can simply type read.csv(\"data/samples.csv\") as opposed to read.csv(\"/home/vargas-poulsen/training_vargas/data/samples.csv\").\nThis is not only convenient for you, but also when working collaboratively. We will talk more about this later, but if Matt makes a copy of my R project that I have published on GitHub, and I am using relative paths, he can run my code exactly as I have written it, without going back and changing /home/vargas-poulsen/training_vargas/data/samples.csv to /home/jones/training_jones/data/samples.csv.\nNote that once you start working in projects you should basically never need to run the setwd() command. If you are in the habit of doing this, stop and take a look at where and why you do it. Could leveraging the working directory concept of R projects eliminate this need? Almost definitely!\nSimilarly, think about how you work with absolute paths. Could you leverage the working directory of your R project to replace these with relative paths and make your code more portable? Probably!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Arctic Data Center and RStudio Server Setup</span>"
    ]
  },
  {
    "objectID": "session_02.html",
    "href": "session_02.html",
    "title": "2  Introduction to Quarto",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "session_02.html#learning-objectives",
    "href": "session_02.html#learning-objectives",
    "title": "2  Introduction to Quarto",
    "section": "",
    "text": "Introduce literate analysis using Quarto (an extension of RMarkdown’s features)\nLearn markdown syntax and run R code using Quarto\nBuild and render an example analysis",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "session_02.html#introduction-to-literate-programming",
    "href": "session_02.html#introduction-to-literate-programming",
    "title": "2  Introduction to Quarto",
    "section": "2.1 Introduction to Literate Programming",
    "text": "2.1 Introduction to Literate Programming\nAll too often, computational methods are written in such a way as to be borderline incomprehensible even to the person who originally wrote the code! The reason for this is obvious, computers interpret information very differently than people do. In 1984, Donald Knuth proposed a reversal of the programming paradigm by introducing the concept of Literate Programming (Knuth 1984).\n\n“Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.”\n\nIf our aim is to make scientific research more transparent, the appeal of this paradigm reversal is immediately apparent. By switching to a literate analysis model, you help enable human understanding of what the computer is doing. As Knuth describes, in the literate analysis model, the author is an “essayist” who chooses variable names carefully, explains what they mean, and introduces concepts in the analysis in a way that facilitates understanding.\nQuarto and RMarkdown are an excellent way to generate literate analysis, and a reproducible workflow. These types of files, combine R the programming language, and markdown, a set of text formatting directives.\nIn an R script, the language assumes that you are writing R code, unless you specify that you are writing prose (using a comment, designated by #). The paradigm shift of literate analysis comes in the switch to RMarkdown or Quarto, where instead of assuming you are writing code, they assume that you are writing prose unless you specify that you are writing code. This, along with the formatting provided by markdown, encourages the “essayist” to write understandable prose to accompany the code that explains to the human-beings reading the document what the author told the computer to do. This is in contrast to writing just R code, where the author telling to the computer what to do with maybe a smattering of terse comments explaining the code to a reader.\nBefore we dive in deeper, let’s look at an example of what a rendered literate analysis can look like using a real example. Here is an example of an analysis workflow written using RMarkdown. Note that if this analysis would be in Quarto, the render version it would be similar, except for formatting and layout (eg: the default font in Quarto is different).\nThere are a few things to notice about this document, which assembles a set of similar data sources on salmon brood tables with different formatting into a single data source.\n\nIt introduces the data sources using in-line images, links, interactive tables, and interactive maps.\nAn example of data formatting from one source using R is shown.\nThe document executes a set of formatting scripts in a directory to generate a single merged file.\nSome simple quality checks are performed (and their output shown) on the merged data.\nSimple analysis and plots are shown.\n\nIn addition to achieving literate analysis, this document also represents a reproducible analysis. Because the entire merging and quality control of the data is done using the R code in the Quarto file, if a new data source and formatting script are added, the document can be run all at once with a single click to re-generate the quality control, plots, and analysis of the updated data.\n\n\n\n\n\n\nA note on reproducibility\n\n\n\nReproducible analysis allow you to automatize how the figures and the statistics in your analysis are generated. This process also helps your collaborators, your readers and your future self to follow your code trail that leads to the original data, increasing the transparency of your science.\nLiterate analysis help reduce the mistakes from copying and pasting across software, keeps results and models in sync, and allows you to provide interested readers with more information about the different approaches and analyses you tried before coming up with the final results (British Ecological Society (2017)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "session_02.html#rmarkdown-and-quarto",
    "href": "session_02.html#rmarkdown-and-quarto",
    "title": "2  Introduction to Quarto",
    "section": "2.2 RMarkdown and Quarto",
    "text": "2.2 RMarkdown and Quarto\nYou can identify a Quarto file with the .qmd extension. On the other hand, an RMarkdown file has a .Rmd extension. Both have similar structures and both combine prose with code. Quarto provides rich support to languages other than R such as Python, Observable, and Julia. It also excels in formatting and layout, allowing users to customize in detail the looks of the rendered documents. On the other hand, RMarkdown is compatible with some languages that Quarto is not, for example bash. Quarto and Rmarkdown are amazing tools to use for collaborative research. During this course we will spend some time learning and using the basics of Quarto and provide some comparisons to RMarkdown.\n\nNow, let’s take a look at the structure of each of these files. They both look, for the most part, the same with minor differences.\n\n\nFinally, lets compare each of these files when knitted/rendered.\n\n\nAgain, we see similar outcomes, with minor differences mainly in formatting (font, style of showing code chunks, etc.)\nBoth type of documents have three main components:\n\nYAML metadata to guide the document’s build process\nCode chunks to run\nProse (Text to display)\n\nToday we are going to use Quarto to run some analysis on data. We are specifically going to focus on the code chunk and text components. We will discuss more about the how the YAML works in a Quarto later in the course.\n\n\n\n\n\n\nThe YAML\n\n\n\nThe YAML is the document’s metadata which sets guidelines on how you want the output of your document to look like. It is located at the top of your file, delineated by three dashes (---) at the top and at the bottom of it. It can be used to specify:\n\nCharacteristics of your documents such at title, author, date of creation.\nArguments to pass on the building process to control the format of the output.\nAdditional information such as the bibliography file (and formatting of the references)\nSpecific parameters for your report (e.g.: just used a subset of the data).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "session_02.html#a-quarto-document",
    "href": "session_02.html#a-quarto-document",
    "title": "2  Introduction to Quarto",
    "section": "2.3 A Quarto Document",
    "text": "2.3 A Quarto Document\nLet’s open a Quarto file by following the instructions below.\n\n\n\n\n\n\nSetup\n\n\n\n\nOpen a new Quarto file using the following prompts: File &gt; New File &gt; Quarto Document\nA popup window will appear.\nGive your file a new title, e.g. “Introduction to Quarto”.\nLeave the output format as HTML and Engine set to Knitr.\nThen click the “Create” button.\n\n\n\nThe first thing to notice is that by opening a file, we see the fourth pane of the RStudio pops up. This is our Quarto document, which is essentially a text editor. We also see in the upper left side that we are looking at the document under the “Visual editor”. This is probably a familiar way of looking at a text document. To introduce the markdown syntax, we are going to move to the source editor and then come back to the visual editor. In the upper left corner, click on Source. See how the formatting changed? In the Source editor we are looking at the same text, but in markdown syntax. The visual editor on the other hand, allows us to see how markdown is rendered, therefore how it is going to look in our output document.\nLet’s have a look at this file — As we saw in the examples above, it looks a little different than an R script. It’s not blank; there is some initial text already provided for you. Let’s identify the three main components we introduced before. We have the YAML at the top, in between the two sets of dashed lines. Then we also see white and grey sections. The grey sections are R code chunks, and the white sections are plain text.\nLet’s go ahead and render this file by clicking the “Render” button, next to the blue arrow at the top of the Quarto file. When you first click this button, RStudio will prompt you to save this file. Save it in into your scripts folder, and name it something that you will remember (like quarto-intro.qmd).\n\n\n\nWhat do you notice between the two?\nFirst, the render process produced a second file (an HTML file) that popped up in a second window in the browser. You’ll also see this file in your directory with the same name as your qmd, but with the .html extension. In its simplest format, Quarto files come in pairs (same as RMarkdown files): the Quarto document, and its rendered version. In this case, we are rendering the file into HTML. You can also knit to PDF or Word files and others.\nNotice how the grey R code chunks are surrounded by 3 back-ticks and {r LABEL}. The first chunk, in this case 1+1, is evaluated and returns the output number (2). Notice the line in the second chunk that says #| echo: false? This is a code chunk option that indicates not to print the code. In the rendered version, we can see the outcome of 2*2, but not the executed code that created the outcome.\nThe table below show some of the options available for customizing outputs (Quarto.org).\n\nCode chunk options\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n#| eval:\nEvaluate the code chunk (if false, just echos the code into the output).\n\n\n#| echo:\nInclude the source code in output\n\n\n#| warning:\nInclude warnings in the output.\n\n\n#| error:\nInclude warnings in the output.\n\n\n#| include:\nCatch all for preventing any output (code or results) from being included (e.g.include: false suppresses all output from the code block).\n\n\n\nNote that you can also combine these options by adding more than one to a code chunk.\n\n\n\n\n\n\nImportant\n\n\n\nOne important difference between Quarto documents and RMarkdown documents is that in Quarto, chunk options are written in special comment format (#|) at the top of code chunks rather than within the wiggly brackets next to ```{r} at the begging of the chunk. For example:\n\nQuarto code options syntax\n\n\n\nRMarkdown code options syntax\n\n\n\n\nIt is important to emphasize one more time that in a Quarto (and RMarkdown) document, the gray areas of the document are code. In this case, it is R code because that is what is indicated in the ```{r} syntax at the start of this gray area. And the white areas of a qmd are in markdown language.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "session_02.html#markdown-syntax",
    "href": "session_02.html#markdown-syntax",
    "title": "2  Introduction to Quarto",
    "section": "2.4 Markdown Syntax",
    "text": "2.4 Markdown Syntax\nLet’s start by talking about markdown. Markdown is a formatting language for plain text, and there are only around 15 rules to know.\nNotice the syntax in the document we just knitted:\n\nHeaders get rendered at multiple levels: #, ##\nBold: **word**\n\nThere are some good cheatsheets to get you started, and here is one built into RStudio: Go to Help &gt; Markdown Quick Reference.\n\n\n\n\n\n\nImportant\n\n\n\nThe hash symbol # is used differently in markdown and in R\n\nIn an R script or inside an R code chunk, a hash indicates a comment that will not be evaluated. You can use as many as you want: # is equivalent to ######. It’s just a matter of style.\nIn markdown, a hash indicates a level of a header. And the number you use matters: # is a “level one header”, meaning the biggest font and the top of the hierarchy. ### is a level three header, and will show up nested below the # and ## headers.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nIn markdown, write some italic text, make a numbered list, and add a few sub-headers. Use the Markdown Quick Reference (in the menu bar: Help &gt; Markdown Quick Reference).\nRe-knit your html file and observe your edits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "session_02.html#the-visual-editor",
    "href": "session_02.html#the-visual-editor",
    "title": "2  Introduction to Quarto",
    "section": "2.5 The Visual Editor",
    "text": "2.5 The Visual Editor\nQuarto has a “what you see is what you mean” (WYSIWYM) editor or Visual editor, which can be a nice way to write markdown without remembering all of the markdown rules. Since there aren’t many rules for markdown, we recommend just learning them especially since markdown is used in many other contexts besides Quarto and RMarkdown. For example, formatting GitHub comments and README files.\nTo access the editor, click the Visual button in the upper left hand corner of your editor pane. You’ll notice that your document is now formatted as you type, and you can change elements of the formatting using the row of icons in the top of the editor pane. Although we don’t really recommend doing all of your markdown composition in the Visual editor, there are two features to this editor that we believe are immensely helpful: adding citations, and adding tables.\n\n2.5.1 Adding citations\nTo add a citation, go to the visual editor and in the insert drop down, select “Citation.” In the window that appears, there are several options in the left hand panel for the source of your citation. If you have a citation manager, such as Zotero, installed, this would be included in that list. For now, select “From DOI”, and in the search bar enter a DOI of your choice (e.g.: 10.1038/s41467-020-17726-z), then select “Insert.”\n\nAfter selecting insert, a couple of things happen. First, the citation reference is inserted into your markdown text as [@oke2020]. Second, a file called references.bib containing the BibTex format of the citation is created. Third, that file is added to the YAML header of your Quarto document (bibliography: references.bib). Adding another citation will automatically update your references.bib file. So easy!\n\n\n2.5.2 Adding table in markdown\nThe second task that the visual editor is convenient for is generating tables. Markdown tables are a bit finicky and annoying to type, and there are a number of formatting options that are difficult to remember if you don’t use them often. In the top icon bar, the “Table” drop down gives several options for inserting, editing, and formatting tables. Experiment with this menu to insert a small table.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "session_02.html#code-chunks-in-quarto",
    "href": "session_02.html#code-chunks-in-quarto",
    "title": "2  Introduction to Quarto",
    "section": "2.6 Code Chunks in Quarto",
    "text": "2.6 Code Chunks in Quarto\nEvery time when opening a new Quarto document, we should start by deleting all template text (everything except for the YAML). Then we save the document into the most convenient folder of our project. Now we are ready to start our work.\nYou can create a new chunk in your Quarto in one of these ways:\n\nGo to Code in the top menu bar, click “Insert Chunk”\nType by hand {r}\nUse the keyboard shortcut\n\nMac:command + option + i\nWindows: Ctrl + Alt + i\n\n\n\n\n\n\n\n\nAbout code chunks\n\n\n\nEach code chunk needs to have an opening syntax ```{r} and a closing syntax ```. Everything in between these lines will be identified as R code.\n\n\nIf I want to write some R code, this is how it would look like:\n\nx &lt;- 4 * 8\n\nheights_ft &lt;- c(5.2, 6.0, 5.7)\n\ncoef &lt;- 3.14\n\nHitting return does not execute this command; remember, it’s just a text file. To execute it, we need to get what we typed in the the R chunk (the grey R code) down into the console. How do we do it? There are several ways (let’s do each of them):\n\nCopy-paste this line into the console (generally not recommended as a primary method)\nSelect the line (or simply put the cursor there), and click “Run”. This is available from:\n\nthe bar above the file (green arrow)\nthe menu bar: Code &gt; Run Selected Line(s)\nkeyboard shortcut: command-return\n\nClick the green arrow at the right of the code chunk",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "session_02.html#practice-literate-analysis-with-ocean-water-samples",
    "href": "session_02.html#practice-literate-analysis-with-ocean-water-samples",
    "title": "2  Introduction to Quarto",
    "section": "2.7 Practice: Literate Analysis with Ocean Water Samples",
    "text": "2.7 Practice: Literate Analysis with Ocean Water Samples\nNow that we have gone over the basics, let’s go a little deeper by building a simple Quarto document that represents a literate analysis using real data. We are going to work with the seawater chemistry data. We are going to use the BGchem2008data.csv data we downloaded in our previous session.\n\n\n2.7.1 Getting Started\n\n\n\n\n\n\nSet up\n\n\n\n\nNavigate to this dataset by Craig Tweedie that is published on the Arctic Data Center. Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X.\nDownload the first csv file called BGchem2008data.csv by clicking the “download” button next to the file.\nClick the “Upload” button in your RStudio server file browser.\nIn the dialog box, make sure the destination directory is the data directory in your R project, click “Choose File,” and locate the BGchem2008data.csv file. Press “OK” to upload the file.\nCheck your file was successfully uploaded by navigating into your data folder in the Files pane.\n\n\n\nExperienced R users who have never used Quarto (or RMarkdown) often struggle a bit in the transition to developing analysis in Prose+Code format — which makes sense! It is switching the code paradigm to a new way of thinking.\nRather than starting an R chunk and putting all of your code in that single chunk, below we describe what we think is a better way.\n\nOpen a document and block out the high-level sections you know you’ll need to include using top level headers.\nAdd bullet points for some high level pseudo-code steps you know you’ll need to take.\nStart filling in, under each bullet point, the code that accomplishes each step. As you write your code, transform your bullet points into prose, and add new bullet points or sections as needed.\n\nFor this mini-analysis, we will have the following sections and code steps:\n\nIntroduction\n\nAbout the data\nSetup\nRead in data\n\nAnalysis\n\nCalculate summary statistics\nCalculate mean Redfield ratio\nPlot Redfield ratio\n\nConclusion\n\n\n\n\n\n\n\nExercise\n\n\n\nUnder “About the data”, write a sentence saying where the data set came from, including a hyperlink to the data. Also mention when the data was downloaded.\nHint: Navigate to Help &gt; Markdown Quick Reference to look-up the hyperlink syntax.\n\n\n\n\n2.7.2 Read in the data\nNow that we have outlined our document, we can start writing code! To read the data into our environment, we will use a function from the readr package.\nTo use a package in our analysis, we need to first make sure it is installed (you can install a package by running install.packages(\"name-of-package\")). Once installed you need to load it into our environment using library(package_name). Even though we have installed it, we haven’t yet told our R session to access it. Because there are so many packages (many with conflicting namespaces) R cannot automatically load every single package you have installed. Instead, you load only the ones you need for a particular analysis. Loading the package is a key part of the reproducible aspect of our literate analysis, so we will include it as an R chunk as part of our Setup.\n\n\n\n\n\n\nBest Practice\n\n\n\nIt is generally good practice to include all of your library() calls in a single, dedicated R chunk near the top of your document. This lets collaborators know what packages they might need to install before they start running your code.\n\n\nThe server should have already installed the two packages we need for now: readr and here. Let’s add a new R chunk below your Setup header that calls these libraries, and run it.\nIt should look like this:\n\nlibrary(readr)\nlibrary(here)\n\n\n\n\n\n\n\nQuarto file path and the here() function\n\n\n\nQuarto has a special way of handling relative paths that can be very handy. When working in a Quarto document, R will set all paths relative to the location of the Quarto file. This can make things easier to read in data if your Quarto document is stored in the same directory or “near” by. However, more often that not, your .qmd file will be stored in a a folder (e.g scripts) and your data in a data folder, (both folder in the main project directory).\nThe here() function helps navigate this file path mix up in a straight forward and reproducible way. This function sets the file path to the project’s directory and builds the rest of the file path from there. Making it easier to find files inside different folders in a project. In this case, because the .qmd file lives in the script folder, here() makes is easy to navigate back into the project’s directory and then into the data folder to read in our file.\n\n\nNow, under “Read data”, add a code chunk that uses the read_csv() with the here() function to read in your data file.\n\n\nRows: 70 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): Station\ndbl  (16): Latitude, Longitude, Target_Depth, CTD_Depth, CTD_Salinity, CTD_T...\ndttm  (1): Time\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nbg_chem &lt;- read_csv(here::here(\"data/BGchem2008data.csv\"))\n\n\n\nWhy read_csv() over read.csv()?\nWe chose to show read_csv() from the readr package to introduce the concept of packages, to show you how to load packages, and read_csv() has several advantages over read.csv() from base R, including:\n\nMore reasonable function defaults (no stringsAsFactors!)\nSmarter column type parsing, especially for dates\nread_csv() is much faster than read.csv(), which is helpful for large files\n\nOnce you run this line in your document, you should see the bg_chem object populate in your environment pane. It also spits out lots of text explaining what types the function parsed each column into. This text is important, and should be examined, but we might not want it in our final document.\n\n\n\n\n\n\nExercise\n\n\n\nHow would you suppress the warnings (so they don’t show in our output file) form a specific code chunk?\nHint: Code chunk options\n\n\n\n\n2.7.3 Calculate Summary Statistics\nAs our “analysis” we are going to calculate some very simple summary statistics and generate a single plot. Using water samples from the Arctic Ocean, we will examine the ratio of nitrogen to phosphate to see how closely the data match the Redfield ratio, which is the consistent 16:1 ratio of nitrogen to phosphorous atoms found in marine phytoplankton.\nLet’s start by exploring the data we just read. Every time we read a new data set, it is important to familiarize yourself with it and make sure that the data looks as expected. Below are some useful functions for exploring your data.\nLet’s start by creating a new R chunk and running the following functions. Because this is just an exploration, and we do not want this chunk to be part of our report, we will indicate that by adding #|eval: false and #| echo: false in the setup of the chunk, that way, the code in this chunk will not run and not be displayed when I knit the final document.\n\n## Prints the column names of my data frame\ncolnames(bg_chem)\n\n## General structure of the data frame - shows class of each column\nstr(bg_chem)\n\n## First 6 lines of the data frame\nhead(bg_chem)\n\n## Summary of each column of data\nsummary(bg_chem)\n\n## Prints unique values in a column (in this case Date)\nunique(bg_chem$Date)\n\nTo peek at the data frame, we can type View(bg_chem) in the console. This will open a tab with our data frame in a tabular format.\nNow that we know more about the data set we are working with, let’s do some analyses. Under the appropriate bullet point in your analysis section, create a new R chunk, and use it to calculate the mean nitrate (NO3), nitrite (NO2), ammonium (NH4), and phosphorous (P) measured.\nSave these mean values as new variables with easily understandable names, and write a (brief) description of your operation using markdown above the chunk. Remember that the $ (aka the subset operator) indicates which column of your data to look into.\n\nnitrate &lt;- mean(bg_chem$NO3)\nnitrite &lt;- mean(bg_chem$NO2)\namm &lt;- mean(bg_chem$NH4)\nphos &lt;- mean(bg_chem$P)\n\nIn another chunk, use those variables to calculate the nitrogen:phosphate ratio (Redfield ratio).\n\nratio &lt;- (nitrate + nitrite + amm)/phos\n\nYou can access this variable in your markdown text by using R in-line in your text. The syntax to call R in-line (as opposed to as a chunk) is a single back tick `, followed by the letter “r”, then whatever your simple R command is — here we will use round(ratio) to print the calculated ratio, and finally a closing back tick `. This allows us to access the value stored in this variable in our explanatory text without resorting to the evaluate-copy-paste method so commonly used for this type of task.\nSo, the text in you Quarto document should look like this:\nThe Redfield ratio for this dataset is approximately: `r round(ratio)`\nAnd the rendered text like this:\nThe Redfield ratio for this dataset is approximately 6.\nFinally, create a simple plot using base R that plots the ratio of the individual measurements, as opposed to looking at mean ratio.\n\nplot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nDecide whether or not you want the plotting code above to show up in your knitted document along with the plot, and implement your decision as a chunk option.\nRender your Quarto document (by pressing the Render button) and observe the results.\n\n\n\n\n\n\n\n\nHow do I decide when to make a new code chunk?\n\n\n\nLike many of life’s great questions, there is no clear cut answer. A rule of thumb is to have one chunk per functional unit of analysis. This functional unit could be 50 lines of code or it could be 1 line, but typically it only does one “thing.” This could be reading in data, making a plot, or defining a function. It could also mean calculating a series of related summary statistics (as we’ll see below). Ultimately, the choice is one related to personal preference and style, but generally you should ensure that code is divided up such that it is easily explainable in a literate analysis as the code is run.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "session_02.html#quarto-and-environments",
    "href": "session_02.html#quarto-and-environments",
    "title": "2  Introduction to Quarto",
    "section": "2.8 Quarto and Environments",
    "text": "2.8 Quarto and Environments\nLet’s walk through an exercise with the document we just created to demonstrate how Quarto handles environments. We will be deliberately inducing some errors here for demonstration purposes.\nFirst, follow these steps:\n\n\n\n\n\n\nSetup\n\n\n\n\nRestart your R session (Session &gt; Restart R)\nRun the last chunk in your Quarto document by pressing the play button on the chunk\n\n\n\nPerhaps not surprisingly, we get an error:\nError in plot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4) : \n  object 'bg_chem' not found\nThis is because we have not run the chunk of code that reads in the bg_chem data. The R part of Quarto works just like a regular R script. You have to execute the code, and the order that you run it in matters. It is relatively easy to get mixed up in a large Quarto document — running chunks out of order, or forgetting to run chunks.\nTo resolve this, follow the next step:\n\n\n\n\n\n\nSetup continued\n\n\n\n\nSelect from the “Run” menu (top right of the editor pane) “Run All.”\nObserve the bg_chem variable in your environment\n\n\n\nThis is a great way to reset and re-run code when things seem to have gone sideways. It is great practice to do periodically since it helps ensure you are writing code that actually runs and it’s reproducible.\n\n\n\n\n\n\nFor the next exercise:\n\n\n\n\nClean your environment by clicking the broom in the environment pane\nRestart your R session (Session &gt; Restart R)\nPress “Render” to run all of the code in your document\nObserve the state of your environment pane\n\nAssuming your document rendered and produced an html page, your code ran. Yet, the environment pane is empty. What happened?\n\n\nThe Render button is rather special — it doesn’t just run all of the code in your document. It actually spins up a fresh R environment separate from the one you have been working in, runs all of the code in your document, generates the output, and then closes the environment. This is one of the best ways Quarto (or RMarkdown) helps ensure you have built a reproducible workflow. If, while you were developing your code, you ran a line in the console as opposed to adding it to your Quarto document, the code you develop while working actively in your environment will still work. However, when you knit your document, the environment RStudio spins up doesn’t know anything about that working environment you were in. Thus, your code may error because it doesn’t have that extra piece of information. Commonly, library() calls are the source of this kind of frustration when the author runs it in the console, but forgets to add it to the script.\nTo further clarify the point on environments, perform the following steps:\n\n\n\n\n\n\nSetup continued\n\n\n\n\nSelect from the “Run” menu (top right of editor pane) “Run All”\nObserve all of the variables in your environment\n\n\n\n\n\n\n\n\n\nWhat about all my R Scripts?\n\n\n\nSome pieces of R code are better suited for R scripts than Quarto or RMarkdown. A function you wrote yourself that you use in many different analyses is probably better to define in an R script than repeated across many Quarto or RMarkdown documents. Some analyses have mundane or repetitive tasks that don’t need to be explained very much. For example, in the document shown in the beginning of this lesson, 15 different excel files needed to be reformatted in slightly different, mundane ways, like renaming columns and removing header text. Instead of including these tasks in the primary Quarto document, the authors chose to write one R script per file and stored them all in a directory. Then, took the contents of one script and included it in the literate analysis, using it as an example to explain what the scripts did, and then used the source() function to run them all from within the Quarto document.\nSo, just because you know Quarto now, doesn’t mean you won’t be using R scripts anymore. Both .R and .qmd have their roles to play in analysis. With practice, it will become more clear what works well in Quarto or RMarkdown, and what belongs in a regular R script.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "session_02.html#additional-quarto-resources",
    "href": "session_02.html#additional-quarto-resources",
    "title": "2  Introduction to Quarto",
    "section": "2.9 Additional Quarto Resources",
    "text": "2.9 Additional Quarto Resources\n\nPosit (the organization that developed Quarto) has great documentation, check out Quarto.org\nR for Data Science (2e) (Wickham et al, 2023), this is an awesome book for all R related things. Chapter 29 and 30 are specific to Quarto.\nQuarto Gallery: Example of different outputs created using Quarto\nHello Quarto: share, collaborate, teach, reimagine. A talk by Julia Stewart Lowndes and Mine Cetinkaya-Runde.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "session_02.html#troubleshooting-my-rmarkdownquarto-doc-wont-knit-to-pdf",
    "href": "session_02.html#troubleshooting-my-rmarkdownquarto-doc-wont-knit-to-pdf",
    "title": "2  Introduction to Quarto",
    "section": "2.10 Troubleshooting: My RMarkdown/Quarto doc Won’t Knit to PDF",
    "text": "2.10 Troubleshooting: My RMarkdown/Quarto doc Won’t Knit to PDF\nIf you get an error when trying to knit to PDF that says your computer doesn’t have a LaTeX installation, one of two things is likely happening:\n\nYour computer doesn’t have LaTeX installed\nYou have an installation of LaTeX but RStudio cannot find it (it is not on the path)\n\nIf you already use LaTeX (like to write papers), you fall in the second category. Solving this requires directing RStudio to your installation - and isn’t covered here.\nIf you fall in the first category - you are sure you don’t have LaTeX installed - can use the R package tinytex to easily get an installation recognized by RStudio, as long as you have administrative rights to your computer.\nTo install tinytex run:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nIf you get an error that looks like destination /usr/local/bin not writable, you need to give yourself permission to write to this directory (again, only possible if you have administrative rights). To do this, run this command in the terminal:\nsudo chown -R `whoami`:admin /usr/local/bin\nand then try the above install instructions again. Learn more about tinytex from Yihui Xie’s online book TinyTeX. ````\n\n\n\n\nBritish Ecological Society, Mike, Croucher. 2017. “A Guide to Reproducible Code in Ecology and Evolution.” British Ecological Society. https://www.britishecologicalsociety.org/wp-content/uploads/2017/12/guide-to-reproducible-code.pdf.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Quarto</span>"
    ]
  },
  {
    "objectID": "session_03.html",
    "href": "session_03.html",
    "title": "3  Data Management Plans",
    "section": "",
    "text": "3.1 Writing Good Data Management Plans",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Plans</span>"
    ]
  },
  {
    "objectID": "session_03.html#writing-good-data-management-plans",
    "href": "session_03.html#writing-good-data-management-plans",
    "title": "3  Data Management Plans",
    "section": "",
    "text": "3.1.1 Learning Objectives\nIn this lesson, you will learn:\n\nWhy to create data management plans\nThe major components of data management plans\nTools that can help create a data management plan\nFeatures and functionality of the DMPTool\n\n\n\n3.1.2 When to Plan: The Data Life Cycle\nShown below is one version of the Data Life Cycle that was developed by DataONE. The data life cycle provides a high level overview of the stages involved in successful management and preservation of data for use and reuse. Multiple versions of the data life cycle exist with differences attributable to variation in practices across domains or communities. It is not necessary for researchers to move through the data life cycle in a cyclical fashion and some research activities might use only part of the life cycle. For instance, a project involving meta-analysis might focus on the Discover, Integrate, and Analyze steps, while a project focused on primary data collection and analysis might bypass the Discover and Integrate steps. However, Plan is at the top of the data life cycle as it is advisable to initiate your data management planning at the beginning of your research process, before any data has been collected.\n\n\n\n3.1.3 Why Plan?\nPlanning data management in advance provides a number of benefits to the researcher.\n\nSaves time and increases efficiency: Data management planning requires that a researcher think about data handling in advance of data collection, potentially raising any challenges before they occur.\nEngages your team: Being able to plan effectively will require conversations with multiple parties, engaging project participants from the outset.\nAllows you to stay organized: It will be easier to organize your data for analysis and reuse if you’ve made a plan about what analysis you want to run, future iterations, and more.\nMeet funder requirements: Most funders require a data management plan (DMP) as part of the proposal process.\nShare data: Information in the DMP is the foundation for archiving and sharing data with community.\n\n\n\n3.1.4 How to Plan\n\nMake sure to plan from the start to avoid confusion, data loss, and increase efficiency. Given DMPs are a requirement of funding agencies, it is nearly always necessary to plan from the start. However, the same should apply to research that is being undertaken outside of a specific funded proposal.\nAs indicated above, engaging your team is a benefit of data management planning. Collaborators involved in the data collection and processing of your research data bring diverse expertise. Therefore, plan in collaboration with these individuals.\nMake sure to utilize resources that are available to assist you in helping to write a good DMP. These might include your institutional library or organization data manager, online resources or education materials such as these.\nUse tools available to you; you don’t have to reinvent the wheel.\nRevise your plan as situations change or as you potentially adapt/alter your project. Like your research projects, DMPs are not static, they require changes and updates throughout the research project process.\n\n\n\n3.1.5 What to include in a DMP\nIf you are writing a DMP as part of a solicitation proposal, the funding agency will have guidelines for the information they want to be provided in the plan. However, in general, a good plan will provide information on the:\n\nstudy design\ndata to be collected\nmetadata\npolicies for access\nsharing & reuse\nlong-term storage & data management\nand budget\n\nA note on Metadata: Both basic metadata (such as title and researcher contact information) and comprehensive metadata (such as complete methods of data collection) are critical for accurate interpretation and understanding. The full definitions of variables, especially units, inside each dataset are also critical as they relate to the methods used for creation. Knowing certain blocking or grouping methods, for example, would be necessary to understand studies for proper comparisons and synthesis.\n\n\n3.1.6 NSF DMP requirements\nIn the 2014 Proposal Preparation Instructions, Section J ‘Special Information and Supplementary Documentation’ NSF put forward the baseline requirements for a DMP. In addition, there are specific division and program requirements that provide additional detail. If you are working on a research project with funding that does not require a DMP, or are developing a plan for unfunded research, the NSF generic requirements are a good set of guidelines to follow.\nThe following questions are the prompting questions in the Arctic Data Center DMP template for NSF projects, excluding the fairly straightforward personnel section.\nFive Sections of the NSF DMP Requirements\n1. What types of data, samples, collections, software, materials, etc. will be produced during your project?\nTypes of data, samples, physical collections, software, curriculum materials, other materials produced during project\n2. What format(s) will data and metadata be collected, processed, and stored in?\nStandards to be used for data and metadata format and content (for initial data collection, as well as subsequent storage and processing)\n3. How will data be accessed and shared during the course of the project?\nProvisions for appropriate protection of privacy, confidentiality, security, intellectual property, or other rights or requirements\n4. How do you anticipate the data for this project will be used?\nIncluding re-distribution and the production of derivatives\n5. What is the long-term strategy for maintaining, curating, and archiving the data?\nPlans for archiving data, samples, research products and for preservation of access\nThe ‘NSF Public Access Plan 2.0’ recently went through public comment, and proposes that DMPs be renamed to Data Management and Sharing Plans (DMSP) and include requirements to ensure that data is submitted to data repositories. Any new requirements will be shared with researchers once this plan goes into effect.\nCurrently, the NSF is revising DMP requirements, and there will be changes coming to NSF DMPs at the release of the next Proposal and Award Policies and Procedures Guide (PAPPG). The draft revisions of the PAPPG is online and accessible for review here. The section on Data Management and Sharing Plans (DMSPs) begins on section II-31 (page 70 in the PDF) and is highlighted in yellow with an introductory comment explaining the revised section.\n\n3.1.6.1 Individual Reflection\nNow that we’ve discussed the data life cycle, how to plan, what to generally include in a DMP, and the NSF DMP requirements - take five minutes to go through each required section for a NSF DMP and write down some initial thoughts on how you would approach completing those sections. What information would you include? How would you plan to answer the questions? What do you need to answer the questions in each section?\nAnother consideration for your data management plan is the extent to which you can document ethical research practices. We’ll go over CARE and FAIR principles in future lessons, but a DMP can be a good place to include context on:\n\npermits needed for your research\ncodes of conduct the research team decided upoon prior to beginning data collection\ninstitutional or local permission needed for sampling\nthe effect on local community members and/or Indigenous lands\nwhether or not authorship expectations are clear for everyone involved in data collection\n\nAfter we’ll get into groups to further discuss.\n\n\n3.1.6.2 Group Discussion\nLet’s split up into five groups; one group for each required section of a NSF DMP. As a group, share your initial thoughts about the section you’ve been assigned to and together as a group discuss how you would complete that section. Select someone in the group to share your approach to the whole class. Take the next 10-15 minutes for group discussion.\nSome guiding questions:\n\nWhat information do you need to complete the section? Think both broadly and detailed.\nDo you need to reference outside materials to complete the section? Is this information already known / found or is additional research required?\nWhat is the relevant, key information necessary for the research to be understood for either your future self or for someone new to the data? What information would you want to know if you were given a new project to work on? Being explicit and including details are important to think about for this question.\nWhat workflows, documentation, standards, maintenance, tools / software, or roles are required?\n\n\n\n\n3.1.7 Tools in Support of Creating a DMP\n\nThe DMPTool and DMP Online are both easy-to-use web based tools that support the development of a DMP. The tools are partnered and share a code base; the DMPTool incorporates templates from US funding agencies and the DMP Online is focused on EU requirements.\n\n\n3.1.7.1 Quick Tips for DMPTool\n\nThere is no requirement to answer all questions in one sitting. Completing a DMP can require information gathering from multiple sources. Saving the plan at any point does not submit the plan, it simply saves your edits. This means you can move between sections in any order or save as you go.\nYou can collaborate in DMPTool which keeps all commentary together, saves time on collaboration, and makes it easy to access the most current version at any time since it is always available in DMPTool.\n\n\n\n\n3.1.8 Arctic Data Center Support for DMPs\nTo support researchers in creating DMPs that fulfills NSF template requirements and provides guidance on how to work with the Arctic Data Center for preservation, we have created an Arctic Data Center template within the DMPTool. This template walks researchers through the questions required by NSF and includes recommendations directly from the Arctic Data Center team.\n\nWhen creating a new plan, indicate that your funding agency is the National Science Foundation and you will then have the option to select a template. Here you can choose the Arctic Data Center.\n\nAs you answer the questions posed, guidance information from the Arctic Data Center will be visible under the ‘NSF’ tab on the right hand side. An example answer is also provided at the bottom. It is not intended that you copy and paste this verbatim. Rather, this is example prose that you can refer to for answering the question.\n\n\n\n3.1.9 Sharing Your DMP\nThe DMPTool allows you to collaborate with authorized individuals on your plan and also to publish it (make it publicly accessible) via the website. If your research activity is funded, it is also useful to share your DMP with the Arctic Data Center. This is not an NSF requirement, but can greatly help the Arctic Data Center team prepare for your data submission. Especially if you are anticipating a high volume of data.\n\n\n3.1.10 Additional Resources\nThe article Ten Simple Rules for Creating a Good Data Management Plan is a great resource for thinking about writing a data management plan and the information you should include within the plan.\n\n\n\n3.1.11 Summary\n\nA good DMP will save you time and effort overall\nDMPs are not static and should be revised throughout your research project\nTake advantage of DMP resources to create your plan\nThe Arctic Data Center curation team is available to assist with the development of your DMP",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Plans</span>"
    ]
  },
  {
    "objectID": "session_04.html",
    "href": "session_04.html",
    "title": "4  Submitting to the Arctic Data Center",
    "section": "",
    "text": "4.1 Learning Objectives\nIn this lesson, you will learn:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Submitting to the Arctic Data Center</span>"
    ]
  },
  {
    "objectID": "session_04.html#learning-objectives",
    "href": "session_04.html#learning-objectives",
    "title": "4  Submitting to the Arctic Data Center",
    "section": "",
    "text": "About open data archives, especially the Arctic Data Center\nWhat science metadata are and how they can be used\nHow data and code can be documented and published in open data archives\n\nWeb-based submission",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Submitting to the Arctic Data Center</span>"
    ]
  },
  {
    "objectID": "session_04.html#data-sharing-and-preservation",
    "href": "session_04.html#data-sharing-and-preservation",
    "title": "4  Submitting to the Arctic Data Center",
    "section": "4.2 Data sharing and preservation",
    "text": "4.2 Data sharing and preservation\n\n\n4.2.1 Data repositories: built for data (and code)\n\nGitHub is not an archival location\nExamples of dedicated data repositories: KNB, Arctic Data Center, tDAR, EDI, Zenodo\n\nRich metadata\nArchival in their mission\nCertification for repositories: https://www.coretrustseal.org/\n\nData papers, e.g., Scientific Data\nList of data repositories: http://re3data.org\n\nRepository finder tool: https://repositoryfinder.datacite.org/\n\n\n\n\n\n4.2.2 DataONE Federation\nDataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses.\nDataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the (currently 44!) repositories one at a time.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Submitting to the Arctic Data Center</span>"
    ]
  },
  {
    "objectID": "session_04.html#metadata",
    "href": "session_04.html#metadata",
    "title": "4  Submitting to the Arctic Data Center",
    "section": "4.3 Metadata",
    "text": "4.3 Metadata\nMetadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where they were collected, and why they were collected.\nFor consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the metadata for a sockeye salmon dataset:\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;eml:eml packageId=\"df35d.442.6\" system=\"knb\" \n    xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.1\"&gt;\n    &lt;dataset&gt;\n        &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through \n            Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt;\n        &lt;creator id=\"1385594069457\"&gt;\n            &lt;individualName&gt;\n                &lt;givenName&gt;Mark&lt;/givenName&gt;\n                &lt;surName&gt;Willette&lt;/surName&gt;\n            &lt;/individualName&gt;\n            &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt;\n            &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt;\n            &lt;address&gt;\n                &lt;city&gt;Soldotna&lt;/city&gt;\n                &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt;\n                &lt;country&gt;USA&lt;/country&gt;\n            &lt;/address&gt;\n            &lt;phone phonetype=\"voice\"&gt;(907)260-2911&lt;/phone&gt;\n            &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt;\n        &lt;/creator&gt;\n        ...\n    &lt;/dataset&gt;\n&lt;/eml:eml&gt;\nThat same metadata document can be converted to HTML format and displayed in a much more readable form on the web: https://knb.ecoinformatics.org/#view/doi:10.5063/F1F18WN4\n And, as you can see, the whole dataset or its components can be downloaded and reused.\nAlso note that the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data.\n\n4.3.1 Overall Guidelines\nAnother way to think about metadata is to answer the following questions with the documentation:\n\nWhat was measured?\nWho measured it?\nWhen was it measured?\nWhere was it measured?\nHow was it measured?\nHow is the data structured?\nWhy was the data collected?\nWho should get credit for this data (researcher AND funding agency)?\nHow can this data be reused (licensing)?\n\n\n\n4.3.2 Bibliographic Guidelines\nThe details that will help your data be cited correctly are:\n\nGlobal identifier like a digital object identifier (DOI)\nDescriptive title that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data\nDescriptive abstract that serves as a brief overview off the specific contents and purpose of the data package\nFunding information like the award number and the sponsor\nPeople and organizations like the creator of the dataset (i.e. who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset\n\n\n\n4.3.3 Discovery Guidelines\nThe details that will help your data be discovered correctly are:\n\nGeospatial coverage of the data, including the field and laboratory sampling locations, place names and precise coordinates\nTemporal coverage of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to\nTaxonomic coverage of the data, including what species were measured and what taxonomy standards and procedures were followed\nAny other contextual information as needed\n\n\n\n4.3.4 Interpretation Guidelines\nThe details that will help your data be interpreted correctly are:\n\nCollection methods for both field and laboratory data the full experimental and project design as well as how the data in the dataset fits into the overall project\nProcessing methods for both field and laboratory samples\nAll sample quality control procedures\nProvenance information to support your analysis and modelling methods\nInformation about the hardware and software used to process your data, including the make, model, and version\nComputing quality control procedures like testing or code review\n\n\n\n4.3.5 Data Structure and Contents\n\nEverything needs a description: the data model, the data objects (like tables, images, matrices, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation.\nVariable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (i.e. 0 = not collected), and any missing values (i.e. 999 = NA).\n\nNot only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable).\nFor example, if you were to search for the character string “carbon dioxide flux” in a data repository, not all relevant results will be shown due to varying vocabulary conventions (i.e., “CO2 flux” instead of “carbon dioxide flux”) across disciplines — only datasets containing the exact words “carbon dioxide flux” are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search.\n\n\n4.3.6 Rights and Attribution\nCorrectly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like:\n\nCitation format to be used when giving credit for the data\nAttribution expectations for the dataset\nReuse rights, which describe who may use the data and for what purpose\nRedistribution rights, which describe who may copy and redistribute the metadata and the data\nLegal terms and conditions like how the data are licensed for reuse.\n\n\n\n4.3.7 Metadata Standards\nSo, how does a computer organize all this information? There are a number of metadata standards that make your metadata machine readable and therefore easier for data curators to publish your data.\n\nEcological Metadata Language (EML)\nGeospatial Metadata Standards (ISO 19115 and ISO 19139)\n\nSee NOAA’s ISO Workbook\n\nDublin Core\nDarwin Core\nPREservation Metadata: Implementation Strategies (PREMIS)\nMetadata Encoding Transmission Standard (METS)\n\nNote this is not an exhaustive list.\n\n\n4.3.8 Data Identifiers\nMany journals require a DOI (a digital object identifier) be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to.\nSome data repositories assign a DOI for each dataset you publish on their repository. But, if you need to update the datasets, check the policy of the data repository. Some repositories assign a new DOI after you update the dataset. If this is the case, researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available.\n\n\n4.3.9 Data Citation\nResearchers should get in the habit of citing the data that they use (even if it’s their own data!) in each publication that uses that data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Submitting to the Arctic Data Center</span>"
    ]
  },
  {
    "objectID": "session_04.html#structure-of-a-data-package",
    "href": "session_04.html#structure-of-a-data-package",
    "title": "4  Submitting to the Arctic Data Center",
    "section": "4.4 Structure of a data package",
    "text": "4.4 Structure of a data package\nNote that the dataset above lists a collection of files that are contained within the dataset. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\n\nThese data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4,and each of the individual files have their own identifiers as well.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Submitting to the Arctic Data Center</span>"
    ]
  },
  {
    "objectID": "session_04.html#publishing-data-from-the-web",
    "href": "session_04.html#publishing-data-from-the-web",
    "title": "4  Submitting to the Arctic Data Center",
    "section": "4.5 Publishing data from the web",
    "text": "4.5 Publishing data from the web\nEach data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the Arctic Data Center, we provide some easy to use web forms for editing and submitting a data package. This section provides a brief overview of some highlights within the data submission process, in advance of a more comprehensive hands-on activity.\nORCiDs\nWe will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button.\n\n\nORCID is a non-profit organization made up of research institutions, funders, publishers and other stakeholders in the research space. ORCID stands for Open Researcher and Contributor ID. The purpose of ORCID is to give researchers a unique identifier which then helps highlight and give credit to researchers for their work. If you click on someone’s ORCID, their work and research contributions will show up (as long as the researcher used ORCID to publish or post their work).\nAfter signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata.\n\nSensitive Data Handling\nUnderneath the Title field, you will see a section titled “Data Sensitivity”. As the primary repository for the NSF Office of Polar Programs Arctic Section, the Arctic Data Center accepts data from all disciplines. This includes data from social science research that may include sensitive data, meaning data that contains personal or identifiable information. Sharing sensitive data can pose challenges to researchers, however sharing metadata or anonymized data contributes to discovery, supports open science principles, and helps reduce duplicate research efforts.\nTo help mitigate the challenges of sharing sensitive data, the Arctic Data Center has added new features to the data submission process influenced by the CARE Principles for Indigenous Data Governance (Collective benefit, Authority to control, Responsibility, Ethics). Researchers submitting data now have the option to choose between varying levels of sensitivity that best represent their dataset. Data submitters can select one of three sensitivity level data tags that best fit their data and/or metadata. Based on the level of sensitivity, guidelines for submission are provided. The data tags range from non-confidential information to maximally sensitive information.\nThe purpose of these tags is to ethically contribute to open science by making the richest set of data available for future research. The first tag, “non-sensitive data”, represents data that does not contain potentially harmful information, and can be submitted without further precaution. Data or metadata that is “sensitive with minimal risk” means that either the sensitive data has been anonymized and shared with consent, or that publishing it will not cause any harm. The third option, “some or all data is sensitive with significant risk” represents data that contains potentially harmful or identifiable information, and the data submitter will be asked to hold off submitting the data until further notice. In the case where sharing anonymized sensitive data is not possible due to ethical considerations, sharing anonymized metadata still aligns with FAIR (Findable, Accessible, Interoperable, Reproducible) principles because it increases the visibility of the research which helps reduce duplicate research efforts. Hence, it is important to share metadata, and to publish or share sensitive data only when consent from participants is given, in alignment with the CARE principles and any IRB requirements.\nYou will continue to be prompted to enter information about your research, and in doing so, create your metadata record. We recommend taking your time because the richer your metadata is, the more easily reproducible and usable your data and research will be for both your future self and other researchers. Detailed instructions are provided below for the hands-on activity.\nResearch Methods\nMethods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software.\n\nAs part of a recent update, researchers are now asked to describe the ethical data practices used throughout their research. The information provided will be visible as part of the metadata record. This feature was added to the data submission process to encourage transparency in data ethics. Transparency in data ethics is a vital part of open science and sharing ethical practices encourages deeper discussion about data reuse and ethics.\nWe encourage you to think about the ethical data and research practices that were utilized during your research, even if they don’t seem obvious at first.\nFile and Variable Level Metadata\nIn addition to providing information about, (or a description of) your dataset, you can also provide information about each file and the variables within the file. By clicking the “Describe” button you can add comprehensive information about each of your measurements, such as the name, measurement type, standard units etc.\n\nProvenance\nThe data submission system also provides the opportunity for you to provide provenance information, describe the relationship between package elements. When viewing your dataset followinng submission, After completing your data description and submitting your dataset you will see the option to add source data and code, and derived data and code.\n\nThese are just some of the features and functionality of the Arctic Data Center submission system and we will go through them in more detail below as part of a hands-on activity.\n\n4.5.1 Download the data to be used for the tutorial\nI’ve already uploaded the test data package, and so you can access the data here:\n\nhttps://demo.arcticdata.io/#view/urn:uuid:0702cc63-4483-4af4-a218-531ccc59069f\n\nGrab both CSV files, and the R script, and store them in a convenient folder.\n\n\n\n4.5.2 Login via ORCID\nWe will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button.\n When you sign in, you will be redirected to orcid.org, where you can either provide your existing ORCID credentials or create a new account. ORCID provides multiple ways to login, including using your email address, an institutional login from many universities, and/or a login from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login.\n\n\n\n4.5.3 Create and submit the dataset\nAfter signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata.\n\n4.5.3.1 Click Add Files to choose the data files for your package\nYou can select multiple files at a time to efficiently upload many files.\n\nThe files will upload showing a progress indicator. You can continue editing metadata while they upload.\n\n\n\n4.5.3.2 Enter Overview information\nThis includes a descriptive title, abstract, and keywords.\n  You also must enter a funding award number and choose a license. The funding field will search for an NSF award identifier based on words in its title or the number itself. The licensing options are CC-0 and CC-BY, which both allow your data to be downloaded and re-used by other researchers.\n\nCC-0 Public Domain Dedication: “…can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.”\nCC-BY: Attribution 4.0 International License: “…free to…copy,…redistribute,…remix, transform, and build upon the material for any purpose, even commercially,…[but] must give appropriate credit, provide a link to the license, and indicate if changes were made.”\n\n \n\n\n4.5.3.3 People Information\nInformation about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people:\n\nCreators - all the people who should be in the citation for the dataset\nContacts - one is required, but defaults to the first Creator if omitted\nPrincipal Investigators\nAny others that are relevant\n\nFor each, please provide their ORCID identifier, which helps link this dataset to their other scholarly works.\n\n\n\n4.5.3.4 Location Information\nThe geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that spatial search will be more precise.\n\nNote that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global:\n\nMission Canyon, Santa Barbara, California, USA\n\n\n\n\n4.5.3.5 Temporal Information\nAdd the temporal coverage of the data, which represents the time period to which data apply. Again, use multiple date ranges if your sampling was discontinuous.\n\n\n\n4.5.3.6 Methods\nMethods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data.\n\n\n\n4.5.3.7 Save a first version with Submit\nWhen finished, click the Submit Dataset button at the bottom.\nIf there are errors or missing fields, they will be highlighted.\nCorrect those, and then try submitting again. When you are successful, you should see a large green banner with a link to the current dataset view. Click the X to close that banner if you want to continue editing metadata.\n Success!\n\n\n\n4.5.4 File and variable level metadata\nThe final major section of metadata concerns the structure and content of your data files. In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure.\nFor example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information.\n The Attributes tab is where you enter variable (aka attribute) information, including:\n\nvariable name (for programs)\nvariable label (for display)\n\n - variable definition (be specific) - type of measurement  - units & code definitions\n You’ll need to add these definitions for every variable (column) in the file. When done, click Done.  Now, the list of data files will show a green checkbox indicating that you have fully described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes.\n After you get the big green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes.\n\n\n4.5.5 Add workflow provenance\nUnderstanding the relationships between files (aka provenance) in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, which are often then used in analysis and visualization code to produce final outputs. In the DataONE network, we support structured descriptions of these relationships, so researchers can see the flow of data from raw data to derived to outputs.\nYou add provenance by navigating to the data table descriptions and selecting the Add buttons to link the data and scripts that were used in your computational workflow. On the left side, select the Add circle to add an input data source to the filteredSpecies.csv file. This starts building the provenance graph to explain the origin and history of each data object.\n The linkage to the source dataset should appear.\n\nThen you can add the link to the source code that handled the conversion between the data files by clicking on Add arrow and selecting the R script:\n   The diagram now shows the relationships among the data files and the R script, so click Submit to save another version of the package.\n Et voilà! A beautifully preserved data package!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Submitting to the Arctic Data Center</span>"
    ]
  },
  {
    "objectID": "session_04.html#bonus-exercise-evaluate-a-data-package-on-the-adc-repository",
    "href": "session_04.html#bonus-exercise-evaluate-a-data-package-on-the-adc-repository",
    "title": "4  Submitting to the Arctic Data Center",
    "section": "4.6 Bonus Exercise: Evaluate a Data Package on the ADC Repository",
    "text": "4.6 Bonus Exercise: Evaluate a Data Package on the ADC Repository\nExplore data packages published on the ADC assess the quality of their metadata. Imagine you’re a data curator!\n\n\n\n\n\n\nSetup\n\n\n\n\nBreak into groups and use the following data packages:\n\nGroup A: ADC Data Portal White spruce (Picea glauca) densities at Brooks Range treelines, Alaska (2019-2022)\nGroup B: ADC Data Portal A 2022 household survey about impacts and human response to climate-related multi-hazards in Anchorage, Fairbanks, and Whitehorse\nGroup C: ADC Data Portal Summertime water quality measurements at beaver ponds and associated locations in Arctic Alaska, 2022-2026\n\n\n\n\nYou and your group will evaluate a data package for its: (1) metadata quality, (2) data documentation quality for reproducibility, and (3) FAIRness and CAREness.\n\n\n\n\n\n\nExercise: Evaluate a data package on the ADC Data Portal\n\n\n\n\nView our Data Package Assessment Rubric and make a copy of it to:\n\nInvestigate the metadata in the provided data\n\nDoes the metadata meet the standards we talked about? How so?\nIf not, how would you improve the metadata based on the standards we talked about?\n\nInvestigate the overall data documentation in the data package\n\nIs the documentation sufficient enough for reproducibility? Why or why not?\nIf not, how would you improve the data documentation? What’s missing?\n\nIdentify elements of FAIR and CARE\n\nIs it clear that the data package used a FAIR and CARE lens?\nIf not, what documentation or considerations would you add?\n\n\nElect someone to share back to the group the following:\n\nHow easy or challenging was it to find the metadata and other data documentation you were evaluating? Why or why not?\nWhat documentation stood out to you? What did you like or not like about it?\nHow well did these data packages uphold FAIR and CARE Principles?\nDo you feel like you understand the research project enough to use the data yourself (aka reproducibility?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Submitting to the Arctic Data Center</span>"
    ]
  },
  {
    "objectID": "session_05.html",
    "href": "session_05.html",
    "title": "5  Introduction to git and GitHub",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_05.html#learning-objectives",
    "href": "session_05.html#learning-objectives",
    "title": "5  Introduction to git and GitHub",
    "section": "",
    "text": "Set global options in your .gitconfig file\nPractice how to set up GitHub Authentication using a Personal Access Token (PAT)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_05.html#set-up-global-options-in-git",
    "href": "session_05.html#set-up-global-options-in-git",
    "title": "5  Introduction to git and GitHub",
    "section": "5.1 Set up global options in Git",
    "text": "5.1 Set up global options in Git\nBefore using Git, you need to tell it who you are, also known as setting the global options. You can do this either on the terminal using git command or in the console using the R package usethis. For this lesson we will use the usethis package. However, you can also find the git commands to reference in the future.\n\n\n\n\n\n\nWhat’s the Terminal?\n\n\n\nTechnically, the Terminal is an interface for the shell, a computer program. To put that simply, we use the Terminal to tell a computer what to do. This is different from the Console in RStudio, which interprets R code and returns a value.\nYou can access the terminal through RStudio by clicking Tools &gt; Terminal &gt; New Terminal.\nA Terminal tab should now be open right next to the Console tab.\n\n\n\n\n\n\n\n\nDon’t be afraid to dip your toes in the Terminal\n\n\n\nMost of our Git operations will be done in RStudio, but there are some situations where you must work in the Terminal and use command line. It may be daunting to code in the Terminal, but as your comfort increases over time, you might find you prefer it. Either way, it’s beneficial to learn enough command line and to feel comfortable in the Terminal.\n\n\nTo introduce ourselves to git we are going to use the function usethis::use_git_config(), followed by usethis::git_default_branch_configure(). This will update our global options with our GitHub user name and email information.\nBelow you will find code to do this in the console using functions from the usethis package and the terminal using git commands.\nMake sure to type your exact GitHub username and email associated to your GitHub account.\n\nConsoleTerminal\n\n\nStep 1: set the user’s global user.name and user.email and define integrate changes from one branch into another branch for all repositories.\n\n\nAnswer\n1usethis::use_git_config(user.name = \"my_user_name\",\n2                        user.email = \"my_email@nceas.ucsb.edu\",\n3                        pull.rebase = \"false\")\n\n\n\n1\n\nAdd you exact same GitHub user name. Case and spelling matters!\n\n2\n\nSet up your email address associated to you GitHub account.\n\n3\n\nSetting “merge” as the default strategy to integrate changes from one branch into another branch (for all repos). Check the note at the end of this chapter for more details.\n\n\n\n\nStep 2: define the name of the branch that gets created when you make the first commit in a new Git repo\n\n\nAnswer\nusethis::git_default_branch_configure(name = \"main\")\n\n\nStep 3: check to make sure everything looks correct\n\n\nAnswer\nusethis::git_sitrep()\n\n\n\n\nStep 1: set the user’s global user.name and user.email and define merge strategy for all repositories.\ngit config --global user.name \"my_user_name\"\nPress enter/return.\nNote that if the code ran successfully, it will look like nothing happened. We will check at the end to make sure it worked.\nThen run:\ngit config --global user.email \"my_email@nceas.ucsb.edu\"\nPress enter/return.\nThen run:\ngit config --global pull.rebase false\nStep 2: define the name of the branch that gets created when you make the first commit in a new Git repo.\ngit config --global init.defaultBranch main\nStep 3: check to make sure everything looks correct.\nThe following command return the global options you have set.\ngit config --global --list\n\n\n\n\n\n\n\n\n\n\nCase and spelling matters!\n\n\n\nWhen you add your username and email to the global options you must use the exact same spelling and case that you used on GitHub otherwise, Git won’t be able to sync to your account.\n\n\n\n\n\n\n\n\nWhy set the default branch name to main?\n\n\n\nPreviously, the default branch name was master and this racist terminology for Git branches motivates us to update our default branch to main instead.\n\n\n\nSet a long timeout for the git cache\nFinally, we will run a step that is only necessary when working on a server. We need to set our credentials to not time out for a very long time. This is related to how our server operating system handles credentials - not doing this will make your Personal Access Token (PAT, which we will set up in the next section) expire after 15 min on the system, even though it is actually valid for at least a month. We will do this configuration in the terminal.\nYou can access the terminal through RStudio by clicking Tools &gt; Terminal &gt; New Terminal.\n\n\n\n\n\n\nTHIS ONLY NEEDS TO BE RUN ON THE SERVER\n\n\n\nDO NOT RUN THE NEXT LINE when setting up Git and GitHub on your Personal Computer\n\n\nBy running the following command we are asking git to store our credential information in the cache for 10 million seconds (almost 4 months).\ngit config --global credential.helper 'cache --timeout=10000000'",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_05.html#github-authentication",
    "href": "session_05.html#github-authentication",
    "title": "5  Introduction to git and GitHub",
    "section": "5.2 GitHub Authentication",
    "text": "5.2 GitHub Authentication\nGitHub recently deprecated password authentication for accessing repositories, so we need to set up a secure way to authenticate.\nThe book Happy Git and GitHub for the useR has a wealth of information related to working with Git in R, and these instructions are based off of Chapter 9 Personal access token for HTTPS.\nWe will be using a Personal Access Token (PAT) in this course. For better security and long term use, we recommend taking the extra steps to set up SSH keys (check out Chapter 10 Set up Keys for SSH).\n\n\n\n\n\n\nSetting up your PAT\n\n\n\n\nRun usethis::create_github_token() in the Console.\nA new browser window should open up to GitHub, showing all the scopes options. You can review the scopes, but you don’t need to worry about which ones to select this time. The previous function automatically pre-selects some recommended scopes. Go ahead and scroll to the bottom and click “Generate Token”.\nCopy the generated token.\nBack in RStudio, run gitcreds::gitcreds_set() in the Console.\nPaste your PAT when the prompt asks for it.\nLast thing, run usethis::git_sitrep() in the Console to check your Git configuration and that you’ve successful stored your PAT. Note: look for Personal access token for 'https://github.com': '&lt;discovered&gt;'\n\nIf you see &lt;unset&gt; instead of &lt;discovered&gt; means your PAT is not correctly set. You need to troubleshoot.\n\n\nCongrats! Now you’ve setup your authentication you should be able to work with GitHub in RStudio now.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_05.html#strategy-to-integrate-changes-from-one-branch-into-another",
    "href": "session_05.html#strategy-to-integrate-changes-from-one-branch-into-another",
    "title": "5  Introduction to git and GitHub",
    "section": "5.3 Strategy to integrate changes from one branch into another",
    "text": "5.3 Strategy to integrate changes from one branch into another\nAbove we configured our global options for all the repositories you create in your server session to use pull.rebase = \"false\" as the strategy to integrate changes from two branches. With this we are saying to merge changes (as opposed to rebasing).\nIt is important to highlight that this configuration can be repo specific. This mean, you can configure how you want git to reconciling two branches at a repository level and not “for all repositories”. Allowing you to control on how git weaves things in when collaborating with others.\nIf you don’t define pull.rebase = \"false\" when setting the global configurations, you will have to define this for each repository you create. You will likely see the following message after you pull, meaning you have not define how to reconciling two branches in your repository.\n\nTo solve this issues you have to run either of the two suggested strategies on the terminal.\ngit config pull.rebase false\nor\ngit config pull.rebase true",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_05.html#learning-objectives-1",
    "href": "session_05.html#learning-objectives-1",
    "title": "5  Introduction to git and GitHub",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nApply the principles of Git to track and manage changes of a project\nUtilize the Git workflow including pulling changes, staging modified files, committing changes, pulling again to incorporate remote changes, and pushing changes to a remote repository\nCreate and configure Git repositories using different workflows",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_05.html#introduction-to-version-control",
    "href": "session_05.html#introduction-to-version-control",
    "title": "5  Introduction to git and GitHub",
    "section": "5.4 Introduction to Version Control",
    "text": "5.4 Introduction to Version Control\n\n\n\n\n\nEvery file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when bugs are discovered. Sometimes those fixes lead to even more bugs, leading to more changes in the code base. Data files get combined together. Sometimes those same files are split and combined again. In just one research project, we can expect thousands of changes to occur.\nThese changes are important to track, and yet, we often use simplistic file names to do so. Many of us have experienced renaming a document or script multiple times with the disingenuous addition of “final” to the file name (like the comic above demonstrates).\nYou might think there is a better way, and you’d be right: version control. Version control provides an organized and transparent way to track changes in code and additional files. This practice was designed for software development, but is easily applicable to scientific programming.\nThere are many benefits to using a version control software including:\n\nMaintain a history of your research project’s development while keeping your workspace clean\nFacilitate collaboration and transparency when working on teams\nExplore bugs or new features without disrupting your team members’ work\nand more!\n\nThe version control system we’ll be diving into is Git, the most widely used modern version control system in the world.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_05.html#introduction-to-git-github",
    "href": "session_05.html#introduction-to-git-github",
    "title": "5  Introduction to git and GitHub",
    "section": "5.5 Introduction to Git + GitHub",
    "text": "5.5 Introduction to Git + GitHub\nBefore diving into the details of Git and how to use it, let’s start with a motivating example that’s representative of the types of problems Git can help us solve.\n\n5.5.1 A Motivating Example\nSay, for example, you’re working on an analysis in R and you’ve got it into a state you’re pretty happy with. We’ll call this version 1:\n\n\n\nYou come into the office the following day and you have an email from your boss, “Hey, you know what this model needs?”\n\n\n\nYou’re not entirely sure what she means but you figure there’s only one thing she could be talking about: more cowbell. So you add it to the model in order to really explore the space.\nBut you’re worried about losing track of the old model so, instead of editing the code in place, you comment out the old code and put as serious a warning as you can muster in a comment above it.\n\n\n\nCommenting out code you don’t want to lose is something probably all of us have done at one point or another but it’s really hard to understand why you did this when you come back years later or you when you send your script to a colleague. Luckily, there’s a better way: Version control. Instead of commenting out the old code, we can change the code in place and tell Git to commit our change. So now we have two distinct versions of our analysis and we can always see what the previous version(s) look like.\n\n\n\nYou may have noticed something else in the diagram above: Not only can we save a new version of our analysis, we can also write as much text as we like about the change in the commit message. In addition to the commit message, Git also tracks who, when, and where the change was made.\nImagine that some time has gone by and you’ve committed a third version of your analysis, version 3, and a colleague emails with an idea: What if you used machine learning instead?\n\n\n\nMaybe you’re not so sure the idea will work out and this is where a tool like Git shines. Without a tool like Git, we might copy analysis.R to another file called analysis-ml.R which might end up having mostly the same code except for a few lines. This isn’t particularly problematic until you want to make a change to a bit of shared code and now you have to make changes in two files, if you even remember to.\nInstead, with Git, we can start a branch. Branches allow us to confidently experiment on our code, all while leaving the old code intact and recoverable.\n\n\n\nSo you’ve been working in a branch and have made a few commits on it and your boss emails again asking you to update the model in some way. If you weren’t using a tool like Git, you might panic at this point because you’ve rewritten much of your analysis to use a different method but your boss wants change to the old method.\n\n\n\nBut with Git and branches, we can continue developing our main analysis at the same time as we are working on any experimental branches. Branches are great for experiments but also great for organizing your work generally.\n\n\n\nAfter all that hard work on the machine learning experiment, you and your colleague could decide to scrap it. It’s perfectly fine to leave branches around and switch back to the main line of development but we can also delete them to tidy up.\n\n\n\nIf, instead, you and your colleague had decided you liked the machine learning experiment, you could also merge the branch with your main development line. Merging branches is analogous to accepting a change in Word’s Track Changes feature but way more powerful and useful.\n\n\n\nA key takeaway here is that Git can drastically increase your confidence and willingness to make changes to your code and help you avoid problems down the road. Analysis rarely follows a linear path and we need a tool that respects this.\n\n\n\nFinally, imagine that, years later, your colleague asks you to make sure the model you reported in a paper you published together was actually the one you used. Another really powerful feature of Git is tags which allow us to record a particular state of our analysis with a meaningful name. In this case, we are lucky because we tagged the version of our code we used to run the analysis. Even if we continued to develop beyond commit 5 (above) after we submitted our manuscript, we can always go back and run the analysis as it was in the past.\n\nWith Git we can enhance our workflow:\n\nEliminate the need for cryptic filenames and comments to track our work.\nProvide detailed descriptions of our changes through commits, making it easier to understand the reasons behind code modifications.\nWork on multiple branches simultaneously, allowing for parallel development, and optionally merge them together.\nUse commits to access and even execute older versions of our code.\nAssign meaningful tags to specific versions of our code.\nAdditionally, Git offers a powerful distributed feature. Multiple individuals can work on the same analysis concurrently on their own computers, with the ability to merge everyone’s changes together.\n\n\n\n\n5.5.2 What exactly are Git and GitHub?\n\nGit:\n\nan open-source distributed version control software\ndesigned to manage the versioning and tracking of source code files and project history\noperates locally on your computer, allowing you to create repositories, and track changes\nprovides features such as committing changes, branching and merging code, reverting to previous versions, and managing project history\nworks directly with the files on your computer and does not require a network connection to perform most operations\nprimarily used through the command-line interface (CLI, e.g. Terminal), but also has various GUI tools available (e.g. RStudio IDE)\n\n\n\n\n\n\nGitHub:\n\nonline platform and service built around Git\nprovides a centralized hosting platform for Git repositories\nallows us to store, manage, and collaborate on their Git repositories in the cloud\noffers additional features on top of Git, such as a web-based interface, issue tracking, project management tools, pull requests, code review, and collaboration features\nenables easy sharing of code with others, facilitating collaboration and contribution to open source projects\nprovides a social aspect, allowing users to follow projects, star repositories, and discover new code\n\n\n\n\n\n\n\n5.5.3 Understanding how local working files, Git, and GitHub all work together\nIt can be a bit daunting to understand all the moving parts of the Git / GitHub life cycle (i.e. how file changes are tracked locally within repositories, then stored for safe-keeping and collaboration on remote repositories, then brought back down to a local machine(s) for continued development). It gets easier with practice, but we’ll explain (first in words, then with an illustration) at a high-level how things work:\n\n5.5.3.1 What is the difference between a “normal” folder vs. a Git repository\nWhether you’re a Mac or a PC user, you’ll likely have created a folder at some point in time for organizing files. Let’s pretend that we create a folder, called myFolder/, and add two files: myData.csv and myAnalysis.R. The contents of this folder are not currently version controlled – meaning, for example, that if we make some changes to myAnalysis.R that don’t quite work out, we have no way of accessing or reverting back to a previous version of myAnalysis.R (without remembering/rewriting things, of course).\nGit allows you to turn any “normal” folder, like myFolder/, into a Git repository – you’ll often see/hear this referenced as “initializing a Git repository”. When you initialize a folder on your local computer as a Git repository, a hidden .git/ folder is created within that folder (e.g. myFolder/.git/) – this .git/ folder is the Git repository. As you use Git commands to capture versions or “snapshots” of your work, those versions (and their associated metadata) get stored within the .git/ folder. This allows you to access and/or recover any previous versions of your work. If you delete .git/, you delete your project’s history.\nHere is our example folder / Git repository represented visually:\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.5.3.2 How do I actually tell Git to preserve versions of my local working files?\nGit was built as a command-line tool, meaning we can use Git commands in the command line (e.g. Terminal, Git Bash, etc.) to take “snapshots” of our local working files through time. Alternatively, RStudio provides buttons that help to easily execute these Git commands.\nGenerally, that workflow looks something like this:\n\nMake changes to a file(s) (e.g. myAnalysis.R) in your working directory.\nStage the file(s) using git add myAnalysis.R (or git add . to stage multiple changed files at once). This lets Git know that you’d like to include the file(s) in your next commit.\nCommit the file(s) using git commit -m \"a message describing my changes\". This records those changes (along with a descriptive message) as a “snapshot” or version in the local repository (i.e. the .git/ folder).\n\n\n\n5.5.3.3 My versioned work is on my local computer, but I want to send it to GitHub. How?\nThe last step is synchronizing the changes made to our local repository with a remote repository (oftentimes, this remote repository is stored on GitHub). The git push command is used to send local commits up to a remote repository. The git pull command is used to fetch changes from a remote repository and merge them into the local repository – pulling will become a regular part of your workflow when collaborating with others, or even when working alone but on different machines (e.g. a laptop at home and a desktop at the office).\nThe processes described in the above sections (i.e. making changes to local working files, recording “snapshots” of them to create a versioned history of changes in a local Git repository, and sending those versions from our local Git repository to a remote repository (which is oftentimes on GitHub)) is illustrated using islands, buildings, bunnies, and packages in the artwork, below:\nA basic git workflow represented as two islands, one with “local repo” and “working directory”, and another with “remote repo.” Bunnies move file boxes from the working directory to the staging area, then with Commit move them to the local repo. Bunnies in rowboats move changes from the local repo to the remote repo (labeled “PUSH”) and from the remote repo to the working directory (labeled “PULL”).\n\n\n\n\nArtwork by Allison Horst\n\n\n\n\n\n5.5.4 Let’s Look at a GitHub Repository\nThis screen shows the copy of a repository stored on GitHub, with its list of files, when the files and directories were last modified, and some information on who made the most recent changes.\n\n\n\nIf we drill into the “commits” for the repository, we can see the history of changes made to all of the files. Looks like kellijohnson was working on the project and fixing errors in December:\n\n\n\nAnd finally, if we drill into one of the changes made on December 20, we can see exactly what was changed in each file:\n\n\n\nTracking these changes, how they relate to released versions of software and files is exactly what Git and GitHub are good for. And we will show how they can really be effective for tracking versions of scientific code, figures, and manuscripts to accomplish a reproducible workflow.\n\n\n5.5.5 Git Vocabulary & Commands\nWe know the world of Git and GitHub can be daunting. Use these tables as references while you use Git and GitHub, and we encourage you to build upon this list as you become more comfortable with these tools.\nThis table contains essential terms and commands that complement intro to Git skills. They will get you far on personal and individual projects.\n\nEssential Git Commands\n\n\n\n\n\n\n\nTerm\nGit Command(s)\nDefinition\n\n\n\n\nAdd/Stage\ngit add [file]\nStaging marks a modified file in its current version to go into your next commit snapshot. You can also stage all modified files at the same time using git add .\n\n\nCommit\ngit commit\nRecords changes to the repository.\n\n\nCommit Message\ngit commit -m \"my commit message\"\nRecords changes to the repository and include a descriptive message (you should always include a commit message!).\n\n\nFetch\ngit fetch\nRetrieves changes from a remote repository but does not merge them into your local working file(s).\n\n\nPull\ngit pull\nRetrieves changes from a remote repository and merges them into your local working file(s).\n\n\nPush\ngit push\nSends local commits to a remote repository.\n\n\nStatus\ngit status\nShows the current status of the repository, including (un)staged files and branch information.\n\n\n\nThis table includes more advanced Git terms and commands that are commonly used in both individual and collaborative projects.\n\nAdvanced Git Commands\n\n\n\n\n\n\n\nTerm\nGit Command(s)\nDefinition\n\n\n\n\nBranch\ngit branch\nLists existing branches or creates a new branch.\n\n\nCheckout\ngit checkout [branch]\nSwitches to a different branch or restores files from a specific commit.\n\n\nClone\ngit clone [repository]\nCreates a local copy of a remote repository.\n\n\nDiff\ngit diff\nShows differences between files, commits, or branches.\n\n\nFork\n-\nCreates a personal copy of a repository under your GitHub account for independent development.\n\n\nLog\ngit log\nDisplays the commit history of the repository.\n\n\nMerge\ngit merge [branch]\nIntegrates changes from one branch into another branch.\n\n\nMerge Conflict\n-\nOccurs when Git cannot automatically merge changes from different branches, requiring manual resolution.\n\n\nPull Request (PR)\n-\nA request to merge changes from a branch into another branch, typically in a collaborative project.\n\n\nRebase\ngit rebase\nIntegrates changes from one branch onto another by modifying commit history.\n\n\nRemote\ngit remote\nManages remote repositories linked to the local repository.\n\n\nRepository\ngit init\nA directory where Git tracks and manages files and their versions.\n\n\nStash\ngit stash\nTemporarily saves changes that are not ready to be committed.\n\n\nTag\ngit tag\nAssigns a label or tag to a specific commit.\n\n\n\nGit has a rich set of commands and features, and there are many more terms beyond either table. Learn more by visiting the git documentation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_05.html#exercise-1-create-a-remote-repository-on-github",
    "href": "session_05.html#exercise-1-create-a-remote-repository-on-github",
    "title": "5  Introduction to git and GitHub",
    "section": "5.6 Exercise 1: Create a remote repository on GitHub",
    "text": "5.6 Exercise 1: Create a remote repository on GitHub\n\n\n\n\n\n\nSetup\n\n\n\n\nLogin to GitHub\nClick the New repository button\nName it {FIRSTNAME}_test\nAdd a short description\nCheck the box to add a README.md file\nAdd a .gitignore file using the R template\nSet the LICENSE to Apache 2.0\n\n\n\nIf you were successful, it should look something like this:\n\n\n\n\n\nYou’ve now created your first repository! It has a couple of files that GitHub created for you: README.md, LICENSE, and .gitignore.\n\n\n\n\n\n\nREADME.md files are used to share important information about your repository\n\n\n\nYou should always add a README.md to the root directory of your repository – it is a markdown file that is rendered as HTML and displayed on the landing page of your repository. This is a common place to include any pertinent information about what your repository contains, how to use it, etc.\n\n\n\n\n \n\nFor simple changes to text files, such as the README.md, you can make edits directly in the GitHub web interface.\n\n\n\n\n\n\nChallenge\n\n\n\nNavigate to the README.md file in the file listing, and edit it by clicking on the pencil icon (top right of file). This is a regular Markdown file, so you can add markdown text. Add a new level-2 header called “Purpose” and add some bullet points describing the purpose of the repo. When done, add a commit message, and hit the Commit changes button.\n\n\n\n\n\n\n\nCongratulations, you’ve now authored your first versioned commit! If you navigate back to the GitHub page for the repository, you’ll see your commit listed there, as well as the rendered README.md file.\n\n\n\n\n\nThe GitHub repository landing page provides us with lots of useful information. To start, we see:\n\nall of the files in the remote repository\nwhen each file was last edited\nthe commit message that was included with each file’s most recent commit (which is why it’s important to write good, descriptive commit messages!)\n\nAdditionally, the header above the file listing shows the most recent commit, along with its commit message, and a unique ID (assigned by Git) called a SHA. The SHA (aka hash) identifies the specific changes made, when they were made, and by who. If you click on the SHA, it will display the set of changes made in that particular commit.\n\n\n\n\n\n\nWhat should I write in my commit message?\n\n\n\nWriting effective Git commit messages is essential for creating a meaningful and helpful version history in your repository. It is crucial to avoid skipping commit messages or resorting to generic phrases like “Updates.” When it comes to following best practices, there are several guidelines to enhance the readability and maintainability of the codebase.\nHere are some guidelines for writing effective Git commit messages:\n\nBe descriptive and concise: Provide a clear and concise summary of the changes made in the commit. Aim to convey the purpose and impact of the commit in a few words.\nUse imperative tense: Write commit messages in the imperative tense, as if giving a command. For example, use “Add feature” instead of “Added feature” or “Adding feature.” This convention aligns with other Git commands and makes the messages more actionable.\nSeparate subject and body: Start with a subject line, followed by a blank line, and then provide a more detailed explanation in the body if necessary. The subject line should be a short, one-line summary, while the body can provide additional context, motivation, or details about the changes.\nLimit the subject line length: Keep the subject line within 50 characters or less. This ensures that the commit messages are easily scannable and fit well in tools like Git logs.\nCapitalize and punctuate properly: Begin the subject line with a capital letter and use proper punctuation. This adds clarity and consistency to the commit messages.\nFocus on the “what” and “why”: Explain what changes were made and why they were made. Understanding the motivation behind a commit helps future researchers and collaborators (including you!) comprehend its purpose.\nUse present tense for subject, past tense for body: Write the subject line in present tense as it represents the current state of the codebase. Use past tense in the body to describe what has been done.\nReference relevant issues: If the commit is related to a specific issue or task, include a reference to it. For example, you can mention the issue number or use keywords like “Fixes,” “Closes,” or “Resolves” followed by the issue number.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_05.html#exercise-2-clone-your-repository-and-use-git-locally-in-rstudio",
    "href": "session_05.html#exercise-2-clone-your-repository-and-use-git-locally-in-rstudio",
    "title": "5  Introduction to git and GitHub",
    "section": "5.7 Exercise 2: clone your repository and use Git locally in RStudio",
    "text": "5.7 Exercise 2: clone your repository and use Git locally in RStudio\nCurrently, our repository just exists on GitHub as a remote repository. It’s easy enough to make changes to things like our README.md file (as demonstrated above), from the web browser, but that becomes a lot harder (and discouraged) for scripts and other code files. In this exercise, we’ll bring a copy of this remote repository down to our local computer (aka clone this repository) so that we can work comfortably in RStudio.\n\n\n\n\n\n\nAn important distinction\n\n\n\nWe refer to the remote copy of the repository that is on GitHub as the origin repository (the one that we cloned from), and the copy on our local computer as the local repository.\n\n\nStart by clicking the green Code button (top right of your file listing) and copying the URL to your clipboard (this URL represents the repository location):\n\n\n\n\n\n\n\nRStudio makes working with Git and version controlled files easy – to do so, you’ll need to be working within an R project folder. The following steps will look similar to those you followed when first creating an R Project, with a slight difference. Follow the instructions in the Setup box below to clone your remote repository to your local computer in RStudio:\n\n\n\n\n\n\nSetup\n\n\n\n\nClick File &gt; New Project\nSelect Version Control and paste the remote repository URL (which should be copied to your clipboard) in the Repository URL field\nPress Tab, which will auto-fill the Project directory name field with the same name as that of your remote repo – while you can name the local copy of the repository anything, it’s typical (and highly recommended) to use the same name as the GitHub repository to maintain the correspondence\n\n\n\n\n\n\n\n\n\nOnce you click Create Project, a new RStudio window will open with all of the files from the remote repository copied locally. Depending on how your version of RStudio is configured, the location and size of the panes may differ, but they should all be present – you should see a Git tab, as well as the Files tab, where you can view all of the files copied from the remote repo to this local repo.\n\n\n\n\nYou’ll note that there is one new file sam_test.Rproj, and three files that we created earlier on GitHub (.gitignore, LICENSE, and README.md).\nIn the Git tab, you’ll note that the one new file, sam_test.Rproj, is listed. This Git tab is the status pane that shows the current modification status of all of the files in the repository. Here, we see sam_test.Rproj is preceded by a ?? symbol to indicate that the file is currently untracked by Git. This means that we have not yet committed this file using Git (i.e. Git knows nothing about the file; hang tight, we’ll commit this file soon so that it’s tracked by Git). As you make version control decisions in RStudio, these icons will change to reflect the current version status of each of the files.\nInspect the history. Click on the History button in the Git tab to show the log of changes that have occurred – these changes will be identical to what we viewed on GitHub. By clicking on each row of the history, you can see exactly what was added and changed in each of the two commits in this repository.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nMake a change to the README.md file – this time from RStudio – then commit the README.md change\nAdd a new section to your README.md called “Creator” using a level-2 header. Under it include some information about yourself. Bonus: Add some contact information and link your email using Markdown syntax.\n\n\n\nOnce you save, you’ll immediately see the README.md file show up in the Git tab, marked as a modification. Select the file in the Git tab, and click Diff to see the changes that you saved (but which are not yet committed to your local repository). Newly made changes are highlighted in green.\n\n\n\n\nCommit the changes. To commit the changes you made to the README.md file using RStudio’s GUI (Graphical User Interface), rather than the command line:\n\nStage (aka add) README.md by clicking the check box next to the file name – this tells Git which changes you want included in the commit and is analogous to using the git command, git add README.md, in the command line\nCommit README.md by clicking the Commit button and providing a descriptive commit message in the dialog box. Press the Commit button once you’re satisfied with your message. This is analogous to using the git command, git commit -m \"my commit message\", in the command line.\n\n\nA few notes about our local repository’s state:\n\nWe still have a file, sam_test.Rproj, that is listed as untracked (denoted by ?? in the Git tab).\nYou should see a message at the top of the Git tab that says, Your branch is ahead of ‘origin/main’ by 1 commit., which tells us that we have 1 commit in the local repository, but that commit has not yet been pushed up to the origin repository (aka remote repository on GitHub).\n\nCommit the remaining project file by staging/adding and committing it with an informative commit message.\n\nWhen finished, you’ll see that no changes remain in the Git tab, and the repository is clean.\nInspect the history. Note that under Changes, the message now says:\nYour branch is ahead of ‘origin/main’ by 2 commits.\nThese are the two commits that we just made, but have not yet been pushed to GitHub.\nClick on the History button to see a total of four commits in the local repository (the two we made directly to GitHub via the web browser and the two we made in RStudio).\nPush these changes to GitHub. Now that we’ve made and committed changes locally, we can push those changes to GitHub using the Push button. This sends your changes to the remote repository (on GitHub) leaving your repository in a totally clean and synchronized state (meaning your local repository and remote repository should look the same).\n\n\n\n\n\n\nIf you are prompted to provide your GitHub username and password when Pushing…\n\n\n\nit’s a good indicator that you did not set your GitHub Personal Access Token (PAT) correctly. You can redo the steps outlined in the GitHub Authentication section to (re)set your PAT, then Push again.\n\n\n\n &lt;––&gt;\n\nIf you look at the History pane again, you’ll notice that the labels next to the most recent commit indicate that both the local repository (HEAD) and the remote repository (origin/HEAD) are pointing at the same version in the history. If we look at the commit history on GitHub, all the commits will be shown there as well.\n\n\n\n5.7.1 Defining Merge Method\n\n\n\n\n\n\nSome Git configuration to surpress warning messages\n\n\n\nGit version 2.27 includes a new feature that allows users to specify the default method for integrating changes from a remote repository into a local repository, without receiving a warning (this warning is informative, but can get annoying). To suppress this warning for this repository only we need to configure Git by running this line of code in the Terminal:\n\n\nAnswer\ngit config pull.rebase false\n\n\npull.rebase false is a default strategy for pulling where Git will first try to auto-merge the files. If auto-merging is not possible, it will indicate a merge conflict (more on resolving merge conflicts in Collaborating with Git and GitHub).\nNote: Unlike when we first configured Git, we do not include the --global flag here (e.g. git config --global pull.rebase false). This sets this default strategy for this repository only (rather than globally for all your repositories). We do this because your chosen/default method of grabbing changes from a remote repository (e.g. pulling vs. rebasing) may change depending on collaborator/workflow preference.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_05.html#exercise-3-setting-up-git-on-an-existing-project",
    "href": "session_05.html#exercise-3-setting-up-git-on-an-existing-project",
    "title": "5  Introduction to git and GitHub",
    "section": "5.8 Exercise 3: Setting up Git on an existing project",
    "text": "5.8 Exercise 3: Setting up Git on an existing project\nThere are a number of different workflows for creating version-controlled repositories that are stored on GitHub. We started with Exercise 1 and Exercise 2 using one common approach: creating a remote repository on GitHub first, then cloning that repository to your local computer (you used your {FIRSTNAME}_test repo).\nHowever, you may find yourself in the situation where you have an existing directory (i.e. a “normal” folder) of code that you want to make a Git repository out of, and then send it to GitHub. In this last exercise, we will practice this workflow using your training_{USERNAME} project.\nFirst, switch to your training_{USERNAME} project using the RStudio project dropdown menu. The project drop down menu is in the upper right corner of your RStudio pane. Click the drop down next to your project name ({FIRSTNAME}_test), and then select the training_{USERNAME} project from the RECENT PROJECTS list.\nThere are a few approaches for turning an existing project folder into a Git repository, then sending it to GitHub – if you’re an R-user, the simplest way is to use the {usethis} package, which is built to automate tasks involved with project setup and development. However, you can also initialize a local git repository and set the remote repository from the command line (a language-agnostic workflow). Steps for both approaches are included below (demonstrated using your training_{USERNAME} project):\n\nUsing R & {usethis}Using the command line\n\n\n\nInstall the {usethis} package (if you haven’t done so already) by running the following in your Console:\n\n\n\nAnswer\ninstall.packages(\"usethis\")\n\n\n\nInitialize training_{USERNAME} as a Git repository by running usethis::use_git() in the Console. Choose yes when asked if it’s okay to commit any uncommitted files. Choose yes again if asked to restart R. Once complete, you should see the Git tab appear in your top left pane in RStudio and a .gitignore file appear in your Files tab.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.gitignore files allow you to specify which files/folders you don’t want Git to track\n\n\n\nA .gitignore file is automatically created in the root directory of your project when you initialize it as a Git repository. You’ll notice that there are already some R / R Project-specific files that have been added by default.\nWhy is this useful? For many reasons, but possibly the greatest use-case is adding large files (GitHub has a file size limit of 2 GB) or files with sensitive information (e.g. keys, tokens) that you don’t want to accidentally push to GitHub.\nHow do I do this? Let’s say I create a file with sensitive information that I don’t want to push to GitHub. I can add a line to my .gitignore file:\n\n\nAnswer\n# added by default when I initalized my RProj as a Git Repository\n.Rproj.user\n.Rhistory\n.Rdata\n.httr-oauth\n.DS_Store\n.quarto\n\n# add file so that it doesn't get pushed to the remote repo (on GitHub); \ncontains_sensitive_info.R\n\n\nIf this file is currently untracked by Git, it should appear in my Git tab. Once I add it to the .gitignore and save the modified .gitignore file, you should see contains_sensitive_info.R disappear from the Git tab, and a modified .gitignore (denoted by a blue M) appear. Stage/commit/push this modified .gitignore file.\n\n\n\nCreate an upstream remote repository on GitHub by running usethis::use_github() in the Console. Your web browser should open up to your new GitHub repository, with the same name as your local Git repo/R Project.\n\n\n\n\n\n\n\n\n\n\n\nEnsure that your default branch is named main rather than master by:\n\nrunning git branch in the Terminal to list all your branches (you should currently only have one, which is your default)\nif it’s named master, run the following line in the Console to update it\n\n\n\n\nAnswer\nusethis::git_default_branch_rename(from = \"master\", to = \"main\")\n\n\nYou can verify that your update worked by running git branch once more in the Terminal.\n\n\n\n\n\n\nWhy are we doing this?\n\n\n\nThe racist “master” terminology for git branches motivates us to update our default branch to “main” instead.\nThere is a push across platforms and software to update this historical default branch name from master to main. GitHub has already done so – you may have noticed that creating a remote repository first (like we did in Exercises 1 & 2) results in a default branch named main. Depending on your version of Git, however, you may need to set update the name manually when creating a local git repository first (as we’re doing here).\n\n\n\nYou’re now ready to edit, stage/add, commit, and push files to GitHub as practiced earlier!\n\n\n\n\n\n\n\nChallenge: add a README.md file to training_{USERNAME}\n\n\n\nGitHub provides a button on your repo’s landing page for quickly adding a README.md file. Click the Add a README button and use markdown syntax to create a README.md. Commit the changes to your repository.\nGo to your local repository (in RStudio) and pull the changes you made.\n\n\n\n\nWhile we’ll be using the RStudio Terminal here, you can use any command-line interface (e.g. Mac Terminal, Git Bash, etc.) that allows for git interactions (if you plan to use a command-line interface that is not the RStudio Terminal, make sure to navigate to your project directory (e.g. using cd file/path/to/project/directory) before initializing your repository.\n\nInitialize training_{USERNAME} as a Git repository by running git init in the Terminal. You should get a message that says something like:\n\n\n\nAnswer\nInitialized empty Git repository in /home/username/training_username/.git/\n\n\n\n\n\n\n\n\nYou may have to quit and reopen your RStudio session on the server for the Git tab to appear\n\n\n\nYou’ll likely need to help included-crab along in recognizing that this R Project has been initialized as a git repository – click Session &gt; Quit Session… &gt; New Session &gt; choose training_{USERNAME} to reopen your project.\n\n\nOnce complete, you should see the Git tab appear in your top left pane in RStudio and a .gitignore file appear in your Files tab.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.gitignore files allow you to specify which files/folders you don’t want Git to track\n\n\n\nA .gitignore file is automatically created in the root directory of your project when you initialize it as a Git repository. You’ll notice that there are already some R / R Project-specific files that have been added by default.\nWhy is this useful? For many reasons, but possibly the greatest use-case is adding large files (GitHub has a file size limit of 2 GB) or files with sensitive information (e.g. keys, tokens) that you don’t want to accidentally push to GitHub.\nHow do I do this? Let’s say I create a file with sensitive information that I don’t want to push to GitHub. I can add a line to my .gitignore file:\n\n\nAnswer\n# added by default when I initalized my RProj as a Git Repository\n.Rproj.user\n.Rhistory\n.Rdata\n.httr-oauth\n.DS_Store\n.quarto\n\n# add file so that it doesn't get pushed to the remote repo (on GitHub); \ncontains_sensitive_info.R\n\n\nIf this file is currently untracked by Git, it should appear in my Git tab. Once I add it to the .gitignore and save the modified .gitignore file, you should see contains_sensitive_info.R disappear from the Git tab, and a modified .gitignore (denoted by a blue M) appear. Stage/commit/push this modified .gitignore file.\n\n\n\nEnsure that your default branch is named main rather than master by:\n\nrunning git branch in the Terminal to list all your branches (you should currently only have one, which is your default)\nif it’s named master, run the following line in the Terminal to update it\n\n\n\n\nAnswer\n# for Git version 2.28+ (check by running `git --version`)\n# this sets the default branch name to `main` for any new repos moving forward\ngit config --global init.defaultBranch main\n\n# for older versions of Git\n# this sets the default branch name to `main` ONLY for this repo \ngit branch -m master main\n\n\nYou can verify that your update worked by running git branch once more in the Terminal.\n\n\n\n\n\n\nWhy are we doing this?\n\n\n\nThe racist “master” terminology for git branches motivates us to update our default branch to “main” instead.\nThere is a push across platforms and software to update this historical default branch name from master to main. GitHub has already done so – you may have noticed that creating a remote repository first (like we did in Exercises 1 & 2) results in a default branch named main. Depending on your version of Git, however, you may need to set update the name manually when creating a local git repository first (as we’re doing here).\n\n\n\nStage/Add your files. It’s helpful to first run git status to check the state of your local repository (particularly if you aren’t using RStudio / have access to a GUI with a Git tab-esque feature) – this will tell you which files have been modified or are untracked and that are currently unstaged (in red). What appears here should look just like what appears in the Git tab:\n\n\n\n\n\n\n\n\n\n\nRun git add . in the Terminal to stage all files at once (or git add {FILENAME} to stage individual files). Running git status again will show you which files have been staged (in green). You may have to refresh your Git tab to see the change in state reflected in the GUI.\n\n\n\n\n\n\n\n\n\n\nCommit your files by running git commit -m \"an informative commit message\" in the Terminal. Refreshing your Git tab will cause them to disappear (just as they do when you commit using RStudio’s GUI buttons). You can run git log in the Terminal to see a history of your past commits (currently, we only have this one).\n\n\n\n\n\n\n\n\n\n\n\nCreate an empty remote repository by logging into GitHub and creating a new repository, following the same steps as in Exercise 1. IMPORTANTLY, DO NOT initialize your remote repo with a README license, or .gitignore file – doing so now can lead to merge conflicts. We can add them after our local and remote repos are linked. Name your remote repository the same as your local repository (i.e. training_{USERNAME}).\nLink your remote (GitHub) repository to your local Git repository. Your empty GitHub repo conveniently includes instructions for doing so. Copy the code under push an existing repository from the command line to your clipboard, paste into your RStudio Terminal, and press return/enter.\n\n\n\n\n\n\n\n\n\n\nThese commands do three things:\n\nAdds the GitHub repository as the remote repository (i.e. links your local repo to the remote repo)\nRenames the default branch to main\nPushes the main branch to the remote GitHub repository\n\nHead back to your browser and refresh your GitHub repository page to see your files appear!\n\nYou’re now ready to edit, stage/add, commit, and push files to GitHub as practiced earlier!\n\n\n\n\n\n\n\nChallenge: add a README.md file to training_{USERNAME}\n\n\n\nGitHub provides a button on your repo’s landing page for quickly adding a README.md file. Click the Add a README button and use markdown syntax to create a README.md. Commit the changes to your repository.\nGo to your local repository (in RStudio) and pull the changes you made.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_05.html#go-further-with-git",
    "href": "session_05.html#go-further-with-git",
    "title": "5  Introduction to git and GitHub",
    "section": "5.9 Go further with Git",
    "text": "5.9 Go further with Git\nThere’s a lot we haven’t covered in this brief tutorial. There are some great and much longer tutorials that cover advanced topics, such as:\n\nUsing Git on the command line\nResolving conflicts\nBranching and merging\nPull requests versus direct contributions for collaboration\nUsing .gitignore to protect sensitive data\nGitHub Issues - how to use them for project management and collaboration\n\nand much, much more.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_05.html#git-resources",
    "href": "session_05.html#git-resources",
    "title": "5  Introduction to git and GitHub",
    "section": "5.10 Git resources",
    "text": "5.10 Git resources\n\nPro Git Book\nHappy Git and GitHub for the useR\nGitHub Documentation\nLearn Git Branching is an interactive tool to learn Git on the command line\nSoftware Carpentry Version Control with Git\nBitbucket’s tutorials on Git Workflows",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_06.html",
    "href": "session_06.html",
    "title": "6  Data Modeling Essentials",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Modeling Essentials</span>"
    ]
  },
  {
    "objectID": "session_06.html#learning-objectives",
    "href": "session_06.html#learning-objectives",
    "title": "6  Data Modeling Essentials",
    "section": "",
    "text": "Review high level best practices for effective data management\nUnderstand the principles of tidy data\nIntroduce the relational data model\nLearn how to design and diagram multi-table data models",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Modeling Essentials</span>"
    ]
  },
  {
    "objectID": "session_06.html#simple-guidelines-for-data-management",
    "href": "session_06.html#simple-guidelines-for-data-management",
    "title": "6  Data Modeling Essentials",
    "section": "6.1 Simple guidelines for data management",
    "text": "6.1 Simple guidelines for data management\nA great paper called ‘Some Simple Guidelines for Effective Data Management’ (Borer et al. 2009) lays out exactly that - guidelines that make your data management, and your reproducible research, more effective. Although not strictly about data modeling, the first six guidelines are worth remembering as foundational guidelines, here slightly revised to reflect changes over the intervening years:\n\n\n\n\n\n\n\n\n\n\n\n\nUse a scripted workflow\n\n\n\nManipulating data using scripts (like R!) gives you much more control over all decisions and changes. Scripts allow you to document, audit, edit, reuse, and reproduce all steps. In contrast, point-and-click manipulations, such as manually editing data in a spreadsheet application, are time consuming, poorly documented, and difficult to reproduce.\n\n\n\n\n\n\n\n\n\n\nKeep a raw version of data\n\n\n\nIn conjunction with using a scripted language, keeping a raw version of your data is essential for maintaining a reproducible workflow. When you keep your raw data, your scripts can read from that raw data and create as many derived data products as you need, and you will always be able to re-run your scripts and know that you will get the same output.\n\n\n\n\n\n\n\n\n\n\nPrefer open file formats (e.g. csv, txt)\n\n\n\nUsing a file that can be read by free, open source software greatly increases the longevity and accessibility of your data. In contrast, proprietary formats bind your data to a particular vendor or software license that may be hard to maintain and may become obsolete.\n\n\n\n\n\n\n\n\n\n\n\n\nUse simple but descriptive names\n\n\n\nFile and variable names should be descriptive, helping your future self and others more quickly understand what type of data they contain, but also simple - in particular, without spaces or special characters - to reduce issues when reading and referring to them in scripts and other programmatic environments.\n\n\n\n\n\n\n\n\n\n\nInclude a header line in your data tables\n\n\n\nWhen creating and managin tabular data, using a single header line of column names as the first row of your table is the most common and easiest way to achieve consistency among files.\n\n\n\n\n\n\n\n\n\n\nUse plain ASCII or UTF-8 for text\n\n\n\nStandard character encodings such as ASCII and UTF-8 are widely used for text files, hence far more likely to be parsed correctly and supported well into the future as compared with more esoteric formats.\n\n\n\n\n\nBefore moving on to discuss the remaining guidelines, here is an example of how you might organize the files themselves following the simple rules above. Note that we have all open formats, plain text formats for data, sortable file names without special characters, scripts, and a special folder for raw files.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Modeling Essentials</span>"
    ]
  },
  {
    "objectID": "session_06.html#tidy-data",
    "href": "session_06.html#tidy-data",
    "title": "6  Data Modeling Essentials",
    "section": "6.2 Tidy data",
    "text": "6.2 Tidy data\nThe final 3 guidelines focus on the structural design of data tables:\n\nDesign tables to add rows, not columns\nEach column should contain only one type of information\nRecord a single piece of data only once; separate information collected at different scales into different tables.\n\nMore recently, these have been popularized under the tidy data principles, which articulate a simple but effective pattern for organizing data tables in a way that allows us to understand, manage, and analyze data efficiently:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\n\n\n\n\n\n\nWhy Tidy Data?\n\n\n\n\nEfficiency: less re-creating the wheel. Easier to apply the same tools to different datasets.\nCollaboration: Makes it easier to work with others as you can work with the same tools in the same ways.\nReuse: It makes it easier to apply similar techniques and analysis across different or new datasets.\nGeneralizability: Tools built for one tidy data set can be used to multiple other datasets. Opening posibilities of data you can work with.\n\n\n“There is a specific advantage to placing varables in columns becasuse it allows R’s vectorized nature to shine. …most buit-in R functions work with vactors of values. That makes transforming tidy data feel particularly natural.” (R for Data Science by Grolemund and Wickham)\n\n\n\n\n\n\n\n\n\nTidy Data: A way of life\n\n\n\n\nTidy data is not a language or tool specific.\nTidy data is not an R thing.\nTidy data is not a tidyverse thing.\n\nTidy Data is a way to organize data that will make life easier for people working with data.\n(Allison Horst & Julia Lowndes)\n\n\nLet’s make sure we understand the basic terms used above, along with a fourth term (“entity”) that we’ll discuss more later.\n\n\n\n\n\n\n\nConcept\nDefinition\n\n\n\n\nVariables\nA characteristic that is being measured, counted, or described with data.\nExample: car type, salinity, year, height, mass.\n\n\nObservations\nA single “data point” for which the measure, count, or description of one or more variables is recorded.\nExample: If you are collecting variables height, species, and location of plants, then each plant is an observation.\n\n\nValue\nThe record measured, count, or description of a variable.\nExample: 3 ft\n\n\nEntity\nEach type of observation is an entity.\nExample: If you are recording the species name and height of plants observed at various sites that each have a site name and location, then plants is an entity and site is an entity.\n\n\n\n\n6.2.1 Tidy data in action\nThe following is an example of tidy data - it’s easy to see the three tidy data principles apply.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.2.2 Recognizing untidy data\nAnything that does not follow the three tidy data principles is untidy data.\nThere are many ways in which data can become untidy. Some are obvious once you know what to look for, while others are more subtle. In this section we will look at some examples of common untidy data situations.\nThe following is a screenshot of an actual dataset that came across NCEAS. We have all seen spreadsheets that look like this - and it is fairly obvious that whatever this is, it isn’t very tidy.\n\nLet’s dive deeper into why we consider this untidy data.\n\nMultiple tables\nFirst, notice there are actually three smaller tables within this table. Although for our human brain it is easy to see and interpret this, it is extremely difficult to get a computer to see it this way. Having multiple tables immediately breaks the tidy data principles, as we will see next.\n\n\n\nInconsistent columns (variables)\nIn tidy data, each column corresponds to a single variable. If you look down a column, and see that multiple variables exist in the table, the data are not tidy. A good test for this can be to see if you think the column consists of only one unit type.\n\n\n\nInconsistent rows (observations)\nIn tidy data, each row must be a single observation. If you look across a single row, and you notice that there are clearly multiple observations in one row, the data are likely not tidy.\n\n\n\nMarginal sums and statistics\nMarginal sums and statistics are not considered tidy. They break the first principle (“Every column is a variable”), because a marginal statistic does not represent the same variable as the values it is summarizing. They also break the second principle (“Every row is an observation”), because they represent a combination of observations, rather than a single one.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Modeling Essentials</span>"
    ]
  },
  {
    "objectID": "session_06.html#introduction-to-the-relational-data-model",
    "href": "session_06.html#introduction-to-the-relational-data-model",
    "title": "6  Data Modeling Essentials",
    "section": "6.3 Introduction to the relational data model",
    "text": "6.3 Introduction to the relational data model\nThe Relational Model, first developed in the late 1960s, provides a highly structured approach for modeling data, and remains the basis for modern relational databases like SQLite, MariaDB, PostgreSQL, and Microsoft Access. It is fully consistent with the tidy data principles, but goes further by providing a formalized framework for decomposing data into multiple tables, while maintaining the relationships between those tables in a way that allows them to be flexibly joined back together on demand.\nA database organized following a relational data model is a relational database. However, you don’t have to be using a relational database to enjoy the benefits of using a relational data model, and your data don’t have to be large or complex for you to benefit. Here are a few of the benefits of using a relational data model:\n\nEnabling powerful search and filtering\nAbility to handle large, complex data sets\nEnforcing data integrity\nDecreasing errors from redundant updates\n\nIn this module, we’ll introduce two concepts integral to the relational model. The first involves normalization, and the difference between denormalized and normalized data. The second involves relationships between tables, and different ways of combining records across tables using operations called joins.\n\n6.3.1 Data Normalization\nData normalization is the process of creating normalized data, which are datasets free from data redundancy to simplify query, analysis, storing, and maintenance. In normalized data we organize data so that:\n\nWe have separate tables for each type of entity measured\nEach table follows the tidy data principles\nEach column represents either an identifying variable or a measured variable\nWe are not repeating information across rows\n\nIn contrast, a denormalized table combines observations about different entities in the same table. A good indication that a data table is denormalized and needs normalization is seeing the same column values repeated across multiple rows. This can sometimes be a convenient way to represent data for certain analysis or visualization tasks, but it is typically an inefficient and error-prone way to manage and store data.\nConsider the following table, which shows data about species observed at a specific site and date. The column headers refer to the following:\n\ndate: date when a species was observed\nsite: site where a species was observed\nname: site’s name\naltitude: site’s altitude\nsp1code, sp2code: species code for two plants observed\nsp1height, sp2height: corresponding height of the plants observed\n\nTake a moment to see why this is not tidy data.\n\n\n\n\n\n\n\nChallenge\n\n\n\nBefore reading on, try to answer the following questions:\n\nWhat are the observed entities in the example above?\nWhat are the measured variables associated with those observations?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nThe table has two entities: Site and plants\nFor plant observations, the table records the species names, plant heights, and dates of observation. For sites, the table records the name and altitude.\n\n\n\n\nThe table above is not in a normalized form (nor is it tidy!). Each row has measurements about both the site at which observations occurred, as well as observations of two individual plants of possibly different species found at that site.\n\nYou’ll often see this described as a wide table, because the observations are spread across a wide number of columns. Note that if we observed a new species in the survey, we would need to add new columns to the table. This is difficult to analyze, understand, and maintain.\nTo solve this problem, we can create a single column for species code, and a single column for species height. The table is now in a long format, which looks like this:\n\nThis was an important first step toward normalizing the data, and technically this is now a tidy table. But we’re not done! Notice that in our new table, the row values for the last three columns are repeated.\n\nThis happens because each row has variables (columns) and values about more than one entity:\n\n1st entity: individual plants found at that site\n2nd entity: sites at which the plants were observed\n\n\nIf we use this information to normalize our data, we should end up with:\n\none tidy table for plants\none tidy table for sites\nnew columns for uniquely identifying observations (such as site ID)\n\nHere’s how our normalized data looks:\n\n\n\n\n\nNow all of the following are true:\n\nSeparate tables for each type of entity measured\nEach row represents a single observed entity\nEach column represents either an identifier, measured variable, or other attribute of that entity\nAll values in a row describe the corresponding observed entity\nAll values in a column are of the same type\nInformation is not repeated across rows\n\nIn addition, notice that each table also satisfies the tidy data principles!\nAnd last but not least, notice that this normalized version of the data meets the three guidelines set by (Borer et al. 2009):\n\nDesign tables to add rows, not columns\nEach column should contain only one type of information\nRecord a single piece of data only once; separate information collected at different scales into different tables.\n\n\n\n\n\n\n\nFrom one table to multiple tables\n\n\n\nNormalizing data by separating it into multiple tables often makes researchers really uncomfortable. This is understandable! The person who designed this study collected all of these measurements for a reason - so that they could analyze the measurements together. Now that our site and plant information are in separate tables, how would we use site altitude as a predictor variable for species composition, for example?\nWe will go over a solution in the next section.\n\n\n\n\n6.3.2 Managing table relationships\nTo reiterate, the relational data model is not just about decomposing your data into tidy tables, but also connecting those tables to one another so that observations of related entities can linked back together as needed.\nThis linkage is accomplished via keys, the cornerstone of relational data models. Keys are columns (or combinations of columns) whose values unambiguously identify observations of an entity.\nTwo types of keys are common within relational data models:\n\nPrimary Key: chosen key for a table, uniquely identifying each observation in the table\nForeign Key: reference to a primary key in another table, used to create links between tables\n\n\n\n\n\n\n\nChallenge\n\n\n\nIn our normalized tables below, identify the following:\n\nthe primary key for each table\nany foreign keys that exist\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe primary key of the top table is id. The primary key of the bottom table is site.\nThe site column is the primary key of that table because it uniquely identifies each row of the table as a unique observation of a site. In the first table, however, the site column is a foreign key that references the primary key from the second table. This linkage tells us that the first height measurement for the DAPU observation occurred at the site with the name Taku.\n\n\n\n\nIn the examples above, each key corresponds to an individual column in a table. However, it’s also possible to define a key based on multiple columns. Imagine a table containing one unique observation for each date from each site. In that case, although neither date nor site alone uniquely identifies each table record, the combination of these two columns contains all the information we need. Keys made up of two or more columns in a table are called composite keys, and you’ll see a lot of these in practice.\nFinally, note that there are various strategies for choosing and/or creating keys when desiging a data model, including natural keys, surrogate keys, and compound keys. Each type of key has advantages and disadvantages. We won’t discuss them more here, but you can read more about it in this article.\n\n\n6.3.3 Joining tables\nFrequently, analysis of data will require merging these separately managed tables back together. There are multiple ways to join the observations in two tables, based on how the rows of one table are merged with the rows of the other. Regardless of the join we will perform, we need to start by identifying the primary key in each table and how these appear as foreign keys in other tables.\nWhen conceptualizing merges, one can think of two tables, one on the left and one on the right.\n\n\n\n\n\n\n6.3.3.1 Inner Join\nThe most common (and often useful) join is when you merge the subset of rows that have matches in both the left table and the right table: this is called an INNER JOIN.\n\n\n\n\n\n\n\n6.3.3.2 Left Join\nA LEFT JOIN takes all of the rows from the left table, and merges on the data from matching rows in the right table. Keys that don’t match from the left table are still provided with a missing value (na) from the right table.\n\n\n\n\n\n\n\n6.3.3.3 Right Join\nA RIGHT JOIN is the same as a left join, except that all of the rows from the right table are included with matching data from the left, or a missing value. Notice that left and right joins can ultimately be the same depending on the positions of the tables.\n\n\n\n\n\n\n\n6.3.3.4 Full Outer Join\nFinally, a FULL OUTER JOIN includes all data from all rows in both tables, and includes missing values wherever necessary.\n\n\n\n\n\nSometimes people represent these as Venn diagrams showing which parts of the left and right tables are included in the results for each join. This representation is useful, but it misses part of the story related to where the missing value comes from in each result.\n\n\n\nImage source: R for Data Science, Wickham & Grolemund.\n\n\nWe suggest reading the Relational Data chapter in the “R for Data Science” book for more examples and best practices about joins.\n\n\n\n6.3.4 Entity Relationship Diagrams\nEntity-relationship modeling is the important process of stepping back to think about your data, deciding what are your entities, their attributes, and the relationships between them. Ideally you will do this early in the process of gather and recording data, but often you’ll need to do it as part of the process of organizing and cleaning up an existing dataset. In either case, an Entity Relationship Diagram (ERD) is a simple but powerful way to compactly describe the structure and relationships of tables in a relational database, including the primary and foreign keys. If you can draw an ERD for your data, it will make it easier to reason about the assumptions you’re making about your data, and validate that your data model is a solid representation of the real-world entities and phenomena captured in your data.\nHere are the steps to building up an ERD:\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Identify your entities\n\n\n\nIdentify the entities in the relational database and draw a box for each one. In our case, the entities are plants and sites, since we are gathering observations about both of these.\n\n\n\n\n\nerDiagram\n    Plants { }\n    Sites { }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Add variables and identify keys\n\n\n\nIn each box, add the variables as a list, identifying all primary keys (PKs) and foreign keys (FKs).\n\n\n\n\n\nerDiagram\n    Plants {\n        numeric id PK\n        date date\n        numeric site FK\n        string sp_code\n        numeric sp_height\n    }\n    Sites {\n        numeric site PK\n        string name\n        numeric altitude\n    }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Add entity relationships and indicate cardinality\n\n\n\nFirst draw a line between each pair of boxes that have a PK-FK relationship, and annotate it to describe how they are related. The “direction” of the relationship is from the table with the FK to the table with the FK. In this example, site is the primary key of Sites and appears as a foreign key in Plants, so we might label Plants as observed_in Sites.\nThen quantify how many items in one entity can be related to each observation in another entity. The options are zero, one, many, or some combination thereof. In this case, if every site has one or more plant, we add the symbol for “One or Many” at the end of the line going from Sites to Plants. In the other direction, a plant must be observed in exactly one site, so we add the symbol for “One and ONLY One” at the end of the line going from Plants to Sites.\n\n\n\n\n\nerDiagram\n    Plants |{--|| Sites : observed_in\n    Plants {\n        numeric id PK\n        date date\n        numeric site FK\n        string sp_code\n        numeric sp_height\n    }\n    Sites {\n        numeric site PK\n        string name\n        numeric altitude\n    }\n\n\n\n\n\n\n\n\n\n\n\nHere is an example of a more complicated ERD, with details about how to indicate the other types of relationships.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you need to produce a publishable ERD such as the one above, Mermaid is a great option. Here is the Mermaid “code” used to produce the ERD above for our simple Plants and Sites database. Just like with R code, we can put this in a Quarto code chunk, and it will be used to generate a visual diagram when we render the document.\n\nerDiagram\n    Plants |{--|| Sites : observed_in\n    Sites {\n        numeric site PK\n        string name\n        numeric altitude\n    }\n    Plants {\n        numeric id PK\n        date date\n        numeric site FK\n        string sp_code\n        numeric sp_height\n    }\n\n\n\n\nerDiagram\n    Plants |{--|| Sites : observed_in\n    Sites {\n        numeric site PK\n        string name\n        numeric altitude\n    }\n    Plants {\n        numeric id PK\n        date date\n        numeric site FK\n        string sp_code\n        numeric sp_height\n    }\n\n\n\n\n\n\nRead more here about how to use this tool to create diagrams.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Modeling Essentials</span>"
    ]
  },
  {
    "objectID": "session_06.html#data-modeling-exercise",
    "href": "session_06.html#data-modeling-exercise",
    "title": "6  Data Modeling Essentials",
    "section": "6.4 Data modeling exercise",
    "text": "6.4 Data modeling exercise\nLet’s practice what we’ve learned!\nIn this exercise, we’ll be working with a tidied up version of a dataset from ADF&G containing commercial catch data from 1878-1997. The dataset and reference to the original source can be viewed at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2. That site includes metadata describing the full data set, including column definitions.\nHere’s the first catch table:\n\n\n\n\n\n\nAnd here’s the region_defs table:\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nUsing the live session started by your instructor, work through the following tasks:\nPart 1. Draw an Entity Relationship diagram for the tables, including labels as relevant for primary and foreign keys, along with entity relationships and corresponding cardinality.\nPart 2. Is the catch table in normal (aka tidy) form? If so, what single type of entity was observed? If not, how might you restructure the data to make it normalized, tidy, and well-modeled overall? Draw a new ERD for your new tables, again detailing all keys and relationships.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPart 1. We can start by creating one box for each of the two tables in the dataset. We have a catch table with annual catch by region for several species, and region_defs table with one record for each region. Next, we list all of the columns contained within each table.\nWhat about keys? The catch table does not have a column uniquely identifying each record! However, given that the observations here are catch within region by year, the combination of region and year does the trick. To denote this composite primary key, we can label both fields as PK in this table.\nMeanwhile, the region table appears to use code to uniquely identify each region, and this field is referenced in the catch table. So we’ll label code as the PK within the region_defs table, and label the corresponding (but differently named!) Region field as FK in the catch table. Note that the region_defs table also somewhat confusingly has a regionCode field, which superficially sounds like a candidate primary key. However, looking at the data, it is clearly that this does not uniquely identify each record in the region_defs table, nor is it referenced from the catch table.\nFinally, let’s document the relationships. Each annual catch record counts fish caught within a particular region, so we definitely need a line between them to express this relationship. What is the cardinality? Clearly each catch record belongs to exactly one region, so we’ll use “one and ONLY one” on the region_defs side. Meanwhile, there can be many catch record per region. We could simple choose to label this as “many” cardinality, but let’s be more specific and label it as “one or many”; this asserts that the region table only contain regions that have at least one catch record.\n\n\n\n\n\n\nPart 2. There are number of changes we might make here! There’s no single correct answer, but here are some improvements you might consider.\n\nThe catch table is clearly not tidy, using a wide format. Let’s first pivot that to a long format, retaining the existing region and year identifier fields, then adding a single species field with a corresponding count (of fish caught) field.\nAs part of the previous step, note that we’ve eliminated the All field. If this is the sum of the species counts, then it a marginal statistic, and not considered tidy. We could always recompute it later by summing over all of the species-specific counts. If this is not (always) the sum of individual counts, then this raises questions about where the values come from, and could lead us to consider bigger changes in how the data is recorded and how the tables is designed. For this exercise, let’s just assume its a marginal sum, and discard the column.\nWhile we’re at it, let’s address the confusion around multiple meanings for “region”, and corresponding inconsistency in table and field names. According to the documention, the catch counts are really by area, and areas are located within regions identified by the regionCode field. So let’s rename our region_defs table to Area, and make its field names more consistent by renaming code to area_id, mgmtArea to area_name, and areaClass to area_class.\nFor consistency with common (although by no means required) convention, let’s also use lower case in all field names, while capitalizing table names.\nFinally, let’s consider the NotesRegCode. Strictly speaking, in the original tables, each value corresponds to the observed catch for a given year and region. If we kept this field in our new Catch table in long format, it would be repeated for each species observation in a given year and area, and therefore would not be tidy. To retain the original semantics, we can put these notes in a new table called Catch_notes, with a composite primary key based on area_id and year. Those two matching columns in the Catch table would therefore be FKs to the Catch_nodes table, and we might say that that catch observations are described_by catch notes. With respect to cardinality, we see in the data that not all observations have an associated note, while it’s clear that with our new long Catch table, any given note will refer to all species-level observations within each area and year. Hence let’s indicate that catch observations are described by “zero or one” notes, and notes describe “one or many” species catch observations.\n\nHere’s an ERD reflecting the changes describe above.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBonus challenge\n\n\n\nIf you have time, take on this extra challenge with your group.\nNavigate to this dataset: Richard Lanctot and Sarah Saalfeld. 2019. Utqiaġvik shorebird breeding ecology study, Utqiaġvik, Alaska, 2003-2018. Arctic Data Center. doi:10.18739/A23R0PT35\nRead the dataset documention, inspect the tables, and then try to create an ERD that includes the following tables from the package:\n\nUtqiagvik_adult_shorebird_banding.csv\nUtqiagvik_egg_measurements.csv\nUtqiagvik_nest_data.csv\nUtqiagvik_shorebird_resightings.csv",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Modeling Essentials</span>"
    ]
  },
  {
    "objectID": "session_06.html#resources",
    "href": "session_06.html#resources",
    "title": "6  Data Modeling Essentials",
    "section": "6.5 Resources",
    "text": "6.5 Resources\n\nBorer et al. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America.\nWhite et al. 2013. Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution 6.\nSoftware Carpentry SQL tutorial\nTidy Data\n\n\n\n\n\nBorer, Elizabeth, Eric Seabloom, Matthew B. Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” Bulletin of the Ecological Society of America 90: 205–14. https://doi.org/10.1890/0012-9623-90.2.205.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Modeling Essentials</span>"
    ]
  },
  {
    "objectID": "session_07.html",
    "href": "session_07.html",
    "title": "7  Cleaning and Wrangling Data",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#learning-objectives",
    "href": "session_07.html#learning-objectives",
    "title": "7  Cleaning and Wrangling Data",
    "section": "",
    "text": "Introduce dplyr and tidyr functions to clean and wrangle data for analysis\nLearn about the Split-Apply-Combine strategy and how it applies to data wrangling\nDescribe the difference between wide vs. long table formats and how to convert between them",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#introduction",
    "href": "session_07.html#introduction",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nThe data we get to work with are rarely, if ever, in the format we need to do our analyses. It’s often the case that one package requires data in one format, while another package requires the data to be in another format. To be efficient analysts, we should have good tools for reformatting data for our needs so we can do further work like making plots and fitting models. The dplyr and tidyr R packages provide a fairly complete and extremely powerful set of functions for us to do this reformatting quickly. Learning these tools well will greatly increase your efficiency as an analyst.\nLet’s look at two motivating examples.\n\n\n\n\n\n\nExample 1\n\n\n\nSuppose you have the following data.frame called length_data with data about salmon length and want to calculate the average length per year.\n\n\n\nyear\nlength_cm\n\n\n\n\n1990\n5.6\n\n\n1991\n3.0\n\n\n1991\n4.5\n\n\n1992\n4.3\n\n\n1992\n5.5\n\n\n1992\n4.9\n\n\n\nBefore thinking about the code, let’s think about the steps we need to take to get to the answer (aka pseudocode).\nNow, how would we code this? The dplyr R library provides a fast and powerful way to do this calculation in a few lines of code:\n\n\nAnswer\nlength_data %&gt;% \n  group_by(year) %&gt;% \n  summarize(mean_length_cm = mean(length_cm))\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\nAnother process we often need to do is to “reshape” our data. Consider the following table that is in what we call “wide” format:\n\n\n\nsite\n1990\n1991\n…\n1993\n\n\n\n\ngold\n101\n109\n…\n112\n\n\nlake\n104\n98\n…\n102\n\n\n…\n…\n…\n…\n…\n\n\ndredge\n144\n118\n…\n145\n\n\n\nYou are probably familiar with data in the above format, where values of the variable being observed are spread out across columns. In this example we have a different column per year. This wide format works well for data entry and sometimes works well for analysis but we quickly outgrow it when using R (and know it is not tidy data!). For example, how would you fit a model with year as a predictor variable? In an ideal world, we’d be able to just run lm(length ~ year). But this won’t work on our wide data because lm() needs length and year to be columns in our table.\nWhat steps would you take to get this data frame in a long format?\nThe tidyr package allows us to quickly switch between wide format and long format using the pivot_longer() function:\n\n\nAnswer\nsite_data %&gt;% \n  pivot_longer(-site, \n               names_to = \"year\", \n               values_to = \"length\")\n\n\n\n\n\nsite\nyear\nlength\n\n\n\n\ngold\n1990\n101\n\n\nlake\n1990\n104\n\n\ndredge\n1990\n144\n\n\n…\n…\n…\n\n\ndredge\n1993\n145\n\n\n\n\n\nThis lesson will cover examples to learn about the functions you’ll most commonly use from the dplyr and tidyr packages:\n\nCommon dplyr functions\n\n\n\n\n\n\nFunction name\nDescription\n\n\n\n\nmutate()\nCreates modify and deletes columns\n\n\ngroup_by()\nGroups data by one or more variables\n\n\nsummarise()\nSummaries each group down to one row\n\n\nselect()\nKeep or drop columns using their names\n\n\nfilter()\nKeeps rows that matches conditions\n\n\narrange()\norder rows using columns variable\n\n\nrename()\nRename a column\n\n\n\n\nCommon tidyr functions\n\n\n\n\n\n\nFunction name\nDescription\n\n\n\n\npivot_longer()\ntransforms data from a wide to a long format\n\n\npivot_wider()\ntransforms data from a long to a wide format\n\n\nunite()\nUnite multiple columns into one by pasting strings together\n\n\nseparate()\nSeparate a character column into multiple columns with a regular expression or numeric locations",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#data-cleaning-basics",
    "href": "session_07.html#data-cleaning-basics",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.2 Data cleaning basics",
    "text": "7.2 Data cleaning basics\nTo demonstrate, we’ll be working with a tidied up version of a data set from Alaska Department of Fish & Game containing commercial catch data from 1878-1997. The data set and reference to the original source can be found at its public archive.\n\n\n\n\n\n\nSetup\n\n\n\nFirst, open a new Quarto document. Delete everything below the setup chunk, and add a library chunk that calls dplyr, tidyr, and readr\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\n\n\n\n\n\n\n\n\n\nA note on loading packages\n\n\n\nYou may have noticed the following messages pop up when you ran your library chunk.\nAttaching package: ‘dplyr’\n\nThe following objects are masked from ‘package:stats’:\n\n    filter, lag\n\nThe following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\nThese are important messages. They are letting you know that certain functions from the stats and base packages (which are loaded by default when you start R) are masked by different functions with the same name in the dplyr package. It turns out, the order that you load the packages in matters. Since we loaded dplyr after stats, R will assume that if you call filter(), you mean the dplyr version unless you specify otherwise.\nBeing specific about which version of filter(), for example, you call is easy. To explicitly call a function by its unambiguous name, we use the syntax package_name::function_name(...). So, if we wanted to call the stats version of filter() in this Rmarkdown document, I would use the syntax stats::filter(...).\n\n\n\n\n\n\n\n\nRemove messages and warnings\n\n\n\nMessages and warnings are important, but we might not want them in our final document. After you have read the packages in, adjust the chunk settings in your library chunk to suppress warnings and messages by adding #| message: false or #| warning: false. Both of these chunk options, when set to false, prevents messages or warnings from appearing in the rendered file.\n\n\nNow that we have introduced some data wrangling libraries, let’s get the data that we are going to use for this lesson.\n\n\n\n\n\n\nSetup\n\n\n\n\nGo to KNB Data Package Alaska commercial salmon catches by management region (1886- 1997)\nFind the data file df35b.302.1. Right click the “Download” button and select “Copy Link Address”\nPaste the copied URL into the read_csv() function\n\nThe code chunk you use to read in the data should look something like this:\n\ncatch_original &lt;- read_csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1\")\n\n\n\n\n\n\n\n\n\n\nThis data set is relatively clean and easy to interpret as-is. While it may be clean, it’s in a shape that makes it hard to use for some types of analyses so we’ll want to fix that first.\n\n\n\n\n\n\nExercise\n\n\n\nBefore we get too much further, spend a minute or two outlining your Quarto document so that it includes the following sections and steps:\n\nData Sources\n\nRead in the data\nExplore data\n\nClean and Reshape data\n\nUsing select() function\nCheck column types\nReplace values in a column with mutate()\nReshape data with pivot_longer() and pivot_wider()\nRename columns rename()\nAdd columns with mutate()\nSummary stats using group_by() and summarize()\nFiltering rows using filter()\nSort data using arrange()\nSplit and combine values in columns with separate() and unite()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#data-exploration",
    "href": "session_07.html#data-exploration",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.3 Data exploration",
    "text": "7.3 Data exploration\nSimilar to what we did in our Literate Analysis lesson, it is good practice to skim through the data you just read in.\nDoing so is important to make sure the data is read as you were expecting and to familiarize yourself with the data.\nSome of the basic ways to explore your data are:\n\n## Prints the column names of my data frame\ncolnames(catch_original)\n\n## First 6 lines of the data frame\nhead(catch_original)\n\n## Summary of each column of data\nsummary(catch_original)\n\n## Prints unique values in a column (in this case, the region)\nunique(catch_original$Region)\n\n## Opens data frame in its own tab to see each row and column of the data (do in console)\nView(catch_original)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#about-the-pipe-operator",
    "href": "session_07.html#about-the-pipe-operator",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.4 About the pipe (%>%) operator",
    "text": "7.4 About the pipe (%&gt;%) operator\nBefore we jump into learning tidyr and dplyr, we first need to explain the pipeline operator %&gt;%.\nBoth the tidyr and the dplyr packages use the pipe operator (%&gt;%), which may look unfamiliar. The pipe is a powerful way to efficiently chain together operations. The pipe will take the output of a previous statement, and use it as the input to the next statement.\nSay you want to both filter() out rows of a data set, and select() certain columns.\nInstead of writing:\n\ndf_filtered &lt;- filter(df, ...)\ndf_selected &lt;- select(df_filtered, ...)\n\nYou can write:\n\ndf_cleaned &lt;- df %&gt;% \n    filter(...) %&gt;%\n    select(...)\n\nIf you think of the assignment operator (&lt;-) as reading like “gets”, then the pipe operator would read like “then”.\nSo you might think of the above chunk being translated as:\n\nThe cleaned data frame gets the original data, and then a filter (of the original data), and then a select (of the filtered data).\n\nThe benefits to using pipes are that you don’t have to keep track of (or overwrite) intermediate data frames. The drawbacks are that it can be more difficult to explain the reasoning behind each step, especially when many operations are chained together. It is good to strike a balance between writing efficient code (chaining operations), while ensuring that you are still clearly explaining, both to your future self and others, what you are doing and why you are doing it.\n\n\n\n\n\n\nQuick Tip\n\n\n\nRStudio has a keyboard shortcut for %&gt;%\n\nWindows: Ctrl + Shift + M\nMac: cmd + shift + M",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#selecting-or-removing-columns-using-select",
    "href": "session_07.html#selecting-or-removing-columns-using-select",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.5 Selecting or removing columns using select()",
    "text": "7.5 Selecting or removing columns using select()\nWe’re ready to go back to our salmon dataset. The first issue is the extra columns All and notesRegCode. Let’s select only the columns we want, and assign this to a variable called catch_data.\n\ncatch_data &lt;- catch_original %&gt;%\n    select(Region, Year, Chinook, Sockeye, Coho, Pink, Chum)\n\nhead(catch_data)\n\n# A tibble: 6 × 7\n  Region  Year Chinook Sockeye  Coho  Pink  Chum\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 SSE     1886 0             5     0     0     0\n2 SSE     1887 0           155     0     0     0\n3 SSE     1888 0           224    16     0     0\n4 SSE     1889 0           182    11    92     0\n5 SSE     1890 0           251    42     0     0\n6 SSE     1891 0           274    24     0     0\n\n\nMuch better!\nThe select() function also allows you to say which columns you don’t want, by passing unquoted column names preceded by minus (-) signs:\n\ncatch_data &lt;- catch_original %&gt;%\n    select(-All,-notesRegCode)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#quality-check",
    "href": "session_07.html#quality-check",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.6 Quality check",
    "text": "7.6 Quality check\nNow that we have the data we are interested in using, we should do a little quality check to see that everything seems as expected. One nice way of doing this is the glimpse() function.\n\ndplyr::glimpse(catch_data)\n\nRows: 1,708\nColumns: 7\n$ Region  &lt;chr&gt; \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\",…\n$ Year    &lt;dbl&gt; 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 18…\n$ Chinook &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"3\", \"4\", \"5\", \"9…\n$ Sockeye &lt;dbl&gt; 5, 155, 224, 182, 251, 274, 207, 189, 253, 408, 989, 791, 708,…\n$ Coho    &lt;dbl&gt; 0, 0, 16, 11, 42, 24, 11, 1, 5, 8, 192, 161, 132, 139, 84, 107…\n$ Pink    &lt;dbl&gt; 0, 0, 0, 92, 0, 0, 8, 187, 529, 606, 996, 2218, 673, 1545, 204…\n$ Chum    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 2, 0, 0, 0, 102, 343…\n\n\n\n\n\n\n\n\nExercise\n\n\n\nExamine the output of the glimpse() function call. Does anything seem amiss with this data set that might warrant fixing?\n\n\nAnswer:\n\nThe Chinook catch data are character class. Let’s fix it using the function mutate() before moving on.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#changing-column-content-using-mutate",
    "href": "session_07.html#changing-column-content-using-mutate",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.7 Changing column content using mutate()",
    "text": "7.7 Changing column content using mutate()\nWe can use the mutate() function to change a column, or to create a new column. First, let’s try to convert the Chinook catch values to numeric type using the as.numeric() function, and overwrite the old Chinook column.\n\ncatch_clean &lt;- catch_data %&gt;%\n    mutate(Chinook = as.numeric(Chinook))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `Chinook = as.numeric(Chinook)`.\nCaused by warning:\n! NAs introduced by coercion\n\nhead(catch_clean)\n\n# A tibble: 6 × 7\n  Region  Year Chinook Sockeye  Coho  Pink  Chum\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 SSE     1886       0       5     0     0     0\n2 SSE     1887       0     155     0     0     0\n3 SSE     1888       0     224    16     0     0\n4 SSE     1889       0     182    11    92     0\n5 SSE     1890       0     251    42     0     0\n6 SSE     1891       0     274    24     0     0\n\n\nWe get a warning \"NAs introduced by coercion\" which is R telling us that it couldn’t convert every value to an integer and, for those values it couldn’t convert, it put an NA in its place. This is behavior we commonly experience when cleaning data sets and it’s important to have the skills to deal with it when it comes up.\nTo investigate, let’s isolate the issue. We can find out which values are NAs with a combination of is.na() and which(), and save that to a variable called i.\n\ni &lt;- which(is.na(catch_clean$Chinook))\ni\n\n[1] 401\n\n\nIt looks like there is only one problem row, lets have a look at it in the original data.\n\ncatch_data[i,]\n\n# A tibble: 1 × 7\n  Region  Year Chinook Sockeye  Coho  Pink  Chum\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 GSE     1955 I            66     0     0     1\n\n\nWell that’s odd: The value in Chinook is the letter I. It turns out that this data set is from a PDF which was automatically converted into a csv and this value of I is actually a 1.\nLet’s fix it by incorporating the if_else() function to our mutate() call, which will change the value of the Chinook column to 1 if the value is equal to I, then will use as.numeric() to turn the character representations of numbers into numeric typed values.\n\ncatch_clean &lt;- catch_data %&gt;%\n    mutate(Chinook = if_else(condition = Chinook == \"I\", \n                             true = \"1\", \n                             false = Chinook),\n           Chinook = as.numeric(Chinook))\n\n##check\ncatch_clean[i, ]\n\n# A tibble: 1 × 7\n  Region  Year Chinook Sockeye  Coho  Pink  Chum\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 GSE     1955       1      66     0     0     1",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#changing-shape-using-pivot_longer-and-pivot_wider",
    "href": "session_07.html#changing-shape-using-pivot_longer-and-pivot_wider",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.8 Changing shape using pivot_longer() and pivot_wider()",
    "text": "7.8 Changing shape using pivot_longer() and pivot_wider()\nThe next issue is that the data are in a wide format and we want the data in a long format instead. The function pivot_longer() from the tidyr package helps us do this conversion. If you do not remember all the arguments that go into pivot_longer() you can always call the help page by typing ?pivot_longer in the console.\n\ncatch_long &lt;- catch_clean %&gt;% \n    #pivot longer all columns except Region and Year\n    pivot_longer(\n        cols = -c(Region, Year),\n        names_to = \"species\",\n        values_to = \"catch\"\n    )\n\nhead(catch_long)\n\n# A tibble: 6 × 4\n  Region  Year species catch\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 SSE     1886 Chinook     0\n2 SSE     1886 Sockeye     5\n3 SSE     1886 Coho        0\n4 SSE     1886 Pink        0\n5 SSE     1886 Chum        0\n6 SSE     1887 Chinook     0\n\n\nThe syntax we used above for pivot_longer() might be a bit confusing so let’s walk though it.\n\nThe first argument to pivot_longer is the columns over which we are pivoting. You can select these by listing either the names of the columns you do want to pivot, or in this case, the names of the columns you are not pivoting over.\nThe names_to argument: this is the name of the column that you are creating from the column names of the columns you are pivoting over.\nThe values_to argument: the name of the column that you are creating from the values in the columns you are pivoting over.\n\nThe opposite of pivot_longer() is the pivot_wider() function. It works in a similar declarative fashion:\n\ncatch_wide &lt;- catch_long %&gt;%\n    pivot_wider(names_from = species,\n                values_from = catch)\n\nhead(catch_wide)\n\n# A tibble: 6 × 7\n  Region  Year Chinook Sockeye  Coho  Pink  Chum\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 SSE     1886       0       5     0     0     0\n2 SSE     1887       0     155     0     0     0\n3 SSE     1888       0     224    16     0     0\n4 SSE     1889       0     182    11    92     0\n5 SSE     1890       0     251    42     0     0\n6 SSE     1891       0     274    24     0     0\n\n\nSame than we did above we can pull up the documentation of the function to remind ourselves what goes in which argument. Type ?pivot_wider in the console.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#renaming-columns-with-rename",
    "href": "session_07.html#renaming-columns-with-rename",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.9 Renaming columns with rename()",
    "text": "7.9 Renaming columns with rename()\nIf you scan through the data, you may notice the values in the catch column are very small (these are supposed to be annual catches). If we look at the metadata we can see that the catch column is in thousands of fish, so let’s convert it before moving on.\nLet’s first rename the catch column to be called catch_thousands:\n\ncatch_long &lt;- catch_long %&gt;%\n    rename(catch_thousands = catch)\n\nhead(catch_long)\n\n# A tibble: 6 × 4\n  Region  Year species catch_thousands\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1 SSE     1886 Chinook               0\n2 SSE     1886 Sockeye               5\n3 SSE     1886 Coho                  0\n4 SSE     1886 Pink                  0\n5 SSE     1886 Chum                  0\n6 SSE     1887 Chinook               0\n\n\n\n\n\n\n\n\nnames() versus rename()\n\n\n\nMany people use the base R function names() to rename columns, often in combination with column indexing that relies on columns being in a particular order. Column indexing is often also used to select columns instead of the select() function from dplyr. Although these methods work just fine, they do have one major drawback: in most implementations they rely on you knowing exactly the column order your data is in.\nTo illustrate why your knowledge of column order isn’t reliable enough for these operations, considering the following scenario:\nYour colleague emails you letting you know that she has an updated version of the conductivity-temperature-depth data from this year’s research cruise, and sends it along. Excited, you re-run your scripts that use this data for your phytoplankton research. You run the script and suddenly all of your numbers seem off. You spend hours trying to figure out what is going on.\nUnbeknownst to you, your colleagues bought a new sensor this year that measures dissolved oxygen. Because of the new variables in the data set, the column order is different. Your script which previously renamed the fourth column, SAL_PSU to salinity now renames the fourth column, O2_MGpL to salinity. No wonder your results looked so weird, good thing you caught it!\nIf you had written your code so that it doesn’t rely on column order, but instead renames columns using the rename() function, the code would have run just fine (assuming the name of the original salinity column didn’t change, in which case the code would have thrown an error in an obvious way). This is an example of a defensive coding strategy, where you try to anticipate issues before they arise, and write your code in such a way as to keep the issues from happening.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#adding-columns-using-mutate",
    "href": "session_07.html#adding-columns-using-mutate",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.10 Adding columns using mutate()",
    "text": "7.10 Adding columns using mutate()\nNow let’s use mutate() again to create a new column called catch with units of fish (instead of thousands of fish).\n\ncatch_long &lt;- catch_long %&gt;%\n    mutate(catch = catch_thousands * 1000)\n\nhead(catch_long)\n\nLet’s remove the catch_thousands column for now since we don’t need it. Note that here we have added to the expression we wrote above by adding another function call (mutate) to our expression. This takes advantage of the pipe operator by grouping together a similar set of statements, which all aim to clean up the catch_clean data frame.\n\ncatch_long &lt;- catch_long %&gt;%\n    mutate(catch = catch_thousands * 1000) %&gt;%\n    select(-catch_thousands)\n\nhead(catch_long)\n\n# A tibble: 6 × 4\n  Region  Year species catch\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 SSE     1886 Chinook     0\n2 SSE     1886 Sockeye  5000\n3 SSE     1886 Coho        0\n4 SSE     1886 Pink        0\n5 SSE     1886 Chum        0\n6 SSE     1887 Chinook     0\n\n\nWe’re now ready to start analyzing the data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#summary-statistics-using-group_by-and-summarize",
    "href": "session_07.html#summary-statistics-using-group_by-and-summarize",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.11 Summary statistics using group_by() and summarize()",
    "text": "7.11 Summary statistics using group_by() and summarize()\nSuppose we are now interested in getting the average catch per region. In our initial data exploration we saw there are 18 regions, we can easily see their names again:\n\nunique(catch_original$Region)\n\n [1] \"SSE\" \"NSE\" \"YAK\" \"GSE\" \"BER\" \"COP\" \"PWS\" \"CKI\" \"BRB\" \"KSK\" \"YUK\" \"NRS\"\n[13] \"KTZ\" \"KOD\" \"CHG\" \"SOP\" \"ALU\" \"NOP\"\n\n\nThink about how we would calculate the average catch per region “by hand”. It would be something like this:\n\nWe start with our table and notice there are multiple regions in the “Regions” column.\nWe split our original table to group all observations from the same region together.\nWe calculate the average catch for each of the groups we form.\nThen we combine the values for average catch per region into a single table.\n\n\n\n\n\n\n\n\nAnalyses like this conform to what is known as the Split-Apply-Combine strategy. This strategy follows the three steps we explained above:\n\nSplit: Split the data into logical groups (e.g., region, species, etc.)\nApply: Calculate some summary statistic on each group (e.g. mean catch by year, number of individuals per species)\nCombine: Combine the statistic calculated on each group back together into a single table\n\nThe dplyr library lets us easily employ the Split-Apply-Combine strategy by using the group_by() and summarize() functions:\n\nmean_region &lt;- catch_long %&gt;%\n    group_by(Region) %&gt;%\n    summarize(mean_catch = mean(catch))\n\nhead(mean_region)\n\n# A tibble: 6 × 2\n  Region mean_catch\n  &lt;chr&gt;       &lt;dbl&gt;\n1 ALU        40384.\n2 BER        16373.\n3 BRB      2709796.\n4 CHG       315487.\n5 CKI       683571.\n6 COP       179223.\n\n\nLet’s see how the previous code implements the Split-Apply-Combine strategy:\n\ngroup_by(Region): this is telling R to split the dataframe and create a group for each different value in the column Region. R just keeps track of the groups, it doesn’t return separate dataframes per region.\nmean(catch): here mean is the function we want to apply to the column catch in each group.\nsummarize(catch = mean(catch)) the function summarize() is used to combine the results of mean(catch) in each group into a single table. The argument mean_catch = mean(catch) indicates that the column having the results of mean(catch) will be named mean_catch.\n\nAnother common use of group_by() followed by summarize() is to count the number of rows in each group. We have to use a special function from dplyr, n().\n\nn_region &lt;- catch_long %&gt;%\n    group_by(Region) %&gt;%\n    summarize(n = n())\n\nhead(n_region)\n\n# A tibble: 6 × 2\n  Region     n\n  &lt;chr&gt;  &lt;int&gt;\n1 ALU      435\n2 BER      510\n3 BRB      570\n4 CHG      550\n5 CKI      525\n6 COP      470\n\n\n\n\n\n\n\n\nTry using count()\n\n\n\nIf you are finding that you are reaching for this combination of group_by(), summarize() and n() a lot, there is a helpful dplyr function count() that accomplishes this in one function!\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nFind another grouping and statistic to calculate for each group.\nFind out if you can group by multiple variables.\n\n\n\nAnswer\n## for example:\ncatch_year_sp &lt;- catch_long %&gt;%\n    group_by(Year, species) %&gt;%\n    summarize(total_year = sum(catch, na.rm = T))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#filtering-rows-using-filter",
    "href": "session_07.html#filtering-rows-using-filter",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.12 Filtering rows using filter()",
    "text": "7.12 Filtering rows using filter()\nWe use the filter() function to filter our data.frame to rows matching some condition. It’s similar to subset() from base R.\nLet’s go back to our original data.frame and do some filter()ing:\n\nsse_catch &lt;- catch_long %&gt;%\n    filter(Region == \"SSE\")\n\nhead(sse_catch)\n\n# A tibble: 6 × 4\n  Region  Year species catch\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 SSE     1886 Chinook     0\n2 SSE     1886 Sockeye  5000\n3 SSE     1886 Coho        0\n4 SSE     1886 Pink        0\n5 SSE     1886 Chum        0\n6 SSE     1887 Chinook     0\n\n\n\n\n\n\n\n\n== and %in% operators\n\n\n\nThe filter() function performs a logical test across all the rows of a dataframe, and if that test is TRUE for a given row, it keeps that row. The == operator tests whether the left hand side and right hand side match - in the example above, does the value of the Region variable match the value \"SSE\"?\nBut if you want to test whether a variable’s value is within a set of possible values, do not use the == operator - it will very likely give false results! Instead, use the %in% operator:\n\ncatch_long %&gt;%\n  filter(Region == c(\"SSE\", \"ALU\")) %&gt;%\n  nrow()\n\n[1] 498\n\ncatch_long %&gt;%\n  filter(Region %in% c(\"SSE\", \"ALU\")) %&gt;%\n  nrow()\n\n[1] 995\n\n\nThis is because the == version “recycles” the vector of allowed values, so it tests whether the first row matches \"SSE\" (yep!), whether the second matches \"ALU\" (nope! this row gets dropped!), and then whether the third is \"SSE\" again and so on.\nNote that the %in% operator actually works for single values too, so you can never go wrong with that!\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nFilter to just catches of over one million fish\nFilter to just Chinook from the SSE region\n\n\n\nAnswer\n## Catches over a million fish\ncatch_million &lt;- catch_long %&gt;%\n    filter(catch &gt; 1000000)\n\n## Chinook from SSE data\nchinook_sse &lt;- catch_long %&gt;%\n    filter(Region == \"SSE\",\n           species == \"Chinook\")\n\n## OR combine tests with & (\"and\") or | (\"or\")... also, we can swap == for %in%\nchinook_sse &lt;- catch_long %&gt;%\n    filter(Region %in% \"SSE\" & species %in% \"Chinook\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#sorting-your-data-using-arrange",
    "href": "session_07.html#sorting-your-data-using-arrange",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.13 Sorting your data using arrange()",
    "text": "7.13 Sorting your data using arrange()\nThe arrange() function is used to sort the rows of a data.frame. Two common cases to use arrange() are:\n\nTo calculate a cumulative sum (with cumsum()) so row order matters\nTo display a table (like in an .qmd document) in sorted order\n\nLet’s re-calculate mean catch by region, and then arrange() the output by mean catch:\n\nmean_region &lt;- catch_long %&gt;%\n    group_by(Region) %&gt;%\n    summarize(mean_catch = mean(catch)) %&gt;%\n    arrange(mean_catch)\n\nhead(mean_region)\n\n# A tibble: 6 × 2\n  Region mean_catch\n  &lt;chr&gt;       &lt;dbl&gt;\n1 BER        16373.\n2 KTZ        18836.\n3 ALU        40384.\n4 NRS        51503.\n5 KSK        67642.\n6 YUK        68646.\n\n\nThe default sorting order of arrange() is to sort in ascending order. To reverse the sort order, wrap the column name inside the desc() function:\n\nmean_region &lt;- catch_long %&gt;%\n    group_by(Region) %&gt;%\n    summarize(mean_catch = mean(catch)) %&gt;%\n    arrange(desc(mean_catch))\n\nhead(mean_region)\n\n# A tibble: 6 × 2\n  Region mean_catch\n  &lt;chr&gt;       &lt;dbl&gt;\n1 SSE      3184661.\n2 BRB      2709796.\n3 NSE      1825021.\n4 KOD      1528350 \n5 PWS      1419237.\n6 SOP      1110942.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#splitting-a-column-using-separate-and-unite",
    "href": "session_07.html#splitting-a-column-using-separate-and-unite",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.14 Splitting a column using separate() and unite()",
    "text": "7.14 Splitting a column using separate() and unite()\nThe separate() function allow us to easily split a single column into numerous. Its complement, the unite() function, allows us to combine multiple columns into a single one.\nThis can come in really handy when we need to split a column into two pieces by a consistent separator (like a dash).\nLet’s make a new data.frame with fake data to illustrate this. Here we have a set of site identification codes with information about the island where the site is (the first 3 letters) and a site number (the 3 numbers). If we want to group and summarize by island, we need a column with just the island information.\n\nsites_df &lt;- data.frame(site = c(\"HAW-101\",\n                                \"HAW-103\",\n                                \"OAH-320\",\n                                \"OAH-219\",\n                                \"MAU-039\"))\n\nsites_df %&gt;%\n    separate(site, c(\"island\", \"site_number\"), \"-\")\n\n  island site_number\n1    HAW         101\n2    HAW         103\n3    OAH         320\n4    OAH         219\n5    MAU         039\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSplit the city column in the data frame cities_df into city and state_code columns\n\n## create `cities_df`\ncities_df &lt;- data.frame(city = c(\"Juneau AK\",\n                                 \"Sitka AK\",\n                                 \"Anchorage AK\"))\n\n\n\nAnswer\ncolnames(cities_df)\n\ncities_clean &lt;- cities_df %&gt;%\n    separate(city, c(\"city\", \"state_code\"), \" \")\n\n\n\n\nThe unite() function does just the reverse of separate(). If we have a data.frame that contains columns for year, month, and day, we might want to unite these into a single date column.\n\ndates_df &lt;- data.frame(\n    year = c(\"1930\",\n             \"1930\",\n             \"1930\"),\n    month = c(\"12\",\n              \"12\",\n              \"12\"),\n    day = c(\"14\",\n            \"15\",\n            \"16\")\n)\n\ndates_df %&gt;%\n    unite(date, year, month, day, sep = \"-\")\n\n        date\n1 1930-12-14\n2 1930-12-15\n3 1930-12-16",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#now-all-together",
    "href": "session_07.html#now-all-together",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.15 Now, all together!",
    "text": "7.15 Now, all together!\nWe just ran through the various things we can do with dplyr and tidyr but if you’re wondering how this might look in a real analysis. Let’s look at that now:\n\ncatch_original &lt;- read_csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1\")\n\nmean_region &lt;- catch_original %&gt;%\n  select(-All, -notesRegCode) %&gt;% \n  mutate(Chinook = if_else(Chinook == \"I\", \"1\", Chinook)) %&gt;% \n  mutate(Chinook = as.numeric(Chinook)) %&gt;% \n  pivot_longer(-c(Region, Year), \n               names_to = \"species\", \n               values_to = \"catch\") %&gt;%\n  mutate(catch = catch * 1000) %&gt;% \n  group_by(Region) %&gt;% \n  summarize(mean_catch = mean(catch)) %&gt;% \n  arrange(desc(mean_catch))\n\nhead(mean_region)\n\n# A tibble: 6 × 2\n  Region mean_catch\n  &lt;chr&gt;       &lt;dbl&gt;\n1 SSE      3184661.\n2 BRB      2709796.\n3 NSE      1825021.\n4 KOD      1528350 \n5 PWS      1419237.\n6 SOP      1110942.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_07.html#write-out-the-results-with-readrwrite_csv",
    "href": "session_07.html#write-out-the-results-with-readrwrite_csv",
    "title": "7  Cleaning and Wrangling Data",
    "section": "7.16 Write out the results with readr::write_csv()",
    "text": "7.16 Write out the results with readr::write_csv()\nNow that we have performed all this data wrangling, we can save out the results for future use using readr::write_csv().\n\nwrite_csv(mean_region, here::here(\"data/mean_catch_by_region.csv\"))\n\nWe have completed our lesson on Cleaning and Wrangling data. Before we break, let’s practice our Git workflow.\n\n\n\n\n\n\nSteps\n\n\n\n\nSave the .qmd you have been working on for this lesson.\nRender the Quarto file. This is a way to test everything in your code is working.\nStage (Add) &gt; Commit &gt; Pull &gt; Push",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Cleaning and Wrangling Data</span>"
    ]
  },
  {
    "objectID": "session_08.html",
    "href": "session_08.html",
    "title": "8  Collaborating with git and GitHub",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collaborating with git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_08.html#learning-objectives",
    "href": "session_08.html#learning-objectives",
    "title": "8  Collaborating with git and GitHub",
    "section": "",
    "text": "Apply the principles, features, and collaboration tools of Git and GitHub to effectively collaborate with colleagues on code\nAnalyze and evaluate common causes of conflicts that arise when collaborating on repositories\nDemonstrate the ability to resolve conflicts using Git conflict resolution techniques\nApply workflows and best practices that minimize conflicts on collaborative repositories",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collaborating with git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_08.html#introduction-to-git-and-github-tools-for-collaboration",
    "href": "session_08.html#introduction-to-git-and-github-tools-for-collaboration",
    "title": "8  Collaborating with git and GitHub",
    "section": "8.1 Introduction to Git and GitHub Tools for Collaboration",
    "text": "8.1 Introduction to Git and GitHub Tools for Collaboration\n\n\n\nArtwork by Allison Horst\n\n\nGit is not only a powerful tool for individual work but also an excellent choice for collaborating with friends and colleagues. Git ensures that after you’ve completed your contributions to a repository, you can confidently synchronize your changes with changes made by others.\nOne of the easiest and most effective ways to collaborate using Git is by utilizing a shared repository on a hosting service like GitHub. This shared repository acts as a central hub, enabling collaborators to effortlessly exchange and merge their changes. With Git and a shared repository, you can collaborate seamlessly and work confidently, knowing that your changes will be integrated smoothly with those of your collaborators.\n\nThere are many advanced techniques for synchronizing Git repositories, but let’s start with a simple example.\nIn this example, the Collaborator will clone a copy of the Owner’s repository from GitHub, and the Owner will grant them Collaborator status, enabling the Collaborator to directly pull and push from the Owner’s GitHub repository.\n\n\n\n\n\nWe’ll be practicing the above workflow in the next exercises – here, a respository (aka repo) owner controls permissions on their remote repo, which is hosted on GitHub. They can push commits from their local repo to the remote repo. Similarly, they can pull commits from the remote repo to their cloned local repo(s) (remember, you can clone your repo to mulitple machines e.g. your laptop and your desktop). The repository owner adds a colleague as a collaborator by sending them an invite from the remote repo on GitHub. This collaborator can now push their own changes from their local repo to the now-shared remote repo (and also pull the Owner’s changes). Git and GitHub provide the tools for both colleagues to create and merge their changes to the shared remote repository.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collaborating with git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_08.html#collaborating-with-a-trusted-colleague-without-conflicts",
    "href": "session_08.html#collaborating-with-a-trusted-colleague-without-conflicts",
    "title": "8  Collaborating with git and GitHub",
    "section": "8.2 Collaborating with a trusted colleague without conflicts",
    "text": "8.2 Collaborating with a trusted colleague without conflicts\nWe start our collaboration by giving a trusted colleague access to our repository on GitHub. In this example, we define the Owner as the individual who owns the repository, and the Collaborator as the person whom the Owner chooses to give permission to make changes to their repository.\nThe Collaborator will make changes to the repository and then push those changes to the shared repository on GitHub. The Owner will then use pull to retrieve the changes without encountering any conflicts. This is the most ideal workflow.\nThe instructors will demonstrate this process in the next section.\n\nStep 1: Owner adds a Collaborator to their repository on GitHub\nThe Owner must change the settings of the remote repository and give the Collaborator access to the repository by inviting them as a collaborator. Once the Collaborator accepts the owner’s invitation, they will have push access to the repository – meaning they can contribute their own changes/commits to the Owner’s repository.\nTo do this, the owner will navigate to their remote repository on GitHub, then choose Settings &gt; Collaborators &gt; Add people, to send an email invitation. The invitation will show as “Pending” until accepted.\n\n\nStep 2: Collaborator clones the remote repository\nIn order to contribute, the Collaborator must clone the repository from the Owner’s GitHub account (Note: as a Collaborator, you won’t see the repository appear under your profile’s Repositories page). To do this, the Collaborator should navigate to the Owner’s repository on GitHub, then copy the clone URL. In RStudio, the Collaborator will create a new project from version control by pasting this clone URL into the appropriate dialog box (see the earlier chapter introducing GitHub).\n\n\nINTERMEDIATE STEP: Collaborator communicates with Owner that they plan to make some changes\nFrequent communication is SO important when collaborating! Letting one another know that you’re about to make and push changes to the remote repo can help to prevent merge conflicts (and reduce headaches). The easiest way to avoid merge conflicts is to ensure that you and your collaborators aren’t working on the same file(s)/section(s) of code at the same time.\n\n\nStep 3: Collaborator edits files locally\nWith the repo cloned locally, the Collaborator can now make changes to the README.md file, adding a line or statement somewhere noticeable near the top. Save the changes.\n\n\nStep 4: Collaborator commits, pulls, and pushs\nIt’s recommended that all collaborators (including the repo Owner) follow this workflow when syncing changes between their local repo and the remote repo (in this example, the Collaborator is now following these steps):\n\nadd and commit your modified file(s) (e.g. the updated README.md)\npull to fetch and merge changes from the remote/origin repository (in an ideal situation, as we’re demonstrating here, any potential changes are merged seamlessly without conflict)\npush your changes to the remote/origin repository\n\n\n\n\n\n\n\nWhy do I need to add and commit files before pulling?\n\n\n\nRemember, git pull is a combination of git fetching remote changes to your local repo and git mergeing those changes from your local repo into your local working file(s).\nThe merge part of git pull will fail if you have uncommitted changes in your local working file(s) to avoid any potential overwriting of your own changes. Because of this, you should always, add/commit then pull, and finally push.\n\n\n\n\n\n\n\n\n\nINTERMEDIATE STEP: Collaborator communicates with Owner that they pushed their changes to GitHub\nRemember, communication is key! The Owner now knows that they can pull those changes down to their local repo.\n\n\nStep 5: Owner pulls new changes from the remote repo to their local repo\nThe Owner can now open their local working copy of the code in RStudio, and pull to fetch and merge those changes into their local copy.\nCongrats, the Owner now has your changes! Now, all three repositories – the remote/origin repository on GitHub, the Owner’s local repository, and the Collaborator’s local repository – should all be in the exact same state.\n\n\nINTERMEDIATE STEP: Owner communicates with Collaborator that they now plan to make some changes\nDid we mention that communication is important? :)\n\n\nStep 6: Owner edits, commits, pulls (just in case!) and pushes\nFollowing the same workflow as the Collaborator did earlier:\n\nadd and commit your modified file(s) (e.g. the updated README.md)\npull to fetch and merge changes from the remote/origin repository (in an ideal situation, as we’re demonstrating here, any potential changes are merged seamlessly without conflict)\npush your changes to the remote/origin repository\n\n\n\nINTERMEDIATE STEP: Owner communicates with Collaborator that they pushed their changes to GitHub\nYes, this seems silly to repeat, yet again – but it’s also easy to forget in practice!\n\n\nStep 7: Collaborator pulls new changes from the remote repo to their local repo\nThe Collaborator can now pull down those changes from the Owner, and all copies are once again fully synced. And just like that, you’ve successfully collaborated!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collaborating with git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_08.html#ex1-no-conflict",
    "href": "session_08.html#ex1-no-conflict",
    "title": "8  Collaborating with git and GitHub",
    "section": "8.3 Exercise 1: With a partner collaborate in a repository without a merge conflict",
    "text": "8.3 Exercise 1: With a partner collaborate in a repository without a merge conflict\n\n\n\n\n\n\nSetup\n\n\n\n\nGet into pairs, then choose one person as the Owner and one as the Collaborator\nBoth login to GitHub\n\nThese next steps are for the Owner:\n\nNavigate to the {FIRSTNAME}_test repository\nGo to Settings and navigate to Collaborators in the Access section on the left-hand side\nUnder Manage Access click the button Add people and type the username of your Collaborator in the search box\nOnce you’ve found the correct username, click Add {Collaborator username} to this repository\n\n\nNow, the Collaborator will follow this step:\n\nCheck your email for an invitation to GitHub or check your notifications (likely under Your Organizations) on GitHub to accept the invite to collaborate.\n\n\n\n\n8.3.1 Defining Merge Method\n\n\n\n\n\n\nSome Git configuration to surpress warning messages\n\n\n\nGit version 2.27 includes a new feature that allows users to specify the default method for integrating changes from a remote repository into a local repository, without receiving a warning (this warning is informative, but can get annoying). To suppress this warning for this repository only we need to configure Git by running this line of code in the Terminal:\n\ngit config pull.rebase false\n\npull.rebase false is a default strategy for pulling where Git will first try to auto-merge the files. If auto-merging is not possible, it will indicate a merge conflict.\nNote: Unlike when we first configured Git, we do not include the --global flag here (e.g. git config --global pull.rebase false). This sets this default strategy for this repository only (rather than globally for all your repositories). We do this because your chosen/default method of grabbing changes from a remote repository (e.g. pulling vs. rebasing) may change depending on collaborator/workflow preference.\n\n\n\n\n\n\n\n\nInstructions\n\n\n\nYou will do the exercise twice, where each person will get to practice being both the Owner and the Collaborator roles.\nRound One: Designate one person as the Owner and one as the Collaborator.\n\nStep 1: Owner adds Collaborator to {FIRSTNAME}_test repository (see Setup block above for detailed steps)\nStep 2: Collaborator clones the Owner’s {FIRSTNAME}_test repository\nStep 3: Collaborator edits the README file:\n\nCollaborator adds a new level 2 heading to README titled “Git Workflow”\n\nStep 4: Collaborator commits and pushes the README file with the new changes to GitHub\nStep 5: Owner pulls the changes that the Collaborator made\nStep 6: Owner edits the README file:\n\nUnder “Git Workflow”, Owner adds the steps of the Git workflow we’ve been practicing\n\nStep 7: Owner commits and pushes the README file with the new changes to GitHub\nStep 8: Collaborator pulls the Owner’s changes from GitHub\n\nRound Two: Swap Owner and Collaborator roles and repeat!\n\nStep 1: Owner adds Collaborator to {FIRSTNAME}_test repository\nStep 2: Collaborator clones the Owner’s {FIRSTNAME}_test repository\nStep 3: Collaborator edits the README file:\n\nCollaborator adds a new level 2 heading to README titled “How to Create a Git Repository from an existing project” and adds the high level steps for this workflow\n\nStep 4: Collaborator commits and pushes the README file with the new changes to GitHub\nStep 5: Owner pulls the changes that the Collaborator made\nStep 6: Owner edits the README file:\n\nUnder “How to Create a Git Repository”, Owner adds the high level steps for this workflow\n\nStep 7: Owner commits and pushes the README file with the new changes to GitHub\nStep 8: Collaborator pulls the Owner’s changes from GitHub\n\nHint: If you don’t remember how to create a Git repository, refer to the chapter Intro to Git and GitHub where we created two Git repositories",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collaborating with git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_08.html#a-note-on-advanced-collaboration-techniques",
    "href": "session_08.html#a-note-on-advanced-collaboration-techniques",
    "title": "8  Collaborating with git and GitHub",
    "section": "8.4 A Note on Advanced Collaboration Techniques",
    "text": "8.4 A Note on Advanced Collaboration Techniques\nThere are many Git and GitHub collaboration techniques, some more advanced than others. We won’t be covering advanced strategies in this course. But here is a table for your reference on a few popular Git collaboration workflow strategies and tools.\n\n\n\n\n\n\n\n\n\nCollaboration Technique\nBenefits\nWhen to Use\nWhen Not to Use\n\n\n\n\nBranch Management Strategies\n1. Enables parallel development and experimentation2. Facilitates isolation of features or bug fixes3. Provides flexibility and control over project workflows\nWhen working on larger projects with multiple features or bug fixes simultaneously.When you want to maintain a stable main branch while developing new features or resolving issues on separate branches.When collaborating with teammates on different aspects of a project and later integrating their changes.\nWhen working on small projects with a single developer or limited codebase.When the project scope is simple and doesn’t require extensive branch management.When there is no need to isolate features or bug fixes.\n\n\nCode Review Practices\n1. Enhances code quality and correctness through feedback2. Promotes knowledge sharing and learning within the team3. Helps identify bugs, improve performance, and ensure adherence to coding standards\nWhen collaborating on a codebase with team members to ensure code quality and maintain best practices.When you want to receive feedback and suggestions on your code to improve its readability, efficiency, or functionality.When working on critical or complex code that requires an extra layer of scrutiny before merging it into the main branch.\nWhen working on personal projects or small codebases with no collaboration involved.When time constraints or project size make it impractical to conduct code reviews.When the codebase is less critical or has low complexity.\n\n\nForking\n1. Enables independent experimentation and development2. Provides a way to contribute to a project without direct access3. Allows for creating separate, standalone copies of a repository\nWhen you want to contribute to a project without having direct write access to the original repository.When you want to work on an independent variation or extension of an existing project.When experimenting with changes or modifications to a project while keeping the original repository intact.\nWhen collaborating on a project with direct write access to the original repository.When the project does not allow external contributions or forking.When the project size or complexity doesn’t justify the need for independent variations.\n\n\nPull Requests\n1. Facilitates code review and discussion2. Allows for collaboration and feedback from team members3. Enables better organization and tracking of proposed changes\nWhen working on a shared repository with a team and wanting to contribute changes in a controlled and collaborative manner.When you want to propose changes to a project managed by others and seek review and approval before merging them into the main codebase.\nWhen working on personal projects or individual coding tasks without the need for collaboration.When immediate changes or fixes are required without review processes.When working on projects with a small team or single developer with direct write access to the repository.\n\n\n\nThe “When Not to Use” column provides insights into situations where it may be less appropriate / unnecessary to use each collaboration technique, helping you make informed decisions based on the specific context and requirements of your project.\nThese techniques provide different benefits and are used in various collaboration scenarios, depending on the project’s needs and team dynamics.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collaborating with git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_08.html#merge-conflicts",
    "href": "session_08.html#merge-conflicts",
    "title": "8  Collaborating with git and GitHub",
    "section": "8.5 Merge conflicts",
    "text": "8.5 Merge conflicts\nMerge conflicts occur when both collaborators make conflicting changes to the same file. Resolving merge conflicts involves identifying the root of the problem and restoring the project to a normal state. Good communication, discussing file sections to work on, and avoiding overlaps can help prevent merge conflicts. However, if conflicts do arise, Git warns about potential issues and ensures that changes from different collaborators based on the same file version are not overwritten. To resolve conflicts, you need to explicitly specify whose changes should be used for each conflicting line in the file.\nIn this image, we see collaborators mbjones and metamattj have both made changes to the same line in the same README.md file. This is causing a merge conflict because Git doesn’t know whose changes came first. To resolve it, we need to tell Git whose changes to keep for that line, and whose changes to discard.\n\n\n8.5.1 Common ways to resolve a merge conflict\n1. Abort, abort, abort…\nSometimes you just made a mistake. When you get a merge conflict, the repository is placed in a “Merging” state until you resolve it. There’s a Terminal command to abort doing the merge altogether:\n\ngit merge --abort\n\nOf course, after doing that you still haven’t synced with your Collaborator’s changes, so things are still unresolved. But at least your repository is now usable on your local machine.\n2. Checkout\nThe simplest way to resolve a conflict, given that you know whose version of the file you want to keep, is to use the command line to tell Git to use either your changes (the person doing the merge), or their changes (the Collaborator).\n\nkeep your Collaborator’s file: git checkout --theirs conflicted_file.Rmd\nkeep your own file: git checkout --ours conflicted_file.Rmd\n\nOnce you have run that command, then run add (staging), commit, pull, and push the changes as normal.\n3. Pull and edit the file\nOption 2, above, requires the command line, however, we have a third option for resolving the merge conflict from RStudio. Using this approach will allow us to pick and choose some of our changes and some of our Collaborator’s changes by letting us manually edit and fix the conflicted file.\nWhen you pull a file with a conflict, Git will provide you with a warning modify the file so that it includes both your own changes and your Collaborator’s changes. The file will also appear in the Git tab with an orange U icon, which indicates that the file is Unmerged and therefore awaiting your help to resolve the conflict. It delimits these blocks of conflicted code with a series of less than and greater than signs, so they are easy to find:\n\n\n\n\n\nIn the above example, &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD marks the start of your changes. The ======= delimiter separates your changes from your Collaborator’s conflicting changes. &gt;&gt;&gt;&gt;&gt;&gt;&gt; mark the end of your Collaborator’s changes.\nTo resolve the conflicts, simply find all of these blocks, and edit them so that the file looks how you want (either pick your lines, your Collaborator’s lines, some combination, or something altogether new), and save. Be sure you removed the delimiter lines that started with\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n=======\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n\nOnce you have made those changes, you simply add (staging), commit, and push the files to resolve the conflict.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collaborating with git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_08.html#producing-and-resolving-merge-conflicts",
    "href": "session_08.html#producing-and-resolving-merge-conflicts",
    "title": "8  Collaborating with git and GitHub",
    "section": "8.6 Producing and resolving merge conflicts",
    "text": "8.6 Producing and resolving merge conflicts\nTo illustrate this process, the instructors are going to carefully create a merge conflict step-by-step, show how to resolve it, and show how to see the results of the successful merge after it is complete. First, the instructors will walk through the exercise to demonstrate the issues. Then, participants will pair up and try the exercise.\n\nStep 1: Owner and Collaborator ensure that their local repos are synced with the remote repo\nPair with the same partner as in Exercise 1 and decide who will be the Owner and who will be the Collaborator. Begin the exercise by ensuring that both the Owner and Collaborator have all of the changes synced from the remote repo to their local repos. This includes doing a git pull to ensure that you have all changes locally, and ensuring that the Git tab in RStudio doesn’t show any changes that need to be committed.\n\n\nStep 2: Owner makes a change and commits locally\nFrom this clean slate, the Owner will first modify and commit a small change. The Owner should add their name on a specific line of the README.md file (we will change the title in line 1). Save and commit the change (but DO NOT push). The Owner should now have a local but unpushed commit that the Collaborator does not yet have access to.\n\n\nStep 3: Collaborator makes a change and commits on the same line\nNow, the Collaborator will modify and commit a small change. The Collaborator should add their name to the same line of the README.md file (we will again change the title in line 1). Save and commit the change (but DO NOT push). The Collaborator should now also have a local but unpushed commit that the Owner does not yet have access to.\nAt this point, both the Owner and Collaborator have committed local changes, but neither have tried to share their changes via GitHub.\n\n\nStep 4: Collaborator pushes the file to GitHub\nSharing starts when the Collaborator pushes their changes to the GitHub repo, which updates GitHub with their version of the file. The Owner is now one revision behind, but doesn’t know it yet.\n\n\nStep 5: Owner pushes their changes and gets an error\nAt this point, the Owner tries to push their change to the repository, which triggers an error from GitHub. While the error message is long, it tells you everything needed (that the Owner’s repository doesn’t reflect the changes on GitHub, and that they need to pull before they can push).\n\n\n\nStep 6: Owner pulls from GitHub to get Collaborator changes\nFollowing the error message, the Owner pulls the changes from GitHub, and gets another, different error message. Here, it indicates that there is a merge conflict because of the conflicting lines.\n\nIn the Git pane of RStudio, the file is also flagged with an orange U, which stands for an unresolved merge conflict.\n\n\n\nStep 7: Owner edits the file to resolve the conflict\nTo resolve the conflict, the Owner now needs to edit the file. Again, as indicated above, Git has flagged the locations in the file where a conflict occurred with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. The Owner should edit the file, merging whatever changes are appropriate until the conflicting lines read how they should, and eliminate all of the marker lines with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;.\n\nOf course, for scripts and programs, resolving the changes means more than just merging the text – whoever is doing the merging should make sure that the code runs properly and that none of the logic of the program has been broken.\n\n\n\nStep 8: Owner commits the resolved changes\nFrom this point forward, things proceed as normal. The Owner first adds the file, which changes the orange U to a blue M for modified. Then, the Owner commits the changes locally. The Owner now has a resolved version of the file on their system.\n\n\n\nStep 9: Owner pushes the resolved changes to GitHub\nThe Owner can now push the changes, without error, to GitHub.\n\n\n\nStep 10: Collaborator pulls the resolved changes from GitHub\nFinally, the Collaborator can pull from GitHub to get the changes (which include the resolved conflicted lines of code) that the Owner made.\n\n\nStep 11: Both can view commit history\nBoth the Collaborator and the Owner can view the history, which includes information about the conflict, the associated branch, and the merged changes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collaborating with git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_08.html#exercise-2-with-a-partner-collaborate-in-a-repository-and-resolve-a-merge-conflict",
    "href": "session_08.html#exercise-2-with-a-partner-collaborate-in-a-repository-and-resolve-a-merge-conflict",
    "title": "8  Collaborating with git and GitHub",
    "section": "8.7 Exercise 2: With a partner collaborate in a repository and resolve a merge conflict",
    "text": "8.7 Exercise 2: With a partner collaborate in a repository and resolve a merge conflict\nNote you will only need to complete the Setup and Git configuration steps again if you are working in a new repository. Return to Exercise 1 for Setup and Git configuration steps.\n\n\n\n\n\n\nInstructions\n\n\n\nNow it’s your turn. In pairs, intentionally create a merge conflict, and then go through the steps needed to resolve the issues and continue developing with the merged files. See the sections above for help with each of the steps below. You will do the exercise twice, where each person will get to practice being both the Owner and the Collaborator roles.\nRound One: Designate one person as the Owner and one as the Collaborator. Both open the Owner’s {FIRSTNAME}_test project.\n\nStep 1: Both Owner and Collaborator pull to ensure both have the most up-to-date changes\nStep 2: Owner edits the README file and makes a change to the title and commits but do not push yet!\nStep 3: On the same line, Collaborator edits the README file and makes a change to the title and commits\nStep 4: Collaborator pushes the file to GitHub\nStep 5: Owner pushes their changes and gets an error\nStep 6: Owner pulls from GitHub to get Collaborator changes\nStep 7: Owner edits the README file to resolve the conflict\nStep 8: Owner commits the resolved changes\nStep 9: Owner pushes the resolved changes to GitHub\nStep 10: Collaborator pulls the resolved changes from GitHub\nStep 11: Both view commit history\n\nRound Two: Swap Owner and Collaborator roles and repeat! Switch to the new Owner’s {FIRSTNAME}_test project.\n\nStep 1: Both Owner and Collaborator pull to ensure both have the most up-to-date changes\nStep 2: Owner edits the README file and makes a change to line 2 and commits but do not push yet!\nStep 3: On the same line, Collaborator edits the README file and makes a change to line 2 and commits\nStep 4: Collaborator pushes the file to GitHub\nStep 5: Owner pushes their changes and gets an error\nStep 6: Owner pulls from GitHub to get Collaborator changes\nStep 7: Owner edits the README file to resolve the conflict\nStep 8: Owner commits the resolved changes\nStep 9: Owner pushes the resolved changes to GitHub\nStep 10: Collaborator pulls the resolved changes from GitHub\nStep 11: Both view commit history",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collaborating with git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_08.html#best-practices-to-avoid-merge-conflicts",
    "href": "session_08.html#best-practices-to-avoid-merge-conflicts",
    "title": "8  Collaborating with git and GitHub",
    "section": "8.8 Best practices to avoid merge conflicts",
    "text": "8.8 Best practices to avoid merge conflicts\nSome basic rules of thumb can avoid the vast majority of merge conflicts, saving a lot of time and frustration. These are words our teams live by:\n\n\n\n\n\nXKCD 1597\n\n\n\nCommunicate often and set up effective communication channels\nTell each other what you are working on\nStart your working session with a pull\nPull immediately after you commit and before you push\nCommit often in small chunks (this helps you organize your work!)\nMake sure you and who you are collaborating with all fully understand the Git workflow you’re using (aka make sure you’re on the same page before you start)!\n\nA good workflow is encapsulated as follows:\nPull -&gt; Edit -&gt; Save -&gt; Add (stage) -&gt; Commit -&gt; Pull -&gt; (OPTIONAL) Fix any merge conflicts -&gt; Push\nIt may take a bit of practice to get comfortable with navigating merge conflicts, but like any other technical skill, they’ll become less intimidating with time. With careful communication and a consistent workflow, conflicts can be largely avoided or resolved when they do occur.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collaborating with git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_09.html",
    "href": "session_09.html",
    "title": "9  Publishing Analysis to the Web",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Publishing Analysis to the Web</span>"
    ]
  },
  {
    "objectID": "session_09.html#learning-objectives",
    "href": "session_09.html#learning-objectives",
    "title": "9  Publishing Analysis to the Web",
    "section": "",
    "text": "How to use Git, GitHub (+Pages), and Quarto to publish an analysis to the web",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Publishing Analysis to the Web</span>"
    ]
  },
  {
    "objectID": "session_09.html#introduction",
    "href": "session_09.html#introduction",
    "title": "9  Publishing Analysis to the Web",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nSharing your work with others in engaging ways is an important part of the scientific process.\nSo far in this course, we’ve introduced a small set of powerful tools for doing open science:\n\nR and its many packages\nRStudio\nGit\nGitHub\nQuarto\n\nQuarto, in particular, is amazingly powerful for creating scientific reports. We’ve already gotten a glimpse into how we can use it to organize, document, and present ideas along with code & code outputs within our RStudio environment. However, we haven’t yet tapped its full potential for sharing our work with others.\nIn this lesson, we’re going to take our training_{USERNAME} GitHub repository and turn it into a beautiful and easy-to-read web page that you can publish and share using the tools listed above.\n\n\n\n\n\n\nSet up\n\n\n\n\nMake sure you are in training_{USERNAME} project\nAdd a new Quarto file at the top level called index.qmd\n\nGo to the RStudio menu File -&gt; New File -&gt; Quarto Document\nThis will bring up a dialog box. Add the title “GitHub Pages Example”, keep the Default Output Format as “HTML”, and then click “OK”\n\nSave the Quarto Document you just created. Use index.qmd as the file name\n\nBe sure to use the exact case (lower case “index”) as different operating systems handle case differently and it can interfere with loading your web page later\n\nPress “Render” and observe the rendered output\n\nNotice the new file in the same directory index.html\nThis is our Quarto file rendered as HTML (a web page)\n\nCommit your changes (for both index.qmd and index.html) with a commit message, and push to GitHub\n\nIf you have a folder called index_files, commit this as well. This folder contains the all the formatting and display settings for your html page.\n\nOpen your web browser to the github.com and navigate to the page for your training_{USERNAME} repository\nActivate GitHub Pages for the main branch\n\nGo to Settings -&gt; Pages (underneath the Code and Automation section)\nKeep the “Source” as “Deploy from a branch”\nUnder “Branch” you’ll see a message that says “GitHub Pages is currently disabled”. To change this, change the branch from “None” to main. Keep the folder as the root and then click “Save”\nYou should see the message change to “Your GitHub Pages site is currently being built from the main branch”\n\n\nNote: index.qmd represents the default file for a web site, and is returned whenever you visit the web site but doesn’t specify an explicit file to be returned.\n\n\nNow, the rendered website version of your repo will show up at a special URL.\nGitHub Pages follows a convention like this:\n\n\n\n\n\nNote that it changes from github.com to github.io\n\nGo to https://{username}.github.io/{repo_name}/ (Note the trailing /)\nObserve the awesome rendered output\n\nNow that we’ve successfully published a web page from an Quarto Document, let’s make a change to our Quarto Document and follow the steps to publish the change on the web:\n\n\n\n\n\n\nUpdate content in your published page\n\n\n\n\nGo back to your index.qmd\nDelete all the content, except the YAML frontmatter\nType “Hello world”\nRender index.qmd\nUse Git workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\nGo back to https://{username}.github.io/{repo_name}/\n\n\n\nNext, we will show how you can link different qmd’s rendered into html so you can easily share different parts of your work.\n\n\n\n\n\n\nExercise\n\n\n\nIn this exercise, you’ll create a table of contents with the lessons of this course on the main page, and link some of the files we have work on so far.\n\nGo back to the RStudio server and to your index.qmd file\nCreate a table of contents with the names of the main technical lessons of this course, like so:\n\n## Course Lessons\n\n- Introduction to Quarto\n- Introduction to git and GitHub\n- Cleaning and Wrangling data\n\n## Course Practice Session\n\n- Practice I\n- Practice II\n- Practice III\nIn the instructions below, we’ll assume you created a Quarto document called quarto-intro.qmd in your scripts directory during the module on Introduction to Quarto, and have pushed that to GitHub within the same repo where you just created the index.qmd file above.\n\n\n\n\n\n\nBut what if I don’t have a scripts/quarto-intro.qmd file?!?\n\n\n\n\n\nIf you created a Quarto document in the intro session but named it something else, no problem! Simply adapt the instructions below to match your file name.\nIf you haven’t created this file at all, nor any other Quarto documents from previous modules, go ahead and create a new Quarto document in RStudio, and save it as quarto-intro.qmd in the scripts directory. Just for quick illustration, add an R code chunk or two, such as:\n```{r}\nlibrary(dplyr)\nlibrary(ggplot2)\nggplot(airquality, aes(Temp, Ozone)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE)\n```\n\n\n\n\nMake sure you have the html version of your quarto-intro.qmd file, and any other Quarto pages you wish to publish. If you only see the qmd version, you need to “Render” your files first.\nIn your index.qmd, add the links to the html file(s) you want to show on our webpage. Do you remember how to create a link using Markdown?\n\n\n\nMarkdown syntax to create a link:\n\n\n[Text you want to hyperlink](link)\n\nExample: [Introduction to Quarto](scripts/quarto-intro.html)\n\n\n\n\nRender index.qmd, then verify that the rendered page looks the way you expect, that the link works, and that the linked page looks good.\nUse Git workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push. Be sure to include the updated qmd files, html files, and any newly created folders created during the render process.\n\nAfter pushing, it may take up to a couple of minutes for GitHub to build your site, but after that is complete, you should be able to refresh your website. Confirm that you see the table of contents, and can successfully navigate to whatever page(s) you linked.\n\n\nQuarto web pages are a great way to share work in progress with your colleagues. Here we showed an example with the materials we have created in this course. However, you can use these same steps to share the different files and progress of a project you’ve been working on. To do so simply requires thinking through your presentation so that it highlights the workflow to be reviewed. You can include multiple pages and build a simple web site and make your work accessible to people who aren’t set up to open your project in R. Your site could look something like this:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Publishing Analysis to the Web</span>"
    ]
  },
  {
    "objectID": "session_10.html",
    "href": "session_10.html",
    "title": "10  Data Visualization",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#learning-objectives",
    "href": "session_10.html#learning-objectives",
    "title": "10  Data Visualization",
    "section": "",
    "text": "Understand the fundamentals of how the ggplot2 package works\nUse ggplot2’s theme and other customization functions to create publication-grade graphics\nIntroduce the leaflet and DT package to create interactive maps and tables respectively",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#overview",
    "href": "session_10.html#overview",
    "title": "10  Data Visualization",
    "section": "10.1 Overview",
    "text": "10.1 Overview\nggplot2 is a popular package for visualizing data in R. From the home page:\n\nggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.\n\nIt’s been around for years and has pretty good documentation and tons of example code around the web (like on StackOverflow). The goal of this lesson is to introduce you to the basic components of working with ggplot2 and inspire you to go and explore this awesome resource for visualizing your data.\n\n\n\n\n\n\nggplot2 vs base graphics in R vs others\n\n\n\nThere are many different ways to plot your data in R. All of them work! However, ggplot2 excels at making complicated plots easy, and easy plots simple enough.\nBase R graphics (plot(), hist(), etc) can be helpful for simple, quick, and dirty plots. ggplot2 can be used for almost everything else. And really, once you become familiar with how it works, you’ll discover it’s great for simple and quick plots as well.\n\n\nLet’s dive into creating and customizing plots with ggplot2.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\nMake sure you’re in the right project (training_{USERNAME}) and use the Git workflow by Pulling to check for any changes. Then, create a new Quarto document, delete the default text, and save this document as intro_visualization.qmd.\nLoad the packages we’ll need for the ggplot2 exploration and exercises.\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(janitor) # expedite cleaning and exploring data\nlibrary(scales) # scale functions for visualization\n\n\nRead in the data table that we’ll be exploring and visualizing below. We’ll read it into R directly over the web from the KNB Data Repository. To get the URL pointing at the publically archived CSV, visit the landing page for the data package at Daily salmon escapement counts from the OceanAK database, Alaska, 1921-2017, hover over the “Download” button for the ADFG_firstAttempt_reformatted.csv, right click, and select “Copy Link”.\n\n\nescape_raw &lt;- read_csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Af119a05b-bbe7-4aea-93c6-85434dcb1c5e\")\n\n\nLearn about the data. For this session we are going to be working with data on daily salmon escapement counts in Alaska. If you didn’t read the data package documentation in the previous step, have a look now.\nFinally, let’s explore the data we just read into our working environment.\n\n\n## Check out column names\ncolnames(escape_raw)\n\n## Peak at each column and class\nglimpse(escape_raw)\n\n## From when to when\nrange(escape_raw$sampleDate)\n\n## Which species?\nunique(escape_raw$Species)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#getting-the-data-ready",
    "href": "session_10.html#getting-the-data-ready",
    "title": "10  Data Visualization",
    "section": "10.2 Getting the data ready",
    "text": "10.2 Getting the data ready\nIn most cases, we need to do some wrangling before we can plot our data the way we want to. Now that we have read in the data and have done some exploration, we’ll put our data wrangling skills to practice getting our data in the desired format.\n\n\n\n\n\n\nSide note on clean column names\n\n\n\njanitor::clean_names() is an awesome function to transform all column names into the same format. The default format for this function is snake_case_format. We highly recommend having clear well formatted column names. It makes your life easier down the line.\nHow it works?\n\nescape &lt;- escape_raw %&gt;% \n    janitor::clean_names()\n\nAnd that’s it! If we look at the column names of the object escape, we can see the columns are now all in a lowercase, snake format.\n\ncolnames(escape)\n\n[1] \"location\"     \"sasap_region\" \"sample_date\"  \"species\"      \"daily_count\" \n[6] \"method\"       \"latitude\"     \"longitude\"    \"source\"      \n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nAdd a new column for the sample year, derived from sample_date\nCalculate annual escapement by species, sasap_region, and year\nFilter the main 5 salmon species (Chinook, Sockeye, Chum, Coho and Pink)\n\n\n\n\nannual_esc &lt;- escape %&gt;%\n    filter(species %in% c(\"Chinook\", \"Sockeye\", \"Chum\", \"Coho\", \"Pink\")) %&gt;%\n    mutate(year = lubridate::year(sample_date)) %&gt;%\n    group_by(species, sasap_region, year) %&gt;%\n    summarize(escapement = sum(daily_count))\n\nhead(annual_esc)\n\n# A tibble: 6 × 4\n# Groups:   species, sasap_region [1]\n  species sasap_region                           year escapement\n  &lt;chr&gt;   &lt;chr&gt;                                 &lt;dbl&gt;      &lt;dbl&gt;\n1 Chinook Alaska Peninsula and Aleutian Islands  1974       1092\n2 Chinook Alaska Peninsula and Aleutian Islands  1975       1917\n3 Chinook Alaska Peninsula and Aleutian Islands  1976       3045\n4 Chinook Alaska Peninsula and Aleutian Islands  1977       4844\n5 Chinook Alaska Peninsula and Aleutian Islands  1978       3901\n6 Chinook Alaska Peninsula and Aleutian Islands  1979      10463\n\n\nThe chunk above used some dplyr commands that we’ve used previously, and some that are new. First, we use a filter with the %in% operator to select only the salmon species. Although we would get the same result if we ran this filter operation later in the sequence of steps, it’s good practice to apply filters as early as possible because they reduce the size of the dataset and can make the subsequent operations faster. The mutate() function then adds a new column containing the year, which we extract from sample_date using the year() function in the helpful lubridate package. Next we use group_by() to indicate that we want to apply subsequent operations separately for each unique combination of species, region, and year. Finally, we apply summarize() to calculate the total escapement for each of these groups.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#plotting-with-ggplot2",
    "href": "session_10.html#plotting-with-ggplot2",
    "title": "10  Data Visualization",
    "section": "10.3 Plotting with ggplot2",
    "text": "10.3 Plotting with ggplot2\n\n10.3.1 Essential components\nFirst, we’ll cover some ggplot2 basics to create the foundation of our plot. Then, we’ll add on to make our great customized data visualization.\n\n\n\n\n\n\nThe basics\n\n\n\n\nInitialize a ggplot() visualization, by calling the ggplot2::ggplot() function.\nSpecify the data we want to plot, by setting data = &lt;my_data&gt;.\nSpecify how columns in our data should map to aesthetics (i.e., visual elements) in the plot, by using aes() function.\nDefine one or more plot geometries, i.e. specific data visualization layers (e.g., scatterplot, histogram, boxplot), by using relevant geom_*() functions.\n\nNote To add layers, theme elements, and various customizations to the ggplot object initialized in the first step above, we use the + operator. You can think of this as a process of building out your desired visualization by starting with a blank canvas and incrementally adding the desired details.\n\n\nFor example, let’s plot total escapement by species. We will show this by creating the same plot in 3 slightly different ways. Each of the options below have the essential pieces of a ggplot.\n\n## Option 1 - data and mapping called in the ggplot() function\nggplot(data = annual_esc,\n       aes(x = species, y = escapement)) +\n    geom_col()\n\n## Option 2 - data called in ggplot function; mapping called in geom\nggplot(data = annual_esc) +\n    geom_col(aes(x = species, y = escapement))\n\n\n## Option 3 - data and mapping called in geom\nggplot() +\n    geom_col(data = annual_esc,\n             aes(x = species, y = escapement))\n\nThey all will create the same plot:\n\n\n\n\n\n\n\n\n\nLet’s take a minute to review a few of the core components of a ggplot2 visualization, and see how they relate back to the data that we are plotting. Consider this small subset of 12 records from our escapement data, and a corresponding scatterplot of daily fish counts across 5 days for a particular region.\n\n\n\n\n\n\n\n\n\n\n10.3.2 Looking at different geoms\nHaving the basic structure with the essential components in mind, we can easily change the type of graph by updating the geom_*().\n\n\n\n\n\n\nggplot2 and the pipe operator\n\n\n\nRemember that the first argument of ggplot is the data input. Just like in dplyr and tidyr, we can also pipe a data.frame directly into the first argument of the ggplot function using the %&gt;% operator. This means we can create expressions that start with data, pipe it through various tidying and restructuring operations, then pipe into a ggplot call which we then build up as described above.\nThis can certainly be convenient, but use it carefully! Combining too many data-tidying or subsetting operations with your ggplot call can make your code more difficult to debug and understand. In general, operations included in the pipe sequence preceding a ggplot call should be those needed to prepare the data for that particular visualization, whereas pre-processing steps that more generally prepare your data for analysis and visualization should be done separately, producing a new named data.frame object.\n\n\nNext, we will use the pipe operator to pass into ggplot() a filtered version of annual_esc, and make a plot with different geometries.\nLine and point\nLet’s start with a line and point visualization.\n\nannual_esc %&gt;%\n    filter(species == \"Sockeye\",\n           sasap_region == \"Bristol Bay\") %&gt;%\n    ggplot(aes(x = year, y = escapement)) +\n    geom_line() +\n    geom_point()\n\n\n\n\n\n\n\n\nHere we are added two layers to the plot. They are drawn in the order given in the expression, which means the points are drawn on top of the lines. In this example, it doesn’t make a difference, but if you were to use different colors for the lines and points, you could look closely and see that the order matters where the lines overlap with the points.\nAlso notice how we included the aesthetic mapping in the ggplot call, which means this mapping was used by both of the layers. If we didn’t supply the mapping in the ggplot call, we would need to pass it into both of the layers separately.\nBoxplot\nNow let’s try a box plot.\n\nannual_esc %&gt;%\n    filter(year == 1974,\n          species %in% c(\"Chum\", \"Pink\")) %&gt;%\n    ggplot(aes(x = species, y = escapement)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nNotice how we again provided aesthetic mappings for both x and y. Let’s think about how the behavior of these two aesthetics is different in this plot compared with the point and line plot.\nWhereas in the previous example we set x to the continuous variable year, here we use the discrete variable species. But it still worked! This is handled by a so-called scale, another component of the ggplot2 grammar. In short, scales convert the values in your data to corresponding aesthetic values in the plot. You typically don’t have to worry about this yourself, because ggplot2 chooses the relevant scale based on your input data type, the corresponding aesthetic, and the type of geom. In the plot above, ggplot2 applied its discrete x-axis scale, which is responsible for deciding how to order and space out the discrete set of species values along the x axis. Later we’ll see how we can customize the scale in some cases.\nThe y aesthetic also seems different in this plot. We mapped the y aesthetic to the escapement variable, but unlike in the point & line diagram, the plot does not have raw escapement values on the y-axis. Instead, it’s showing statistical properties of the data! How does that happen? The answer involves stats, one of the final key components of the ggplot2 grammar. A stat is a statistical summarization that ggplot2 applies to your data, producing a new dataframe that it then uses internally to produce the plot. For geom_boxplot, ggplot2 internally applies a stat_boxplot to your input data, producing the statistics needed to draw the boxplots. Fortunately, you will rarely ever need to invoke a stat_* function yourself, as each geom_* has a corresponding stat_* that will almost always do the job. But it’s still useful to know this is happening under the hood.\n\n\n\n\n\n\nThe identity stat\n\n\n\nIf ggplot2 applies a statistical transformation to your data before plotting it, then how does the geom_point plot only your raw values? The answer is that this and other geometries use stat_identity, which leaves the data unchanged! Although this might seem unnecessary, it allows ggplot to have a consistent behavior where all input dataframes are processed by a stat_* function before plotting, even if sometimes that step doesn’t actually modify the data.\n\n\nViolin plot\nFinally, let’s look at a violin plot. Other than the geom name itself, this expression is identical to what we used to produce the boxplot above. This is a nice example of how we can quickly switch visualization types – and in this case, the corresponding statistical summarization of our data – with a small change to the code.\n\nannual_esc %&gt;%\n    filter(year == 1974,\n           species %in% c(\"Chum\", \"Pink\")) %&gt;%\n    ggplot(aes(x = species, y = escapement)) +\n    geom_violin()\n\n\n\n\n\n\n\n\n\n\n10.3.3 Customizing our plot\nLet’s go back to our base bar graph. What if we want our bars to be blue instead of gray? You might think we could run this:\n\nggplot(annual_esc,\n       aes(x = species, y = escapement,\n           fill = \"blue\")) +\n    geom_col()\n\n\n\n\n\n\n\n\nWhy did that happen?\nNotice that we tried to set the fill color of the plot inside the mapping aesthetic call. What we have done, behind the scenes, is create a column filled with the word “blue” in our data frame, and then mapped it to the fill aesthetic, which then chose the default fill color of red.\nWhat we really wanted to do was just change the color of the bars. To do this, we can call the fill color option in the geom_col() function, outside of the mapping aesthetics function call.\n\nggplot(annual_esc,\n       aes(x = species, y = escapement)) +\n    geom_col(fill = \"blue\")\n\n\n\n\n\n\n\n\nWhat if we did want to map the color of the bars to a variable, such as region? ggplot() is really powerful because we can easily get this plot to visualize more aspects of our data.\n\nggplot(annual_esc,\n       aes(x = species, y = escapement,\n           fill = sasap_region)) +\n    geom_col()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeep in mind\n\n\n\n\nIf you want to map a plot aesthetic to a variable in your data (e.g., point color should be based on a specific region), put it within the aes() expression passed to the geom_ layer via its mapping argument.\nIf you want to set a plot aesthetic to a constant value (e.g., “Make ALL the points BLUE”), pass it as an argument directly to the relevant geom_ layer.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThe geom_col plot above introduces another useful ggplot layer control: position adjustments. Consider that at each position along the y axis, our data has potentially many escapement values to plot. Why don’t we simply see many overlapping bars? This is handled by the position argument of geom_col, which takes either a position_* function or its corresponding string alias. The default value for column charts is position_stack(), which is equivalent to calling geom_col(..., position = \"stack\"), and produces the stacked bar chart you see above.\nTry repeating the plot with other available position adjustments relevant for this type of geom: “dodge”, “dodge2”, and “fill”.\n\n\n\n10.3.3.1 Creating multiple plots\nWe know that in the graph we just plotted, each bar includes escapements for multiple years. Let’s leverage the power of ggplot to plot more aspects of our data in one plot.\nAn easy way to plot another “dimension” of your data is by using facets. Faceting involves splitting your data into desigated groups, and then having ggplot2 create a multi-paneled figure in which all panels contain the same type of plot, but each visualizing data from only one of the groups. The simplest ggplot2 faceting function is facet_wrap(), which accepts a mapping to a grouping variable using the syntax ~{variable_name}. The ~ (tilde) is a model operator which tells facet_wrap() to model each unique value within variable_name to a facet in the plot.\nThe default behavior of facet wrap is to put all facets on the same x and y scale. You can use the scales argument to specify whether to allow different scales between facet plots (e.g scales = \"free_y\" to free the y axis scale). You can also specify the number of columns using the ncol = argument or number of rows using nrow =.\nTo demonstrate how this works, first let’s create a smaller data.frame containing escapement data from 2000 to 2016.\n\n## Subset with data from years 2000 to 2016\n\nannual_esc_2000s &lt;- annual_esc %&gt;%\n    filter(year %in% c(2000:2016))\n\n## Quick check\nunique(annual_esc_2000s$year)\n\n [1] 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014\n[16] 2015 2016\n\n\nNow let’s plot escapement by species over time, from 2000 to 2016, with separate faceted panels for each region.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\n10.3.3.2 Setting ggplot themes\nNow let’s work on making this plot look a bit nicer. We are going to:\n\nAdd a title using labs()\nAdjust labels using labs()\nInclude a built in theme using theme_bw()\n\nThere are a wide variety of built in themes in ggplot that help quickly set the look of the plot. Use the RStudio autocomplete theme_ &lt;TAB&gt; to view a list of theme functions.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nYou can see that the theme_bw() function changed a lot of the aspects of our plot! The background is white, the grid is a different color, etc. There are lots of other built in themes like this that come with the ggplot2 package.\n\n\n\n\n\n\nExercise\n\n\n\nUse the RStudio auto complete, the ggplot2 documentation, a cheat sheet, or good old Google to find other built in themes. Pick out your favorite one and add it to your plot.\n\n\n\n\nThemes\n## Useful baseline themes are\ntheme_minimal()\ntheme_light()\ntheme_classic()\n\n\nThe built in theme functions (theme_*()) change the default settings for many elements that can also be changed individually using the theme() function. The theme() function is a way to further fine-tune the look of your plot. This function takes MANY arguments (just have a look at ?theme). Luckily there are many great ggplot resources online so we don’t have to remember all of these, just Google “ggplot cheat sheet” and find one you like.\nLet’s look at an example of a theme() call, where we change the position of the legend from the right side to the bottom, and remove its title.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    theme_light() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank())\n\n\n\n\n\n\n\n\nNote that the theme() call needs to come after any built-in themes like theme_bw() are used. Otherwise, theme_bw() will likely override any theme elements that you changed using theme().\nYou can also save the result of a series of theme() function calls to an object to use on multiple plots. This prevents needing to copy paste the same lines over and over again!\n\nmy_theme &lt;- theme_light() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank())\n\nSo now our code will look like this:\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    my_theme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nUsing whatever method you like, figure out how to rotate the x-axis tick labels to a 45-degree angle.\n\nHint: You can start by looking at the documentation of the function by typing ?theme() in the console. And googling is a great way to figure out how to do the modifications you want to your plot.\n\nWhat changes do you expect to see in your plot by adding the following line of code? Discuss with your neighbor and then try it out!\n\nscale_x_continuous(breaks = seq(2000, 2016, 2))\n\n\n\n\nAnswer\n## Useful baseline themes are\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    scale_x_continuous(breaks = seq(2000, 2016, 2)) +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    my_theme +\n    guides(x=guide_axis(angle = 45))\n\n\n\n\n10.3.3.3 Smarter tick labels using scales\nFixing tick labels in ggplot can be super annoying. The y-axis labels in the plot above don’t look great. We could manually fix them, but it would likely be tedious and error prone.\nThe scales package provides some nice helper functions to easily rescale and relabel your plots. Here, we use scale_y_continuous() from ggplot2, with the argument labels, which is assigned to the function name comma, from the scales package. This will format all of the labels on the y-axis of our plot with comma-formatted numbers.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    scale_x_continuous(breaks = seq(2000, 2016, 2),\n        guide=guide_axis(angle = 45)) +\n    scale_y_continuous(labels = comma) +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    my_theme\n\n\n\n\n\n\n\n\nLet’s look at one more version of this graphic, with even fancier label customization. First, we’ll supply names to the scale_*() calls, which ggplot then uses as the default axis titles. Secondly, we’ll add an even fancier label specification for the y axis, dynamically adjusting the units. Finally, we’ll fix the issue with truncated text in the long facet title, by using a labeller function. While we’re at it, will also use a smaller size for the points.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point(size=1) +\n    scale_x_continuous(\"Year\",\n        breaks = seq(2000, 2016, 4),\n        guide = guide_axis(angle = 45)) +\n    scale_y_continuous(\"Escapement\",\n        label = label_comma(scale_cut = cut_short_scale())) +\n    facet_wrap( ~ sasap_region, scales = \"free_y\",\n        labeller = labeller(sasap_region = label_wrap_gen())) +\n    labs(title = \"Annual Salmon Escapement by Region\") +\n    my_theme\n\n\n\n\n\n\n\n\n\n\n\n10.3.3.4 Saving plots\nSaving plots using ggplot is easy! The ggsave() function will save either the last plot you created, or any plot that you have saved to a variable. You can specify what output format you want, size, resolution, etc. See ?ggsave() for documentation.\n\nggsave(\"figures/annualsalmon_esc_region.jpg\", width = 8, height = 6, units = \"in\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#interactive-visualization",
    "href": "session_10.html#interactive-visualization",
    "title": "10  Data Visualization",
    "section": "10.4 Interactive visualization",
    "text": "10.4 Interactive visualization\nNow that we know how to make great static visualizations, let’s introduce two other packages that allow us to display our data in interactive ways.\n\n\n10.4.1 Tables with DT\nThe DT package provides an R interface to DataTables, a Javascript library for rendering interactive tables in HTML documents. In this quick example, we’ll create an interactive table of unique salmon sampling locations using DT. Start by loading the package:\n\nlibrary(DT) # interactive tables\n\nUsing the escape data.frame we worked with above, create a derived version that conains unique sampling locations with no missing values. To do this, we’ll use two new functions from dplyr and tidyr: distinct() and drop_na().\n\nlocations &lt;- escape %&gt;%\n    distinct(location, latitude, longitude) %&gt;%\n    drop_na()\n\nNow let’s display this data as an interactive table using datatable() from the DT package.\n\ndatatable(locations)\n\n\n\n\n\n\n\n10.4.2 Maps with leaflet\nThe leaflet package is similar to the DT package in that it wraps a Javscript library for creating interactive data widgets that you can embed in an HTML document. However, whereas DT is used to produce a tabular data viewer, leaflet used to creating an interactive map-based viewer in cases where you have geographic coordinates in your data.\nAs usual, start by loading the package:\n\nlibrary(leaflet) # interactive maps\n\nNow let’s do some mapping! Similar to ggplot2, you can make a basic leaflet map using just a couple lines of code. Also like ggplot2, the leaflet syntaxs follows a same pattern of first initializing the main plot object, and then adding layers. However, unlike ggplot2, the leaflet package uses pipe operators (%&gt;%) rather than the addition operator (+).\nThe addTiles() function without arguments will add base tiles to your map from OpenStreetMap. addMarkers() will add a marker at each location specified by the latitude and longitude arguments. Note that the ~ symbol is used here to model the coordinates to the map (similar to facet_wrap() in ggplot).\n\nleaflet(locations) %&gt;%\n    addTiles() %&gt;%\n    addMarkers(\n        lng = ~ longitude,\n        lat = ~ latitude,\n        popup = ~ location\n    )\n\n\n\n\n\nAlthough it’s beyond the scope of this lesson, leaflet can do more than simply mapping point data with latitude and longitude coordinates; it can be used more generally to visualize geospatial datasets containing points, lines, or polygons (i.e, geospatial vector data).\nYou can also use leaflet to import Web Map Service (WMS) tiles. Here is an example that uses the General Bathymetric Map of the Oceans (GEBCO) WMS tiles. In this example, we also demonstrate how to create a more simple circle marker, the look of which is explicitly set using a series of style-related arguments.\n\nleaflet(locations) %&gt;%\n    addWMSTiles(\n        \"https://www.gebco.net/data_and_products/gebco_web_services/web_map_service/mapserv?request=getmap&service=wms&BBOX=-90,-180,90,360&crs=EPSG:4326&format=image/jpeg&layers=gebco_latest&width=1200&height=600&version=1.3.0\",\n        layers = 'GEBCO_LATEST',\n        attribution = \"Imagery reproduced from the GEBCO_2022 Grid, WMS 1.3.0 GetMap, www.gebco.net\"\n    ) %&gt;%\n    addCircleMarkers(\n        lng = ~ longitude,\n        lat = ~ latitude,\n        popup = ~ location,\n        radius = 5,\n        # set fill properties\n        fillColor = \"salmon\",\n        fillOpacity = 1,\n        # set stroke properties\n        stroke = TRUE,\n        weight = 0.5,\n        color = \"white\",\n        opacity = 1\n    )\n\n\n\n\n\n\nLeaflet has a ton of functionality that can enable you to create some beautiful, functional, web-based maps with relative ease. Here is an example of some we created as part of the State of Alaskan Salmon and People (SASAP) project, created using the same tools we showed you here.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#publish-the-data-visualization-lesson-to-your-webpage",
    "href": "session_10.html#publish-the-data-visualization-lesson-to-your-webpage",
    "title": "10  Data Visualization",
    "section": "10.5 Publish the Data Visualization lesson to your webpage",
    "text": "10.5 Publish the Data Visualization lesson to your webpage\nAs alluded above, these visualization widgets really shine when included in published web pages. Let’s combine what we learned in this session with what you’ve previously learned about publishing analyses using GitHub Pages.\n\n\n\n\n\n\nSteps\n\n\n\n\nSave the qmd you have been working on for this lesson.\nRender the qmd file, then view it in the browser to verify everything in your code is working as expected.\nOpen the index.qmd file you created in the Publishing Analysis to the Web module, and add a new link to the html file you just created in the previous step.\nSave and render index.qmd to an html, and again verify that it looks & behaves as expected.\nUse the Git workflow to publish all new & updated files to GitHub: Stage &gt; Commit &gt; Pull &gt; Push\n\n\n\nCongrats! You’ve created a publically available web page that anyone can visit to interact with your tables and maps.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_10.html#ggplot2-resources",
    "href": "session_10.html#ggplot2-resources",
    "title": "10  Data Visualization",
    "section": "10.6 ggplot2 Resources",
    "text": "10.6 ggplot2 Resources\n\nggplot2: Elegant Graphics for Data Analysis, Hadley Wickham et al (online edition)\nCustomized Data Visualization in ggplot2 by Allison Horst.\nA ggplot2 tutorial for beautiful plotting in R by Cedric Scherer.\nWhy not to use two axes, and what to use instead: The case against dual axis charts by Lisa Charlotte Rost.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_11.html",
    "href": "session_11.html",
    "title": "11  Practice Session I",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Practice Session I</span>"
    ]
  },
  {
    "objectID": "session_11.html#learning-objectives",
    "href": "session_11.html#learning-objectives",
    "title": "11  Practice Session I",
    "section": "",
    "text": "Practice using common cleaning and wrangling functions\nPractice creating plots using common visualization functions in ggplot\nPractice saving and sharing data visualizations\nPractice Git and GitHub workflow and collaborating with a collegue\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThese exercises are adapted from Allison Horst’s EDS 221: Scientific Programming Essentials Course for the Bren School’s Master of Environmental Data Science program.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Practice Session I</span>"
    ]
  },
  {
    "objectID": "session_11.html#about-the-data",
    "href": "session_11.html#about-the-data",
    "title": "11  Practice Session I",
    "section": "About the data",
    "text": "About the data\nThese exercises will be using data on abundance, size, and trap counts (fishing pressure) of California spiny lobster (Panulirus interruptus) and were collected along the mainland coast of the Santa Barbara Channel by Santa Barbara Coastal LTER researchers (LTER, Reed, and Miller 2022).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Practice Session I</span>"
    ]
  },
  {
    "objectID": "session_11.html#exercise-collaborate-on-an-analysis-and-create-a-report-to-publish-using-github-pages",
    "href": "session_11.html#exercise-collaborate-on-an-analysis-and-create-a-report-to-publish-using-github-pages",
    "title": "11  Practice Session I",
    "section": "11.1 Exercise: Collaborate on an analysis and create a report to publish using GitHub Pages",
    "text": "11.1 Exercise: Collaborate on an analysis and create a report to publish using GitHub Pages\n\n\n\n\n\n\nSetup\n\n\n\n\nCreate a new repository with a partner\n\nDetermine who is the Owner and who is the Collaborator\nThe Owner creates a repository on GitHub titled with both your names (i.e. If Casey and Camila were partners, and Casey is the Owner, she would create a repo called casey-camila)\n\nWhen creating the repository, add a brief description (i.e. R Practice Session: Collaborating on, Wrangling & Visualizing Data), keep the repo Public, and Initialize the repo with a README file and an R .gitignore template.\n\nThe Owner adds the Collaborator to the repo\nBoth the Collaborator and the Owner clone the repo into their RStudio \n\n\n**Step 2 and Step 3 are meant to be completed at the same time. - Collaborator completes Step 2 - Owner completes Step 3\n\nCollaborator creates new files for exercise\n\nThe Collaborator creates the following directory (folder):\n\nanalysis\n\nAfter creating the directories, create the following Quarto Documents and store them in the listed folders:\n\nTitle it: “Owner Analysis”, save it as: owner-analysis.qmd, and store in analysis folder\nTitle it: “Collaborator Analysis”, save it as: collaborator-analysis.qmd, and store in analysis folder\nTitle it: “Lobster Report” and save it as: lobster-report.qmd and store in analysis folder\n\nAfter creating the files, the Collaborator will stage (add), commit, write a commit message, pull, and push the files to the remote repository (on GitHub)\nThe Owner pulls the changes and Quarto Documents into their local repository (their workspace)\n\nOwner downloads data from the EDI Data Portal SBC LTER: Reef: Abundance, size and fishing effort for California Spiny Lobster (Panulirus interruptus), ongoing since 2012.\n\nCreate two new directories one called data and one called figs\n\nNote: Git does not track empty directories, so you won’t see figs when you push to GitHub\n\nDownload the following data and upload them to the data folder:\n\nTime-series of lobster abundance and size\nTime-series of lobster trap buoy counts\n\nAfter creating the data folder and adding the data, the Owner will stage (add), commit, write a commit message,pull, and push the files to the remote repository (on GitHub)\nThe Collaborator pulls the changes and data into their local repository (their workspace)\n\n\n\n\n\n11.1.1 Explore, clean and wrangle data\nFor this portion of the exercise, the - Owner will be working with the lobster abundance and size data - Collaborator will be working with the lobster trap buoy counts data\nQuestions 1-3 you will be working independently since you’re working with different data frames, but you’re welcome to check in with each other.\n\nOwnerCollaborator\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nOpen the Quarto Document owner-analysis.qmd\n\nCheck the YAML and add your name to the author field\nCreate a new section with a level 2 header and title it “Exercise: Explore, Clean, and Wrangle Data”\n\nLoad the following libraries at the top of your Quarto Document\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(here)\n\n\nRead in the data and store the data frame as lobster_abundance\n\n\nlobster_abundance &lt;- read_csv(\"data/Lobster_Abundance_All_Years_20220829.csv\")\n\n\nLook at your data. Take a minute to explore what your data structure looks like, what data types are in the data frame, or use a function to get a high-level summary of the data you’re working with.\nUse the Git workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\n\nNote: You also want to Pull when you first open a project\n\n\n\n\n\n11.2 Convert missing values using mutate() and na_if()\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe variable SIZE_MM uses -99999 as the code for missing values (see metadata or use unique()). This has the potential to cause conflicts with our analyses, so let’s convert -99999 to an NA value. Do this using mutate() and na_if(). Look up the help page to see how to use na_if(). Check your output data using unique().\n\n\n\n\n11.3 filter() practice\n\n\n\n\n\n\nQuestion 2\n\n\n\nCreate and store a subset that does NOT include observations from Naples Reef (NAPL). Check your output data frame to ensure that NAPL is NOT in the data frame.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCreate and store a subset with lobsters at Arroyo Quemado (AQUE) AND with a carapace length greater than 70 mm. Check your output.\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nFind the maximum carapace length using max() and group by SITE and MONTH. Think about how you want to treat the NA values in SIZE_MM (Hint: check the arguments in max()). Check your output.\n\n\n\n\n\n\n\n\nSave your work and don’t forget the Git and GitHub workflow!\n\n\n\nAfter you’ve completed the exercises or reached a significant stopping point, use the workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\n\n\n\n11.3.1 Answers (don’t cheat!)\n\n# `group_by() %&gt;% summarize()` practice\nlobster_abundance &lt;- lobster_abundance %&gt;% \n    mutate(SIZE_MM = na_if(SIZE_MM, -99999))\n\nnot_napl &lt;- lobster_abundance %&gt;% \n    filter(SITE != \"NAPL\")\n\naque_70mm &lt;- lobster_abundance %&gt;% \n    filter(SITE == \"AQUE\" & SIZE_MM &gt;= 70)\n\nmax_lobster &lt;- lobster_abundance %&gt;% \n  group_by(SITE, MONTH) %&gt;% \n  summarize(MAX_LENGTH = max(SIZE_MM, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nOpen the Quarto Document collaborator-analysis.qmd\n\nCheck the YAML and add your name to the author field\nCreate a new section with a level 2 header and title it “Exercise: Explore, Clean, and Wrangle Data”\n\nLoad the following libraries at the top of your Quarto Document.\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(here)\n\n\nRead in the data and store the data frame as lobster_traps\n\n\nlobster_traps &lt;- read_csv(\"data/Lobster_Trap_Counts_All_Years_20210519.csv\")\n\n\nLook at your data. Take a minute to explore what your data structure looks like, what data types are in the data frame, or use a function to get a high-level summary of the data you’re working with.\nUse the Git workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\n\nNote: You also want to Pull when you first open a project\n\n\n\n\n\n11.4 Convert missing values using mutate() and na_if()\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe variable TRAPS uses -99999 as the code for missing values (see metadata or use unique()). This has the potential to cause conflicts with our analyses, so let’s convert -99999 to an NA value. Do this using mutate() and na_if(). Look up the help page to see how to use na_if(). Check your output data using unique().\n\n\n\n\n11.5 filter() practice\n\n\n\n\n\n\nQuestion 2\n\n\n\nCreate and store a subset that does NOT include observations from Naples Reef (NAPL). Check your output data frame to ensure that NAPL is NOT in the data frame.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCreate and store a subset with lobsters at Carpinteria Reef (CARP) AND number of commercial trap floats is greater than 20. Check your output.\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nFind the maximum number of commercial trap floats using max() and group by SITE and MONTH. Think about how you want to treat the NA values in TRAPS (Hint: check the arguments in max()). Check your output.\n\n\n\n\n\n\n\n\nSave your work and Don’t forget the Git and GitHub workflow!\n\n\n\nAfter you’ve completed the exercises or reached a significant stopping point, use the workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\n\n\n\n11.5.1 Answer (dont’ cheat!)\n\n# `group_by() %&gt;% summarize()` practice\nlobster_traps &lt;- lobster_traps %&gt;% \n    mutate(TRAPS = na_if(TRAPS, -99999))\n\nnot_napl &lt;- lobster_traps %&gt;% \n    filter(SITE != \"NAPL\")\n\ncarp_20_traps &lt;- lobster_traps %&gt;% \n    filter(SITE == \"CARP\" & TRAPS &gt; 20)\n\nmax_lobster_traps &lt;- lobster_traps %&gt;% \n    group_by(SITE, MONTH) %&gt;%\n    summarize(MAX_TRAPS = max(TRAPS, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.5.2 Create visually appealing and informative data visualization\n\nOwnerCollaborator\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nStay in the Quarto Document owner-analysis.qmd and create a new section with a level 2 header and title it “Exercise: Data Visualization”\n\nStructure of the data visualization exercises:\n\nIn this section, you will first have you create the necessary subsets to create the data visualizations, as well as the basic code to create a visualization.\nThe next step is to return to the data visualization code you’ve written and add styling code to it. For this exercise, only add styling code to the visualization you want to include in the lobster-report.qmd (start with just one plot and if there’s time add styling code to another plot).\nLastly, save the final visualizations to the figs folder before collaborating on the lobster-report.qmd.\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nCreate a multi-panel plot of lobster carapace length (SIZE_MM) using ggplot(), geom_histogram(), and facet_wrap(). Use the variable SITE in facet_wrap(). Use the object lobster_abundance.\n\n\n\n\nPlots\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nCreate a line graph of the number of total lobsters observed (y-axis) by year (x-axis) in the study, grouped by SITE.\n\n\nFirst, you’ll need to create a new dataset subset called lobsters_summarize:\n\nGroup the data by SITE AND YEAR\nCalculate the total number of lobsters observed using count()\n\nNext, create a line graph using ggplot() and geom_line(). Use geom_point() to make the data points more distinct, but ultimately up to you if you want to use it or not. We also want SITE information on this graph, do this by specifying the variable in the color argument. Where should the color argument go? Inside or outside of aes()? Why or why not?\n\n# line plot\n\n\n\nPlots\n\n\n\n\n\n\n\n\n\n\nLine plot\n\n\n\n\n\n\n\nLine and point plot\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nCreate a bar graph that shows the amount of small and large sized carapace lobsters at each SITE from 2019-2021. Note: The small and large metrics are completely made up and are not based on any known facts.\n\n\nFirst, you’ll need to create a new dataset subset called lobster_size_lrg:\n\nfilter() for the years 2019, 2020, and 2021\nAdd a new column called SIZE_BIN that contains the values “small” or “large”. A “small” carapace size is &lt;= 70 mm, and a “large” carapace size is greater than 70 mm. Use mutate() and if_else(). Check your output\nCalculate the number of “small” and “large” sized lobsters using group() and summarize(). Check your output\nRemove the NA values from the subsetted data. Hint: check out drop_na(). Check your output\n\nNext, create a bar graph using ggplot() and geom_bar(). Note that geom_bar() automatically creates a stacked bar chart. Try using the argument position = \"dodge\" to make the bars side by side. Pick which bar position you like best.\n\n\nPlots\n\n\n\n\n\n\n\n\n\n\nBar plot\n\n\n\n\n\n\n\nDodged bar plot\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\n\nGo back to your visualization code and add some styling code (aka make your plots pretty!). Again, start with one plot and if there’s time add styling code to additional plots. Here’s a list of functions to help you get started (this is not an exhaustive list!) or revisit the data visualization lesson:\n\n\nlabs(): modifying axis, legend and plot labels\ntheme_(): add a complete theme to your plot (i.e. theme_light())\ntheme(): use to customize non-data components of a plot. We’ve listed out some parameters here, but run ?theme to see the full list (there’s a lot of customization you can do!)\n\naxis.title.y\npanel.background\nplot.background\npanel.grid.major.*\ntext\n\nscale_*_date(): use with dates and update breaks, limits, and labels\nscale_*_continuous(): use with continuous variables and update breaks, limits, and labels\nscale_*_discrete(): use with discrete variables and update breaks, limits, and labels\nscales package: use this within the above scale functions and you can do things like add percents to axes labels\ngeom_() within a geom function you can modify:\n\nfill: updates fill colors (e.g. column, density, violin, & boxplot interior fill color)\ncolor: updates point & border line colors (generally)\nshape: update point style\nalpha: update transparency (0 = transparent, 1 = opaque)\nsize: point size or line width\nlinetype: update the line type (e.g. “dotted”, “dashed”, “dotdash”, etc.)\n\n\n\nOnce you’re happy with how your plot looks, assign it to an object, and save it to the figs directory using ggsave()\n\n\n\n\n\n\n\n\n\nSave your work and Don’t forget the Git and GitHub workflow!\n\n\n\nAfter you’ve completed the exercises or reached a significant stopping point, use the workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\n\n\n\n11.5.3 Answers\n\nggplot(data = lobster_abundance, \n       aes(x = SIZE_MM)) +\n    geom_histogram() +\n    facet_wrap(~SITE)\n\nlobsters_summarize &lt;- lobster_abundance %&gt;% \n  group_by(SITE, YEAR) %&gt;% \n  summarize(COUNT = n())\n\nggplot(data = lobsters_summarize, aes(x = YEAR, y = COUNT)) +\n  geom_line(aes(color = SITE)) \n\n# line and point plot\nggplot(data = lobsters_summarize, aes(x = YEAR, y = COUNT)) +\n  geom_point(aes(color = SITE)) +\n  geom_line(aes(color = SITE)) \n\nlobster_size_lrg &lt;- lobster_abundance %&gt;%\n    filter(YEAR %in% c(2019, 2020, 2021)) %&gt;%\n    mutate(SIZE_BIN = if_else(SIZE_MM &lt;= 70, true = \"small\", false = \"large\")) %&gt;%\n    group_by(SITE, SIZE_BIN) %&gt;%\n    summarize(COUNT = n()) %&gt;%\n    drop_na()\n# bar plot\nggplot(data = lobster_size_lrg, aes(x = SITE, y = COUNT, fill = SIZE_BIN)) +\n    geom_col()\n\n# dodged bar plot\nggplot(data = lobster_size_lrg, aes(x = SITE, y = COUNT, fill = SIZE_BIN)) +\n    geom_col(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nStay in the Quarto Document collaborator-analysis.qmd and create a new section with a level 2 header and title it “Exercise: Data Visualization”\n\nStructure of the data visualization exercises:\n\nFirst you will create the necessary subsets to create the data visualizations, as well as the basic code to create a visualization.\nThen, you will return to the data visualization code you’ve written and add styling code to it. For this exercise, only add styling code to the visualization you want to include in the lobster-report.qmd (start with just one plot and if there’s time add styling code to another plot).\nLastly, save the final visualizations to the figs folder before collaborating on the lobster-report.qmd.\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nCreate a multi-panel plot of lobster commercial traps (TRAPS) grouped by year, using ggplot(), geom_histogram(), and facet_wrap(). Use the variable YEAR in facet_wrap(). Use the object lobster_traps.\n\n\n\n\nPlots\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nCreate a line graph of the number of total lobster commercial traps observed (y-axis) by year (x-axis) in the study, grouped by SITE.\n\n\nFirst, you’ll need to create a new dataset subset called lobsters_traps_summarize:\n\nGroup the data by SITE AND YEAR\nCalculate the total number of lobster commercial traps observed using sum(). Look up sum() if you need to. Call the new column TOTAL_TRAPS. Don’t forget about NAs here!\n\nNext, create a line graph using ggplot() and geom_line(). Use geom_point() to make the data points more distinct, but ultimately up to you if you want to use it or not. We also want SITE information on this graph, do this by specifying the variable in the color argument. Where should the color argument go? Inside or outside of aes()? Why or why not?\n\n\nPlots\n\n\n\n\n\n\n\n\n\n\nLine plot\n\n\n\n\n\n\n\nLine and point plot\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nCreate a bar graph that shows the amount of high and low fishing pressure of lobster commercial traps at each SITE from 2019-2021. Note: The high and low fishing pressure metrics are completely made up and are not based on any known facts.\n\n\nFirst, you’ll need to create a new dataset subset called lobster_traps_fishing_pressure:\n\nfilter() for the years 2019, 2020, and 2021\nAdd a new column called FISHING_PRESSURE that contains the values “high” or “low”. A “high” fishing pressure has exactly or more than 8 traps, and a “low” fishing pressure has less than 8 traps. Use mutate() and if_else(). Check your output\nCalculate the number of “high” and “low” observations using group() and summarize(). Check your output\nRemove the NA values from the subsetted data. Hint: check out drop_na(). Check your output\n\nNext, create a bar graph using ggplot() and geom_bar(). Note that geom_bar() automatically creates a stacked bar chart. Try using the argument position = \"dodge\" to make the bars side by side. Pick which bar position you like best.\n\n\nPlots\n\n\n\n\n\n\n\n\n\n\nBar plot\n\n\n\n\n\n\n\nDodged bar plot\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\n\nGo back to your visualization code and add some styling code (aka make your plots pretty!). Again, start with one plot and if there’s time add styling code to additional plots. Here’s a list of functions to help you get started (this is not an exhaustive list!) or revisit the data visualization lesson:\n\n\nlabs(): modifying axis, legend and plot labels\ntheme_(): add a complete theme to your plot (i.e. theme_light())\ntheme(): use to customize non-data components of a plot. We’ve listed out some parameters here, but run ?theme to see the full list (there’s a lot of customization you can do!)\n\naxis.title.y\npanel.background\nplot.background\npanel.grid.major.*\ntext\n\nscale_*_date(): use with dates and update breaks, limits, and labels\nscale_*_continuous(): use with continuous variables and update breaks, limits, and labels\nscale_*_discrete(): use with discrete variables and update breaks, limits, and labels\nscales package: use this within the above scale functions and you can do things like add percents to axes labels\ngeom_() within a geom function you can modify:\n\nfill: updates fill colors (e.g. column, density, violin, & boxplot interior fill color)\ncolor: updates point & border line colors (generally)\nshape: update point style\nalpha: update transparency (0 = transparent, 1 = opaque)\nsize: point size or line width\nlinetype: update the line type (e.g. “dotted”, “dashed”, “dotdash”, etc.)\n\n\n\nOnce you’re happy with how your plot looks, assign it to an object, and save it to the figs directory using ggsave()\n\n\n\n\n\n\n\n\n\nSave your work and Don’t forget the Git and GitHub workflow!\n\n\n\nAfter you’ve completed the exercises or reached a significant stopping point, use the workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\n\n\n\n11.5.4 Answers\n\nggplot(data = lobster_traps, aes(x = TRAPS)) +\n    geom_histogram() +\n    facet_wrap( ~ YEAR)\n\nlobsters_traps_summarize &lt;- lobster_traps %&gt;% \n  group_by(SITE, YEAR) %&gt;% \n  summarize(TOTAL_TRAPS = sum(TRAPS, na.rm = TRUE))\n\nggplot(data = lobsters_traps_summarize, aes(x = YEAR, y = TOTAL_TRAPS)) +\n    geom_line(aes(color = SITE))\n\n# line and point plot\nggplot(data = lobsters_traps_summarize, aes(x = YEAR, y = TOTAL_TRAPS)) +\n    geom_point(aes(color = SITE)) +\n    geom_line(aes(color = SITE))\n\nlobster_traps_fishing_pressure &lt;- lobster_traps %&gt;% \n    filter(YEAR %in% c(2019, 2020, 2021)) %&gt;%\n    mutate(FISHING_PRESSURE = if_else(TRAPS &gt;= 8, true = \"high\", false = \"low\")) %&gt;%\n    group_by(SITE, FISHING_PRESSURE) %&gt;%\n    summarize(COUNT = n()) %&gt;%\n    drop_na()\n# bar plot\nggplot(data = lobster_traps_fishing_pressure, aes(x = SITE, y = COUNT, fill = FISHING_PRESSURE)) +\n    geom_col()\n\n# dodged bar plot\nggplot(data = lobster_traps_fishing_pressure, aes(x = SITE, y = COUNT, fill = FISHING_PRESSURE)) +\n    geom_col(position = \"dodge\")\n\n\n\n\n\n\n\n11.5.5 Collaborate on a report and publish using GitHub pages\nThe final step! Time to work together again. Collaborate with your partner in lobster-report.qmd to create a report to publish to GitHub pages.\n\n\n\n\n\n\nCode Review\n\n\n\nAs you’re working on the lobster-report.qmd you will be conducting two types of code reviews: (1) pair programming and (2) lightweight code review.\n\nPair programming is where two people develop code together at the same workstation. One person is the “driver” and one person is the “navigator”. The driver writes the code while the navigator observes the code being typed, points out any immediate quick fixes, and will also Google / troubleshoot if errors occur. Both the Owner and the Collaborator should experience both roles, so switch halfway through or at a meaningful stopping point.\nA lightweight code review is brief and you will be giving feedback on code readability and code logic as you’re adding Owner and Collaborator code from their respective analysis.qmds to the lobster-report.qmd. Think of it as a walk through of your the code for the data visualizations you plan to include in the report (this includes the code you wrote to create the subset for the plot and the code to create the plot) and give quick feedback.\n\n\n\nMake sure your Quarto Document is well organized and includes the following elements:\n\ncitation of the data\nbrief summary of the abstract (i.e. 1-2 sentences) from the EDI Portal\nOwner analysis and visualizations (you choose which plots you want to include)\n\nTry adding alternative text to your plots (See Quarto Documentation)\nPlots can be added either with the data visualization code or with Markdown syntax (calling a saved image) - it’s up to you if you want to include the code or not.\n\nCollaborator analysis and visualizations (you choose which plots you want to include)\n\nTry adding alternative text to your plots (See Quarto Documentation)\nplots can be added either with the data visualization code or with Markdown syntax (calling a saved image) - it’s up to you if you want to include the code or not.\n\n\nFinally, publish on GitHub pages (from Owner’s repository). Refer back to Chapter 12 for steps on how to publish using GitHub pages.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Practice Session I</span>"
    ]
  },
  {
    "objectID": "session_11.html#convert-missing-values-using-mutate-and-na_if",
    "href": "session_11.html#convert-missing-values-using-mutate-and-na_if",
    "title": "11  Practice Session I",
    "section": "11.2 Convert missing values using mutate() and na_if()",
    "text": "11.2 Convert missing values using mutate() and na_if()\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe variable SIZE_MM uses -99999 as the code for missing values (see metadata or use unique()). This has the potential to cause conflicts with our analyses, so let’s convert -99999 to an NA value. Do this using mutate() and na_if(). Look up the help page to see how to use na_if(). Check your output data using unique().",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Practice Session I</span>"
    ]
  },
  {
    "objectID": "session_11.html#filter-practice",
    "href": "session_11.html#filter-practice",
    "title": "11  Practice Session I",
    "section": "11.3 filter() practice",
    "text": "11.3 filter() practice\n\n\n\n\n\n\nQuestion 2\n\n\n\nCreate and store a subset that does NOT include observations from Naples Reef (NAPL). Check your output data frame to ensure that NAPL is NOT in the data frame.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCreate and store a subset with lobsters at Arroyo Quemado (AQUE) AND with a carapace length greater than 70 mm. Check your output.\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nFind the maximum carapace length using max() and group by SITE and MONTH. Think about how you want to treat the NA values in SIZE_MM (Hint: check the arguments in max()). Check your output.\n\n\n\n\n\n\n\n\nSave your work and don’t forget the Git and GitHub workflow!\n\n\n\nAfter you’ve completed the exercises or reached a significant stopping point, use the workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\n\n\n\n11.3.1 Answers (don’t cheat!)\n\n# `group_by() %&gt;% summarize()` practice\nlobster_abundance &lt;- lobster_abundance %&gt;% \n    mutate(SIZE_MM = na_if(SIZE_MM, -99999))\n\nnot_napl &lt;- lobster_abundance %&gt;% \n    filter(SITE != \"NAPL\")\n\naque_70mm &lt;- lobster_abundance %&gt;% \n    filter(SITE == \"AQUE\" & SIZE_MM &gt;= 70)\n\nmax_lobster &lt;- lobster_abundance %&gt;% \n  group_by(SITE, MONTH) %&gt;% \n  summarize(MAX_LENGTH = max(SIZE_MM, na.rm = TRUE))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Practice Session I</span>"
    ]
  },
  {
    "objectID": "session_11.html#convert-missing-values-using-mutate-and-na_if-1",
    "href": "session_11.html#convert-missing-values-using-mutate-and-na_if-1",
    "title": "11  Practice Session I",
    "section": "11.4 Convert missing values using mutate() and na_if()",
    "text": "11.4 Convert missing values using mutate() and na_if()\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe variable TRAPS uses -99999 as the code for missing values (see metadata or use unique()). This has the potential to cause conflicts with our analyses, so let’s convert -99999 to an NA value. Do this using mutate() and na_if(). Look up the help page to see how to use na_if(). Check your output data using unique().",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Practice Session I</span>"
    ]
  },
  {
    "objectID": "session_11.html#filter-practice-1",
    "href": "session_11.html#filter-practice-1",
    "title": "11  Practice Session I",
    "section": "11.5 filter() practice",
    "text": "11.5 filter() practice\n\n\n\n\n\n\nQuestion 2\n\n\n\nCreate and store a subset that does NOT include observations from Naples Reef (NAPL). Check your output data frame to ensure that NAPL is NOT in the data frame.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCreate and store a subset with lobsters at Carpinteria Reef (CARP) AND number of commercial trap floats is greater than 20. Check your output.\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nFind the maximum number of commercial trap floats using max() and group by SITE and MONTH. Think about how you want to treat the NA values in TRAPS (Hint: check the arguments in max()). Check your output.\n\n\n\n\n\n\n\n\nSave your work and Don’t forget the Git and GitHub workflow!\n\n\n\nAfter you’ve completed the exercises or reached a significant stopping point, use the workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\n\n\n\n11.5.1 Answer (dont’ cheat!)\n\n# `group_by() %&gt;% summarize()` practice\nlobster_traps &lt;- lobster_traps %&gt;% \n    mutate(TRAPS = na_if(TRAPS, -99999))\n\nnot_napl &lt;- lobster_traps %&gt;% \n    filter(SITE != \"NAPL\")\n\ncarp_20_traps &lt;- lobster_traps %&gt;% \n    filter(SITE == \"CARP\" & TRAPS &gt; 20)\n\nmax_lobster_traps &lt;- lobster_traps %&gt;% \n    group_by(SITE, MONTH) %&gt;%\n    summarize(MAX_TRAPS = max(TRAPS, na.rm = TRUE))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Practice Session I</span>"
    ]
  },
  {
    "objectID": "session_11.html#bonus-add-marine-protected-area-mpa-designation-to-the-data",
    "href": "session_11.html#bonus-add-marine-protected-area-mpa-designation-to-the-data",
    "title": "11  Practice Session I",
    "section": "11.6 Bonus: Add marine protected area (MPA) designation to the data",
    "text": "11.6 Bonus: Add marine protected area (MPA) designation to the data\nThe sites IVEE and NAPL are marine protected areas (MPAs). Add this designation to your data set using a new function called case_when(). Then create some new plots using this new variable. Does it change how you think about the data? What new plots or analysis can you do with this new variable?\n\nLobster Abundance & Size DataLobster Trap Buoy Counts Data\n\n\nUse the object lobster_abundance and add a new column called DESIGNATION that contains “MPA” if the site is IVEE or NAPL, and “not MPA” for all other values.\n\nlobster_mpa &lt;- lobster_abundance %&gt;% \n    mutate(DESIGNATION = case_when(\n    SITE %in% c(\"IVEE\", \"NAPL\") ~ \"MPA\",\n    SITE %in% c(\"AQUE\", \"CARP\", \"MOHK\") ~ \"not MPA\"\n  ))\n\n\n\nUse the object lobster_traps and add a new column called DESIGNATION that contains “MPA” if the site is IVEE or NAPL, and “not MPA” for all other values.\n\nlobster_traps_mpa &lt;- lobster_traps %&gt;%\n    mutate(DESIGNATION = case_when(\n    SITE %in% c(\"IVEE\", \"NAPL\") ~ \"MPA\",\n    SITE %in% c(\"AQUE\", \"CARP\", \"MOHK\") ~ \"not MPA\"\n  ))\n\n\n\n\n\n\n\n\nLTER, Santa Barbara Coastal, Daniel C Reed, and Robert J Miller. 2022. “SBC LTER: Reef: Abundance, Size and Fishing Effort for California Spiny Lobster (Panulirus Interruptus), Ongoing Since 2012.” Environmental Data Initiative. https://doi.org/10.6073/PASTA/25AA371650A671BAFAD64DD25A39EE18.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Practice Session I</span>"
    ]
  },
  {
    "objectID": "session_12.html",
    "href": "session_12.html",
    "title": "12  Writing Functions",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_12.html#learning-objectives",
    "href": "session_12.html#learning-objectives",
    "title": "12  Writing Functions",
    "section": "",
    "text": "Explain the importance of using and developing functions\nCreate custom functions using R code\nDocument functions to improve understanding and code communication",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_12.html#function-fundamentals",
    "href": "session_12.html#function-fundamentals",
    "title": "12  Writing Functions",
    "section": "12.1 Function fundamentals",
    "text": "12.1 Function fundamentals\nMany people write R code as a single, continuous stream of commands, often drawn from the R console itself and simply pasted into a script. While any script is preferable to non-scripted solutions, there are advantages to breaking code into small, reusable modules. This is the role of a function in R. In this lesson, we will review the advantages of coding with functions, practice by creating some functions and show how to call them, and then do some exercises to build other simple functions.\nIn general, a function is a defined set of code statements or expressions that are organized together to perform a specific task, and can be used (aka “called”) whenever needed. Functions are typically designed to accept some input(s), do something with it, and return a useful output. Sometimes a function may be designed to have one or more side effects, such as printing something to the console or saving a file to disk, but today we will focus only on functions that simply return a result. Functions are common in all major programming languages, including R.\nThere are many advantages to writing small functions that only complete one logical task and do it well:\n\nWhen you need to modify your code, you only need to update it in one place.\nYou can make the code easier to understand by giving an informative name to your function.\nYou can take advantage of iteration techniques that apply a function to each member of a collection. For example, dplyr::across allows you to apply the same function to multiple columns of a data frame all in one expression.\n\nFunctions are essential for following the DRY principle in coding and softward development: Don’t Repeat Yourself!\n\n\n\n\n\n\nTip\n\n\n\n\n“You should consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code).”\nChapter 19 Functions in R for Data Science (Grolemund & Wickham)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_12.html#functions-in-r",
    "href": "session_12.html#functions-in-r",
    "title": "12  Writing Functions",
    "section": "12.2 Functions in R",
    "text": "12.2 Functions in R\nIn R, functions are objects that can be created and assigned to a name so that they can be used throughout code by reference – just like vectors, data frames, and other types of objects! To create a function in R, you use the function function (so meta!). Here is the general template for a function definition:\n&lt;function_name&gt; &lt;- function(&lt;argument1&gt;, &lt;argument2&gt;, ...) {\n  &lt;whatever code your function needs to run&gt;\n  return(&lt;something&gt;)\n}\nThe function arguments, specified in parentheses after the function keyword and separated by commas, are placeholders for the inputs that we want our function to accept. A function might have just one argument, or many arguments, or even zero arguments if the function does not require any inputs when called! After listing the arguments, we provide the function body enclosed in squiggly braces ({}). This is the code that will be run everytime the function is called. Typically your code will operate on (or otherwise use) the provided arguments. Note that the final line of the function body is a return(&lt;something&gt;) expression, where &lt;something&gt; is a single object - e.g. a vector, a list, a data frame, or even another function! Technically, if you don’t include a return(), R will simply return the result of the last expression it evaluates in the function body. However, it’s good practice to use an explicit return in most cases. Finally, notice that we assign the entire function definition, i.e. function(...) {...}, to a name. Just like when creating vector and data frames, we need to assign a function to a name in order to be able to refer to it again later.\nNote that if your function body refers to named objects that you have neither passed in as arguments nor created inside your function body, R will apply so-called scoping rules that dictate where it will look outside the function for that named object. Scoping rules are, ahem, outside the scope of this module. But as a general rule, to avoid complexity and bugs, your function body should only operate on variables that you have passed in as objects.\n\n\n\n\n\n\nNaming Functions\n\n\n\nThe name of a function is important. A function name should be short, while still clearly communicating its purpose.\nBest Practices from Chapter 19 Functions in R for Data Science:\n\nFunction names should be verbs and arguments should be nouns (there are exceptions).\nUse the snake_case naming convention for functions that are multiple words.\nFor a “family” of functions, use a common prefix to indicate that they are connected - e.g., functions from the stringr package all use the prefix str_, as in str_detect() and str_replace().",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_12.html#exercise-temperature-conversion",
    "href": "session_12.html#exercise-temperature-conversion",
    "title": "12  Writing Functions",
    "section": "12.3 Exercise: Temperature Conversion",
    "text": "12.3 Exercise: Temperature Conversion\nImagine you have some multiple sets of temperature readings in units of Fahrenheit, and need to convert it to Celsius. You might write an R script that does this for you.\n\ninterior_f &lt;- c(175, 134, 181)\ninterior_c &lt;- (interior_f - 32) * 5/9\n\nexterior_f &lt;- c(77, 78, 77)\nexterior_c &lt;- (exterior_f - 32) * 5/9\n\nsurface_f &lt;- c(103, 102, 99)\nsurface_c &lt;- (surface_f - 32) * 5/9\n\nNote the duplicated code, which repeats the same formula three times. What if you realized your formula had an error? You would need to correct it in three places. Moreover, it would be hard to notice if one of the conversion statements contained a typo in the formula. Overall, the code would be more compact and more reliable if we didn’t repeat ourselves.\n\nWrite a temperature conversion function\nLet’s create a function that calculates Celsius temperature outputs from Fahrenheit temperature inputs.\n\nconvert_f_to_c &lt;- function(fahr) {\n  celsius &lt;- (fahr - 32) * 5/9\n  return(celsius)\n}\n\nBy running this code, we have created a function object and stored it in the R global environment. The fahr argument to the function function indicates that our conversion function expects a single argument (the temperature in Fahrenheit), and the return statement indicates that the function should return the value in the celsius variable that was calculated inside the function. Let’s use it, and check if we got the same value as before:\n\nsurface_c_v2 &lt;- convert_f_to_c(fahr = surface_f)\nsurface_c_v2\n\n[1] 39.44444 38.88889 37.22222\n\nidentical(surface_c_v2, surface_c)\n\n[1] TRUE\n\n\nExcellent! Now we have our own local function for converting a vector of temperatures in Fahrenheit to temperatures in Celsius. Note also that we explicitly named the argument inside the function call (convert_f_to_c(fahr = surface_f)), but in this simple case, R can figure it out if we didn’t explicitly tell it the argument name (convert_f_to_c(surface_f)). More on this later!\n\n\nYour Turn: Create a Function that Converts Celsius to Fahrenheit\n\n\n\n\n\n\nExercise\n\n\n\nCreate a function named convert_c_to_f that does the reverse, taking temperatures in Celsius as input and returning them in Fahrenheit.\nCreate the function convert_c_to_f in a new code chunk or even a separate R script file.\nThen use that formula to convert the Celsius vectors back into Fahrenheit values, and compare the results to the original Fahrenheit vectorto ensure that your answers are correct.\nHint: the formula for Celsius to Fahrenheit conversions is celsius * 9/5 + 32.\n\n\nDid you encounter any issues with rounding or precision?\n\n\n\n\n\n\nSolution, but don’t peek!\n\n\n\n\n\nDon’t peek until you write your own…\n\nconvert_c_to_f &lt;- function(celsius) {\n  fahr &lt;- celsius * 9/5 + 32\n  return(fahr)\n}\n\nsurface_f_v2 &lt;- convert_c_to_f(surface_c)\nidentical(surface_f, surface_f_v2)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_12.html#exercise-minimizing-work-with-functions",
    "href": "session_12.html#exercise-minimizing-work-with-functions",
    "title": "12  Writing Functions",
    "section": "12.4 Exercise: Minimizing Work with Functions",
    "text": "12.4 Exercise: Minimizing Work with Functions\nFunctions can be as simple or complex as needed. They can be very effective in repeatedly performing calculations, or for bundling a group of commands that are used on many different input data sources. For example, we might create a simple function that takes Fahrenheit temperatures as input, and calculates both Celsius and Kelvin temperatures. All three values are then returned in a list, making it very easy to create a comparison table among the three scales.\n\nconvert_temps &lt;- function(fahr) {\n  celsius &lt;- (fahr - 32) * 5/9\n  kelvin &lt;- celsius + 273.15\n  return(list(fahr = fahr, celsius = celsius, kelvin = kelvin))\n}\n\nsurface_temps_df &lt;- data.frame(convert_temps(fahr = surface_f))\n\n\n\n\n\n\n\nBut what if we wanted to make this function more flexible? For example, what if wanted to allow the user to provide temperatures in either Fahrenheit or Celsius? Let’s add an additional argument so the user can specify the input units. While we’re at it, let’s also add some error checking, and update the function to return the result as a data frame rather than a list.\n\nconvert_temps2 &lt;- function(temp, unit = \"F\") {\n\n  ### Error checking:\n  unit &lt;- toupper(unit)  ## try to anticipate common mistakes!\n  if (!unit %in% c(\"F\", \"C\")) {\n    stop(\"The units must be either F or C!\")\n  }\n\n  if (unit == \"F\") {\n    fahr &lt;- temp\n    celsius &lt;- (fahr - 32) * 5/9\n  } else {\n    celsius &lt;- temp\n    fahr &lt;- celsius * 9 / 5 + 32\n  }\n  kelvin &lt;- celsius + 273.15\n\n  out_df &lt;- data.frame(fahr, celsius, kelvin)\n  return(out_df)\n}\n\n# run on the Celsius values, using named arguments\nsurface_temps_df1 &lt;- convert_temps2(temp = surface_c, unit = \"C\")\n\n# run on the Fahrenheit values, using positional arguments\nsurface_temps_df2 &lt;- convert_temps2(surface_f, \"F\")\n\n# run on the Fahrenheit values, using default `unit` of \"F\"\nsurface_temps_df3 &lt;- convert_temps2(surface_f)\n\n# check that the last output matches our earlier data frame\nidentical(surface_temps_df, surface_temps_df3)\n\n[1] TRUE\n\n\nNotice that we added some other new features here as well. In the arguments list of our function definition, we provided a default value for the unit argument (unit = \"F\"). This convenience means that the function caller can opt to skip that argument, and R will use the default value. Because temp does not have a default, the user cannot skip that one! Also note that R interprets the arguments in order, so we can even skip naming them, though when calling novel or complex functions it is helpful to explicitly name the arguments.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_12.html#functions-in-the-tidyverse",
    "href": "session_12.html#functions-in-the-tidyverse",
    "title": "12  Writing Functions",
    "section": "12.5 Functions in the tidyverse",
    "text": "12.5 Functions in the tidyverse\nIf you frequently work with the tidyverse package and all its amazing functionality, understanding how those tidyverse functions are designed can help you write your own tidyverse style functions. There are two common use cases:\n\nA function that can be used to calculate inside a mutate() or summarize()\nA function that can be used seamlessly in a piped tidyverse-style workflow\n\n\n12.5.0.1 Functions for mutate or summarize\nThis kind of function should take a vector (or multiple vectors) and return a single vector. Functions that return a vector the same length as the input would be useful for mutate(); functions that return a vector of length 1 (e.g., mean() or sd()) would be useful for summarize(). We’ve already created two functions like that.\n\ndata.frame(f = surface_f) %&gt;%\n  mutate(c = convert_f_to_c(fahr = f),\n         f2 = convert_c_to_f(celsius = c))\n\n    f        c  f2\n1 103 39.44444 103\n2 102 38.88889 102\n3  99 37.22222  99\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhy wouldn’t our convert_temps() function work here?\n\n\n\n\n12.5.0.2 Functions for piped workflows\nA common workflow in the tidyverse is to use the magrittr pipe operator %&gt;% (or the newer built-in |&gt;) to pass a data frame into a function like select(), filter(), or mutate(), and then pass the results from that into another function, and so on:\n\nsurface_temps_df %&gt;%\n  select(fahr, celsius) %&gt;%\n  mutate(rankine = fahr + 459.67)\n\n  fahr  celsius rankine\n1  103 39.44444  562.67\n2  102 38.88889  561.67\n3   99 37.22222  558.67\n\n\nFor this to work:\n\nEvery dplyr and tidyr function takes a data frame (or variant such as a tibble) as its first argument.\nEvery dplyr and tidyr function returns a data frame (or variant).\n\nThe pipe operator %&gt;% says, take the preceding object (a data frame, such as one returned by a dplyr function) and pass it to the next function (such as another dplyr function) as the first argument.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_12.html#exercise-make-a-tidyverse-style-function",
    "href": "session_12.html#exercise-make-a-tidyverse-style-function",
    "title": "12  Writing Functions",
    "section": "12.6 Exercise: Make a tidyverse style function",
    "text": "12.6 Exercise: Make a tidyverse style function\nLet’s make a function that can take a dataframe and calculate a new column that tells whether a temperature is hot or cold, based on some threshold.\n\nadd_hot_or_cold &lt;- function(df, thresh = 70) {\n  ### error check:\n  if (!\"fahr\" %in% names(df)) {\n    stop('The data frame must have a column called `fahr`!')\n  }\n\n  out_df &lt;- df %&gt;%\n    mutate(hotcold = ifelse(fahr &gt; thresh, \"hot\", \"cold\"))\n\n  return(out_df)\n}\n\nsurface_temps_df %&gt;%\n  select(fahr, celsius) %&gt;%\n  add_hot_or_cold(thresh = 100) %&gt;%\n  arrange(desc(fahr))\n\n  fahr  celsius hotcold\n1  103 39.44444     hot\n2  102 38.88889     hot\n3   99 37.22222    cold",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_12.html#functions-and-ggplot",
    "href": "session_12.html#functions-and-ggplot",
    "title": "12  Writing Functions",
    "section": "12.7 Functions and ggplot()",
    "text": "12.7 Functions and ggplot()\nOnce we have a dataset like that, we might want to plot it. One thing that we do repeatedly is set a consistent set of display elements for creating graphs and plots. By using a function to create a custom ggplot theme, we can enable to keep key parts of the formatting flexible. For example, in the custom_theme function, we provide a base_size argument that defaults to using a font size of 9 points. Because it has a default set, it can safely be omitted. But if it is provided, then that value is used to set the base font size for the plot.\n\ncustom_theme &lt;- function(base_size = 9) {\n  ### NOTE: functions used *inside* a function need to be available,\n  ### e.g., attached with library() or called using namespace (package::function)\n  ggplot2::theme(\n    text = element_text(family = \"serif\",\n                        color = \"gray30\",\n                        size = base_size),\n    plot.title = element_text(size = rel(1.25),\n                              hjust = 0.5,\n                              face = \"bold\"),\n    panel.background = element_rect(fill = \"azure\"),\n    panel.border = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(colour = \"grey90\",\n                                    linewidth = 0.25),\n    legend.position = \"right\",\n    legend.key = element_rect(colour = NA,\n                              fill = NA),\n    axis.ticks = element_blank(),\n    axis.line = element_blank()\n  )\n}\n\nlibrary(ggplot2)\n\nggplot(surface_temps_df, aes(x = fahr, y = celsius, color = kelvin)) +\n  geom_point() +\n  custom_theme(10)\n\n\n\n\n\n\n\n\nIn this case, we set the font size to 10, and plotted the air temperatures. The custom_theme function can be used anywhere that one needs to consistently format a plot.\nBut we can go further. One can wrap the entire call to ggplot in a function, enabling one to create many plots of the same type with a consistent structure. For example, we can create a scatterplot function that takes a data frame as input, along with a point_size for the points on the plot, and a font_size for the text.\n\nscatterplot &lt;- function(df, point_size = 2, font_size = 9) {\n  ggplot(df, mapping = aes(x = fahr, y = celsius, color = kelvin)) +\n    geom_point(size = point_size) +\n    custom_theme(font_size)\n}\n\nCalling that lets us, in a single line of code, create a highly customized plot but maintain flexibility via the arguments passed in to the function. Let’s set the point size to 3 and font to 16 to make the plot more legible.\n\nscatterplot(surface_temps_df, point_size = 3, font_size = 16)\n\n\n\n\n\n\n\n\nOnce these functions are set up, all of the plots built with them can be reformatted by changing the settings in just the functions, whether they were used to create 1, 10, or 100 plots.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_12.html#summary",
    "href": "session_12.html#summary",
    "title": "12  Writing Functions",
    "section": "12.8 Summary",
    "text": "12.8 Summary\n\nUse functions to make code less repetitive, more understandable, and ultimately more robust\nBuild functions in R with function()\nDesign R functions to work with tidyverse flow\n\n\n\n\n\n\n\nWorkflow for Creating Functions\n\n\n\n\nHave a clear goal (sometimes it helps to create a visual).\nOutline the plan and then add more detailed steps or tasks.\nBuild it up bit-by-bit and start with a minimum viable example. As your function becomes more complex, it can harder to track all the bits.\nAlways check correctness of intermediate steps within the function.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "session_13.html",
    "href": "session_13.html",
    "title": "13  Writing Packages",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Writing Packages</span>"
    ]
  },
  {
    "objectID": "session_13.html#learning-objectives",
    "href": "session_13.html#learning-objectives",
    "title": "13  Writing Packages",
    "section": "",
    "text": "The advantages of using R packages for organizing code\nSimple techniques for creating R packages\nApproaches to documenting code in packages",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Writing Packages</span>"
    ]
  },
  {
    "objectID": "session_13.html#why-packages",
    "href": "session_13.html#why-packages",
    "title": "13  Writing Packages",
    "section": "13.1 Why packages?",
    "text": "13.1 Why packages?\nMost R users are familiar with loading and utilizing packages in their work. And they know how rich CRAN is in providing for many conceivable needs. Most people have never created a package for their own work, and most think the process is too complicated. Really it’s pretty straighforward and super useful in your personal work. Creating packages serves two main use cases:\n\nMechanism to redistribute reusable code (even if just for yourself)\nMechanism to reproducibly document analysis and models and their results\n\nEven if you don’t plan on writing a package with such broad appeal such as, say, ggplot2 or dplyr, you still might consider creating a package to contain:\n\nUseful utility functions you write (i.e. a Personal Package). Having a place to put these functions makes it much easier to find and use them later.\nA set of shared routines for your lab or research group, making it easier to remain consistent within your team and also to save time.\nThe analysis accompanying a thesis or manuscript, making it all that much easier for others to reproduce your results.\n\n\n\n\n\n\n\nPackages for Creating and Maintaining Packages\n\n\n\nThe usethis, devtools and roxygen2 packages make creating and maintining a package to be a straightforward experience.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Writing Packages</span>"
    ]
  },
  {
    "objectID": "session_13.html#create-a-basic-package",
    "href": "session_13.html#create-a-basic-package",
    "title": "13  Writing Packages",
    "section": "13.2 Create a Basic Package",
    "text": "13.2 Create a Basic Package\nTo create a package we’re going to use the following packages:\n\ndevtools: Provides R functions that make package development easier by expediting common development tasks.\nusethis: Commonly referred to as a “workflow package” and provides functions that automate common tasks in project setup and development for both R packages and non-package projects.\nroxygen2: Provides a structure for describing your functions in the scripts you’re creating them in. It will additionally process the source code and the documentation within it to automatically create the necessary files for the documentation to appear in your R Package.\n\nThanks to the great usethis package, it only takes one function call to create the skeleton of an R package using create_package(). Which eliminates pretty much all reasons for procrastination. To create a package called mytools, all you do is:\n\nusethis::create_package(\"~/mytools\")\n\n✔ Creating /home/ruser/mytools/.\n✔ Setting active project to \"/home/ruser/mytools\".\n✔ Creating R/.\n✔ Writing DESCRIPTION.\nPackage: mytools\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R (parsed):\n    * First Last &lt;first.last@example.com&gt; [aut, cre]\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to\n    pick a license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.3.2\n✔ Writing NAMESPACE.\n✔ Writing mytools.Rproj.\n✔ Adding \"^mytools\\\\.Rproj$\" to .Rbuildignore.\n✔ Adding \".Rproj.user\" to .gitignore.\n✔ Adding \"^\\\\.Rproj\\\\.user$\" to .Rbuildignore.\n✔ Opening /home/ruser/mytools/ in new RStudio session.\n✔ Setting active project to \"&lt;no active project&gt;\".\n\n\n\n\n\n\nWhat did the create_package() function do?\n\n\n\n\nOpen a new project called mytools (the name of the package) in a new RStudio session.\nCreate a top-level directory structure, including a number of critical files under the standard R package structure:\n\nDESCRIPTIONfile: The most important file, which provides metadata about your package. Edit this file to provide reasonable values for each of the fields, including your contact information.\nNAMESPACE file declares the functions your package exports for external use and the external functions your package imports from other packages.\nR/ directory is where you save all your function scripts and other .R files.\n.Rbuildignore lists files that we need to have around but that should not be included when building the R package from source.\n.Rproj.user is a directory used internally by RStudio.\n\nAdd the Build Tab to the Environment Pane.\n\n\n\n\n13.2.1 Add a License\nInformation about choosing a LICENSE is provided in the R Package (2e) book Chapter 12: Licensing.\nThe DESCRIPTION file expects the license to be chose from a predefined list, but you can use its various utility methods for setting a specific license file, such as the MIT license or the Apache 2 license:\n\nusethis::use_apache_license()\n\n✔ Setting active project to \"/home/ruser/mytools\".\n✔ Adding \"Apache License (&gt;= 2)\" to License.\n✔ Writing LICENSE.md.\n✔ Adding \"^LICENSE\\\\.md$\" to .Rbuildignore.\nOnce your license has been chosen, and you’ve edited your DESCRIPTION file with your contact information, a title, and a description, it will look like this:\n\n\n\n\n\n\nPackage: mytools\nTitle: My Amazing Utility R Functions\nVersion: 0.0.0.9000\nAuthors@R:\n    person(\"R\", \"User\", email = \"ruser@somewhere.edu\", role = c(\"aut\", \"cre\"),\n           comment = c(ORCID = \"YOUR-ORCID-ID\"))\nDescription: A collection of useful R functions that I use for general utilities.\nLicense: Apache License (&gt;= 2)\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.3.2\n\n\n\n\n\n13.2.2 Add Code\nThe skeleton package created contains a directory R which should contain your source files. Add your functions and classes in files to this directory, attempting to choose names that don’t conflict with existing packages. For example, you might add a file custom_theme that contains a function custom_theme() that you might want to reuse. The usethis::use_r() function will help set up you files in the right places. For example, running:\n\nusethis::use_r(\"custom_theme\")\n\n✔ Setting active project to '/home/ruser/mytools'\n• Modify 'R/custom_theme.R'\n• Call `use_test()` to create a matching test file\ncreates the file R/custom_theme and stores it in the R directory, which you can then modify as needed:\n\ncustom_theme &lt;- function(base_size = 9) {\n  ggplot2::theme(\n    axis.ticks = ggplot2::element_blank(),\n    text = ggplot2::element_text(family = 'Helvetica',\n                                 color = 'gray30',\n                                 size = base_size),\n    plot.title = ggplot2::element_text(size = ggplot2::rel(1.25),\n                                       hjust = 0.5,\n                                       face = 'bold'),\n    panel.background = ggplot2::element_blank(),\n    legend.position = 'right',\n    panel.border = ggplot2::element_blank(),\n    panel.grid.minor = ggplot2::element_blank(),\n    panel.grid.major = ggplot2::element_line(colour = 'grey90',\n                                             linewidth = .25),\n    legend.key = ggplot2::element_rect(colour = NA,\n                                       fill = NA),\n    axis.line = ggplot2::element_blank()\n  )\n}\n\n\n\n\n\n\n\nPower of Packages\n\n\n\nRemember when we created custom_theme() in the Writing Functions session? Now that we’ve added it to our mytools package, we don’t have to worry about coyping the code from another file, sourcing the file from another directory, or copying the script from an R Project.\nInstead we can leverage the portable functionality of a package to easily access our custom functions and maintain the code in one location.\n\n\n\n\n13.2.3 Add Dependencies\nIf your R code depends on functions from another package, you must declare it. In the Imports section in the DESCRIPTION file, list all the packages your functions depend upon.\nIn our custom_theme() function, we depend on the ggplot2 package, and so we need to list it as a dependency.\nOnce again, usethis provides a handy helper method:\n\nusethis::use_package(\"ggplot2\")\n\n✔ Adding 'ggplot2' to Imports field in DESCRIPTION\n• Refer to functions with `ggplot2::fun()`\nTake a look at the DESCRIPTION file again, and you’ll see a new Imports section has been added, with listed ggplot2 underneath.\n\n\n\n\n\n\nPackage: mytools\nTitle: My Amazing Utility R Functions\nVersion: 0.0.0.9000\nAuthors@R:\n    person(\"R\", \"User\", email = \"ruser@somewhere.edu\", role = c(\"aut\", \"cre\"),\n           comment = c(ORCID = \"YOUR-ORCID-ID\"))\nDescription: A collection of useful R functions that I use for general utilities.\nLicense: Apache License (&gt;= 2)\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.3.2\nImports:\n    ggplot2\n\n\n\n\n\n13.2.4 Add Documentation\nDocumentation is crucial to add to each of your functions. In the Functions Lesson, we did this using the roxygen2 package and that same package and approach can be used for packages.\nThe roxygen2 approach allows us to add comments in the source code, where are then converted into Help pages that we can access by typing ?function_name in the Console.\nLet’s add documentation for the custom_theme() function.\n\n#' My custom ggplot theme\n#'\n#' @param base_size Numeric value of font size of all text elements in plot\n#'\n#' @return A theme used for ggplot point or line plots\n#' @export\n#'\n#' @examples\n#' library(ggplot2)\n#' ggplot(data = mtcars, aes(x = mpg, y = disp)) +\n#'   geom_point() +\n#'   custom_theme(base_size = 30)\ncustom_theme &lt;- function(base_size = 9) {\n  ggplot2::theme(\n    axis.ticks = ggplot2::element_blank(),\n    text = ggplot2::element_text(family = 'Helvetica',\n                                 color = 'gray30',\n                                 size = base_size),\n    plot.title = ggplot2::element_text(size = ggplot2::rel(1.25),\n                                       hjust = 0.5,\n                                       face = 'bold'),\n    panel.background = ggplot2::element_blank(),\n    legend.position = 'right',\n    panel.border = ggplot2::element_blank(),\n    panel.grid.minor = ggplot2::element_blank(),\n    panel.grid.major = ggplot2::element_line(colour = 'grey90',\n                                             linewidth = .25),\n    legend.key = ggplot2::element_rect(colour = NA,\n                                       fill = NA),\n    axis.line = ggplot2::element_blank()\n  )\n}\n\nOnce your files are documented, you can then process the documentation using devtools::document() to generate the appropriate .Rd files that your package needs. The .Rd files will appear in the man/ directory, which is automatically created by devtools::document().\n\ndevtools::document()\n\nℹ Updating mytools documentation\nℹ Loading mytools\nWriting NAMESPACE\nWriting custom_theme.Rd\nWe now have a package that we can check() and install() and release(). These functions come from the devtools package, but first let’s do some testing.\n\n\n13.2.5 Testing\nYou can test your code using the testthat package’s testing framework. The ussethis::use_testthat() function will set up your package for testing, and then you can use the use_test() function to setup individual test files. For example, in the Functions Lesson we created some tests for our fahr_to_celsius functions but ran them line by line in the console.\nFirst, let’s add that function to our package. Run the use_r function in the console:\n\nusethis::use_r(\"fahr_to_celsius\")\n\nThen copy the function and documentation into the R script that opens and save the file.\n\n#' Convert temperature values from Fahrenheit to Celsius\n#'\n#' @param fahr Numeric vector in degrees Fahrenheit\n#'\n#' @return Numeric vector in degrees Celsius\n#' @export\n#'\n#' @examples\n#' fahr_to_celsius(32)\n#' fahr_to_celsius(c(32, 212, 72))\nfahr_to_celsius &lt;- function(fahr) {\n  celsius &lt;- (fahr - 32) * 5 / 9\n  return(celsius)\n}\n\nNow, set up your package for testing:\n\nusethis::use_testthat()\n\n✔ Adding testthat to Suggests field in DESCRIPTION.\n✔ Adding \"3\" to Config/testthat/edition.\n✔ Creating tests/testthat/.\n✔ Writing tests/testthat.R.\n☐ Call usethis::use_test() to initialize a basic test file and open it for editing.\nThen write a test for fahr_to_celsius:\n\nusethis::use_test(\"fahr_to_celsius\")\n\n✔ Writing 'tests/testthat/test-fahr_to_celsius.R'\n• Modify 'tests/testthat/test-fahr_to_celsius.R'\nNotice that this has created a new R script named test-fahr_to_celsius.R in the tests/testthat/ director. This script contains a sample test that has nothing to do with your function. Let’s replace it with a more meaningful test:\n\ntest_that(\"fahr_to_celsius works\", {\n  expect_equal(fahr_to_celsius(32), 0)\n  expect_equal(fahr_to_celsius(212), 100)\n})\n\nOnce we have a test defined, we can run all tests using devtools::test(). Let’s give it a try:\n\ndevtools::test()\n\nℹ Testing mytools\n✔ | F W  S  OK | Context\n✔ |          2 | fahr_to_celsius\n\n══ Results ═══════════════════════════════════════════════════════════════════════════════════════\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]\nHooray, all tests passed!\n\n\n13.2.6 Checking and Installing\nNow that you’ve completed testing your package, you can check it for consistency and completeness using devtools::check().\n\ndevtools::check()\n\n══ Documenting ════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\nℹ Updating mytools documentation\nℹ Loading mytools\nWriting NAMESPACE\nWriting fahr_to_celsius.Rd\n\n══ Building ═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\nSetting env vars:\n• CFLAGS    : -Wall -pedantic -fdiagnostics-color=always\n• CXXFLAGS  : -Wall -pedantic -fdiagnostics-color=always\n• CXX11FLAGS: -Wall -pedantic -fdiagnostics-color=always\n• CXX14FLAGS: -Wall -pedantic -fdiagnostics-color=always\n• CXX17FLAGS: -Wall -pedantic -fdiagnostics-color=always\n• CXX20FLAGS: -Wall -pedantic -fdiagnostics-color=always\n── R CMD build ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n✔  checking for file ‘/home/ruser/mytools/DESCRIPTION’ (657ms)\n─  preparing ‘mytools’:\n✔  checking DESCRIPTION meta-information\n─  checking for LF line-endings in source and make files and shell scripts (598ms)\n─  checking for empty or unneeded directories\n─  building ‘mytools_0.0.0.9000.tar.gz’\n\n══ Checking ═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\nSetting env vars:\n• _R_CHECK_CRAN_INCOMING_REMOTE_               : FALSE\n• _R_CHECK_CRAN_INCOMING_                      : FALSE\n• _R_CHECK_FORCE_SUGGESTS_                     : FALSE\n• _R_CHECK_PACKAGES_USED_IGNORE_UNUSED_IMPORTS_: FALSE\n• NOT_CRAN                                     : true\n── R CMD check ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n─  using log directory ‘/tmp/RtmpTM1qoX/file1e21d33c132b73/mytools.Rcheck’ (731ms)\n─  using R version 4.4.2 (2024-10-31)\n─  using platform: x86_64-pc-linux-gnu\n─  R was compiled by\n       gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n       GNU Fortran (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n─  running under: Ubuntu 22.04.5 LTS\n─  using session charset: UTF-8\n─  using options ‘--no-manual --as-cran’\n✔  checking for file ‘mytools/DESCRIPTION’\n─  this is package ‘mytools’ version ‘0.0.0.9000’\n─  package encoding: UTF-8\n✔  checking package namespace information\n✔  checking package dependencies (2.5s)\n✔  checking if this is a source package ...\n✔  checking if there is a namespace\n✔  checking for executable files ...\n✔  checking for hidden files and directories\n✔  checking for portable file names\n✔  checking for sufficient/correct file permissions\n✔  checking serialization versions\n✔  checking whether package ‘mytools’ can be installed (3.5s)\n✔  checking installed package size ...\n✔  checking package directory\n✔  checking for future file timestamps ...\n✔  checking DESCRIPTION meta-information (599ms)\n✔  checking top-level files ...\n✔  checking for left-over files\n✔  checking index information\n✔  checking package subdirectories ...\n✔  checking code files for non-ASCII characters ...\n✔  checking R files for syntax errors ...\n✔  checking whether the package can be loaded (468ms)\n✔  checking whether the package can be loaded with stated dependencies ...\n✔  checking whether the package can be unloaded cleanly ...\n✔  checking whether the namespace can be loaded with stated dependencies ...\n✔  checking whether the namespace can be unloaded cleanly (484ms)\n✔  checking loading without being on the library search path (585ms)\n✔  checking dependencies in R code (1.8s)\n✔  checking S3 generic/method consistency (566ms)\n✔  checking replacement functions ...\n✔  checking foreign function calls ...\n✔  checking R code for possible problems (5.8s)\n✔  checking Rd files (516ms)\n✔  checking Rd metadata ...\n✔  checking Rd line widths ...\n✔  checking Rd cross-references ...\n✔  checking for missing documentation entries ...\n✔  checking for code/documentation mismatches (1s)\n✔  checking Rd \\usage sections (855ms)\n✔  checking Rd contents ...\n✔  checking for unstated dependencies in examples ...\n✔  checking examples (3.1s)\n✔  checking for unstated dependencies in ‘tests’ ...\n─  checking tests (480ms)\n✔  Running ‘testthat.R’ (1.9s)\n✔  checking for non-standard things in the check directory\n✔  checking for detritus in the temp directory\n\n── R CMD check results ──────────────────────────────────────────────────────────────────────────────────── mytools 0.0.0.9000 ────\nDuration: 27.3s\n\n0 errors ✔ | 0 warnings ✔ | 0 notes ✔\nWow, that was a lot! Basically this check() first helpfully completed a few documentation steps we had forgotten to do ourselves: creating the R documentation for fahr_to_celsius, and exporting the function in our NAMESPACE file. It then attempts to build a package from the source code, and runs a slew of checks to make sure everything looks good.\nNext you can install the package locally using devtools::install(), which needs to be run from the parent directory of your module\n\ndevtools::install()\n\n── R CMD build ───────────────────────────────────────────────────────────────────────────────────\n✔  checking for file ‘/home/ruser/mytools/DESCRIPTION’ (557ms)\n─  preparing ‘mytools’:\n✔  checking DESCRIPTION meta-information\n─  checking for LF line-endings in source and make files and shell scripts (621ms)\n─  checking for empty or unneeded directories\n─  building ‘mytools_0.0.0.9000.tar.gz’\n\nRunning /opt/R/4.4.2/lib/R/bin/R CMD INSTALL /tmp/RtmpTM1qoX/mytools_0.0.0.9000.tar.gz \\\n  --install-tests\n* installing to library ‘/home/ruser/R/x86_64-pc-linux-gnu-library/4.4’\n* installing *source* package ‘mytools’ ...\n** using staged installation\n** R\n** tests\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (mytools)\nAfter installing, your package is now available for use in your local environment, and you should be able to either use mytools::fahr_to_celsius(c(32, 212, 72)) or import your package and use your functions directly.\n\n\n\n\n\n\nCheck out the Build Tab\n\n\n\nRemember when we ran usethis::create_package() and after we ran it we saw the Build Tab added to the Environment pane?\nIn the Build Tab, each of the buttons correspond with one of the devtools functions we ran, meaning:\n\nTest button is equivalent to running devtools::test() in the Console\nCheck button is equivalent to running devtools::check() in the Console\nInstall button is equivalent to running devtools::install() in the Console\n\n\n\n\n\n13.2.7 Sharing and Releasing\n\nGitHub: The simplest way to share your package with others is to upload it to a public GitHub repository. Others would be able clone and build the package locally. Or even more conveniently, they could use the devtools::install_github() function. For example, if you pushed your ‘mytools’ package to GitHub under your hypothetical GitHub account with username ‘ruser’, then anyone would be able to install it by calling devtools::install_github(\"ruser/mytools\").\nCRAN: If your package might be broadly useful, also consider releasing it to CRAN, using the release() method from devtools(). Releasing a package to CRAN requires a significant amount of work to ensure it follows the standards set by the R community, but it is entirely tractable and a valuable contribution to the science community. If you are considering releasing a package more broadly, you may find that the supportive community at ROpenSci provides incredible help and valuable feeback through their onboarding process.\nR-Universe: A newer approach is to link your package release to R-Universe, which is an effective way to make it easy to test and maintain packages so that many people can install them using the familiar install.packages() function in R. In R-Universe, people and organizations can create their own universe of packages, which represent a collection of packages that appear as a CRAN-compatible repository in R. For example, for DataONE we maintain the DataONE R-Universe, which lists the packages we actively maintain as an organization. So, any R-user that wants to install these packages can do so by adding our universe to their list of repositories, and then installing packages as normal. For example, to install the codyn package, one could use:\n\n\ninstall.packages('codyn',\n    repos = c('https://dataoneorg.r-universe.dev', 'https://cloud.r-project.org'))\n\n\n\n13.2.8 Exercise: Add More Functions\nAdd additional temperature conversion functions to the mytools package and:\n\nAdd full documentation for each function\nWrite tests to ensure the functions work properly\nRebuild the package using document(), check(), and install()\n\n\n\n\n\n\n\nDon’t forget to update the version number before you install!\n\n\n\nVersion information is located in the DESCRIPTION file and when you first create a package the version is 0.0.0.9000.\nThis version number follows the format major.minor.patch.dev. The different parts of the version represent different things:\n\nMajor: A significant change to the package that would be expected to break users code. This is updated very rarely when the package has been redesigned in some way.\nMinor: A minor version update means that new functionality has been added to the package. It might be new functions to improvements to existing functions that are compatible with most existing code.\nPatch: Patch updates are bug fixes. They solve existing issues but don’t do anything new.\nDev: Dev versions are used during development and this part is missing from release versions. For example you might use a dev version when you give someone a beta version to test. A package with a dev version can be expected to change rapidly or have undiscovered issues.\n\nAfter you’ve made some changes to a package, but before you install run the code:\n\nusethis::use_version()\n\nCurrent version is 0.0.0.9000.\nWhat should the new version be? (0 to exit)\n\n1: major --&gt; 1.0.0\n2: minor --&gt; 0.1.0\n3: patch --&gt; 0.0.1\n4:   dev --&gt; 0.0.0.9001\nSince we’re adding new functions, we can consider this a minor change and can select option 2.\nSelection: 2\n✔ Setting Version field in DESCRIPTION to '0.1.0'\nSource: COMBINE’s R package workshop, Ch 9: Versioning",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Writing Packages</span>"
    ]
  },
  {
    "objectID": "session_13.html#additional-resources",
    "href": "session_13.html#additional-resources",
    "title": "13  Writing Packages",
    "section": "13.3 Additional Resources",
    "text": "13.3 Additional Resources\n\nHadley Wickham and Jenny Bryan’s awesome book: R Packages\nROpenSci Blog Post: How to create your personal CRAN-like repository on R-universe\nKarl Broman’s: R package primer: a minimal tutorial on writing R packages\nThomas Westlake’s Short Tutorial: Writing an R package from scratch (his post is an updated version of Hilary Parker’s blog post)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Writing Packages</span>"
    ]
  },
  {
    "objectID": "session_14.html",
    "href": "session_14.html",
    "title": "14  Collaboration and Thinking Preferences",
    "section": "",
    "text": "15 Thinking preferences",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Collaboration and Thinking Preferences</span>"
    ]
  },
  {
    "objectID": "session_14.html#learning-objectives",
    "href": "session_14.html#learning-objectives",
    "title": "14  Collaboration and Thinking Preferences",
    "section": "15.1 Learning Objectives",
    "text": "15.1 Learning Objectives\nAn activity and discussion that will help participants identify and appreciate the diverse thinking styles within a group and understand how those differences can influence collaboration.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Collaboration and Thinking Preferences</span>"
    ]
  },
  {
    "objectID": "session_14.html#thinking-preferences-activity",
    "href": "session_14.html#thinking-preferences-activity",
    "title": "14  Collaboration and Thinking Preferences",
    "section": "15.2 Thinking Preferences Activity",
    "text": "15.2 Thinking Preferences Activity\n\n15.2.1 About the Whole Brain Thinking System\nEveryone thinks differently. The way individuals think guides the way they work, and the way groups of individuals think guides how teams work. Understanding thinking preferences facilitates effective collaboration and team work.\nThe Whole Brain Model, developed by Ned Herrmann, builds upon early conceptualizations of brain functioning. For example, the left and right hemispheres were thought to be associated with different types of information processing while our neocortex and limbic system would regulate different functions and behaviours.\n\nThe Herrmann Brain Dominance Instrument (HBDI) provides insight into dominant characteristics based on thinking preferences. There are four major thinking styles that reflect the left cerebral, left limbic, right cerebral and right limbic.\n\nAnalytical (Blue)\nPractical (Green)\nRelational (Red)\nExperimental (Yellow)\n\n\nThese four thinking styles are characterized by different traits. Those in the BLUE quadrant have a strong logical and rational side. They analyze information and may be technical in their approach to problems. They are interested in the ‘what’ of a situation. Those in the GREEN quadrant have a strong organizational and sequential side. They like to plan details and are methodical in their approach. They are interested in the ‘when’ of a situation. The RED quadrant includes those that are feelings-based in their apporach. They have strong interpersonal skills and are good communicators. They are interested in the ‘who’ of a situation. Those in the YELLOW quadrant are ideas people. They are imaginative, conceptual thinkers that explore outside the box. Yellows are interested in the ‘why’ of a situation.\n\n\n15.2.2 Pre-Breakout Activity (~10 mins):\nStep 1: - Read through the statements contained within this document and determine which descriptors resonate with you. Make a note of them. - You are working towards identifying your top 20. If you have more than 20, discard the descriptors that resonate the least. - Using the letter codes in the right hand column, count the number of descriptors that fall into the categories A B C and D.\nStep 2: Determine your dominant thinking style based on your counts - A: Analytical (Blue) - B: Practical (Green) - C: Relational (Red) - D: Experimental (Yellow)\nStep 3: Reflect and share out: What is your dominant thinking style? Are there any traits you don’t feel resonate with you that fall into your thinking style?\n\n\n15.2.3 Breakout Activity (~15 mins):\nScenario: Your interdisciplinary research team has been given discretionary funds to support a large-scale research project focused on identifying, monitoring, and mapping climate-driven permafrost thaw throughout the Arctic. The data should be useful to both Arctic researchers and local communities who are directly affected.\nInstructions: Come up with a detailed plan (from project conception to completion) for your team to tackle this project given the guidelines for your group listed below. Your group will be a mix of people with different thinking preferences.\n\nGroup 1: Focus on brainstorming unconventional ideas.\nGroup 2: Break down the problem into data, facts, and logical steps.\nGroup 3: Find realistic, actionable solutions that can be implemented immediately without thinking about the long-term.\nGroup 4: Look at the problem from a bigger picture or long-term strategy perspective, do not focus on short-term goals or obstacles.\n\nIn each room, the goal is for each member of the group to approach the problem from their thinking style’s perspective, but encourage them to listen to others’ perspectives and try to incorporate all viewpoints.\nAfter ~15 minutes we will reconvene and have each group share their findings and solutions. As each group presents, ask them to explain how their thinking preferences influenced their approach. Guiding questions below.\n\n\n15.2.4 Discussion\nAfter all groups have presented their project summaries, we will discuss the following questions:\n\nGive a brief explanation of your solution and how you worked through the problem as a team.\nHow did your different thinking preferences shape your group’s solution?\nDid you learn anything about working with others who have a different thinking style than you?\nIf you were missing someone with a certain thinking style in your group, how did this impact your thinking and decision making?\n\n\n\n15.2.5 Conclusion\nCollaboration is strengthened when we recognize other’s strengths and receive input from creative, analytical, practical, and conceptual thinkers.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Collaboration and Thinking Preferences</span>"
    ]
  },
  {
    "objectID": "session_14.html#developing-a-code-of-conduct",
    "href": "session_14.html#developing-a-code-of-conduct",
    "title": "14  Collaboration and Thinking Preferences",
    "section": "16.1 Developing a Code of Conduct",
    "text": "16.1 Developing a Code of Conduct\nWhether you are joining a lab group or establishing a new collaboration, articulating a set of shared agreements about how people in the group will treat each other will help create the conditions for successful collaboration. If agreements or a code of conduct do not yet exist, invite a conversation among all members to create them. Co-creation of a code of conduct will foster collaboration and engagement as a process in and of itself, and is important to ensure all voices heard such that your code of conduct represents the perspectives of your community. If a code of conduct already exists, and your community will be a long-acting collaboration, you might consider revising the code of conduct. Having your group ‘sign off’ on the code of conduct, whether revised or not, supports adoption of the principles.\n\n\n\n\n\n\nWhen developing a code of conduct, keep in mind:\n\n\n\n\nInvite a conversation among all members to create grounding guidelines\nCo-creation of a code of conduct will foster collaboration and engagement as a process in it self.\nIt is important to ensure all voices are heard\nThings to consider\n\nBehaviors you want to encourage\nBehaviors that will not be tolerated\n\n\n\n\nFor example, the Openscapes code of conduct includes\n\nBe respectful\nBe direct but professional\nBe inclusive\nUnderstand different perspectives\nAppreciate and Accommodate Our Similarities and Differences\nLead by Example\n\n\nUnderstand Different Perspectives  Our goal should not be to “win” every disagreement or argument. A more productive goal is to be open to ideas that make our own ideas better. Strive to be an example for inclusive thinking. “Winning” is when different perspectives make our work richer and stronger. (openscapes.org)\n\nBelow are other example codes of conduct:\n\nNCEAS Code of Conduct\nCarpentries Code of Conduct\nArctic Data Center Code of Conduct\nMozilla Community Participation Guidelines\nEcological Society of America Code of Conduct",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Collaboration and Thinking Preferences</span>"
    ]
  },
  {
    "objectID": "session_14.html#authorship-and-credit-policies",
    "href": "session_14.html#authorship-and-credit-policies",
    "title": "14  Collaboration and Thinking Preferences",
    "section": "16.2 Authorship and Credit Policies",
    "text": "16.2 Authorship and Credit Policies\n\nNavigating issues of intellectual property and credit can be a challenge, particularly for early career researchers. Open communication is critical to avoiding misunderstandings and conflicts. Talk to your coauthors and collaborators about authorship, credit, and data sharing early and often. This is particularly important when working with new collaborators and across lab groups or disciplines which may have divergent views on authorship and data sharing. If you feel uncomfortable talking about issues surrounding credit or intellectual property, seek the advice or assistance of a mentor to support you in having these important conversations.\nThe “Publication” section of the Ecological Society of America’s Code of Ethics is a useful starting point for discussions about co-authorship, as are the International Committee of Medical Journal Editors guidelines for authorship and contribution. You should also check guidelines published by the journal(s) to which you anticipate submitting your work.\nFor collaborative research projects, develop an authorship agreement for your group early in the project and refer to it for each product. This example authorship agreement from the Arctic Data Center provides a useful template. It builds from information contained within Weltzin et al (2006) and provides a rubric for inclusion of individuals as authors. Your collaborative team may not choose to adopt the agreement in the current form, however it will prompt thought and discussion in advance of developing a consensus. Some key questions to consider as you are working with your team to develop the agreement:\n\nWhat roles do we anticipate contributors will play? e.g., the NISO Contributor Roles Taxonomy (CRediT) identifies 14 distinct roles:\n\nConceptualization\nData curation\nFormal Analysis\nFunding acquisition\nInvestigation\nMethodology\nProject administration\nResources\nSoftware\nSupervision\nValidation\nVisualization\nWriting – original draft\nWriting – review & editing\n\nWhat are our criteria for authorship? (See the ICMJE guidelines for potential criteria)\nWill we extend the opportunity for authorship to all group members on every paper or product?\nDo we want to have an opt in or opt out policy? (In an opt out policy, all group members are considered authors from the outset and must request removal from the paper if they don’t want think they meet the criteria for authorship)\nWho has the authority to make decisions about authorship? Lead author? PI? Group?\nHow will we decide authorship order?\nIn what other ways will we acknowledge contributions and extend credit to collaborators?\nHow will we resolve conflicts if they arise?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Collaboration and Thinking Preferences</span>"
    ]
  },
  {
    "objectID": "session_14.html#data-sharing-and-reuse-policies",
    "href": "session_14.html#data-sharing-and-reuse-policies",
    "title": "14  Collaboration and Thinking Preferences",
    "section": "16.3 Data Sharing and Reuse Policies",
    "text": "16.3 Data Sharing and Reuse Policies\nAs with authorship agreements, it is valuable to establish a shared agreement around handling of data when embarking on collaborative projects. Data collected as part of a funded research activity will typically have been managed as part of the Data Management Plan (DMP) associated with that project. However, collaborative research brings together data from across research projects with different data management plans and can include publicly accessible data from repositories where no management plan is available. For these reasons, a discussion and agreement around the handling of data brought into and resulting from the collaboration is warranted and management of this new data may benefit from going through a data management planning process. Below we discuss example data agreements.\nThe example data policy template provided by the Arctic Data Center addresses three categories of data.\n\nIndividual data not in the public domain\nIndividual data with public access\nDerived data resulting from the project\n\nFor the first category, the agreement considers conditions under which those data may be used and permissions associated with use. It also addresses access and sharing. In the case of individual, publicly accessible data, the agreement stipulates that the team will abide by the attribution and usage policies that the data were published under, noting how those requirements we met. In the case of derived data, the agreement reads similar to a DMP with consideration of making the data public; management, documentation and archiving; pre-publication sharing; and public sharing and attribution. As research data objects receive a persistent identifier (PID), often a DOI, there are citable objects and consideration should be given to authorship of data, as with articles.\nThe following example lab policy from the Wolkovich Lab combines data management practices with authorship guidelines and data sharing agreements. It provides a lot of detail about how this lab approaches data use, attribution and authorship.\nBelow an example from Wolkovich’s Lab Data Management Policies, Section 6: Co-authorship & data.\n\n\n\nData Management Policies for the Wolkovich Lab - Preview\nThis policy is communicated with all incoming lab members, from undergraduate to postdocs and visiting scholars, and is shared here with permission from Dr Elizabeth Wolkovich.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Collaboration and Thinking Preferences</span>"
    ]
  },
  {
    "objectID": "session_14.html#slide-deck",
    "href": "session_14.html#slide-deck",
    "title": "14  Collaboration and Thinking Preferences",
    "section": "16.4 Slide Deck",
    "text": "16.4 Slide Deck\n&lt;iframe width=“560” height=“380”src=“https://docs.google.com/presentation/d/1LnrUxmfrZsDuFEnZ49PfNY_gXrWca96dfSJNeh62ymc/preview&gt;",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Collaboration and Thinking Preferences</span>"
    ]
  },
  {
    "objectID": "session_15.html",
    "href": "session_15.html",
    "title": "15  FAIR/CARE and Ethical Data Collection",
    "section": "",
    "text": "15.1 The FAIR and CARE Principles\nThe idea behind these principles is to increase access and usage of complex and large datasets for innovation, discovery, and decision-making. This means making data available to machines, researchers, Indigenous communities, policy makers, and more.\nWith the need to improve the infrastructure supporting the reuse of data, a group of diverse stakeholders from academia, funding agencies, publishers and industry came together to jointly endorse measurable guidelines that enhance the reusability of data (Wilkinson et al. (2016)). These guidelines became what we now know as the FAIR Data Principles.\nFollowing the discussion about FAIR and incorporating activities and feedback from the Indigenous Data Sovereignty network, the Global Indigenous Data Alliance developed the CARE principles (Carroll et al. (2021)). The CARE principles for Indigenous Data Governance complement the more data-centric approach of the FAIR principles, introducing social responsibility to open data management practices.\nTogether, these two principle encourage us to push open and other data movements to consider both people and purpose in their advocacy and pursuits. The goal is that researchers, stewards, and any users of data will be FAIR and CARE (Carroll et al. (2020)).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>FAIR/CARE and Ethical Data Collection</span>"
    ]
  },
  {
    "objectID": "session_15.html#the-fair-and-care-principles",
    "href": "session_15.html#the-fair-and-care-principles",
    "title": "15  FAIR/CARE and Ethical Data Collection",
    "section": "",
    "text": "Source: Global Indigenous Data Alliance\n\n\n\n\n\n\n\n15.1.1 What is FAIR?\nWith the rise of open science and more accessible data, it is becoming increasingly important to address accessibility and openness in multiple ways. The FAIR principles focuses on how to prepare your data so that it can be reused by others (versus just open access of research outputs). In 2016, the data stewardship community published principles surrounding best practices for open data management, including FAIR. FAIR stands for Findable, Accessible, Interoperable, and Reproducible. It is best to think about FAIR as a set of comprehensive standards for you to use while curating your data. And each principle of FAIR can be translated into a set of actions you can take during the entire lifecycle of research data management.\n\n\n\n\n\n\n\nSource: Fair Teaching Handbook\n\n\n\n\n\n\n\n\n\nFAIR\nDefinition\n\n\n\n\n(F) Findable\nMetadata and data should be easy to find for both humans and computers.\n\n\n(A) Accessible\nOnce someone finds the required data, they need to know how the data can be accessed.\n\n\n(I) Interoperable\nThe data needs to be easily integrated with other data for analysis, storage, and processing.\n\n\n(R) Reusable\nData should be well-described so they can be reused and replicated in different settings.\n\n\n\n\n\n15.1.2 FAIR Principles in Practice\nThis is not an exhaustive list of actions for applying FAIR Principles to your research, but these are important big picture concepts you should always keep in mind. We’ll be going through the resources linked below so that you know how to use them in your own work.\n\nIt’s all about the metadata. To make your data and research as findable and as accessible as possible, it’s crucial that you are providing rich metadata. This includes, using a field-specific metadata standard (i.e. EML or Ecological Metadata Language for earth and environmental sciences), adding a globally unique identifier (i.e. a Digital Object Identifier) to your datasets, and more. As discussed earlier, quality metadata goes a long way in making your data FAIR. One tool to help implement FAIR principles to non-FAIR data is the FAIRification process. This workflow was developed by GoFAIR, a self-governed initiative that aims to help implement the FAIR data principles.\nAssess the FAIRness of your research. The FAIR Principles are a lens to apply to your work. And it’s important to ask yourself questions about finding and accessing your data, about how machine-readable your datasets and metadata are, and how reusable it is throughout the entirety of your project. This means you should be re-evaluating the FAIRness of your work over and over again. One way to check the FAIRness of your work, is to use tools like FAIR-Aware and the FAIR Data Maturity Model. These tools are self-assessments and can be thought of as a checklists for FAIR and will provide guidance if you’re missing anything.\nMake FAIR decisions during the planning process. You can ensure FAIR Principles are going to implemented in your work by thinking about it and making FAIR decisions early on and throughout the data life cycle. As you document your data always keep in mind the FAIR lense.\n\n\n\n15.1.3 What is CARE?\nThe CARE Principles for Indigenous Data Governance were developed by the International Indigenous Data Sovereignty Interest Group in consultation with Indigenous Peoples, scholars, non-profit organizations, and governments (Carroll et al. (2020)). They address concerns related to the people and purpose of data. It advocates for greater Indigenous control and oversight in order to share data on Indigenous Peoples’ terms. These principles are people and purpose-oriented, reflecting the crucial role data have in advancing Indigenous innovation and self-determination. CARE stands for Collective benefits, Authority control, Responsibility and Ethics. It details that the use of Indigenous data should result in tangible benefits for Indigenous collectives through inclusive development and innovation, improved governance and citizen engagement, and result in equitable outcomes.\n\n\n\n\n\n\n\nSource: Carroll, S.R., et al, 2020. The CARE Principles for Indigenous Data Governance\n\n\n\n\n\n\n\n\n\nCARE\nDefinition\n\n\n\n\n(C) Collective Benefit\nData ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data.\n\n\n(A) Authority to Control\nIndigenous Peoples’ rights and interests in Indigenous data must be recognized and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledge and geographical indicators, are represented and identified within data.\n\n\n(R) Responsibility\nThose working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples.\n\n\n(E) Ethics\nIndigenous Peoples’ rights and well being should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n\n\n\n\n\n15.1.4 CARE Principles in Practice\n\nMake your data access to Indigenous groups. Much of the CARE Principles are about sharing and making data accessible to Indigenous Peoples. To do so, consider publish your data on Indigenous founded data repositories such as:\n\nCollaborative Indigenous Research Digital Garden (CIRDG)\nMukurtu Wumpurrarni-kari Archive\n\nUse Traditional Knowledge (TK) and Biocultural (BC) Labels How do we think of intellectual property for Traditional and Biocultural Knowledge? Knowledge that outdates any intellectual property system. In many cases institution, organizations, outsiders hold the copy rights of this knowledge and data that comes from their lands, territories, waters and traditions. Traditional Knowledge and Biocultural Labels are digital tags that establish Indigenous cultural authority and governance over Indigenous data and collections by adding provenance information and contextual metadata (including community names), protocols, and permissions for access, use, and circulation. This way mark cultural authority so is recorded in a way that recognizes the inherent sovereignty that Indigenous communities have over knowledge. Giving Indigenous groups more control over their cultural material and guide users what an appropriate behavior looks like. A global initiative that support Indigenous communities with tools that attribute their cultural heritage is Local Contexts.\nAssess the CAREness of your research. Like FAIR, CARE Principles are a lens to apply to your work. With CARE, it’s important to center human well-being in addition to open science and data sharing. To do this, reflect on how you’re giving access to Indigenous groups, on who your data impacts and the relationships you have with them, and the ethical concerns in your work. The Arctic Data Center, a data repository for Arctic research, now requires an Ethical Research Practices Statement when submitting data to them. They also have multiple guidelines on how to write and what to include in an Ethical Research Practices Statement.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>FAIR/CARE and Ethical Data Collection</span>"
    ]
  },
  {
    "objectID": "session_15.html#research-data-publishing-ethics",
    "href": "session_15.html#research-data-publishing-ethics",
    "title": "15  FAIR/CARE and Ethical Data Collection",
    "section": "15.2 Research Data Publishing Ethics",
    "text": "15.2 Research Data Publishing Ethics\nFor over 20 years, the Committee on Publication Ethics (COPE) has provided trusted guidance on ethical practices for scholarly publishing. The COPE guidelines have been broadly adopted by academic publishers across disciplines, and represent a common approach to identify, classify, and adjudicate potential breaches of ethics in publication such as authorship conflicts, peer review manipulation, and falsified findings, among many other areas. Despite these guidelines, there has been a lack of ethics standards, guidelines, or recommendations for data publications, even while some groups have begun to evaluate and act upon reported issues in data publication ethics.\n\n\n\nData retractions\n\n\nTo address this gap, the Force 11 Working Group on Research Data Publishing Ethics was formed as a collaboration among research data professionals and the Committee on Publication Ethics (COPE) “to develop industry-leading guidance and recommended best practices to support repositories, journal publishers, and institutions in handling the ethical responsibilities associated with publishing research data.” The group released the “Joint FORCE11 & COPE Research Data Publishing Ethics Working Group Recommendations” (Puebla, Lowenberg, and WG 2021), which outlines recommendations for four categories of potential data ethics issues:\n\nAuthorship and Contribution Conflicts\n\nAuthorship omissions\nAuthorship ordering changes / conflicts\nInstitutional investigation of author finds misconduct\n\nLegal/regulatory restrictions\n\nCopyright violation\nInsufficient rights for deposit\nBreaches of national privacy laws (GPDR, CCPA)\nBreaches of biosafety and biosecurity protocols\nBreaches of contract law governing data redistribution\n\nRisks of publication or release\n\nRisks to human subjects\n\nLack of consent\nBreaches of himan rights\nRelease of personally identifiable information (PII)\n\nRisks to species, ecosystems, historical sites\n\nLocations of endangered species or historical sites\n\nRisks to communities or societies\n\nData harvested for profit or surveillance\nBreaches of data sovereignty\n\n\nRigor of published data\n\nUnintentional errors in collection, calculation, display\nUn-interpretable data due to lack of adequate documentation\nErrors of of study design and inference\nData manipulation or fabrication\n\n\nGuidelines cover what actions need to be taken, depending on whether the data are already published or not, as well as who should be involved in decisions, who should be notified of actions, and when the public should be notified. The group has also published templates for use by publishers and repositories to announce the extent to which they plan to conform to the data ethics guidelines.\n\n\n\nForce11/COPE",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>FAIR/CARE and Ethical Data Collection</span>"
    ]
  },
  {
    "objectID": "session_15.html#exercise-evaluate-a-data-package-on-the-edi-repository",
    "href": "session_15.html#exercise-evaluate-a-data-package-on-the-edi-repository",
    "title": "15  FAIR/CARE and Ethical Data Collection",
    "section": "15.3 Exercise: Evaluate a Data Package on the EDI Repository",
    "text": "15.3 Exercise: Evaluate a Data Package on the EDI Repository\nExplore data packages published on EDI assess the quality of their metadata. Imagine you’re a data curator!\n\n\n\n\n\n\nSetup\n\n\n\n\nBreak into groups and use the following data packages:\n\nGroup A: EDI Data Portal SBC LTER: Reef: Abundance, size and fishing effort for California Spiny Lobster (Panulirus interruptus), ongoing since 2012\nGroup B: EDI Data Portal Physiological stress of American pika (Ochotona princeps) and associated habitat characteristics for Niwot Ridge, 2018 - 2019\nGroup C: EDI Data Portal Ecological and social interactions in urban parks: bird surveys in local parks in the central Arizona-Phoenix metropolitan area\n\n\n\n\nYou and your group will evaluate a data package for its: (1) metadata quality, (2) data documentation quality for reproducibility, and (3) FAIRness and CAREness.\n\n\n\n\n\n\nExercise: Evaluate a data package on EDI Data Portal\n\n\n\n\nView our Data Package Assessment Rubric and make a copy of it to:\n\nInvestigate the metadata in the provided data\n\nDoes the metadata meet the standards we talked about? How so?\nIf not, how would you improve the metadata based on the standards we talked about?\n\nInvestigate the overall data documentation in the data package\n\nIs the documentation sufficient enough for reproducibility? Why or why not?\nIf not, how would you improve the data documentation? What’s missing?\n\nIdentify elements of FAIR and CARE\n\nIs it clear that the data package used a FAIR and CARE lens?\nIf not, what documentation or considerations would you add?\n\n\nElect someone to share back to the group the following:\n\nHow easy or challenging was it to find the metadata and other data documentation you were evaluating? Why or why not?\nWhat documentation stood out to you? What did you like or not like about it?\nHow well did these data packages uphold FAIR and CARE Principles?\nDo you feel like you understand the research project enough to use the data yourself (aka reproducibility?)\n\n\nIf your group finishes early, check out more datasets in the bonus question.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>FAIR/CARE and Ethical Data Collection</span>"
    ]
  },
  {
    "objectID": "session_15.html#bonus-investigate-metadata-and-data-documentation-in-other-data-repositories",
    "href": "session_15.html#bonus-investigate-metadata-and-data-documentation-in-other-data-repositories",
    "title": "15  FAIR/CARE and Ethical Data Collection",
    "section": "15.4 Bonus: Investigate metadata and data documentation in other Data Repositories",
    "text": "15.4 Bonus: Investigate metadata and data documentation in other Data Repositories\nNot all environmental data repositories document and publish datasets and data packages in the same way. Nor do they have the same submission requirements. It’s helpful to become familiar with metadata and data documentation jargon so it’s easier to identify the information you’re looking for. It’s also helpful for when you’re nearing the end of your project and are getting ready to publish your datasets.\nEvaluate the following data packages at these data repositories:\n\nKNB Arthropod pitfall trap biomass captured (weekly) and pitfall biomass model predictions (daily) near Toolik Field Station, Alaska, summers 2012-2016\nDataOne USDA-NOAA NWS Daily Climatological Data\nArctic Data Center Landscape evolution and adapting to change in ice-rich permafrost systems 2021-2022\n\nHow different are these data repositories from the EDI Data Portal? Would you consider publishing you data at one or multiple of these repositories?\n :::",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>FAIR/CARE and Ethical Data Collection</span>"
    ]
  },
  {
    "objectID": "session_15.html#resources",
    "href": "session_15.html#resources",
    "title": "15  FAIR/CARE and Ethical Data Collection",
    "section": "15.5 Resources",
    "text": "15.5 Resources\n\nCheruvelil, K. S., Soranno, P. A., Weathers, K. C., Hanson, P. C., Goring, S. J., Filstrup, C. T., & Read, E. K. (2014). Creating and maintaining high-performing collaborative research teams: The importance of diversity and interpersonal skills. Frontiers in Ecology and the Environment, 12(1), 31-38. DOI: 10.1890/130001 \n\n\n\n\n\nCarroll, Stephanie Russo, Ibrahim Garba, Oscar L. Figueroa-Rodríguez, Jarita Holbrook, Raymond Lovett, Simeon Materechera, Mark Parsons, et al. 2020. “The CARE Principles for Indigenous Data Governance.” Data Science Journal 19 (1): 43. https://doi.org/10.5334/dsj-2020-043.\n\n\nCarroll, Stephanie Russo, Edit Herczog, Maui Hudson, Keith Russell, and Shelley Stall. 2021. “Operationalizing the CARE and FAIR Principles for Indigenous Data Futures.” Scientific Data 8 (1): 108. https://doi.org/10.1038/s41597-021-00892-0.\n\n\nPuebla, Iratxe, Daniella Lowenberg, and FORCE11 Research Data Publishing Ethics WG. 2021. “Joint FORCE11 & COPE Research Data Publishing Ethics Working Group Recommendations.” Zenodo. https://doi.org/10.5281/zenodo.5391293.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>FAIR/CARE and Ethical Data Collection</span>"
    ]
  },
  {
    "objectID": "session_16.html",
    "href": "session_16.html",
    "title": "16  R Practice II",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>R Practice II</span>"
    ]
  },
  {
    "objectID": "session_16.html#learning-objectives",
    "href": "session_16.html#learning-objectives",
    "title": "16  R Practice II",
    "section": "",
    "text": "Integrate knowledge on writing functions in R\nUnderstand how functions can be used when cleaning data",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>R Practice II</span>"
    ]
  },
  {
    "objectID": "session_16.html#about-the-data",
    "href": "session_16.html#about-the-data",
    "title": "16  R Practice II",
    "section": "About the data",
    "text": "About the data\nFor this practice session we will use data on shorebird breeding ecology collected in Utqiaġvik, Alaska, 2003-2018 by Richard Lanctot and Sarah Saalfeld. This data is publicly available at the Arctic Data Center.\n\nRichard Lanctot and Sarah Saalfeld. 2019. Utqiaġvik shorebird breeding ecology study, Utqiaġvik, Alaska, 2003-2018. Arctic Data Center. doi:10.18739/A23R0PT35\n\nOne of the features if this dataset is that it has many files with similar formatting, most of which contain the column species which is comprised of the Bird Banding Laboratory species codes. These four letter codes aren’t very intuitive to most people, so the main goal for this session is to write a function that can be used on any file in this dataset that contains a species code.\n\n\n\n\n\n\nSetup\n\n\n\n\nMake sure you’re in the right project (training_{USERNAME}) and use the Git workflow by Pulling to check for any changes in the remote repository (aka repository on GitHub).\nCreate a new Quarto Document.\n\nTitle it “R Practice: Functions”.\nSave the file and name it “r-practice-functions”.\n\nInsert a Setup r chunck and load the necessary libraries. Note here we introduce a new package called rvest. This package enables easy scraping and handling of information from websites.\n\n\nlibrary(rvest)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\n\n\nLoad the species table using the following code. This code scrapes a table from a url and uses some cleaning and wrangling functions to get the table into our Environment in the format we want.\n\n\nwebpage &lt;- rvest::read_html(\"https://www.pwrc.usgs.gov/BBL/Bander_Portal/login/speclist.php\")\n\ntbls &lt;- rvest::html_nodes(webpage, \"table\") %&gt;% \n    rvest::html_table(fill = TRUE)\n\nspecies &lt;- tbls[[1]] %&gt;% \n    janitor::clean_names() %&gt;% \n    select(alpha_code, common_name) %&gt;% \n    mutate(alpha_code = tolower(alpha_code))\n\nhead(species, 3)\n\nNote: After running the chunk above you should have an object class data.frame in your global environment. Explore this data frame! What are the column names?\n\nObtain data from the the Arctic Data Center Utqiaġvik shorebird breeding ecology study, Utqiaġvik. Make sure to click “Show more items in this data set” and then download the following files:\n\nUtqiagvik_predator_surveys.csv\nUtqiagvik_nest_data.csv\nUtqiagvik_egg_measurements.csv\n\n\nNote: It’s up to you on how you want to download and load the data! You can either use the download links (obtain by right-clicking the “Download” button and select “Copy Link Address” for each data entity) or manually download the data and then upload the files to RStudio server.\n\nUse the Git workflow. After you’ve set up your project and uploaded your data go through the workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\n\n\n\n\n\n\n\n\n\nA note on rvest\n\n\n\nThis is a handy package that requires a moderate amount of knowledge of html to use. We used it here because we don’t have a plain text version of the BBL species codes. Ideally, to build a reproducible and long lived workflow, we would want to run this code and then store a plain text version of the data in a long lived location, which cites the original source appropriately.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>R Practice II</span>"
    ]
  },
  {
    "objectID": "session_16.html#exercise",
    "href": "session_16.html#exercise",
    "title": "16  R Practice II",
    "section": "Exercise",
    "text": "Exercise",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>R Practice II</span>"
    ]
  },
  {
    "objectID": "session_16.html#question-1",
    "href": "session_16.html#question-1",
    "title": "16  R Practice II",
    "section": "16.1 Question 1",
    "text": "16.1 Question 1\n\n\n\n\n\n\nRead and explore data\n\n\n\nRead in each data file and store the data frame as nest_data, predator_survey, and egg_measures accordingly. After reading the data, insert a new chunk or in the console, explore the data using any function we have used during the lessons (eg. colname(), glimpse())\n\n\n\n\nAnswer\n## When reading from a file in your data folder in your Rpoj\nnest_data &lt;-  read_csv(\"data/Utqiagvik_nest_data.csv\")\n\npredator_survey &lt;- read_csv(\"data/  Utqiagvik_predator_surveys.csv\")\n\negg_measures &lt;- read_csv(\"data/Utqiagvik_egg_measurements.csv\")\n\n\n## When reading using the url\nnest_data &lt;-  read_csv(\"https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A982bd2fc-4edf-4da7-96ef-0d11b853102d\")\n\npredator_survey &lt;- read_csv(\"https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A9ffec04c-7e2d-41dd-9e88-b6c2e8c4375e\")\n\negg_measures &lt;- read_csv(\"https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A4b219711-2282-420a-b1d6-1893fe4a74a6\")\n\n## Exploring the data (these functions can also be used to explore nest_data & egg_measures) \n\ncolnames(predator_survey)\nglimpse(predator_survey)\nunique(predator_survey$species)\nsummary(predator_survey)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>R Practice II</span>"
    ]
  },
  {
    "objectID": "session_16.html#question-2",
    "href": "session_16.html#question-2",
    "title": "16  R Practice II",
    "section": "16.2 Question 2",
    "text": "16.2 Question 2\n\n\n\n\n\n\nHow would you translate species codes into common names for one of the data frames?\n\n\n\nBefore thinking of how to write a function, first discuss what are you trying to achieve and how would you get there. Write and run the code that would allow you to combine the species data frame with the predator_survey so that the outcome data frame has the species code and common names.\nHint: joins\n\n\n\n\nAnswer\npredator_comm_names &lt;- left_join(predator_survey,\n                                 species,\n                                 by = c(\"species\" = \"alpha_code\"))",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>R Practice II</span>"
    ]
  },
  {
    "objectID": "session_16.html#question-3",
    "href": "session_16.html#question-3",
    "title": "16  R Practice II",
    "section": "16.3 Question 3",
    "text": "16.3 Question 3\n\n\n\n\n\n\nWrite a functions to add species common name to any data frame.\n\n\n\nHow can you generalize the code from the previous question and make it into a function?\nThe idea is that you can use this function in any data frame that has a column named species with the Bird Banding Laboratory Species Code.\n\n\n\n\nAnswer\nassign_species_name &lt;- function(df, species){\n    return_df &lt;- left_join(df, species, by = c(\"species\" = \"alpha_code\"))\n    return(return_df)\n}",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>R Practice II</span>"
    ]
  },
  {
    "objectID": "session_16.html#question-4",
    "href": "session_16.html#question-4",
    "title": "16  R Practice II",
    "section": "16.4 Question 4",
    "text": "16.4 Question 4\n\n\n\n\n\n\nDocument your funtion inserting Roxygen skeleton and adding the necesary description.\n\n\n\nPlace the cursor inside your function, In the top menu go to Code &gt; Insert Roxygen skeleton. Document parameters, return and write one example.\n\n\n\n\nAnswer\n#' Title\n#'\n#' @param df A data frame containing BBL species codes in column `species`\n#' @param species A data frame defining BBL species codes with columns `alpha_code` and `common_name` \n#'\n#' @return A data frame with original data df, plus the common name of species\n#' @export\n#'\n#' @examples `*provide an example*`\n\n\nassign_species_name &lt;- function(df, species){\n    return_df &lt;- left_join(df, species, by = c(\"species\" = \"alpha_code\"))\n    return(return_df)\n}",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>R Practice II</span>"
    ]
  },
  {
    "objectID": "session_16.html#question-5",
    "href": "session_16.html#question-5",
    "title": "16  R Practice II",
    "section": "16.5 Question 5",
    "text": "16.5 Question 5\n\n\n\n\n\n\nUse your function to clean names of each data frame\n\n\n\nCreate clean versions of the three data frames by applying the function you created and removing columns that you think are note necessary(aka selecting the ones you want to keep) and filter out NA values.\n\n\n\n\nAnswer\n## This is one solution. \npredator_clean &lt;- assign_species_name(predator_survey, species) %&gt;% \n    select(year, site, date, common_name, count) %&gt;% \n    filter(!is.na(common_name))\n\nnest_location_clean &lt;- assign_species_name(nest_data, species) %&gt;% \n    select(year, site, nestID, common_name, lat_corrected, long_corrected) %&gt;% \n    filter(!is.na(common_name))\n\neggs_clean &lt;- assign_species_name(egg_measures, species) %&gt;% \n    select(year, site, nestID, common_name, length, width) %&gt;% \n    filter(!is.na(common_name))\n\n\nCongrats! Now you have clean data sets ready for analysis.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>R Practice II</span>"
    ]
  },
  {
    "objectID": "session_16.html#optional-challenge",
    "href": "session_16.html#optional-challenge",
    "title": "16  R Practice II",
    "section": "16.6 Optional Challenge",
    "text": "16.6 Optional Challenge\n\n\n\n\n\n\nChallenge\n\n\n\nFor a little extra challenge, try to incorporate an if statement that looks for NA values in the common name field you are adding. What other conditionals might you include to make your function smarter?\n\n\n\n\nAnswer\n#' Function to add common name to data.frame according to the BBL list of species codes\n\n#' @param df A data frame containing BBL species codes in column `species`\n#' @param species A data frame defining BBL species codes with columns `alpha_code` and `common_name`\n#' @return A data frame with original data df, plus the common name of species\n\nassign_species_name &lt;- function(df, species){\n    if (!(\"alpha_code\" %in% names(species)) |\n        !(\"species\" %in% names(df)) |\n        !(\"common_name\" %in% names(species))){\n      stop(\"Tables appear to be formatted incorrectly.\")\n    }  \n  \n    return_df &lt;- left_join(df, species, by = c(\"species\" = \"alpha_code\"))\n    \n    if (nrow(return_df) &gt; nrow(df)){\n      warning(\"Joined table has more rows than original table. Check species table for duplicated code values.\")\n    }\n    \n    if (length(which(is.na(return_df$common_name))) &gt; 0){\n      x &lt;- length(which(is.na(return_df$common_name)))\n      warning(paste(\"Common name has\", x, \"rows containing NA\"))\n    }\n    \n    return(return_df)\n        \n}\n\n\n\n\n\n\n\n\nWhy do we need to use a function for this task?\n\n\n\nYou will likely at some point realize that the function we asked you to write is pretty simple. The code can in fact be accomplished in a single line. So why write your own function for this? There are a couple of answers. The first and most obvious is that we want to you practice writing function syntax with simple examples. But there are other reasons why this operation might benefit from a function:\n\nFollow the DRY principles!\n\nIf you find yourself doing the same cleaning steps on many of your data files, over and over again, those operations are good candidates for functions. This falls into that category, since we need to do the same transformation on all of the files we use here, and if we incorporated more files from the dataset it would come in even more use.\n\nAdd custom warnings and quality control.\n\nFunctions allow you to incorporate quality control through conditional statements coupled with warnings. Instead of checking for NA’s or duplicated rows after you run a join, you can check within the function and return a warning if any are found.\n\nCheck your function input more carefully\n\nSimilar to custom warnings, functions allow you to create custom errors too. Writing functions is a good way to incorporate defensive coding practices, where potential issues are looked for and the process is stopped if they are found.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>R Practice II</span>"
    ]
  },
  {
    "objectID": "session_17.html",
    "href": "session_17.html",
    "title": "17  Data Portals",
    "section": "",
    "text": "17.1 What is a Portal?\nData portals are a new feature on the Arctic Data Center. Researchers can now easily view project information and datasets all in one place.\nA portal is a collection of Arctic Data Center data packages on a unique webpage.\nTypically, a research project’s website won’t be maintained beyond the life of the project, and all the information on the website that provides context for the data collection is lost. Arctic Data Center portals can provide a means to preserve information regarding the projects’ objectives, scopes, and organization and couple this with the data files so it’s clear how to use and interpret the data for years to come.\nPortals also leverage Arctic Data Center’s metric features, which create statistics describing the project’s data packages. Information such as total size of data, proportion of data file types, and data collection periods are immediately available from the portal webpage.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Portals</span>"
    ]
  },
  {
    "objectID": "session_17.html#portal-uses",
    "href": "session_17.html#portal-uses",
    "title": "17  Data Portals",
    "section": "17.2 Portal Uses",
    "text": "17.2 Portal Uses\nPortals allow users to bundle supplementary information about their group, data, or project along with the data packages. Data contributors can organize their project-specific data packages into a unique portal and customize the portal’s theme and structure according to the needs of that project.\nResearchers can also use portals to compare their public data packages and highlight or share them with other teams, as well as the broader Arctic research audience.\nTo see an example of a portal, please view the Toolik Field Station’s portal.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Portals</span>"
    ]
  },
  {
    "objectID": "session_17.html#portal-features",
    "href": "session_17.html#portal-features",
    "title": "17  Data Portals",
    "section": "17.3 Portal Features",
    "text": "17.3 Portal Features\nPublished portals vary in their design according to the needs and preferences of the individual or group. However, when constructing a portal there are three core elements: a data page, a metrics page, and customizable free-form pages.\n\nFlexible Content Creation\nPortals can be constructed as a website providing information about the research and products. Using our flexible ‘free-form’ pages (written in markdown), you can add and re-order pages to meet your needs. These pages might be used as a home, or ‘landing’ page with general project information. They are also used to showcase research products, and communicate news and upcoming events.\n\n\n\n\nCurated Collections of Data\nThe data page is arguably the most important component of the Arctic Data Center portal system. This is where users will display the data packages of their choice. Whether these data reflect research products from your research group (collated based on contributor IDs), or thematic areas of research interest (collated based on keyword and search terms), will depend upon the intended use of the portal but in all cases, you are refining the suite of data viewed by your audience. The data page looks and performs just like the main Arctic Data Center catalog - with some added bonuses, see below.\n\n\n\nCustomized Search Capabilities\nYou can also build more precise search capabilities into your portal, leveraging the rich metadata associated with data products preserved at the Arctic Data Center. For example, in the example below the research group have identified seven primary search categories and within these, enable users to search within specific metadata fields or to select from a drop down list of options. In doing so, your audience can construct refined search queries drilling down to the data of interest. (Note that although the SASAP portal is hosted by DataONE, the same functionality exists at the Arctic Data Center. More on this later).\n\n\n\n\nMetrics, Metrics, Metrics\nAs with the full Arctic Data Center catalog, we aggregate metrics for the collection of data packages within a portal. This page is not customizable - it comes as a default with the portal - but you can choose to delete it. The metrics provided include a summary of the holdings: number, volume, time period, format of datasets, metadata assessment scores, citations across all packages, and counts of downloads and views. These latter metrics can be particularly useful if wanting to track the usage or reach of your project or group’s activities.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Portals</span>"
    ]
  },
  {
    "objectID": "session_17.html#enhancing-access-to-social-science-research-data",
    "href": "session_17.html#enhancing-access-to-social-science-research-data",
    "title": "17  Data Portals",
    "section": "17.4 Enhancing Access to Social Science Research Data",
    "text": "17.4 Enhancing Access to Social Science Research Data\nMany of the portal examples provided above are organizational, individual or project portals created by members of the Arctic research community. The ability to group relevant datasets and customize search criteria increases data discoverability and accessibility among target audiences. The Arctic Data Center has leveraged these features to create a specific portal for social science data.\n\nWithin this portal, users can subset by social science discipline, data type and other metadata fields built into the portal. These search features depend on sufficient user contributed metadata describing the dataset, and as can be seen from the ‘with data’ toggle, the portal does not require the data themselves to be uploaded to the Arctic Data Center.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Portals</span>"
    ]
  },
  {
    "objectID": "session_17.html#relationship-between-arctic-data-center-portals-and-dataone",
    "href": "session_17.html#relationship-between-arctic-data-center-portals-and-dataone",
    "title": "17  Data Portals",
    "section": "17.5 Relationship between Arctic Data Center portals and DataONE",
    "text": "17.5 Relationship between Arctic Data Center portals and DataONE\nBoth DataONE and the Arctic Data Center use Metacat and MetacatUI software and both have the capability for individuals and groups to develop portals. This is true of other repositories running this software. The difference between a portal at the Arctic Data Center and one through DataONE is the corpus of data that can be pulled into your portal. Arctic Data Center portals expose only data held in the Arctic Data Center repository. A portal in DataONE can expose data from across the full network of data repositories - including DataONE. This is particularly useful for interdisciplinary research projects, labs that have published data to multiple repositories etc. However, unlike at the Arctic Data Center, there is a cost associated with a DataONE portal as they are part of the organizations sustainability model.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Portals</span>"
    ]
  },
  {
    "objectID": "session_17.html#creating-portals",
    "href": "session_17.html#creating-portals",
    "title": "17  Data Portals",
    "section": "17.6 Creating Portals",
    "text": "17.6 Creating Portals\nA step-by-step guide on how to navigate the Arctic Data Center and create a new portal. For video tutorials on how to create your first portal, please visit the Arctic Data Center’s website.\n\nGetting Started with Portals\nIf you are on the Arctic Data Center’s primary website, select the button on the top right titled ‘Sign in with ORCID’. Sign in with your ORCID, which will then take you to the ADC data catalog.\n\nYou can then navigate to Data -&gt; Portals.\n\nYou’ll then see a list of ‘My Portals’. After the page loads, select the green button ‘+ New Portal’ to add a new portal. You’ll automatically be directed to a fresh edit session.\n\n\n\nPortal Settings Page\nIn a new edit session, the first page you’ll be taken to is the settings page where you’ll be able to add details about your portal.\n\n\nPortal URL\n\nIdentify a short name for your portal that will become part of the URL. If the name is available, a label will indicate it’s available and if the name is taken already, it will note that the name is already taken. This feature ensures the portals are unique.\n\nPortal description\nSharing options\n\nFor the purposes of this training, please leave your portal in ‘Private’ status. You are welcome to return and make the portal public when the portal is complete and is useful to you and others.\n\nPermissions\n\nAdding collaborators to help you create your portal is as straightforward as copying and pasting in their ORCID into the box below the permissions section. You can choose whether the collaborator can view, edit, or is an owner of the portal. You can have multiples of each role.\n\nPartner Logos\n\n\n\n\nAdding Data to Portals\nWhen selecting the data tab you will see a page with two sections. These are titled with instructive statements to help explain their function:\n\nAdd filters to help people find data within your collection\n\nbuild search features for others to use within your portal\n\nAdd data to your collection\n\nconstruct a search to populate your portal with data\n\n\n We’re going to start with the second section: Adding data to your collection.\nWhen adding data to your collection, you can include any of the datasets that are available at the Arctic Data Center. You build rules based on metadata to define which datasets should be included. Of course, where metadata is incomplete or abbreviated, this will impact your results. Data added to the network in the future that match these rules will also be added to your collection.\nThe first thing we notice is the ability to include/exclude data based on all/any metadata criteria. This setting applies across all rules.\nIn the default view, you have the starting point for a single rule, and a view showing 0 datasets.\n\nAs we build rules, the page will refresh to show how many datasets are included based on your rule structure. You can continue to add rules (and rule groups) to create complex queries that are specific to your needs.\n\nThe metadata fields available for rule construction are easily visible in the dropdown option, and grouped for ease of use. You also have the option to define ‘any metadata field’, though your results may be less precise.\n\n\n\nBuilding Search Options for Your Audience\nThis section covers the first part of the ‘data’ page: “Add filters to help people find data within your collection”. Although it appears first, I recommend constructing these filters after you have defined your data, as you will have a better understanding of the metadata fields that are relevant to your portal collection. It appears at the top of this editor page, as when published, it will be at the top for users. Hence, the editor page reflects the layout of the published page.\nWhen selecting “add a search filter” you will be presented with a pop-up that comprises three primary elements:\n\nThe metadata filed you will be querying\nThe way in which you want the user to interact with that metadata\nLanguage settings for the filter you are building\n\n\nAs you select options for number 1 - the metadata field, the pop-up will refresh to show only those relevant options. Save this filter to close the pop-up, return to the main editor and add another search filter.\n\n\n\nData Package Metrics\nAs stated above, the metrics page is a default function provided by the Arctic Data Center. This page cannot be edited and cannot be viewed while editing. Users do have the option to delete the page if they’d like. To delete the page, select the arrow next to the word ‘Metrics’ in the tab and choose ‘Delete’ from the dropdown list. You can always change your mind and add a metrics page with the ‘+’ tab.\nTo see metric summaries, navigate to your portal in view mode. See ‘Saving and Editing Portals’ for more information on how to view portals.\n\n\n\nCreating Unique Freeform Pages\nTo watch a tutorial on creating a new freeform page see this video:Creating a Freeform Text Page\nTo add a freeform page to a portal, select the ‘+’ tab next to the data and metric tabs and then choose the freeform option that appears on screen. A freeform page will then populate.\n\nEasily customize your banner with a unique image, title, and page description. To change the name of the tab, click on the arrow in the ‘Untitled’ tab and select ‘Rename’ from the dropdown list.\n\nBelow the banner, there is a markdown text box with some examples on how to use the markdown formatting directives to customize the text display. There is also a formatting header at the top to assist if you’re unfamiliar with markdown. As you write, toggle through the Edit and Preview modes in the markdown text box to make sure your information is displaying as intended. Portals are flexible and can accommodate as many additional freeform pages as needed.\nThe markdown header structure helps to generate the table of contents for the page.\nPlease see these additional resources for help with markdown:\n\nMarkdown reference\nTen minute tutorial\nFor a longer example where you can also preview the results, checkout the Showdown Live Editor\n\n\n\nSaving and Editing Portals\nBe sure to save your portal when you complete a page to ensure your progress is retained.\n\nWhenever a portal is saved, a dialogue box will pop up at the top of the page prompting users to view their private portal in view mode. You can choose to ignore this and continue editing.\n\nTo delete a page from your portal, select the arrow in the tab and choose ‘Delete’ from the dropdown.\n\nUsers can view and edit their portal from their ‘My Portals’ tab.\nFirst, click the arrow your name in the top-right corner to drop down your menu options. Then, select ‘My Portals’ from the dropdown underneath your name. See the section on Getting Started with Portals for more details.\n \nClick on the portal title to view it or select the edit button to make changes.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Portals</span>"
    ]
  },
  {
    "objectID": "session_17.html#how-to-publish-portals",
    "href": "session_17.html#how-to-publish-portals",
    "title": "17  Data Portals",
    "section": "17.7 How to Publish Portals",
    "text": "17.7 How to Publish Portals\nNew portals are automatically set to private and only visible to the portal creator. The portal will remain private until the owner decides to make it public.\nTo make your portal public, go into the settings of your portal. Under the description, you’ll see a new section called ‘Sharing Options’. You can toggle between your portal being private and your portal being public there.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Portals</span>"
    ]
  },
  {
    "objectID": "session_17.html#sharing-portals",
    "href": "session_17.html#sharing-portals",
    "title": "17  Data Portals",
    "section": "17.8 Sharing Portals",
    "text": "17.8 Sharing Portals\nIn order to share a published portal, users can simply direct recipients to their unique identifier. Portal identifiers are embedded into the Arctic Data Center’s portal URL: https://arcticdata.io/catalog/portals/portal-identifier\nTo view or edit your portal identifier, go into edit mode in your portal. In the settings page, your portal URL will be the first item on the page.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Portals</span>"
    ]
  },
  {
    "objectID": "session_17.html#tutorial-videos",
    "href": "session_17.html#tutorial-videos",
    "title": "17  Data Portals",
    "section": "17.9 Tutorial Videos",
    "text": "17.9 Tutorial Videos\nFor video tutorials on how to create your first portal, please visit the Arctic Data Center video tutorial page.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Portals</span>"
    ]
  },
  {
    "objectID": "session_17.html#acknowledgements",
    "href": "session_17.html#acknowledgements",
    "title": "17  Data Portals",
    "section": "17.10 Acknowledgements",
    "text": "17.10 Acknowledgements\nMuch of this documentation was composed by ESS-DIVE, which can be found here.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Portals</span>"
    ]
  },
  {
    "objectID": "session_18.html",
    "href": "session_18.html",
    "title": "18  Working with Spatial Data",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Working with Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_18.html#learning-objectives",
    "href": "session_18.html#learning-objectives",
    "title": "18  Working with Spatial Data",
    "section": "",
    "text": "How to use the sf package to wrangle spatial data\nStatic mapping with ggplot\nAdding basemaps to static maps\nInteractive mapping with leaflet",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Working with Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_18.html#brief-introduction-to-sf",
    "href": "session_18.html#brief-introduction-to-sf",
    "title": "18  Working with Spatial Data",
    "section": "18.1 Brief introduction to sf",
    "text": "18.1 Brief introduction to sf\nFrom the sf vignette:\n\nSimple features or simple feature access refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects. It also describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them.\n\nThe sf package is an R implementation of Simple Features. This package incorporates:\n\na new spatial data class system in R\n\nfunctions for reading and writing spatial data\n\ntools for spatial operations on vectors\n\nMost of the functions in this package starts with prefix st_ which stands for spatial and temporal.\nIn this lesson, our goal is to use a shapefile of Alaska regions and rivers, and data on population in Alaska by community to create a map that looks like this:",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Working with Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_18.html#about-the-data",
    "href": "session_18.html#about-the-data",
    "title": "18  Working with Spatial Data",
    "section": "18.2 About the data",
    "text": "18.2 About the data\nAll of the data used in this tutorial are simplified versions of real datasets available on the KNB Data Repository. We are using simplified datasets to ease the processing burden on all our computers since the original geospatial datasets are high-resolution. These simplified versions of the datasets may contain topological errors.\nThe spatial data we will be using to create the map are:\n\n\n\nData\nOriginal datasets\n\n\n\n\nAlaska regional boundaries\nJared Kibele and Jeanette Clark. 2018. State of Alaska’s Salmon and People Regional Boundaries. Knowledge Network for Biocomplexity. doi:10.5063/F1125QWP.\n\n\nCommunity locations and population\nJeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018. Languages used in Alaskan households, 1990-2015. Knowledge Network for Biocomplexity. doi:10.5063/F11G0JHX.\n\n\nAlaska rivers\nThe rivers shapefile is a simplified version of Jared Kibele and Jeanette Clark. Rivers of Alaska grouped by SASAP region, 2018. Knowledge Network for Biocomplexity. doi:10.5063/F1SJ1HVW.\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nNavigate to this dataset on KNB’s test site and download the zip folder.\nUpload the zip folder to the data folder in the training_{USERNAME} project. You don’t need to unzip the folder ahead of time, uploading will automatically unzip the folder.\n\nAlternatively, programmatically download and extract the demo data with: \n\n\n\nknb_url &lt;- \"https://dev.nceas.ucsb.edu/knb/d1/mn/v2/object/urn%3Auuid%3Aaceaecb2-1ce0-4d41-a839-d3607d32bb58\"\n\ndownload.file(url = knb_url, destfile = 'shapefile_demo_data.zip')\n\nunzip('shapefile_demo_data.zip', exdir = 'data')\n\nfile.remove('shapefile_demo_data.zip')\n\n\nCreate a new Quarto file.\n\nTitle it “Working with Spatial Data in R”\nSave the file and name it “intro-to-spatial-data”.\n\nLoad the following libraries at the top of your Quarto file.\n\n\nlibrary(readr)\nlibrary(here)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(leaflet)\nlibrary(scales)\nlibrary(ggspatial)\nlibrary(dplyr)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Working with Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_18.html#exploring-the-data-using-plot-and-st_crs",
    "href": "session_18.html#exploring-the-data-using-plot-and-st_crs",
    "title": "18  Working with Spatial Data",
    "section": "18.3 Exploring the data using plot() and st_crs()",
    "text": "18.3 Exploring the data using plot() and st_crs()\nFirst let’s read in the shapefile of regional boundaries in Alaska using read_sf() and then create a basic plot of the data plot(). Here we’re adding a _sf suffix to our object name, to remind us that this is a Simple Features object with spatial information.\n\n# read in shapefile using read_sf()\nak_rgns_sf &lt;- read_sf(here(\"data/ak_regions_simp.shp\"))\n\n\n# quick plot\nplot(ak_rgns_sf)\n\n\n\n\n\n\n\n\nWe can also examine its class using class().\n\nclass(ak_rgns_sf)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nsf objects usually have two types of classes: sf and data.frame.\nSince our shapefile object has the data.frame class, viewing the contents of the object using the head() function or other exploratory functions shows similar results as if we read in data using read.csv() or read_csv().\nBut, unlike a typical data.frame, an sf object has spatial metadata (geometry type, dimension, bbox, epsg (SRID), proj4string) and an additional column typically named geometry that contains the spatial data.\n\nhead(ak_rgns_sf)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.2296 ymin: 51.15702 xmax: 179.8567 ymax: 71.43957\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  region_id region           mgmt_area                                  geometry\n      &lt;int&gt; &lt;chr&gt;                &lt;dbl&gt;                        &lt;MULTIPOLYGON [°]&gt;\n1         1 Aleutian Islands         3 (((-171.1345 52.44974, -171.1686 52.4174…\n2         2 Arctic                   4 (((-139.9552 68.70597, -139.9893 68.7051…\n3         3 Bristol Bay              3 (((-159.8745 58.62778, -159.8654 58.6137…\n4         4 Chignik                  3 (((-155.8282 55.84638, -155.8049 55.8655…\n5         5 Copper River             2 (((-143.8874 59.93931, -143.9165 59.9403…\n6         6 Kodiak                   3 (((-151.9997 58.83077, -152.0358 58.8271…\n\nglimpse(ak_rgns_sf)\n\nRows: 13\nColumns: 4\n$ region_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n$ region    &lt;chr&gt; \"Aleutian Islands\", \"Arctic\", \"Bristol Bay\", \"Chignik\", \"Cop…\n$ mgmt_area &lt;dbl&gt; 3, 4, 3, 3, 2, 3, 4, 4, 2, 4, 2, 1, 4\n$ geometry  &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-171.1345 5..., MULTIPOLYGON (((-139.9552 6.…\n\n\n\n18.3.1 Coordinate Reference System (CRS)\n\n\n\nSource: ESRI\n\n\nEvery sf object needs a coordinate reference system (or crs) defined in order to work with it correctly. A coordinate reference system contains both a datum and a projection.\n\n\n\n\n\nThe datum is how you georeference your points (in 3 dimensions!) onto a spheroid, or the Earth. The Earth is not a perfect sphere and there are many ways to describe its shape. For example, is the Earth shaped like a lemon, lime, or orange? The shape, or datum, that you choose will depend on the scope of your project (for instance, local vs. global) and the specific locations.\nThe projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. Since you will visualize a 3D object onto a 2D space, there will be some distortions depending on the projection that you choose. Analogously, how do you peel the fruit (representing the Earth) and flatten the peel?\n\n\n\nSource: ESRI\n\n\nAll coordinate reference systems require a datum. However, some coordinate reference systems are “unprojected” (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984.\nESRI has a blog post that explains these concepts in more detail with very helpful diagrams and examples.\nYou can view what crs is set by using the function st_crs().\n\nst_crs(ak_rgns_sf)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nThis looks pretty confusing. Without getting into the details, that long string says that this data has a geographic coordinate system (WGS84) with no projection. A convenient way to reference crs quickly is by using the EPSG code, a number that represents a standard projection and datum. You can check out a list of (lots!) of EPSG codes here.\nWe will use multiple EPSG codes in this lesson. Here they are, along with their more readable names:\n\n3338: Alaska Albers (projected CRS)\n4326: WGS84 (World Geodetic System 1984), used in GPS (unprojected CRS)\n3857: Pseudo-Mercator, used in Google Maps, OpenStreetMap, Bing, ArcGIS, ESRI (projected CRS)\n\nYou will often need to transform your geospatial data from one coordinate system to another. The st_transform() function does this quickly for us. You may have noticed the maps above looked wonky because of the dateline. We might want to set a different projection for this data so it plots nicer. A good one for Alaska is called the Alaska Albers projection, with an EPSG code of 3338.\n\nak_rgns_3338_sf &lt;- ak_rgns_sf %&gt;%\n    st_transform(crs = 3338)\n\nst_crs(ak_rgns_3338_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3338 \n  wkt:\nPROJCRS[\"NAD83 / Alaska Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Alaska Albers (meter)\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-154,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",55,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",65,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Topographic mapping (small scale).\"],\n        AREA[\"United States (USA) - Alaska.\"],\n        BBOX[51.3,172.42,71.4,-129.99]],\n    ID[\"EPSG\",3338]]\n\n\n\nplot(ak_rgns_3338_sf)\n\n\n\n\n\n\n\n\nMuch better!",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Working with Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_18.html#sf-the-tidyverse",
    "href": "session_18.html#sf-the-tidyverse",
    "title": "18  Working with Spatial Data",
    "section": "18.4 sf & the Tidyverse",
    "text": "18.4 sf & the Tidyverse\nsf objects can be used as a regular data.frame object in many operations. We already saw the results of plot() and head().\nSince sf objects are data.frames, they play nicely with packages in the tidyverse. Here are a couple of simple examples:\n\n18.4.1 select()\n\n# returns the names of all the columns in dataset\ncolnames(ak_rgns_3338_sf)\n\n[1] \"region_id\" \"region\"    \"mgmt_area\" \"geometry\" \n\n\n\nak_rgns_3338_sf %&gt;%\n    select(region)\n\nSimple feature collection with 13 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -2175328 ymin: 405653 xmax: 1579226 ymax: 2383770\nProjected CRS: NAD83 / Alaska Albers\n# A tibble: 13 × 2\n   region                                                               geometry\n   &lt;chr&gt;                                                      &lt;MULTIPOLYGON [m]&gt;\n 1 Aleutian Islands     (((-1156666 420855.1, -1159837 417990.3, -1161898 41694…\n 2 Arctic               (((571289.9 2143072, 569941.5 2142691, 569158.2 2142146…\n 3 Bristol Bay          (((-339688.6 973904.9, -339302 972297.3, -339229.2 9710…\n 4 Chignik              (((-114381.9 649966.8, -112866.8 652065.8, -108836.8 65…\n 5 Copper River         (((561012 1148301, 559393.7 1148169, 557797.7 1148492, …\n 6 Kodiak               (((115112.5 983293, 113051.3 982825.9, 110801.3 983211.…\n 7 Kotzebue             (((-678815.3 1819519, -677555.2 1820698, -675557.8 1821…\n 8 Kuskokwim            (((-1030125 1281198, -1029858 1282333, -1028980 1284032…\n 9 Cook Inlet           (((35214.98 1002457, 36660.3 1002038, 36953.11 1001186,…\n10 Norton Sound         (((-848357 1636692, -846510 1635203, -840513.7 1632225,…\n11 Prince William Sound (((426007.1 1087250, 426562.5 1088591, 427711.6 1089991…\n12 Southeast            (((1287777 744574.1, 1290183 745970.8, 1292940 746262.7…\n13 Yukon                (((-375318 1473998, -373723.9 1473487, -373064.8 147393…\n\n\nNote the sticky geometry column stays with the region column! The geometry column will stay with your sf object even if it is not called explicitly.\n\n\n18.4.2 filter()\nRecall that == is problematic if you’re testing whether a variable might match multiple values - use %in% for that situation!\n\nunique(ak_rgns_3338_sf$region)\n\n [1] \"Aleutian Islands\"     \"Arctic\"               \"Bristol Bay\"         \n [4] \"Chignik\"              \"Copper River\"         \"Kodiak\"              \n [7] \"Kotzebue\"             \"Kuskokwim\"            \"Cook Inlet\"          \n[10] \"Norton Sound\"         \"Prince William Sound\" \"Southeast\"           \n[13] \"Yukon\"               \n\n\n\nak_rgns_3338_sf %&gt;%\n    filter(region == \"Southeast\")\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 559475.7 ymin: 722450 xmax: 1579226 ymax: 1410576\nProjected CRS: NAD83 / Alaska Albers\n# A tibble: 1 × 4\n  region_id region    mgmt_area                                         geometry\n*     &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;                               &lt;MULTIPOLYGON [m]&gt;\n1        12 Southeast         1 (((1287777 744574.1, 1290183 745970.8, 1292940 …",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Working with Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_18.html#spatial-joins",
    "href": "session_18.html#spatial-joins",
    "title": "18  Working with Spatial Data",
    "section": "18.5 Spatial Joins",
    "text": "18.5 Spatial Joins\nYou can also use the sf package to create spatial joins, useful for when you want to utilize two datasets together.\n\n\n\n\n\n\nExercise: How many people live in each of these Alaska regions?\n\n\n\nWe have some population data, but it gives the population by city, not by region. To determine the population per region we will need to:\n\nRead in the population data from a csv and turn it into an sf object\nUse a spatial join (st_join()) to assign each city to a region\nUse group_by() and summarize() to calculate the total population by region\nSave the spatial object you created using write_sf()\n\n\n\n1. Read in alaska_population.csv using read_csv()\nHere we’ll add a _df suffix to remind us that this is just a regular data frame, not a spatial data frame. It does contain spatial variables (longitude and latitude), but as far as it knows, those are just numbers, not recognized as spatial geometry… yet!\n\n# read in population data\npop_df &lt;- read_csv(here(\"data/alaska_population.csv\"))\n\nTurn pop into a spatial object\nThe st_join() function is a spatial left join. The arguments for both the left and right tables are objects of class sf which means we will first need to turn our population data.frame with latitude and longitude coordinates into an sf object.\nWe can do this easily using the st_as_sf() function, which takes as arguments the coordinates and the crs. The remove = F specification here ensures that when we create our geometry column, we retain our original lat lng columns, which we will need later for plotting. Although it isn’t said anywhere explicitly in the file, let’s assume that the coordinate system used to reference the latitude longitude coordinates is WGS84, which has a crs number of 4326.\nNote that we’re adding a _sf suffix to our new object, because now it is a Simple Features spatial data frame!\n\npop_4326_sf &lt;- st_as_sf(pop_df,\n                        coords = c('lng', 'lat'),\n                        crs = 4326,\n                        remove = F)\n\nhead(pop_4326_sf)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -176.6581 ymin: 51.88 xmax: -154.1703 ymax: 62.68889\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 6\n   year city       lat   lng population             geometry\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;          &lt;POINT [°]&gt;\n1  2015 Adak      51.9 -177.        122    (-176.6581 51.88)\n2  2015 Akhiok    56.9 -154.         84 (-154.1703 56.94556)\n3  2015 Akiachak  60.9 -161.        562 (-161.4314 60.90944)\n4  2015 Akiak     60.9 -161.        399 (-161.2139 60.91222)\n5  2015 Akutan    54.1 -166.        899 (-165.7731 54.13556)\n6  2015 Alakanuk  62.7 -165.        777 (-164.6153 62.68889)\n\n\n2. Join population data with Alaska regions data using st_join()\nNow we can do our spatial join! You can specify what geometry function the join uses (st_intersects, st_within, st_crosses, st_is_within_distance…) in the join argument. The geometry function you use will depend on what kind of operation you want to do, and the geometries of your shapefiles.\nIn this case, we want to find what region each city falls within, so we will use st_within.\n\npop_joined_sf &lt;- st_join(pop_4326_sf, \n                         ak_rgns_3338_sf, \n                         join = st_within)\n\nThis gives an error!\nError: st_crs(x) == st_crs(y) is not TRUE\nTurns out, this won’t work right now because our coordinate reference systems are not the same. Luckily, this is easily resolved using st_transform(), and projecting our population object into Alaska Albers.\n\npop_3338_sf &lt;- st_transform(pop_4326_sf, \n                            crs = 3338)\n\n\npop_joined_sf &lt;- st_join(pop_3338_sf, \n                         ak_rgns_3338_sf, \n                         join = st_within)\n\nhead(pop_joined_sf)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -1537925 ymin: 472626.9 xmax: -10340.71 ymax: 1456223\nProjected CRS: NAD83 / Alaska Albers\n# A tibble: 6 × 9\n   year city     lat   lng population             geometry region_id region     \n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;          &lt;POINT [m]&gt;     &lt;int&gt; &lt;chr&gt;      \n1  2015 Adak    51.9 -177.        122  (-1537925 472626.9)         1 Aleutian I…\n2  2015 Akhiok  56.9 -154.         84 (-10340.71 770998.4)         6 Kodiak     \n3  2015 Akiac…  60.9 -161.        562  (-400885.5 1236460)         8 Kuskokwim  \n4  2015 Akiak   60.9 -161.        399  (-389165.7 1235475)         8 Kuskokwim  \n5  2015 Akutan  54.1 -166.        899 (-766425.7 526057.8)         1 Aleutian I…\n6  2015 Alaka…  62.7 -165.        777  (-539724.9 1456223)        13 Yukon      \n# ℹ 1 more variable: mgmt_area &lt;dbl&gt;\n\n\n\n\n\n\n\n\nExploring types of joins\n\n\n\nThere are many different types of joins you can do with geospatial data. Examine the help page for these joins (?st_within() will get you there). What other joins types might be appropriate for examining the relationship between points and polygons? What about two sets of polygons?\n\n\n3. Calculate the total population by region using group_by() and summarize()\nNext we compute the total population for each region. In this case, we want to do a group_by() and summarize() as if this were a regular data.frame, without the spatial information - otherwise all of our point geometries would be included in the aggregation, which is not what we want. We remove the sticky geometry using st_drop_geometry(). Here we’re adding a _df suffix because it’s no longer a spatial data frame.\n\npop_rgn_df &lt;- pop_joined_sf %&gt;%\n    st_drop_geometry() %&gt;%\n    group_by(region) %&gt;%\n    summarize(total_pop = sum(population))\n\nhead(pop_rgn_df)\n\n# A tibble: 6 × 2\n  region           total_pop\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Aleutian Islands      8840\n2 Arctic                8419\n3 Bristol Bay           6947\n4 Chignik                311\n5 Cook Inlet          408254\n6 Copper River          2294\n\n\nAnd use a regular left_join() to get the information back to the Alaska region shapefile. Note that we need this step in order to regain our region geometries so that we can make some maps.\n\npop_rgn_3338_sf &lt;- left_join(ak_rgns_3338_sf, \n                             pop_rgn_df, \n                             by = \"region\")\n\n# plot to check\nplot(pop_rgn_3338_sf[\"total_pop\"])\n\n\n\n\n\n\n\n\nSo far, we have learned how to use sf and dplyr to use a spatial join on two datasets and calculate a summary metric from the result of that join.\n\n\n\n\n\n\nsf and tidyverse best practices\n\n\n\nThe group_by() and summarize() functions can also be used on sf objects to summarize within a dataset and combine geometries. Many of the tidyverse functions have methods specific for sf objects, some of which have additional arguments that wouldn’t be relevant to the data.frame methods. You can run ?sf::tidyverse to get documentation on the tidyverse sf methods.\n\n\nSay we want to calculate the population by Alaska management area, as opposed to region.\n\npop_mgmt_3338_sf &lt;- pop_rgn_3338_sf %&gt;%\n    group_by(mgmt_area) %&gt;%\n    summarize(total_pop = sum(total_pop))\n\nplot(pop_mgmt_3338_sf[\"total_pop\"])\n\n\n\n\n\n\n\n\nNotice that the region geometries were combined into a single polygon for each management area.\nIf we don’t want to combine geometries, we can specify do_union = F as an argument.\n\npop_mgmt_3338_sf &lt;- pop_rgn_3338_sf %&gt;%\n    group_by(mgmt_area) %&gt;%\n    summarize(total_pop = sum(total_pop), do_union = F)\n\nplot(pop_mgmt_3338_sf[\"total_pop\"])\n\n\n\n\n\n\n\n\n4. Save the spatial object to a new file using write_sf()\nSave the spatial object to disk using write_sf() and specifying the filename. Writing your file with the extension .shp will assume an ESRI driver driver, but there are many other format options available.\n\nwrite_sf(pop_rgn_3338_sf, here(\"data/ak_regions_population.shp\"))",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Working with Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_18.html#visualize-with-ggplot",
    "href": "session_18.html#visualize-with-ggplot",
    "title": "18  Working with Spatial Data",
    "section": "18.6 Visualize with ggplot",
    "text": "18.6 Visualize with ggplot\nggplot2 now has integrated functionality to plot sf objects using geom_sf().\nWe can plot sf objects just like regular data.frames using geom_sf.\n\nggplot(pop_rgn_3338_sf) +\n    geom_sf(aes(fill = total_pop)) +\n    labs(fill = \"Total Population\") +\n    scale_fill_continuous(low = \"khaki\",\n                          high =  \"firebrick\",\n                          labels = comma) +\n    theme_bw()\n\n\n\n\n\n\n\n\nWe can also plot multiple shapefiles in the same plot. Say if we want to visualize rivers in Alaska, in addition to the location of communities, since many communities in Alaska are on rivers. We can read in a rivers shapefile, double-check the crs to make sure it is what we need, and then plot all three shapefiles\n\nthe regional population (polygons),\nthe locations of cities (points), and\nthe rivers (linestrings).\n\n\nrivers_3338_sf &lt;- read_sf(here(\"data/ak_rivers_simp.shp\"))\n\n\nst_crs(rivers_3338_sf)\n\nCoordinate Reference System:\n  User input: Albers \n  wkt:\nPROJCRS[\"Albers\",\n    BASEGEOGCRS[\"GCS_GRS 1980(IUGG, 1980)\",\n        DATUM[\"unknown\",\n            ELLIPSOID[\"GRS80\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1,\n                    ID[\"EPSG\",9001]]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",50,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-154,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",55,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",65,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nNote that although no EPSG code is set explicitly, with some sleuthing we can determine that this is EPSG:3338. This site is helpful for looking up EPSG codes.\n\nggplot() +\n    geom_sf(data = pop_rgn_3338_sf, \n            aes(fill = total_pop)) +\n    geom_sf(data = pop_3338_sf, \n            size = 0.5) +\n    geom_sf(data = rivers_3338_sf,\n            aes(linewidth = StrOrder)) +\n    scale_linewidth(range = c(0.05, 0.5),\n                    guide = \"none\") +\n    labs(title = \"Total Population by Alaska Region\",\n         fill = \"Total Population\") +\n    scale_fill_continuous(low = \"khaki\",\n                          high =  \"firebrick\",\n                          labels = comma) +\n    theme_bw() \n\n\n\n\n\n\n\n\n\n18.6.1 Incorporate base maps into static maps using ggspatial\nThe ggspatial package has a function that can add tile layers from a few predefined tile sources like OpenStreetMap. Making sure that the tiles will plot correctly can be a finicky, so we will reproject our population data into the OpenStreetMap projection, Pseudo-Mercator (EPSG 3857), first.\nThen we will add ggspatial::annotation_map_tile() function into ggplot to add a base map to our map. This can take a couple of minutes to load.\n\npop_3857_sf &lt;- st_transform(pop_3338_sf, \n                         crs = 3857)\n\n\nggplot(data = pop_3857_sf) +\n    ggspatial::annotation_map_tile(type = \"osm\", zoom = 4, progress = 'none') + # higher zoom values are more detailed \n    geom_sf(aes(color = population),\n            fill = NA) +\n    scale_color_continuous(low = \"darkkhaki\",\n                           high =  \"firebrick\",\n                           labels = comma)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Working with Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_18.html#visualize-sf-objects-with-leaflet",
    "href": "session_18.html#visualize-sf-objects-with-leaflet",
    "title": "18  Working with Spatial Data",
    "section": "18.7 Visualize sf objects with leaflet",
    "text": "18.7 Visualize sf objects with leaflet\nWe can also make an interactive map from our data above using leaflet.\nleaflet (unlike ggplot) will project data for you. The catch is that you have to give it both a projection (like Alaska Albers), and that your shapefile must use a geographic coordinate system. This means that we need to use our shapefile with the 4326 EPSG code. Remember you can always check what crs you have set using st_crs.\nHere we define a leaflet projection for Alaska Albers, and save it as a variable to use later.\n\nepsg3338 &lt;- leaflet::leafletCRS(\n    crsClass = \"L.Proj.CRS\",\n    code = \"EPSG:3338\",\n    proj4def =  \"+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\",\n    resolutions = 2 ^ (16:7)\n)\n\nYou might notice that this looks familiar! The syntax is a bit different, but most of this information is also contained within the crs of our shapefile:\n\nst_crs(pop_rgn_3338_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3338 \n  wkt:\nPROJCRS[\"NAD83 / Alaska Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Alaska Albers (meter)\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-154,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",55,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",65,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Topographic mapping (small scale).\"],\n        AREA[\"United States (USA) - Alaska.\"],\n        BBOX[51.3,172.42,71.4,-129.99]],\n    ID[\"EPSG\",3338]]\n\n\nSince leaflet requires that we use an unprojected coordinate system, let’s use st_transform() yet again to get back to WGS84.\n\npop_rgn_4326_sf &lt;- pop_rgn_3338_sf %&gt;% \n    st_transform(crs = 4326)\n\n\nm &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;%\n    addPolygons(data = pop_rgn_4326_sf,\n                fillColor = \"gray\",\n                weight = 1)\n\nm\n\n\n\n\n\nWe can add labels, legends, and a color scale.\n\npal &lt;- colorNumeric(palette = \"Reds\", domain = pop_rgn_4326_sf$total_pop)\n\nm &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;%\n    addPolygons(\n        data = pop_rgn_4326_sf,\n        fillColor = ~ pal(total_pop),\n        weight = 1,\n        color = \"black\",\n        fillOpacity = 1,\n        label = ~ region\n    ) %&gt;%\n    addLegend(\n        position = \"bottomleft\",\n        pal = pal,\n        values = range(pop_rgn_4326_sf$total_pop),\n        title = \"Total Population\"\n    )\n\nm\n\n\n\n\n\nWe can also add the individual communities, with popup labels showing their population, on top of that!\n\npal &lt;- colorNumeric(palette = \"Reds\", domain = pop_rgn_4326_sf$total_pop)\n\nm &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;%\n    addPolygons(\n        data = pop_rgn_4326_sf,\n        fillColor = ~ pal(total_pop),\n        weight = 1,\n        color = \"black\",\n        fillOpacity = 1\n    ) %&gt;%\n    addCircleMarkers(\n        data = pop_4326_sf,\n        lat = ~ lat,\n        lng = ~ lng,\n        radius = ~ log(population / 500),\n        # arbitrary scaling\n        fillColor = \"gray\",\n        fillOpacity = 1,\n        weight = 0.25,\n        color = \"black\",\n        label = ~ paste0(pop_4326_sf$city, \", population \", comma(pop_4326_sf$population))\n    ) %&gt;%\n    addLegend(\n        position = \"bottomleft\",\n        pal = pal,\n        values = range(pop_rgn_4326_sf$total_pop),\n        title = \"Total Population\"\n    )\n\nm",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Working with Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_18.html#more-spatial-resources",
    "href": "session_18.html#more-spatial-resources",
    "title": "18  Working with Spatial Data",
    "section": "18.8 More Spatial Resources",
    "text": "18.8 More Spatial Resources\nThere is a lot more functionality to sf including the ability to intersect polygons, calculate distance, create a buffer, and more. Here are some more great resources and tutorials for a deeper dive into this great package:\n\nRaster analysis in R\n\nSpatial analysis in R with the sf package\n\nIntro to Spatial Analysis\n\nsf github repo\n\nTidy spatial data in R: using dplyr, tidyr, and ggplot2 with sf\n\nmapping-fall-foliage-with-sf",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Working with Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_18.html#basemaps-resources",
    "href": "session_18.html#basemaps-resources",
    "title": "18  Working with Spatial Data",
    "section": "18.9 Basemaps Resources",
    "text": "18.9 Basemaps Resources\n\nDocumentation for the ggmap package",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Working with Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_19.html",
    "href": "session_19.html",
    "title": "19  Reproducibility and Provenance",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility and Provenance</span>"
    ]
  },
  {
    "objectID": "session_19.html#learning-objectives",
    "href": "session_19.html#learning-objectives",
    "title": "19  Reproducibility and Provenance",
    "section": "",
    "text": "Discuss the concept of reproducible workflows including computational reproducibility and provenance metadata\nLearn how to use R to package your work by building a reproducible paper in RMarkdown/Quarto\nIntroduce tools and techniques for reproducibility supported by the NCEAS and DataONE\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn this lesson, we will be leveraging RMarkdown instead of Quarto so that we can use a very cool R package called rticles. Quarto has the same functionality as RMarkdown with rticles - making journal formatted articles from a code notebook - but it is done from the command line without additional R packages. See the Quarto documentation for details\n\n\n\n19.0.1 Reproducible Research: Recap\nWorking in a reproducible manner:\n\nIncreases research efficiency, accelerating the pace of your research and collaborations.\nProvides transparency by capturing and communicating scientific workflows.\nEnables research to stand on the shoulders of giants (build on work that came before).\nAllows credit for secondary usage and supports easy attribution.\nIncreases trust in science.\n\nTo enable others to fully interpret, reproduce or build upon our research, we need to provide more comprehensive information than is typically included in a figure or publication. The methods sections of papers are typically inadequate to fully reproduce the work described in the paper.\n\nFor example, the figure above conveys multiple messages. But, by looking at the figure we don’t get the full story of the process the scientist used to make this plot. What data were used in this study? What methods were applied? What were the parameter settings? What documentation or code are available to us to evaluate the results? Can we trust these data and methods? Are the results reproducible?\nComputational reproducibility is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions.\nPractically speaking, reproducibility includes:\n\nPreserving the data\nPreserving the software workflow\nDocumenting what you did\nDescribing how to interpret it all\n\nA recent study of publicly-available datasets in the Harvard Database repository containing R files found that only 26% of R files ran without error in the initial execution. 44% were able to be run after code cleaning, showing the importance of good programming practice (Trisovic et al. 2022). The figure below from Trisovic et al. shows a sankey diagram of how code cleaning was able to fix common errors.\n\n\n\n19.0.2 Computational Provenance and Workflows\nComputational provenance refers to the origin and processing history of data including:\n\nInput data\nWorkflow/scripts\nOutput data\nFigures\nMethods, dataflow, and dependencies\n\nWhen we put these all together with formal documentation, we create a computational workflow that captures all of the steps from initial data cleaning and integration, through analysis, modeling, and visualization. In other words, computational provenance is a formalized description of a workflow from the origin of the data to its final outcome.\nHere’s an example of a computational workflow from Mark Carls: Mark Carls. Analysis of hydrocarbons following the Exxon Valdez oil spill, Gulf of Alaska, 1989 - 2014. Gulf of Alaska Data Portal. urn:uuid:3249ada0-afe3-4dd6-875e-0f7928a4c171., that represents a three step workflow comprising four source data files and two output visualizations.\n\n\nThis image is a screenshot of an interactive user interface of a workflow built by DataONE. You can clearly see which data files were inputs to the process, the scripts that are used to process and visualize the data, and the final output objects that are produced, in this case two graphical maps of Prince William Sound in Alaska.\n\n\n19.0.3 From Provenance to Reproducibility\n\nDataONE provides a tool to track and visualize provenance. It facilitates reproducible science through provenance by:\n\nTracking data derivation history\nTracking data inputs and outputs of analyses\nPreserving and documenting software workflows\nTracking analysis and model executions\nLinking all of these to publications\n\n\nOne way to illustrate this is to look into the structure of a data package. A data package is the unit of publication of your data, including datasets, metadata, software and provenance. The image below represents a data package and all its components and how these components relate to one another.\n\n\n\n\n19.0.4 Data Citation and Transitive Credit\nWe want to move towards a model such that when a user cites a research publication we will also know:\n\nWhich data produced it\nWhat software produced it\nWhat was derived from it\nWho to credit down the attribution stack\n\n\nThis is transitive credit. And it changes the way in which we think about science communication and traditional publications.\n\n\n19.0.5 Reproducible Papers with rrtools\nA great overview of this approach to reproducible papers comes from:\n\nBen Marwick, Carl Boettiger & Lincoln Mullen (2018) Packaging Data Analytical Work Reproducibly Using R (and Friends), The American Statistician, 72:1, 80-88, doi:10.1080/00031305.2017.1375986\n\nThe key idea in Marwick et al. (2018) is that of the research compendium: A single container for not just the journal article associated with your research but also the underlying analysis, data, and even the required software environment required to reproduce your work.\nResearch compendium makes it easy for researchers to do their work but also for others to inspect or even reproduce the work because all necessary materials are readily at hand due to being kept in one place. Rather than a constrained set of rules, the research compendium is a scaffold upon which to conduct reproducible research using open science tools such as:\n\nR\nRMarkdown\nQuarto\ngit and GitHub\n\nFortunately for us, Ben Marwick (and others) have written an R package called rrtools that helps us create a research compendium from scratch.\n\n\n\n\n\n\nSet up\n\n\n\nTo start a reproducible paper with rrtools:\n\nClose your username-training project. Go to the project switcher dropdown, just click “close project.” This will set your working directory back to your home directory.\nIn console run the following line of code\n\n\n## \"mypaper\" is the name of the Rproj with my research compendia\nrrtools::use_compendium(\"mypaper\")\n\nrrtools has created the beginnings of a research compendium for us. The structure of this compendium is similar to the one needed to built an R package. That’s because it uses the same underlying folder structure and metadata and therefore it technically is an R package (called mypaper). And this means our research compendium could be easy to install in someone elses’ computer, similar to an R package.\n\nrrtools also helps you set up some key information:\n\n\nSet up a README file in the RMarkdown format\nCreate an analysis folder to hold our reproducible paper\n\n\nusethis::use_apl2_license()\nrrtools::use_readme_qmd()\nrrtools::use_analysis()\n\nThis creates a standard, predictable layout for our code and data and outputs that multiple people can understand. At this point, we’re technically ready to start writing the paper. But.. What about GitHub?\n\n\n\n19.0.5.1 Creating a git and GitHub repository with usethis\n\nusethis is a package that facilitates interactive workflows for R project creation and development. It automates repetitive tasks that arise during project setup and development.\n\nWe are going to use two functions to start tracking our work in git, create a remote repository in GitHub and be able to push and pull between the local version and the remote. To learn more about this package checkout the package documentation.\n\n\n\n\n\n\nSet up\n\n\n\n\nMake sure your are in “mypaper” Rproj.\nIn the Console run usethis::use_git() to create a local git repo. Choose yes to both questions when prompted (to commit files, and to restart R).\nThen, in the Console, run usethis::use_github() to create an upstream remote repo (in GitHub).\n\nAnd that’s it! Now your have your research compendium in your local computer and your changes are being tracked by git and your can pull and push to GitHub.\n\n\nLet’s explore the structure rrtools has put in place for us. Inside the analysis folder we have 5 folders. Different parts of our project will go into this different folders. Our data into the data folder, when the time comes to save any figure, we should save them into the figures folder, and so on.\n\n\n\nResearch compendia from Marwick et al.\n\n\nYou’ll notice a analysis/templates directory that contains journal citation style language (CSL) files which set the style of citations and reference list for the journal (the Journal of Archaeological Science, in this example). The template.Rmd renders into the template.docx. This document is called in the paper.qmd YAML to style the output of the paper created in paper.qmd.\nWhat if I want a template from another journal, different from the Journal of Archeological Science? We can create other journal’s template with the rticles package. This package will provide the templates and necessary information to render your paper in the journal of your choice (note: not all journal are in the rticles package). With that in mind, we will delete the existing paper directory and create a new one shortly.\n\n\n\n19.0.6 RMarkdown templates with rticles\nThe rticles package provides a lot of other great templates for formatting your paper specifically to the requirements of many journals. In addition to a custom CSL file for reference customization, rticles supports custom LATeX templates that fit the formatting requirements of each journals.\n\n\n\n\n\n\nTinytex and rendering to PDF\n\n\n\nTo be able to render your document to PDF you need to have tinytex installed in your machine.\nIn the console run:\n\ninstall.packages('tinytex') ## this package is already installed in our server\n\ntinytex::install_tinytex() ## this may take several minutes\n\n\n\n\n\n\n\n\n\nSet up\n\n\n\n\nIf you do not have rticles installed, go ahead and install calling the following function in the console: install.packages('rticles') Restart your RStudio session.\nTo create a new file from rticlescustom templates, got to File | New File | R Markdown... menu, which shows the following dialog:\n\n\n\nGo to “From Template” in the left side menu.\nSelect the “PNAS” template, give the file a name and set the location of the files to be mypaper/analysis, and click “OK”.\nYou can now Knit the Rmd file to see a highly-targeted article format, like this one for PNAS:\n\n\n\n\n\n\n19.0.7 Workflow in a nutshell\n\n\n\n\n\n\nSummary\n\n\n\n\nUse rrtools to generate the core directory layout and approach to data handling.\nThen use rticles to create the structure of the paper itself. The combination is incredibly flexible.\n\n\n\nThings we can do with our research compendium:\n\nEdit ./analysis/paper/paper.qmd to begin writing your paper and your analysis in the same document\nAdd any citations to ./analysis/paper/pnas-sample.bib\nAdd any longer R scripts that don’t fit in your paper in an R folder at the top level\nAdd raw data to ./data/raw_data\nWrite out any derived data (generated in paper.qmd) to ./data/derived_data\nWrite out any figures in ./analysis/figures\n\nYou can then write all of your R code in your RMarkdown/Quarto, and generate your manuscript all in the format needed for your journal (using its .csl file, stored in the paper directory).\n\n\n19.0.8 Adding renv to conserve your environment\n\nrrtools has a couple more tricks up its sleeve to help your compendium be as reproducible and portable as possible.\nTo capture the R packages and versions this project depends on, we can use the renv package.\nRunning renv::init(), will initiate tracking of the R packages in your project.\nThis action will create a new folder called renv in your top directory.\nrenv::init() automatically detects dependencies in your code (by looking for library calls, at the DESCRIPTION file, etc.) and installs them to a private project specific library. This means that your project mypaper can use a different version of dplyr than another project which may need an older version without any hassle.\nrenv also write the package dependencies to a special file in the repository called renv.lock.\nIf any of your packages you are using is updated, while your are working on your project, you can run renv::snapshot() to update the renv.lock file and your project-installed packages.\nYou can read the renv.lock file using renv::restore(), when needed. This will install the versions of the packages needed.\n\n\n\n19.0.9 Conserve your computational environement with Docker\n\nThe rrtools package then uses this renv.lock file to build what is called a Dockerfile.\nDocker allows you to build containers, a standard unit of software that packages up code and all its dependencies so an application runs quickly and reliably from one computing environment to another.\nA container is an “image” of all the software specified, and this image can be run on other computers such that the software stack looks exactly as you specify.\nThis is important when it comes to reproducibility, because when running someone else code, you may get different results or errors if you are using different versions of software (like an old version of dplyr).\nA Dockerfile contains the instructions for how to recreate the computational environment where your analysis was run.\n\nIn practice\n\nOnce you have your research compendium, you can call rrtools::use_dockerfile(). If needed, re-install rrtools directly from GitHub remotes::install_github(\"benmarwick/rrtools\")\nThis first creates a Dockerfile that loads a standard image for using R with the tidyverse.\nAnd then has more instructions for how to create the environment so that it has the very specific R packages and versions you need.\nIf we look at the Dockerfile (example below), it calls to renv::restore(), as described above.\nThe last line of the docker file renders our Quarto/RMarkdown reproducible paper!\n\n# get the base image, the rocker/verse has R, RStudio and pandoc\nFROM rocker/verse:4.2.2\n\n# required\nMAINTAINER Your Name &lt;your_email@somewhere.com&gt;\n\nCOPY . /&lt;REPO&gt;\n\n# go into the repo directory\nRUN . /etc/environment \\\n  # Install linux depedendencies here\n  # e.g. need this for ggforce::geom_sina\n  && sudo apt-get update \\\n  && sudo apt-get install libudunits2-dev -y \\\n  # build this compendium package\n  && R -e \"install.packages('remotes', repos = c(CRAN = 'https://cloud.r-project.org'))\" \\\n  && R -e \"remotes::install_github(c('rstudio/renv', 'quarto-dev/quarto-r'))\" \\\n  # install pkgs we need\n  && R -e \"renv::restore()\" \\\n  # render the manuscript into a docx, you'll need to edit this if you've\n  # customised the location and name of your main qmd file\n  && R -e \"quarto::quarto_render('/&lt;REPO&gt;/analysis/paper/paper.qmd')\"\n\nAfter running rrtools::use_dockerfile(), the package also sets up GitHub Actions for you.\nActions are processes that are triggered in GitHub events (like a push) and run automatically.\nIn this case, the Action that is set up will build your Docker image on GitHub.\nThis means that the code that knits your paper is run, and an updated version of your paper is knit.\nThis is called continuous integration, and is extremely convenient for developing products like this, since the build step can be taken care of automatically as you push to your repository.\n\n\n\n\n19.0.10 The 5th Generation of Reproducible Papers\n\nWhole Tale is a project that aims to simplify computational reproducibility. It enables researchers to easily package and share ‘Tales’. Tales are executable research objects captured in a standards-based tale format complete with metadata. They can contain:\n\nData (references)\nCode (computational methods)\nNarrative (traditional science story)\nCompute environment (e.g. RStudio, Jupyter)\n\n\nBy combining data, code and the compute environment, Tales allow researchers to:\n\nRe-create the computational results from a scientific study\nAchieve computational reproducibility\n“Set the default to reproducible.”\n\nFull circle reproducibility can be achieved by publishing data, code AND the computational environment.\n\n\n19.0.11 Resources\n\nrrtools documentation\nThe rticles\nusethis documentation\n\n\n\n\n\nTrisovic, Ana, Matthew K. Lau, Thomas Pasquier, and Mercè Crosas. 2022. “A Large-Scale Study on Research Code Quality and Execution.” Scientific Data 9 (1). https://doi.org/10.1038/s41597-022-01143-6.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility and Provenance</span>"
    ]
  }
]