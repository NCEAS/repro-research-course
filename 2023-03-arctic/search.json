[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "About",
    "text": "About\nThis 5-day in-person workshop will provide researchers with an introduction to advanced topics in computationally reproducible research in python, including software and techniques for working with very large datasets. This includes working in cloud computing environments, docker containers, and parallel processing using tools like parsl and dask. The workshop will also cover concrete methods for documenting and uploading data to the Arctic Data Center, advanced approaches to tracking data provenance, responsible research and data management practices including data sovereignty and the CARE principles, and ethical concerns with data-intensive modeling and analysis.\n\n\n\nSchedule\n\n\n\n\n\nCode of Conduct\nPlease note that by participating in this activity you agree to abide by the NCEAS Code of Conduct."
  },
  {
    "objectID": "index.html#setting-up",
    "href": "index.html#setting-up",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "Setting Up",
    "text": "Setting Up\nIn this course, we will be using Python (3.9.13) as our primary language, and VS Code as our IDE. Below are instructions on how to get VS Code set up to work for the course. If you are already a regular Python user, you may already have another IDE set up. We strongly encourage you to set up VS Code with us, because we will use your local VS Code instance to write and execute code on one of the NCEAS servers.\n\nDownload VS Code and Remote - SSH Extension\nFirst, download VS Code if you do not already have it installed.\nYou’ll also need to download the Remote - SSH extension.\n\n\nLog in to the server\nTo connect to the server using VS Code follow these steps, from the VS Code window:\n\nopen the command pallette (Cmd + Shift + P)\nenter “Remote SSH: Connect to Host”\n\n\n\nselect “Add New SSH Host”\nenter the ssh command to connect to the host as if in a terminal (ssh username@included-crab.nceas.ucsb.edu)\n\nNote: you will only need to do this step once\n\n\n\n\nselect the SSH config file to update with the name of the host. You should select the one in your user directory (eg: /Users/jclark/.ssh/config)\nclick “Connect” in the popup in the lower right hand corner\n\nNote: If the dialog box does not appear, reopen the command palette (Cmd + Shift + P), type in “Remote-SSH: Connect to Host…”, choose included-crab.nceas.ucsb.edu from the options of configured SSH hosts, then enter your password into the dialog box that appears\n\nenter your password in the dialog box that pops up\n\nWhen you are connected, you will see in the lower left hand corner of the window a green bar that says “SSH: included-crab.nceas.ucsb.edu.”\n\n\n\nInstall extensions on the server\nAfter connecting to the server, in the extensions pane (View > Extensions) search for, and install, the following extensions:\n- Python\n- Jupyter\n- Jupyter Keymap\nNote that these extensions will be installed on the server, and not locally.\n\n\nCreate a (free) Google Earth Engine (GEE) account\nIn order to code along during the Google Earth Engine lesson (Ch 15) on Thursday, you’ll need to sign up for an account at https://signup.earthengine.google.com. Once submitted, you’ll receive an email with some helpful links and a message that it may take a few days for your account to be up and running. Please be sure to do this a few days ahead of needing to use GEE.\n\n\n\n\n\n\nImportant\n\n\n\nGEE authentication (more on that in Lesson 15) uses Cloud Projects. Some organizations control who can create Cloud Projects, which may prevent you from completing the authentication process. To circumvent authentication issues, we recommend creating your GEE account using a non-organizational account (e.g. a personal email account). Check out GEE’s authentication troubleshooting recommendations if you continue to run into issues.\n\n\n\n\nTest your local setup (Optional)\nWe are going to be working on the server exclusively, but if you are interested in setting up VS Code to work for you locally with Python, you can follow these instructions. This local setup section summarizes the official VS Code tutorial. For more detailed instructions and screenshots, see the source material. This step is 100% optional, if you already have an IDE set up to work locally that you like, or already have VS code set up to work locally, you are welcome to skip this.\nLocally (not connected to the server), check to make sure you have Python installed if you aren’t sure you do. File > New Window will open up a new VS Code window locally.\nTo check your python, from the terminal run:\npython3 --version\nIf you get an error, it means you need to install Python. Here are instructions for getting installed, depending on your operating system. Note: There are many ways to install and manage your Python installations, and advantages and drawbacks to each. If you are unsure about how to proceed, feel free to reach out to the instructor team for guidance.\n\nWindows: Download and run an installer from Python.org.\nMac: Install using homebrew. If you don’t have homebrew installed, follow the instructions from their webpage.\n\nbrew install python3\n\n\nAfter you run your install, make sure you check that the install is on your system PATH by running python3 --version again.\nNext, install the Python extension for VS Code.\nOpen a terminal window in VS Code from the Terminal drop down in the main window. Run the following commands to initialize a project workspace in a directory called training. This example will show you how to do this locally. Later, we will show you how to set it up on the remote server with only one additional step.\nmkdir training\ncd training\ncode .\nNext, select the Python interpreter for the project. Open the Command Palette using Command + Shift + P (Control + Shift + P for windows). The Command Palette is a handy tool in VS Code that allows you to quickly find commands to VS Code, like editor commands, file edit and open commands, settings, etc. In the Command Palette, type “Python: Select Interpreter.” Push return to select the command, and then select the interpreter you want to use (your Python 3.X installation).\nTo make sure you can write and execute code in your project, create a Hello World test file.\n\nFrom the File Explorer toolbar, or using the terminal, create a file called hello.py\nAdd some test code to the file, and save\n\nmsg = \"Hello World\"\nprint(msg)\n\nExecute the script using either the Play button in the upper-right hand side of your window, or by running python3 hello.py in the terminal.\n\nFor more ways to run code in VS Code, see the tutorial\n\n\nFinally, to test Jupyter, download the Jupyter extension. You’ll also need to install ipykernel. From the terminal, run pip install ipykernel.\nYou can create a test Jupyter Notebook document from the command pallete by typing “Create: New Jupyter Notebook” and selecting the command. This will open up a code editor pane with a notebook that you can test."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "About this book",
    "text": "About this book\nThese written materials reflect the continuous development of learning materials at the Arctic Data Center and NCEAS to support individuals to understand, adopt, and apply ethical open science practices. In bringing these materials together we recognize that many individuals have contributed to their development. The primary authors are listed alphabetically in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: S. Jeanette Clark, Matthew B. Jones, Samantha Csik, Carmen Galaz García, Bryce Mecum, Natasha Haycock-Chavez, Daphne Virlar-Knight, Juliet Cohen, Anna Liljedahl. 2023. Scalable and Computationally Reproducible Approaches to Arctic Research. Arctic Data Center. doi:10.18739/A2QF8JM2V\nAdditional contributors: Amber E. Budden, Noor Johnson, Robyn Thiessen-Bock\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "sections/adc-intro.html#arctic-data-center-overview",
    "href": "sections/adc-intro.html#arctic-data-center-overview",
    "title": "1  Welcome and Overview",
    "section": "1.1 Arctic Data Center Overview",
    "text": "1.1 Arctic Data Center Overview\nThe Arctic Data Center is the primary data and software repository for the Arctic section of National Science Foundation’s Office of Polar Programs (NSF OPP).\nWe’re best known in the research community as a data archive – researchers upload their data to preserve it for the future and make it available for re-use. This isn’t the end of that data’s life, though. These data can then be downloaded for different analyses or synthesis projects. In addition to being a data discovery portal, we also offer top-notch tools, support services, and training opportunities. We also provide data rescue services.\n\nNSF has long had a commitment to data reuse and sharing. Since our start in 2016, we’ve grown substantially – from that original 4 TB of data from ACADIS to now over 76 TB. In 2021 alone, we saw 16% growth in dataset count, and about 30% growth in data volume. This increase has come from advances in tools – both ours and of the scientific community, plus active community outreach and a strong culture of data preservation from NSF and from researchers. We plan to add more storage capacity in the coming months, as researchers are coming to us with datasets in the terabytes, and we’re excited to preserve these research products in our archive. We’re projecting our growth to be around several hundred TB this year, which has a big impact on processing time. Give us a heads up if you’re planning on having larger submissions so that we can work with you and be prepared for a large influx of data.\n\nThe data that we have in the Arctic Data Center comes from a wide variety of disciplines. These different programs within NSF all have different focuses – the Arctic Observing Network supports scientific and community-based observations of biodiversity, ecosystems, human societies, land, ice, marine and freshwater systems, and the atmosphere as well as their social, natural, and/or physical environments, so that encompasses a lot right there in just that one program. We’re also working on a way right now to classify the datasets by discipline, so keep an eye out for that coming soon.\n\nAlong with that diversity of disciplines comes a diversity of file types. The most common file type we have are image files in four different file types. Probably less than 200-300 of the datasets have the majority of those images – we have some large datasets that have image and/or audio files from drones. Most of those 6600+ datasets are tabular datasets. There’s a large diversity of data files, though, whether you want to look at remote sensing images, listen to passive acoustic audio files, or run applications – or something else entirely. We also cover a long period of time, at least by human standards. The data represented in our repository spans across centuries.\n\nWe also have data that spans the entire Arctic, as well as the sub-Arctic, regions.\n\n\n1.1.1 Data Discovery Portal\nTo browse the data catalog, navigate to arcticdata.io. Go to the top of the page and under data, go to search. Right now, you’re looking at the whole catalog. You can narrow your search down by the map area, a general search, or searching by an attribute.\n\nClicking on a dataset brings you to this page. You have the option to download all the files by clicking the green “Download All” button, which will zip together all the files in the dataset to your Downloads folder. You can also pick and choose to download just specific files.\n\nAll the raw data is in open formats to make it easily accessible and compliant with FAIR principles – for example, tabular documents are in .csv (comma separated values) rather than Excel documents.\nThe metrics at the top give info about the number of citations with this data, the number of downloads, and the number of views. This is what it looks like when you click on the Downloads tab for more information.\n\nScroll down for more info about the dataset – abstract, keywords. Then you’ll see more info about the data itself. This shows the data with a description, as well as info about the attributes (or variables or parameters) that were measured. The green check mark indicates that those attributes have been annotated, which means the measurements have a precise definition to them. Scrolling further, we also see who collected the data, where they collected it, and when they collected it, as well as any funding information like a grant number. For biological data, there is the option to add taxa.\n\n\n1.1.2 Tools and Infrastructure\nAcross all our services and partnership, we are strongly aligned with the community principles of making data FAIR (Findable, Accesible, Interoperable and Reusable).\n\nWe have a number of tools available to submitters and researchers who are there to download data. We also partner with other organizations, like Make Data Count and DataONE, and leverage those partnerships to create a better data experience.\n\nOne of those tools is provenance tracking. With provenance tracking, users of the Arctic Data Center can see exactly what datasets led to what product, using the particular script that the researcher ran.\n\nAnother tool are our Metadata Quality Checks. We know that data quality is important for researchers to find datasets and to have trust in them to use them for another analysis. For every submitted dataset, the metadata is run through a quality check to increase the completeness of submitted metadata records. These checks are seen by the submitter as well as are available to those that view the data, which helps to increase knowledge of how complete their metadata is before submission. That way, the metadata that is uploaded to the Arctic Data Center is as complete as possible, and close to following the guideline of being understandable to any reasonable scientist.\n\n\n\n1.1.3 Support Services\nMetadata quality checks are the automatic way that we ensure quality of data in the repository, but the real quality and curation support is done by our curation team. The process by which data gets into the Arctic Data Center is iterative, meaning that our team works with the submitter to ensure good quality and completeness of data. When a submitter submits data, our team gets a notification and beings to evaluate the data for upload. They then go in and format it for input into the catalog, communicating back and forth with the researcher if anything is incomplete or not communicated well. This process can take anywhere from a few days or a few weeks, depending on the size of the dataset and how quickly the researcher gets back to us. Once that process has been completed, the dataset is published with a DOI (digital object identifier).\n\n\n\n1.1.4 Training and Outreach\nIn addition to the tools and support services, we also interact with the community via trainings like this one and outreach events. We run workshops at conferences like the American Geophysical Union, Arctic Science Summit Week and others. We also run an intern and fellows program, and webinars with different organizations. We’re invested in helping the Arctic science community learn reproducible techniques, since it facilitates a more open culture of data sharing and reuse.\n\nWe strive to keep our fingers on the pulse of what researchers like yourselves are looking for in terms of support. We’re active on Twitter to share Arctic updates, data science updates, and specifically Arctic Data Center updates, but we’re also happy to feature new papers or successes that you all have had with working with the data. We can also take data science questions if you’re running into those in the course of your research, or how to make a quality data management plan. Follow us on Twitter and interact with us – we love to be involved in your research as it’s happening as well as after it’s completed.\n\n\n\n1.1.5 Data Rescue\nWe also run data rescue operations. We digitiazed Autin Post’s collection of glacier photos that were taken from 1964 to 1997. There were 100,000+ files and almost 5 TB of data to ingest, and we reconstructed flight paths, digitized the images of his notes, and documented image metadata, including the camera specifications.\n\n\n\n1.1.6 Who Must Submit\nProjects that have to submit their data include all Arctic Research Opportunities through the NSF Office of Polar Programs. That data has to be uploaded within two years of collection. The Arctic Observing Network has a shorter timeline – their data products must be uploaded within 6 months of collection. Additionally, we have social science data, though that data often has special exceptions due to sensitive human subjects data. At the very least, the metadata has to be deposited with us.\nArctic Research Opportunities (ARC)\n\nComplete metadata and all appropriate data and derived products\nWithin 2 years of collection or before the end of the award, whichever comes first\n\nARC Arctic Observation Network (AON)\n\nComplete metadata and all data\nReal-time data made public immediately\nWithin 6 months of collection\n\nArctic Social Sciences Program (ASSP)\n\nNSF policies include special exceptions for ASSP and other awards that contain sensitive data\nHuman subjects, governed by an Institutional Review Board, ethically or legally sensitive, at risk of decontextualization\nMetadata record that documents non-sensitive aspects of the project and data\n\nTitle, Contact information, Abstract, Methods\n\n\nFor more complete information see our “Who Must Submit” webpage\nRecognizing the importance of sensitive data handling and of ethical treatment of all data, the Arctic Data Center submission system provides the opportunity for researchers to document the ethical treatment of data and how collection is aligned with community principles (such as the CARE principles). Submitters may also tag the metadata according to community develop data sensitivity tags. We will go over these features in more detail shortly.\n\n\n\n1.1.7 Supporting data reuse\nAs the Arctic Data Center has grown in size, we have envisioned new capabilities to support efficient reuse of these valuable data. We are developing new tools both within the Arctic Data Center, and through collaborative projects with other researchers in the community. In house, some of the new features we plan to support include:\n\nEfficient submission and access to multi-Terabyte datasets\nData Quality assessment services\nAutomated workflows for building derived data products\n\nAs a concrete example, we are collaborating with the Permafrost Discovery Gateway to build new capacity for computing and visualization at sub-meter spatial resolution across the global Arctic. The services we are building support workflows for pre-processing massive image data collections to prepare them for modeling, as well as automating machine learning and other computations across large data on distributed compute clusters, and finally post-processing the results to create viable pathways for results mapping and visualization at global scales.\n\n\n\n1.1.8 Summary\nAll the above informtion can be found on our website or if you need help, ask our support team at support@arcticdata.io or tweet us @arcticdatactr!\n\n\n\n 6,958 Datasets\n\n\n \n\n\n 2,584 Creators\n\n\n\n\n 76 TB\n\n\n \n\n\n 16,162 Users"
  },
  {
    "objectID": "sections/adc-intro.html#scalable-computing-topics",
    "href": "sections/adc-intro.html#scalable-computing-topics",
    "title": "1  Welcome and Overview",
    "section": "1.2 Scalable Computing Topics",
    "text": "1.2 Scalable Computing Topics\nThe overall objective of this course is to facilitate effective analysis and modeling of the massive data resources that we curate for the Arctic research community. We assume a baseline proficiency, and plan to build on that to better support time-consuming computations with multi-terabyte datasets.\nActivities will include:\n\nTechnical tutorials with a lot of hands-on time in Python\nLectures and discussions on core concepts for scalable computing\nSemi-structured group projects to practice and cement key skills\n\nKey topics will include:\n\nArctic Data Center services and tools\nOverview and review of core computing concepts\nFundamentals of concurrent programming\n\nconcurrent.futures\nParsl\nDask\n\nFundamentals of working with big data and imagery\nGood practices in research software design for efficiency, reproducibility, and reuse\nFundamentals of cloud computing\n\nDocker, containers, reproducibility, and more\n\n\nAs this is a lot to cover in a short survey course, we will break it down to fundamentals daily, and provide ample time for hands-on practice."
  },
  {
    "objectID": "sections/remote-computing.html#learning-objectives",
    "href": "sections/remote-computing.html#learning-objectives",
    "title": "2  Remote Computing",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\n\nUnderstand the basic architecture of computer networks\nLearn how to connect to a remote computer via a shell\nBecome familiarized with Bash Shell programming to navigate your computer’s file system, manipulate files and directories, and automate processes"
  },
  {
    "objectID": "sections/remote-computing.html#introduction",
    "href": "sections/remote-computing.html#introduction",
    "title": "2  Remote Computing",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nScientific synthesis and our ability to effectively and efficiently work with big data depends on the use of computers and the internet. Working on a personal computer may be sufficient for many tasks, but as data get larger and analyses more computationally intensive, scientists often find themselves needing more computing resources than they have available locally. Remote computing, or the process of connecting to a computer(s) in another location via a network link is becoming more and more common in overcoming big data challenges.\nIn this lesson, we’ll learn about the architecture of computer networks and explore some of the different remote computing configurations that you may encounter, we’ll learn how to securely connect to a remote computer via a shell, and we’ll become familiarized with using Bash Shell to efficiently manipulate files and directories. We will begin working in the VS Code IDE (integrated development environment), which is a versatile code editor that supports many different languages."
  },
  {
    "objectID": "sections/remote-computing.html#servers-networking",
    "href": "sections/remote-computing.html#servers-networking",
    "title": "2  Remote Computing",
    "section": "2.3 Servers & Networking",
    "text": "2.3 Servers & Networking\nRemote computing typically involves communication between two or more “host” computers. Host computers connect via networking equipment and can send messages to each other over communication protocols (aka an Internet Protocol, or IP). Host computers can take the role of client or server, where servers share their resources with the client. Importantly, these client and server roles are not inherent properties of a host (i.e. the same machine can play either role).\n\nClient: the host computer intiating a request\nServer: the host computer responding to a request\n\n\n\n\nFig 1. Examples of different remote computing configurations. (a) A client uses secure shell protocol (SSH) to login/connect to a server over the internet. (b) A client uses SSH to login/connect to a computing cluster (i.e. a set of computers (nodes) that work together so that they can be viewed as a single system) over the internet. In this example, servers A - I are each nodes on this single cluster. The connection is first made through a gateway node (i.e. a computer that routes traffic from one network to another). (c) A client uses SSH to login/connect to a computing cluser where each node is a virtual machine (VM). In this example, the cluster comprises three servers (A, B, and C). VM1 (i.e. node 1) runs on server A while VM4 runs on server B, etc. The connection is first made through a gateway node."
  },
  {
    "objectID": "sections/remote-computing.html#ip-addressing",
    "href": "sections/remote-computing.html#ip-addressing",
    "title": "2  Remote Computing",
    "section": "2.4 IP addressing",
    "text": "2.4 IP addressing\nHosts are assigned a unique numerical address used for all communication and routing called an Internet Protocol Address (IP Address). They look something like this: 128.111.220.7. Each IP Address can be used to communicate over various “ports”, which allow multiple applications to communicate with a host without mixing up traffic.\n\n\n\n\n\n\nPort numbers are divided into three ranges:\n\n\n\n\nwell-known ports, range from 0 through 1023 and are reserved for the most commonly used services (see table below for examples of some well-known port numbers)\nregistered ports, range from 1024 through 49151 and are not assigned or controlled, but can be registered (e.g. by a vendor for use with thier own server application) to prevent duplication\ndynamic ports, range from 49152 through 65535 and are not assigned, controlled, or registered but may instead be used as temporary or private ports\n\n\n\n\n\n\n\n\nwell-known port\nassignment\n\n\n\n\n20, 21\nFile Transfer Protocol (FTP), for transfering files between a client & server\n\n\n22\nsecure shell (SSH), to create secure network connections\n\n\n53\nDomain Name System (DNS) service, to match domain names to IP addresses\n\n\n80\nHypertext Transfer Protocol (HTTP), used in the World Wide Web\n\n\n443\nHTTP Secure (HTTPS), an encrypted version of HTTP\n\n\n\n\n\nBecause IP addresses can be difficult to remember, they are also assigned hostnames, which are handled through the global Domain Name System (DNS). Clients first look up a hostname in the DNS to find the IP address, then open a connection to the IP address.\n\n\n\n\n\n\nIn order to connect to remote servers, computing clusters, virtual machines, etc., you need to know their IP address (or hostname)\n\n\n\nA couple important ones:\n\nThroughout this course, we’ll be working on a server with the hostname, included-crab and IP address, 128.111.85.28 (in just a little bit, we’ll learn how to connect to included-crab using SSH)\n\nlocalhost is a hostname that refers to your local computer and is assigned the IP address 127.0.0.1 – the concept of localhost is important for tasks such as website testing, and is also important to understand when provisioning local execution resources (e.g. we’ll practice this during the section 6 exercise when working with Parsl.)"
  },
  {
    "objectID": "sections/remote-computing.html#bash-shell-programming",
    "href": "sections/remote-computing.html#bash-shell-programming",
    "title": "2  Remote Computing",
    "section": "2.5 Bash Shell Programming",
    "text": "2.5 Bash Shell Programming\nWhat is a shell? From Wikipedia:\n\n“a computer program which exposes an operating system’s services to a human user or other programs. In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), depending on a computer’s role and particular operation.”\n\n\n\n\nWhat is Bash? Bash, or Bourne-again Shell, is a command line tool (language) commonly used to manipulate files and directories. Accessing and using bash is slightly different depending on what type of machine you work on:\n\nMac: bash via the Terminal, which comes ready-to-use with all Macs and Linux machines\nWindows: running bash depends on which version of Windows you have – newer versions may ship with bash or may require a separate install (e.g. Windows Subsystem for Linux (WSL) or Git Bash), however there are a number of different (non-bash) shell options as well (they all vary slightly; e.g. PowerShell, Command Prompt).\n\n\n\n\n\n\n\nNote\n\n\n\nMac users may have to switch from Z Shell, or zsh, to bash. Use the command exec bash to switch your default shell to bash (or exec zsh to switch back).\n\n\n\n2.5.1 Some commonly used (and very helpful) bash commands:\nBelow are just a few bash commands that you’re likely to use. Some may be extended with options (more on that in the next section) or even piped together (i.e. where the output of one command gets sent to the next command, using the | operator). You can also find some nice bash cheat sheets online, like this one. Alternatively, the Bash Reference Manual has all the content you need, albeit a bit dense.\n\n\n\n\n\n\n\nbash command\nwhat it does\n\n\n\n\npwd\nprint your current working directory\n\n\ncd\nchange directory\n\n\nls\nlist contents of a directory\n\n\ntree\ndisplay the contents of a directory in the form of a tree structure (not installed by default)\n\n\necho\nprint text that is passed in as an argument\n\n\nmv\nmove or rename a file\n\n\ncp\ncopy a file(s) or directory(ies)\n\n\ntouch\ncreate a new empty file\n\n\nmkdir\ncreate a new directory\n\n\nrm/rmdir\nremove a file/ empty directory (be careful – there is no “trash” folder!)\n\n\ngrep\nsearches a given file(s) for lines containing a match to a given pattern list\n\n\nawk\na text processing language that can be used in shell scripts or at a shell prompt for actions like pattern matching, printing specified fields, etc.\n\n\nsed\nstands for Stream Editor; a versatile command for editing files\n\n\ncut\nextract a specific portion of text in a file\n\n\njoin\njoin two files based on a key field present in both\n\n\ntop, htop\nview running processes in a Linux system (press Q to quit)\n\n\n\n\n\n2.5.2 General command syntax\nBash commands are typically are written as: command [options] [arguments] where the command must be an executable on your PATH and where options (settings that change the shell and/or script behavior) take one of two forms: short form (e.g. command -option-abbrev) or long form (e.g. command --option-name or command -o option-name). An example:\n# the `ls` command lists the files in a directory\nls file/path/to/directory\n\n# adding on the `-a` or `--all` option lists all files (including hidden files) in a directory\nls -a file/path/to/directory # short form\nls --all file/path/to/directory # long form\nls -o all file/path/to/directory # long form\n\n\n2.5.3 Some useful keyboard shortcuts\nIt can sometimes feel messy working on the command line. These keyboard shortcuts can make it a little easier:\n\nCtrl + L: clear your terminal window\nCtrl + U: delete the current line\nCtrl + C: abort a command\nup & down arrow keys: recall previously executed commands in chronological order\nTAB key: autocompletion"
  },
  {
    "objectID": "sections/remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "href": "sections/remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "title": "2  Remote Computing",
    "section": "2.6 Connecting to a remote computer via a shell",
    "text": "2.6 Connecting to a remote computer via a shell\nIn addition to navigating your computer/manipulating your files, you can also use a shell to gain accesss to and remotely control other computers. To do so, you’ll need the following:\n\na remote computer (e.g. server) which is turned on\nclient and server ssh clients installed/enabled\nthe IP address or name of the remote computer\nthe necessary permissions to access the remote computer\n\nSecure Shell, or SSH, is a network communication protocol that is often used for securely connecting to and running shell commands on a remote host, tremendously simplifying remote computing."
  },
  {
    "objectID": "sections/remote-computing.html#git-via-a-shell",
    "href": "sections/remote-computing.html#git-via-a-shell",
    "title": "2  Remote Computing",
    "section": "2.7 Git via a shell",
    "text": "2.7 Git via a shell\nGit, a popular version control system and command line tool can be accessed via a shell. While there are lots of graphical user interfaces (GUIs) that faciliatate version control with Git, they often only implement a small subset of Git’s most-used functionality. By interacting with Git via the command line, you have access to all Git commands. While all-things Git is outside the scope of this workshop, we will use some basic Git commands in the shell to clone GitHub (remote) repositories to the server and save/store our changes to files. A few important Git commands:\n\n\n\n\n\n\n\nGit command\nwhat it does\n\n\n\n\ngit clone\ncreate a copy (clone) of repository in a new directory in a different location\n\n\ngit add\nadd a change in the working directory to the staging area\n\n\ngit commit\nrecord a snapshot of a repository; the -m option adds a commit message\n\n\ngit push\nsend commits from a local repository to a remote repository\n\n\ngit fetch\ndownloads contents (e.g. files, commits, refs) from a remote repo to a local repo\n\n\ngit pull\nfetches contents of a remote repo and merges changes into the local repo"
  },
  {
    "objectID": "sections/remote-computing.html#lets-practice",
    "href": "sections/remote-computing.html#lets-practice",
    "title": "2  Remote Computing",
    "section": "2.8 Let’s practice!",
    "text": "2.8 Let’s practice!\nWe’ll now use bash commands to do the following:\n\nconnect to the server (included-crab) that we’ll be working on for the remainder of this course\nnavigate through directories on the server and add/change/manipulate files\nclone a GitHub repository to the server\nautomate some of the above processes by writing a bash script\n\n\n2.8.1 Exercise 1: Connect to a server using the ssh command (or using VS Code’s command palette)\nLet’s connect to a remote computer (included-crab) and practice using some of above commands.\n\nLaunch a terminal in VS Code\n\n\nThere are two options to open a terminal window, if a terminal isn’t already an open pane at the bottom of VS Code\n\nClick on Terminal > New Terminal in top menu bar\nClick on the + (dropdown menu) > bash in the bottom right corner\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou don’t need to use the VS Code terminal to ssh into a remote computer, but it’s conveniently located in the same window as your code when working in the VS Code IDE.\n\n\n\nConnect to a remote server\n\n\nYou can choose to SSH into the server (included-crab.nceas.ucsb.edu) through (a) the command line by using the ssh command, or (b) through VS Code’s command palette. If you prefer the latter, please refer back to the Log in to the server section. To do so via the command line, use the ssh command followed by yourusername@included-crab.nceas.ucsb.edu. You’ll be prompted to type/paste your password to complete the login. It should look something like this:\n\nyourusername:~$ ssh yourusername@included-crab.nceas.ucsb.edu \nyourusername@included-crab.nceas.ucsb.edu's password: \nyourusername@included-crab:~$ \n\n\n\n\n\n\nImportant\n\n\n\nYou won’t see anything appear as you type or paste your password – this is a security feature! Type or paste your password and press enter/return when done to finish connecting to the server.\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo log out of the server, type exit – it should look something like this:\nyourusername@included-crab.nceas.ucsb.edu:$ exit\nlogout\nConnection to included-crab.nceas.ucsb.edu closed.\n(base) .....\n\n\n\n\n2.8.2 Exercise 2: Practice using some common bash commands\n\nUse the pwd command to print your current location, or working directory. You should be in your home directory on the server (e.g. /home/yourusername).\nUse the ls command to list the contents (any files or subdirectories) of your home directory\nUse the mkdir command to create a new directory named bash_practice:\n\nmkdir bash_practice\n\nUse the cd command to move into your new bash_practice directory:\n\n# move from /home/yourusername to home/yourusername/bash_practice\ncd bash_practice\n\nTo move up a directory level, use two dots, .. :\n\n# move from /home/yourusername/bash_practice back to /home/yourusername\n$ cd ..\n\n\n\n\n\n\nNote\n\n\n\nTo quickly navigate back to your home directory from wherever you may be on your computer, use a tilde, ~ :\n# e.g. to move from from some subdirectory, /home/yourusername/Projects/project1/data, back to your home directory, home/yourusername\n$ cd ~\n\n# or use .. to back out three subdirectories\n$ cd ../../..\n\n\n\nAdd some .txt files (file1.txt, file2.txt, file3.txt) to your bash_practice subdirectory using the touch command (Note: be sure to cd into bash_practice if you’re not already there):\n\n# add one file at a time\ntouch file1.txt\ntouch file2.txt\ntouch file3.txt\n\n# or add all files simultanously like this:\ntouch file{1..3}.txt\n\n# or like this:\ntouch file1.txt file2.txt file3.txt\n\nYou can also add other file types (e.g. .py, .csv, etc.)\n\ntouch mypython.py mycsv.csv\n\nPrint out all the .txt files in bash_practice using a wildcard, *:\n\nls *.txt\n\nCount the number of .txt files in bash_practice by combining the ls and wc (word count) funtions using the pipe, |, operator:\n\n# `wc` returns a word count (lines, words, chrs)\n# the `-l` option only returns the number of lines\n# use a pipe, `|`, to send the output from `ls *.txt` to `wc -l`\nls *.txt | wc -l\n\nDelete mypython.py using the rm command:\n\nrm mypython.py \n\nCreate a new directory inside bash_practice called data and move mycsv.csv into it.\n\nmkdir data\nmv mycsv.csv ~/bash_practice/data\n\n# add the --interactive option (-i for short) to prevent a file from being overwritten by accident (e.g. in case there's a file with the same name in the destination location)\nmv -i mycsv.csv ~/bash_practice/data\n\nUse mv to rename mycsv.csv to mydata.csv\n\nmv mycsv.csv mydata.csv\n\nAdd column headers col1, col2, col3 to mydata.csv using echo + the > operator\n\necho \"col1, col2, col3\" > mydata.csv\n\n\n\n\n\n\nTip\n\n\n\nYou can check to see that mydata.csv was updated using GNU nano, a text editor for the command line that comes preinstalled on Linux machines (you can edit your file in nano as well). To do so, use the nano command followed by the file you want to open/edit:\nnano mydata.csv\nTo save and quit out of nano, use the control + X keyboard shortcut.\nYou can also create and open a file in nano in just one line of code. For example, running nano hello_world.sh is the same as creating the file first using touch hello_world.sh, then opening it with nano using nano hello_world.sh.\n\n\n\nAppend a row of data to mydata.csv using echo + the >> operator\n\n# using `>` will overwrite the contents of an existing file; `>>` appends new information to an existing file\necho \"1, 2, 3\" >> mydata.csv\n\n\n2.8.3 Exercise 3: Clone a GitHub repository to the server\nIDEs commonly have helper buttons for cloning (i.e. creating a copy of) remote repositories to your local computer (or in this case, a server), but using git commands in a terminal can be just as easy. We can practice that now, following the steps below:\n\nGo to the scalable-computing-examples repository on GitHub at https://github.com/NCEAS/scalable-computing-examples – this repo contains example files for you to edit and practice in throughout this course. Fork (make your own copy of the repository) this repo by clicking on the Fork button (top right corner of the repository’s page).\n\n\n\n\n\n\nOnce forked, click on the green Code button (from the forked version of the GitHub repo) and copy the URL to your clipboard.\n\n\n\n\n\n\nIn the VS Code terminal, use the git clone command to create a copy of the scalable-computing-examples repository in the top level of your user directory (i.e. your home directory) on the server (Note: use pwd to check where you are; use cd ~/ to navigate back to your home directory if you find that you’re somewhere else).\n\ngit clone <url-of-forked-repo>\n\nYou should now have a copy of the scalable-computing-examples repository to work on on the server. Use the tree command to see the structure of the repo (you need to be in the scalable-computing-examples directory for this to work) – there should be a subdirectory called 02-bash-babynames that contains (i) a README.MD file, (ii) a KEY.sh file (this is a functioning bash script available for reference; we’ll be recreating it together in the next exercise) and (iii) a namesbystate folder containing 51 .TXT files and a StateReadMe.pdf file with some metadata.\n\n\n\n2.8.4 Bonus Exercise: Automate data processing with a Bash script\nAs we just demonstrated, we can use bash commands in the terminal to accomplish a variety of tasks like navigating our computer’s directories, manipulating/creating/adding files, and much more. However, writing a bash script allows us to gather and save our code for automated execusion.\nWe just cloned the scalable-computing-examples GitHub repository to the server in Exercise 3 above. This contains a 02-bash-babynames folder with 51 .TXT files (one for each of the 50 US states + The District of Columbia), each with the top 1000 most popular baby names in that state. We’re going to use some of the bash commands we learned in Exercise 2 to concatenate all rows of data from these 51 files into a single babynames_allstates.csv file.\nLet’s begin by creating a simple bash script that when executed, will print out the message, “Hello, World!” This simple script will help us determine whether or not things are working as expected before writing some more complex (and interesting) code.\n\nOpen a terminal window and determine where you are by using the pwd command – we want to be in scalable-computing-examples/02-bash-babynames. If necessary, navigate here using the cd command.\nNext, we’ll create a shell script called mybash.sh using the touch command:\n\n$ touch mybash.sh\n\nThere are a number of ways to edit a file or script – we’ll use Nano, a terminal-based text editor, as we did earlier. Open your mybash.sh with nano by running the following in your terminal:\n\n$ nano mybash.sh\n\nWe can now start to write our script. Some important considerations:\n\n\nAnything following a # will not be executed as code – these are useful for adding comments to your scripts\nThe first line of a Bash script starts with a shebang, #!, followed by a path to the Bash interpreter – this is used to tell the operating system which interpreter to use to parse the rest of the file. There are two ways to use the shebang to set your interpreter (read up on the pros & cons of both methods on this Stack Overflow post):\n\n# (option a): use the absolute path to the bash binary\n#!/bin/bash\n\n# (option b): use the env untility to search for the bash executable in the user's $PATH environmental variable\n#!/usr/bin/env bash\n\nWe’ll first specify our bash interpreter using the shebang, which indicates the start of our script. Then, we’ll use the echo command, which when executed, will print whatever text is passed as an argument. Type the following into your script (which should be opened with nano), then save (Use the keyboard shortcut control + X to exit, then type Y when it asks if you’d like to save your work. Press enter/return to exit nano).\n\n# specify bash as the interpreter\n#!/bin/bash\n\n# print \"Hello, World!\"\necho \"Hello, World!\"\n\nTo execute your script, use the bash command followed by the name of your bash script (be sure that you’re in the same working directory as your mybash.sh file or specify the file path to it). If successful, “Hello, World!” should be printed in your terminal window.\n\nbash mybash.sh\n\nNow let’s write our script. Re-open your script in nano by running nano mybash.sh. Using what we practiced above and the hints below, write a bash script that does the following:\n\n\nprints the number of .TXT files in the namesbystate subdirectory\nprints the first 10 rows of data from the CA.TXT file (HINT: use the head command)\nprints the last 10 rows of data from the CA.TXT file (HINT: use the tail command)\ncreates an empty babynames_allstates.csv file in the namesbystate subdirectory (this is where the concatenated data will be saved to)\nadds the column names, state, gender, year, firstname, count, in that order, to the babynames_allstates.csv file\nconcatenates data from all .TXT files in the namesbystate subdirectory and appends those data to the babynames_allstates.csv file (HINT: use the cat command to concatenate files)\n\nHere’s a script outline to fill in (Note: The echo statements below are not necessary but can be included as progress indicators for when the bash script is executed – these also make it easier to diagnose where any errors occur during execution):\n#!bin/bash\necho \"THIS IS THE START OF MY SCRIPT!\"\n\necho \"-----Verify that we have .TXT files for all 50 states + DC-----\"\n<add your code here>\n\necho \"-----Printing head of CA.TXT-----\"\n<add your code here>\n\necho \"-----Printing tail of CA.TXT-----\"\n<add your code here>\n\necho \"-----Creating empty .csv file to concatenate all data-----\"\n<add your code here>\n\necho \"-----Adding column headers to csv file-----\"\n<add your code here>\n\necho \"-----Concatenating files-----\"\n<add your code here>\n\necho \"DONE!\"\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n#!bin/bash\necho \"THIS IS THE START OF MY SCRIPT!\"\n\necho \"-----Verify that we have .TXT files for all 50 states + DC-----\"\nls namesbystate/*.TXT | wc -l\n\necho \"-----Printing head of CA.TXT-----\"\nhead namesbystate/CA.TXT\n\necho \"-----Printing tail of CA.TXT-----\"\ntail namesbystate/CA.TXT\n\necho \"-----Creating empty .csv file to concatenate all data-----\"\ntouch namesbystate/babynames_allstates.csv\n\necho \"-----Adding column headers to csv file-----\"\necho \"state, gender, year, firstname, count\" > namesbystate/babynames_allstates.csv\n\necho \"-----Concatenating files-----\"\ncat namesbystate/*.TXT >> namesbystate/babynames_allstates.csv\n\necho \"DONE!\""
  },
  {
    "objectID": "sections/python-intro.html#learning-objectives",
    "href": "sections/python-intro.html#learning-objectives",
    "title": "3  Python Programming on Clusters",
    "section": "3.1 Learning Objectives",
    "text": "3.1 Learning Objectives\n\nBasic Python review\nUsing virtual environments\nWriting in Jupyter notebooks\nWriting functions in Python"
  },
  {
    "objectID": "sections/python-intro.html#introduction",
    "href": "sections/python-intro.html#introduction",
    "title": "3  Python Programming on Clusters",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nWe’ve chosen to use VS Code in this training, in part, because it has great support for developing on remote machines. Hopefully, your VS Code setup went easily, and you were able to connect to our server included-crab. Once connected, the VS Code interface looks just like you were working locally, and connection to the server is seamless.\nOther aspects of VS Code that we like: it supports all languages thanks to the extensive free extension library, it has built in version control integration, and it is highly flexible/configurable.\nWe will also be working quite a bit in Jupyter notebooks in this course. Notebooks are great ways to interleave rich text (markdown formatted text, equations, images, links) and code in a way that a ‘literate analysis’ is generated. Although Jupyter notebooks are not subsitutes for python scripts, they can be great communication tools, and can also be convenient for code development."
  },
  {
    "objectID": "sections/python-intro.html#connect-to-the-server-if-you-arent-already-connected",
    "href": "sections/python-intro.html#connect-to-the-server-if-you-arent-already-connected",
    "title": "3  Python Programming on Clusters",
    "section": "3.3 Connect to the server (if you aren’t already connected)",
    "text": "3.3 Connect to the server (if you aren’t already connected)\nTo get set up for the course, let’s connect to the server again. If you were able to work through the setup for the lesson without difficulty, follow these steps to connect:\n\nopen VS Code\nopen the command pallette (Cmd + Shift + P)\nenter “Remote SSH: Connect to Host”\nselect included-crab\nenter your password in the dialog box that pops up\n\nAlternatively, you may see a popup window that says “Cannot reconnect. Please reload the window”. Choose the blue Reload Window button and enter your password, if prompted.\nTo open the scalable-computing-examples workspace, select File > Open Workspace from File. Then navigate to and select ~/scalable-computing-examples/scalable-computing-examples.code-workspace. If you cannot find the scalable-computing-examples directory, in a terminal you can use the ls command to list out the contents of your home directory and ensure that you have a clone of the respository. If not, follow the instructions in Exercise 3 in the Remote Computing session).\nOnce connected and in the workslace, use the pwd command in the terminal to make sure you’re in your project directory (/home/yourusername/scalable-computing-examples)."
  },
  {
    "objectID": "sections/python-intro.html#virtual-environments",
    "href": "sections/python-intro.html#virtual-environments",
    "title": "3  Python Programming on Clusters",
    "section": "3.4 Virtual Environments",
    "text": "3.4 Virtual Environments\n\nWhen you install a python library, let’s say pandas, via pip, unless you specify otherwise, pip will go out and grab the most recent version of the library, and install it somewhere on your system path (where, exactly, depends highly on how you install python originally, and other factors). That library is going to sit alongside every other python library you have ever installed (probably a lot of them) in your site-packages directory. This might work okay, until you start a new project, have no idea what version of pandas you had installed, realize that version conflicts with a new library that you need, and then instead of writing code you are spending days trying to untangle the sphaghetti mess of your python install, libraries and their versions. Sound familiar? Virtual environments help to solve this issue without making the all to common situation in the comic above even more complicated.\nA virtual environment is a folder structure which creates a symlink (pointer) to all of the libraries that you specify into the folder. The three main components will be: the python distribution itself, its configuration, and a site-packages directory (where your libraries like pandas live). So the folder is a self contained directory of all the version-specific python software you need for your project.\nVirtual environments are very helpful to create reproducible workflows, and we’ll talk more about this concept of reproducible environements later in the course. Perhaps most importantly though, virtual environments also help you maintain your sanity when python programming. Because they are just folders, you can create and delete new ones at will, without worrying about bungling your underlying python setup.\nIn this course, we are going to use virtualenv as our tool to create and manage virtual environments. Other virtual environment tools used commonly are conda and pipenv. One reason we like using virtualenv is there is an extension to it called virtualenvwrapper, which provides easy to remember wrappers around common virtualenv operations that make creating, activating, and deactivating a virtual environment very easy.\nFirst we will create a .bash_profile file to create variables that point to the install locations of python and virtualenvwrapper. .bash_profile is just a text file that contains bash commands that are run every time you start up a new terminal. Although setting up this file is not required to use virtualenvwrapper, it is convenient because it allows you to set up some reasonable defaults to the commands (meaning less typing, overall), and it makes sure that the package is available every time you start a new terminal.\n\n3.4.0.1 Setup\n\nIn VS Code, select ‘File > New Text File’\nPaste this text into the file:\n\nexport VIRTUALENVWRAPPER_VIRTUAWORKON_HOME=$HOME/.virtualenvs\nsource /usr/share/virtualenvwrapper/virtualenvwrapper.sh\nThe first line points virtualenvwrapper to the directory where your virtual environments will be stored. We point it to a hidden directory (.virtualenvs) in your home directory. The last line sources a bash script that ships with virtualenvwrapper, which makes all of virtualenvwrapper commands available in your terminal session.\n\nSave the file in the top of your home directory as .bash_profile.\nRestart your terminal (Terminal > New Terminal)\nCheck to make sure it was installed and configured correctly by running this in the terminal:\n\nmkvirtualenv --version\nIt should return some content that looks like this (with more output, potentially).\nvirtualenv 20.13.0+ds from /usr/lib/python3/dist-packages/virtualenv/__init__.py\n\n\n3.4.0.2 Course environment\nNow we can create the virtual environment we will use for the course. In the terminal run:\nmkvirtualenv -p python3.9 scomp\nHere, we’ve specified explicitly which python version to use by using the -p flag, and the path to the python 3.9 installation on the server. After making a virtual environment, it will automatically be activated. You’ll see the name of the env you are working in on the left side of your terminal prompt in parentheses. To deactivate your environment (like if you want to work on a different project), just run deactivate. To activate it again, run:\nworkon scomp\nYou can get a list of all available environments by just running:\nworkon\nNow let’s install the dependencies for this course into that environment. To install our libraries we’ll use pip. As of Python 3.4, pip is automatically included with your python installation. pip is a package manager for python, and you might have used it already to install common python libraries like pandas or numpy. pip goes out to PyPI, the Python Package Index, to download the code and put it in your site-packages directory. Note that on this shared server, your user directory will ahve a site-packages directory, in addition to one that our systems administrator manages as the root of the system.\npip install -r requirements.txt\n\n\n3.4.0.3 Installing locally (optional)\nvirtualenvwrapper was already installed on the server we are working on. To install on your local computer, run:\npip3 install virtualenvwrapper\nAnd then follow the instructions as described above, making sure that you have the correct paths set when you edit your .bash_profile."
  },
  {
    "objectID": "sections/python-intro.html#brief-overview-of-python-syntax",
    "href": "sections/python-intro.html#brief-overview-of-python-syntax",
    "title": "3  Python Programming on Clusters",
    "section": "3.5 Brief overview of python syntax",
    "text": "3.5 Brief overview of python syntax\nWe’ll very briefly go over some basic python syntax and the base variable types. First, open a python script. From the File menu, select New File, type “python”, then save it as ‘python-intro.py’ in the top level of your directory.\nIn your file, assign a value to a variable using = and print the result.\n\nx = 4\nprint(x)\n\n4\n\n\nTo run this code in python we can:\n\nexecute python python-intro.py in the terminal\nclick the Play button in the upper right hand corner of the file editor\nright click any line and select: “Run to line in interactive terminal”\n\nIn that interactive window you can then run python code interactively, which is what we’ll use for the next bit of exploring data types.\nThere are 5 standard data types in python\n\nNumber (int, long, float, complex)\nString\nList\nTuple\nDictionary\n\nWe already saw a number type, here is a string:\n\nstr = 'Hello World!'\nprint(str)\n\nHello World!\n\n\nLists in python are very versatile, and are created using square brackets []. Items in a list can be of different data types.\n\nlist = [100, 50, -20, 'text']\nprint(list)\n\n[100, 50, -20, 'text']\n\n\nYou can access items in a list by index using the square brackets. Note indexing starts with 0 in python. The slice operator enables you to easily access a portion of the list without needing to specify every index.\n\nlist[0] # print first element\nlist[1:3] # print 2nd until 4th elements\nlist[:2] # print first until the 3rd\nlist[2:] # print last elements from 3rd\n\n100\n\n\n[50, -20]\n\n\n[100, 50]\n\n\n[-20, 'text']\n\n\nThe + and * operators work on lists by creating a new list using either concatenation (+) or repetition (*).\n\nlist2 = ['more', 'things']\n\nlist + list2\nlist * 3\n\n[100, 50, -20, 'text', 'more', 'things']\n\n\n[100, 50, -20, 'text', 100, 50, -20, 'text', 100, 50, -20, 'text']\n\n\nTuples are similar to lists, except the values cannot be changed in place. They are constructed with parentheses.\n\ntuple = ('a', 'b', 'c', 'd')\ntuple[0]\ntuple * 3\ntuple + tuple\n\n'a'\n\n\n('a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'a', 'b', 'c', 'd')\n\n\n('a', 'b', 'c', 'd', 'a', 'b', 'c', 'd')\n\n\nObserve the difference when we try to change the first value. It works for a list:\n\nlist[0] = 'new value'\nlist\n\n['new value', 50, -20, 'text']\n\n\n…and errors for a tuple.\n\ntuple[0] = 'new value'\n\nTypeError: 'tuple' object does not support item assignment\nDictionaries consist of key-value pairs, and are created using the syntax {key: value}. Keys are usually numbers or strings, and values can be any data type.\n\ndict = {'name': ['Jeanette', 'Matt'],\n        'location': ['Tucson', 'Juneau']}\n\ndict['name']\ndict.keys()\n\n['Jeanette', 'Matt']\n\n\ndict_keys(['name', 'location'])\n\n\nTo determine the type of an object, you can use the type() method.\n\ntype(list)\ntype(tuple)\ntype(dict)\n\nlist\n\n\ntuple\n\n\ndict"
  },
  {
    "objectID": "sections/python-intro.html#jupyter-notebooks",
    "href": "sections/python-intro.html#jupyter-notebooks",
    "title": "3  Python Programming on Clusters",
    "section": "3.6 Jupyter notebooks",
    "text": "3.6 Jupyter notebooks\nTo create a new notebook, from the file menu select File > New File > Jupyter Notebook. Go ahead and save this notebook at the top level of your scalable-computing-examples directory.\nAt the top of your notebook, add a first level header using a single hash. Practice some markdown text by creating:\n\na list\nbold text\na link\n\nUse the Markdown cheat sheet if needed.\nYou can click the plus button below any chunk to add a chunk of either markdown or python.\n\n3.6.1 Load libraries\nIn your first code chunk, lets load in some modules. We’ll use pandas, numpy, matplotlib.pyplot, requests, skimpy, and exists from os.path.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport urllib\nimport skimpy\nimport os\n\nA note on style: There are a few ways to construct import statements. The above code uses three of the most common:\nimport module\nimport module as m\nfrom module import function\nThe first way of importing will make the module a function comes from more explicitly clear, and is the simplest. However for very long module names, or ones that are used very frequently (like pandas, numpy, and matplotlib.plot), the code in the notebook will be more cluttered with constant calls to longer module names. So module.function() instead is written as m.function()\nThe second way of importing a module is a good style to use in cases where modules are used frequently, or have extremely long names. If you import every single module with a short name, however, you might have a hard time remembering which modules are named what, and it might be more confusing for others trying to read your code. Many of the most commonly used libraries for python data science have community-driven styling for how they are abbreviated in import statements, and these community norms are generally best followed.\nFinally, the last way to import a single object from a module can be helpful if you only need that one piece from a larger module, but again, like the first case, results in less explicit code and therefore runs the risk of your or someone else misremembering the usage and source.\n\n\n3.6.2 Read in a csv\nCreate a new code chunk that will download the csv that we are going to use for this tutorial.\n\nNavigate to Rohi Muthyala, Åsa Rennermalm, Sasha Leidman, Matthew Cooper, Sarah Cooley, et al. 2022. 62 days of Supraglacial streamflow from June-August, 2016 over southwest Greenland. Arctic Data Center. doi:10.18739/A2XW47X5F.\nRight click the download button for ‘Discharge_timeseries.csv’\nClick ‘copy link address’\n\nCreate a variable called URL and assign it the link copied to your clipboard. Then use urllib.request.urlretrieve to download the file, and open to write it to disk, to a directory called data/. We’ll write this bundled in an if statement so that we only download the file if it doesn’t yet exist. First, we create the directory if it doesn’t exist:\n\nif not os.path.exists ('data/'):\n        os.mkdir('data/')\n\n\nif not os.path.exists('data/discharge_timeseries.csv'):\n\n        url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3Ae248467d-e1f9-4a32-9e38-a9b4fb17cefb'\n\n        msg = urllib.request.urlretrieve(url, 'data/discharge_timeseries.csv')\n\nNow we can read in the data from the file.\n\ndf = pd.read_csv('data/discharge_timeseries.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Total Pressure [m]\n      Air Pressure [m]\n      Stage [m]\n      Discharge [m3/s]\n      temperature [degrees C]\n    \n  \n  \n    \n      0\n      6/13/2016 0:00\n      9.816\n      9.609775\n      0.206225\n      0.083531\n      -0.1\n    \n    \n      1\n      6/13/2016 0:05\n      9.810\n      9.609715\n      0.200285\n      0.077785\n      -0.1\n    \n    \n      2\n      6/13/2016 0:10\n      9.804\n      9.609656\n      0.194344\n      0.072278\n      -0.1\n    \n    \n      3\n      6/13/2016 0:15\n      9.800\n      9.609596\n      0.190404\n      0.068756\n      -0.1\n    \n    \n      4\n      6/13/2016 0:20\n      9.793\n      9.609537\n      0.183463\n      0.062804\n      -0.1\n    \n  \n\n\n\n\nThe column names are a bit messy so we can use clean_columns from skimpy to make them cleaner for programming very quickly. We can also use the skim function to get a quick summary of the data.\n\n\nclean_df = skimpy.clean_columns(df)\nskimpy.skim(clean_df)\n\n6 column names have been cleaned\n\n\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 17856  │ │ float64     │ 5     │                                                          │\n│ │ Number of columns │ 6      │ │ string      │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name              ┃ NA  ┃ NA %    ┃ mean     ┃ sd     ┃ p0        ┃ p25    ┃ p75   ┃ p100  ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━┩  │\n│ │ total_pressure_m         │   0 │       0 │      9.9 │   0.12 │       9.6 │    9.8 │    10 │    10 │ ▁▅█▇▅▂ │  │\n│ │ air_pressure_m           │   0 │       0 │      9.6 │   0.06 │       9.5 │    9.6 │   9.7 │   9.7 │ ▂▅▄▆█▃ │  │\n│ │ stage_m                  │   0 │       0 │     0.28 │   0.12 │   0.00056 │   0.17 │  0.37 │  0.56 │ ▁█▇▇▆▁ │  │\n│ │ discharge_m_3_s          │   0 │       0 │     0.22 │   0.19 │   4.7e-08 │  0.055 │  0.35 │  0.96 │  █▄▃▁  │  │\n│ │ temperature_degrees_     │   8 │   0.045 │   -0.034 │  0.053 │      -0.1 │   -0.1 │     0 │   0.2 │   ▅█   │  │\n│ └──────────────────────────┴─────┴─────────┴──────────┴────────┴───────────┴────────┴───────┴───────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name               ┃ NA      ┃ NA %       ┃ words per row                ┃ total words              ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ date                      │       0 │          0 │                            2 │                    36000 │  │\n│ └───────────────────────────┴─────────┴────────────┴──────────────────────────────┴──────────────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nWe can see that the date column is classed as a string, and not a date, so let’s fix that.\n\n\nclean_df['date'] = pd.to_datetime(clean_df['date'])\nskimpy.skim(clean_df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 17856  │ │ float64     │ 5     │                                                          │\n│ │ Number of columns │ 6      │ │ datetime64  │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name              ┃ NA  ┃ NA %    ┃ mean     ┃ sd     ┃ p0        ┃ p25    ┃ p75   ┃ p100  ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━┩  │\n│ │ total_pressure_m         │   0 │       0 │      9.9 │   0.12 │       9.6 │    9.8 │    10 │    10 │ ▁▅█▇▅▂ │  │\n│ │ air_pressure_m           │   0 │       0 │      9.6 │   0.06 │       9.5 │    9.6 │   9.7 │   9.7 │ ▂▅▄▆█▃ │  │\n│ │ stage_m                  │   0 │       0 │     0.28 │   0.12 │   0.00056 │   0.17 │  0.37 │  0.56 │ ▁█▇▇▆▁ │  │\n│ │ discharge_m_3_s          │   0 │       0 │     0.22 │   0.19 │   4.7e-08 │  0.055 │  0.35 │  0.96 │  █▄▃▁  │  │\n│ │ temperature_degrees_     │   8 │   0.045 │   -0.034 │  0.053 │      -0.1 │   -0.1 │     0 │   0.2 │   ▅█   │  │\n│ └──────────────────────────┴─────┴─────────┴──────────┴────────┴───────────┴────────┴───────┴───────┴────────┘  │\n│                                                    datetime                                                     │\n│ ┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name        ┃ NA    ┃ NA %     ┃ first            ┃ last                           ┃ frequency      ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩  │\n│ │ date               │     0 │        0 │    2016-06-13    │      2016-08-13 23:55:00       │ 5T             │  │\n│ └────────────────────┴───────┴──────────┴──────────────────┴────────────────────────────────┴────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nIf we wanted to calculate the daily mean flow (as opposed to the flow every 5 minutes), we need to:\n\ncreate a new column with only the date\ngroup by that variable\nsummarize over it by taking the mean\n\nFirst we should probably rename our existing date/time column to prevent from getting confused.\n\nclean_df = clean_df.rename(columns = {'date': 'datetime'})\n\nNow create the new date column\n\nclean_df['date'] = clean_df['datetime'].dt.date\n\nFinally, we use group by to split the data into groups according to the date. We can then apply the mean method to calculate the mean value across all of the columns. Note that there are other methods you can use to calculate different statistics across different columns (eg: clean_df.groupby('date').agg({'discharge_m_3_s': 'max'})).\n\ndaily_flow = clean_df.groupby('date', as_index = False).mean(numeric_only = True)\n\n\ncreate a simple plot\n\n\nvar = 'discharge_m_3_s'\nvar_labs = {'discharge_m_3_s': 'Total Discharge'}\n\nfig, ax = plt.subplots(figsize=(7, 7))\nplt.plot(daily_flow['date'], daily_flow[var])\nplt.xticks(rotation = 45)\nax.set_ylabel(var_labs.get('discharge_m_3_s'))\n\n(array([16967., 16974., 16983., 16990., 16997., 17004., 17014., 17021.,\n        17028.]),\n [Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, '')])\n\n\nText(0, 0.5, 'Total Discharge')"
  },
  {
    "objectID": "sections/python-intro.html#functions",
    "href": "sections/python-intro.html#functions",
    "title": "3  Python Programming on Clusters",
    "section": "3.7 Functions",
    "text": "3.7 Functions\nThe plot we made above is great, but what if we wanted to make it for each variable? We could copy paste it and replace some things, but this violates a core tenet of programming: Don’t Repeat Yourself! Instead, we’ll create a function called myplot that accepts the data frame and variable as arguments.\n\ncreate myplot.py\n\n\nimport matplotlib.pyplot as plt\n\ndef myplot(df, var):\n\n        var_labs = {'discharge_m_3_s': 'Total Discharge (m^3/s)',\n                    'total_pressure_m': 'Total Pressure (m)',\n                    'air_pressure_m': 'Air Pressure (m)',\n                    'stage_m': 'Stage (m)',\n                    'temperature_degrees_c': 'Temperature (C)'}\n\n        fig, ax = plt.subplots(figsize=(7, 7))\n        plt.plot(df['date'], df[var])\n        plt.xticks(rotation = 45)\n        ax.set_ylabel(var_labs.get(var))\n\n\nload myplot into jupyter notebook (from myplot import myplot)\nadd libraries\nreplace old plot method with new function\n\n\nmyplot(daily_flow, 'temperature_degrees_c')\n\n\n\n\nWe’ll have more on functions in the software design sections."
  },
  {
    "objectID": "sections/python-intro.html#summary",
    "href": "sections/python-intro.html#summary",
    "title": "3  Python Programming on Clusters",
    "section": "3.8 Summary",
    "text": "3.8 Summary\nIn this lesson we learned all about virtual environments, how to use them, and why. We got our environments set up for the course, did a brief python syntax review, and then jumped into Jupyter notebooks, pandas, and writing functions."
  },
  {
    "objectID": "sections/parallel-programming.html#learning-objectives",
    "href": "sections/parallel-programming.html#learning-objectives",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.1 Learning Objectives",
    "text": "4.1 Learning Objectives\n\nUnderstand what parallel computing is and when it may be useful\nUnderstand how parallelism can work\nReview sequential loops and map functions\nBuild a parallel program using concurrent.futures\nBuild a parallel program using parsl\nUnderstand Thread Pools and Process pools"
  },
  {
    "objectID": "sections/parallel-programming.html#introduction",
    "href": "sections/parallel-programming.html#introduction",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nProcessing large amounts of data with complex models can be time consuming. New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America:\n\n\n\nLevy et al. 2016. doi:10.5063/F1Z899CZ\n\n\nThere are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization.\nAlternatively, think of remote sensing data. Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years.\n\n\n\nNEON Data Cube"
  },
  {
    "objectID": "sections/parallel-programming.html#why-parallelism",
    "href": "sections/parallel-programming.html#why-parallelism",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.3 Why parallelism?",
    "text": "4.3 Why parallelism?\nMuch R code runs fast and fine on a single processor. But at times, computations can be:\n\ncpu-bound: Take too much cpu time\nmemory-bound: Take too much memory\nI/O-bound: Take too much time to read/write from disk\nnetwork-bound: Take too much time to transfer\n\nTo help with cpu-bound computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire set of those processors. Plus, these machines also have large amounts of memory to avoid memory-bound computing jobs."
  },
  {
    "objectID": "sections/parallel-programming.html#processors-cpus-cores-and-threads",
    "href": "sections/parallel-programming.html#processors-cpus-cores-and-threads",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.4 Processors (CPUs), Cores, and Threads",
    "text": "4.4 Processors (CPUs), Cores, and Threads\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with mutliple processors, each of which in turn can contain multiple cores. These processors and cores are available to perform computations. But, just what’s the difference between processors and cores? A computer with one processor may still have 4 cores (quad-core), allowing 4 (or possibly more) computations to be executed at the same time.\n\n\nMicroprocessor: an integrated circuit that contains the data processing logic and control for a computer.\nMulti-core processor: a microprocessor containing multiple processing units (cores) on a single integrated circuit. Each core in a multi-core processor can execute program instructions at the same time.\nProcess: an instance of a computer program (including instructions, memory, and other resources) that is executed on a microprocessor.\nThread: a thread of execution is the smallest sequence of program instructions that can be executed independently, and is typically a component of a process. The threads in a process can be executed concurrently and typically share the same memory space. They are faster to create than a process.\nCluster: a set of multiple, physically distinct computing systems, each with its own microprocessors, memory, and storage resources, connected together by a (fast) network that allows the nodes to be viewed as a single system.\n\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here we show four quad-core processors for a total of 16 cores in this machine.\n\nYou can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later).\nHistorically, many languages only utilized one processor, which makes them single-threaded. Which is a shame, because the 2019 MacBook Pro that I am writing this on is much more powerful than that, and has mutliple cores that would support concurrent execution of multiple threads:\njones@powder:~$ sysctl hw.ncpu hw.physicalcpu\nhw.ncpu: 12\nhw.physicalcpu: 6\nTo interpret that output, this machine powder has 6 physical CPUs, each of which has two processing cores, for a total of 12 cores for computation. I’d sure like my computations to use all of that processing power. Because its all on one machine, we can easily use multicore processing tools to make use of those cores. Now let’s look at the computational server included-crab at NCEAS:\njones@included-crab:~$ lscpu | egrep 'CPU\\(s\\)|per core|per socket' \nCPU(s):                          88\nOn-line CPU(s) list:             0-87\nThread(s) per core:              1\nCore(s) per socket:              1\nNUMA node0 CPU(s):               0-87\nNow that’s more compute power! included-crab has 384 GB of RAM, and ample storage. All still under the control of a single operating system.\nFinally, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now:\n\n\n\nStampede2 at TACC\n\n4200 KNL nodes: 285,600 cores\n1736 SKX nodes: 83,328 cores\n224 ICX nodes: 17,920 cores\nTOTAL: 386,848 cores\n\nDelta at NCSA\n\n124 CPU Milan nodes (15,872 cores)\n100 quad A100 GPU nodes (6400 cores + 400 GPUs)\n100 quad A40 GPU nodes (6400 cores + 400 GPUs)\n5 eight-way A100 GPU nodes (640 cores + 40 GPUs):\n1 MI100 GPU node (128 cores + 8 GPUs)\n7 PB of disk-based Lustre storage\n3 PB of flash based storage\nTOTAL: 29,440 cores, 848 gpus\n\n\n\n\n\n\n\nDelta Supercomputer\n\n\n\n\nNote that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster. One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine. But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster."
  },
  {
    "objectID": "sections/parallel-programming.html#parallel-processing-in-the-shell",
    "href": "sections/parallel-programming.html#parallel-processing-in-the-shell",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.5 Parallel processing in the shell",
    "text": "4.5 Parallel processing in the shell\nShell programming helps massively speed up data management tasks, and even more so with simple use of the GNU parallel utility to execute bash commands in parallel. In its simplest form, this can be used to speed up common file operations, such as renaming, compression, decompression, and file transfer. Let’s look at a common example – calculating checksums to verify file integrity for data files. Calculating a hash checksum using the shasum command can be time consuming, especially when you have a lot of large files to work on. But it is a classic processor-limited task, and one that can be massively faster using parallel.\n$ for fn in `ls *.gpkg`; do shasum -a 256 ${fn}; done\n\nreal    35.081s\nuser    32.745s\nsystem  2.336s\n\n$ ls *.gpkg | parallel \"shasum -a 256 {}\"\n\nreal    2.97s \nuser    37.16s \nsystem  2.70s\nThe first invocation takes 35 seconds to execute the tasks one at a time serially, while the second version only takes 🎉 3 seconds 🎉 to do the same tasks. Note that the computational time spent in user and system processing is about the same, with the major difference being that the user-space tasks were conducted on multiple cores in parallel, resulting in more than 10x faster performance, Using htop, you can see processor cores spiking in usage when the command is run:\n\n\n\nProcessor usage for parallel tasks."
  },
  {
    "objectID": "sections/parallel-programming.html#modes-of-parallelization",
    "href": "sections/parallel-programming.html#modes-of-parallelization",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.6 Modes of parallelization",
    "text": "4.6 Modes of parallelization\nSeveral different approaches can be taken to structuring a computer program to take advantage of the hardware capabilities of multi-core processors. In the typical, and simplest, case, each task in a computation is executed serially in order of first to last. The total computation time is the sum of the time of all of the subtasks that are executed. In the next figure, a single core of the processor is used to sequentially execute each of the five tasks, with time flowing from left to right.\n\n\n\nSerial and parallel execution of tasks using threads and processes.\n\n\nIn comparison, the middle panel shows two approaches to parallelization on a single computer: Parallel Threads and Parallel Processes. With multi-threaded execution, a separate thread of execution is created for each of the 5 tasks, and these are executed concurrently on 5 of the cores of the processor. All of the threads are in the same process and share the same memory and resources, so one must take care that they do not interfere with each other.\nWith multi-process execution, a separate process is created for each of the 5 tasks, and these are executed concurrently on the cores of the processor. The difference is that each process has it’s own copy of the program memory, and changes are merged when each child process completes. Because each child process must be created and resources for that process must be marshalled and unmarshalled, there is more overhead in creating a process than a thread. “Marshalling” is the process of transforming the memory representation of an object into another format, which allows communication between remote objects by converting an object into serialized form.\nFinally, cluster parallel execution is shown in the last panel, in which a cluster with multiple computers is used to execute multiple processes for each task. Again, there is a setup task associated with creating and mashaling resources for the task, which now includes the overhead of moving data from one machine to the others in the cluster over the network. This further increases the cost of creating and executing multiple processes, but can be highly advantageous when accessing exceedingly large numbers of processing cores on clusters.\nThe key to performance gains is to ensure that the overhead associated with creating new threads or processes is small relative to the time it takes to perform a task. Somewhat unintuitively, when the setup overhead time exceeds the task time, parallel execution will likely be slower than serial."
  },
  {
    "objectID": "sections/parallel-programming.html#task-parallelization-in-python",
    "href": "sections/parallel-programming.html#task-parallelization-in-python",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.7 Task parallelization in Python",
    "text": "4.7 Task parallelization in Python\nPython also provides a number of easy to use packages for concurrent processing. We will review two of these, concurrent.futures and parsl, to show just how easy it can be to parallelize your programs. concurrent.futures is built right into the python3 release, and is a great starting point for learning concurrency.\nWe’re going to start with a task that is a little expensive to compute, and define it in a function. All this task(x) function does is to use numpy to create a fairly large range of numbers, and then sum them.\n\ndef task(x):\n    import numpy as np\n    result = np.arange(x*10**8).sum()\n    return result\n\nWe can start by executing this task function serially ten times with varying inputs. In this case, we create a function run_serial that takes a list of inputs to be run, and it calls the task function for each of those inputs. The @timethis decorator is a simple way to wrap the function with timing code so that we can see how long it takes to execute.\n\nimport numpy as np\n\n@timethis\ndef run_serial(task_list):\n    return [task(x) for x in task_list]\n\nrun_serial(np.arange(10))\n\nrun_serial: 160920.7260608673 ms\n\n\n[0,\n 4999999950000000,\n 19999999900000000,\n 44999999850000000,\n 79999999800000000,\n 124999999750000000,\n 179999999700000000,\n 244999999650000000,\n 319999999600000000,\n 404999999550000000]\n\n\nIn this case, it takes around 25 seconds to execute 10 tasks, depending on what else is happening on the machine and network.\nSo, can we make this faster using a multi-threaded parallel process? Let’s try with concurrent.futures. The main concept in this package is one of a future, which is a structure which represents the value that will be created in a computation in the future when the function completes execution. With concurrent.futures, tasks are scheduled and do not block while they await their turn to be executed. Instead, threads are created and executed asynchronously, meaning that the function returns it’s future potentially before the thread has actually been executed. Using this approach, the user schedules a series of tasks to be executed asynchronously, and keeps track of the futures for each task. When the future indicates that the execution has been completed, we can then retrieve the result of the computation.\nIn practice this is a simple change from our serial implementation. We will use the ThreadPoolExecutor to create a pool of workers that are available to process tasks. Each worker is set up in its own thread, so it can execute in parallel with other workers. After setting up the pool of workers, we use concurrent.futures map() to schedule each task from our task_list (in this case, an input value from 1 to 10) to run on one of the workers. As for all map() implementations, we are asking for each value in task_list to be executed in the task function we defined above, but in this case it will be executed using one of the workers from the executor that we created.\n\nfrom concurrent.futures import ThreadPoolExecutor\n\n@timethis\ndef run_threaded(task_list):\n    with ThreadPoolExecutor(max_workers=20) as executor:\n        return executor.map(task, task_list)\n\nresults = run_threaded(np.arange(10))\n[x for x in results]\n\nrun_threaded: 4440.109491348267 ms\n[0,\n 4999999950000000,\n 19999999900000000,\n 44999999850000000,\n 79999999800000000,\n 124999999750000000,\n 179999999700000000,\n 244999999650000000,\n 319999999600000000,\n 404999999550000000]\nThis execution took about 🎉 4 seconds 🎉, which is about 6.25x faster than serial. Congratulations, you wrote your a multi-threaded python program!"
  },
  {
    "objectID": "sections/parallel-programming.html#exercise-parallel-downloads",
    "href": "sections/parallel-programming.html#exercise-parallel-downloads",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.8 Exercise: Parallel downloads",
    "text": "4.8 Exercise: Parallel downloads\nIn this exercise, we’re going to parallelize a simple task that is often very time consuming – downloading data. And we’ll compare performance of simple downloads using first a serial loop, and then using two parallel execution libraries: concurrent.futures and parsl. We’re going to see an example here where parallel execution won’t always speed up this task, as this is likely an I/O bound task if you’re downloading a lot of data. But we still should be able to speed things up a lot until we hit the limits of our disk arrays.\nThe data we are downloading is a pan-Arctic time series of TIF images containing rasterized Arctic surface water indices from:\n\nElizabeth Webb. 2022. Pan-Arctic surface water (yearly and trend over time) 2000-2022. Arctic Data Center doi:10.18739/A2NK3665N.\n\n\n\n\nWebb surface water index data\n\n\nFirst, let’s download the data serially to set a benchmark. The data files are listed in a table with their filename and identifier, and can be downloaded directly from the Arctic Data Center using their identifier. To make things easier, we’ve already provided a data frame with the names and identifiers of each file that could be downloaded.\n\n\n\n\n\n\n  \n    \n      \n      filename\n      identifier\n    \n  \n  \n    \n      0\n      SWI_2007.tif\n      urn:uuid:5ee72c9c-789d-4a1c-95d8-cb2b24a20662\n    \n    \n      1\n      SWI_2019.tif\n      urn:uuid:9cd1cdc3-0792-4e61-afff-c11f86d3a9be\n    \n    \n      2\n      SWI_2021.tif\n      urn:uuid:14e1e509-77c0-4646-9cc3-d05f8d84977c\n    \n    \n      3\n      SWI_2020.tif\n      urn:uuid:1ba473ff-8f03-470b-90d1-7be667995ea1\n    \n    \n      4\n      SWI_2001.tif\n      urn:uuid:85150557-05fd-4f52-8bbd-ec5a2c27e23d\n    \n  \n\n\n\n\n\n4.8.1 Serial\nWhen you have a list of repetitive tasks, you may be able to speed it up by adding more computing power. If each task is completely independent of the others, then it is pleasingly parallel and a prime candidate for executing those tasks in parallel, each on its own core. For example, let’s build a simple loop that downloads the data files that we need for an analysis. First, we start with the serial implementation.\n\nimport urllib\n\ndef download_file(row):\n    service = \"https://arcticdata.io/metacat/d1/mn/v2/object/\"\n    pid = row[1]['identifier']\n    filename = row[1]['filename']\n    url = service + pid\n    print(\"Downloading: \" + filename)\n    msg = urllib.request.urlretrieve(url, filename)\n    return filename\n\n@timethis\ndef download_serial(table):\n    results = [download_file(row) for row in table.iterrows()]\n    return results\n    \nresults = download_serial(file_table[0:5])\nprint(results)\n\nDownloading: SWI_2007.tif\n\n\nDownloading: SWI_2019.tif\n\n\nDownloading: SWI_2021.tif\n\n\nDownloading: SWI_2020.tif\n\n\nDownloading: SWI_2001.tif\n\n\ndownload_serial: 73280.58290481567 ms\n['SWI_2007.tif', 'SWI_2019.tif', 'SWI_2021.tif', 'SWI_2020.tif', 'SWI_2001.tif']\n\n\nIn this code, we have one function (download_file) that downloads a single data file and saves it to disk. It is called iteratively from the function download_serial. The serial execution takes about 20-25 seconds, but can vary considerably based on network traffic and other factors.\nThe issue with this loop is that we execute each download task sequentially, which means that only one of our processors on this machine is in use. In order to exploit parallelism, we need to be able to dispatch our tasks and allow each to run at the same time, with one task going to each core. To do that, we can use one of the many parallelization libraries in python to help us out.\n\n\n4.8.2 Multi-threaded with concurrent.futures\nIn this case, we’ll use the same download_file function from before, but let’s switch up and create a download_threaded() function to use concurrent.futures with a ThreadPoolExecutor just as we did earlier.\n\nfrom concurrent.futures import ThreadPoolExecutor\n\n@timethis\ndef download_threaded(table):\n    with ThreadPoolExecutor(max_workers=15) as executor:\n        results = executor.map(download_file, table.iterrows(), timeout=60)\n        return results\n\nresults = download_threaded(file_table[0:5])\nfor result in results:\n    print(result)\n\nDownloading: SWI_2007.tif\nDownloading: SWI_2019.tif\nDownloading: SWI_2021.tif\nDownloading: SWI_2020.tif\nDownloading: SWI_2001.tif\n\n\ndownload_threaded: 44622.36833572388 ms\nSWI_2007.tif\nSWI_2019.tif\nSWI_2021.tif\nSWI_2020.tif\nSWI_2001.tif\n\n\nNote how the “Downloading…” messages were printed immediately, but then it still took over 20 seconds to download the 5 files. This could be for several reasons, including that one of the files alone took that long (e.g., due to network congestion), or that there was a bottleneck in writing the files to disk (i.e., we could have been disk I/O limited). Or maybe the multithreaded executor pool didn’t do a good job parallelizing the tasks. The trick is figuring out why you did or didn’t get a speedup when parallelizing. So, let’s try this another way, using a multi-processing approach, rather than multi-threading.\n\n\n4.8.3 Multi-process with concurrent.futures\nYou’ll remember from earlier that you can execute tasks concurrently by creating multiple threads within one process (multi-threaded), or by creating and executing muliple processes. The latter creates more independence, as each of the executing tasks has their own memory and process space, but it also takes longer to set up. With concurrent.futures, we can switch to a multi-process pool by using a ProcessPoolExecutor, analogously to how we used ThreadPoolExecutor previously. So, some simple changes, and we’re running multiple processes.\n\nfrom concurrent.futures import ProcessPoolExecutor\n\n@timethis\ndef download_process(table):\n    with ProcessPoolExecutor(max_workers=15) as executor:\n        results = executor.map(download_file, table.iterrows(), timeout=60)\n        return results\n\nresults = download_process(file_table[0:5])\nfor result in results:\n    print(result)\n\nDownloading: SWI_2020.tif\n\n\nDownloading: SWI_2019.tif\n\n\nDownloading: SWI_2001.tif\n\n\nDownloading: SWI_2021.tif\n\n\nDownloading: SWI_2007.tif\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndownload_process: 39312.90817260742 ms\nSWI_2007.tif\nSWI_2019.tif\nSWI_2021.tif\nSWI_2020.tif\nSWI_2001.tif\n\n\nAgain, the output messages print almost immediately, but then later the processes finish and report that it took between 10 to 15 seconds to run. Your mileage may vary. When I increase the number of files being downloaded to 10 or even to 20, I notice it is actually about the same, around 10-15 seconds. So, part of our time now is the overhead of setting up multiple processes. But once we have that infrastructure in place, we can make effective euse of the pool of processes to handle many downloads."
  },
  {
    "objectID": "sections/parallel-programming.html#parallel-processing-with-parsl",
    "href": "sections/parallel-programming.html#parallel-processing-with-parsl",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.9 Parallel processing with parsl",
    "text": "4.9 Parallel processing with parsl\nconcurrent.futures is great and powerful, but it has its limits. Particularly as you try to scale up into the thousands of concurrent tasks, other libraries like Parsl (docs), Dask, Ray, and others come into play. They all have their strengths, but Parsl makes it particularly easy to build parallel workflows out of existing python code through it’s use of decorators on existing python functions.\nIn addition, Parsl supports a lot of different kinds of providers, allowing the same python code to be easily run multi-threaded using a ThreadPoolExecutor and via multi-processing on many different cluster computing platforms using the HighThroughputExecutor. For example, Parsl includes providers supporting local execution, and on Slurm, Condor, Kubernetes, AWS, and other platforms. And Parsl handles data staging as well across these varied environments, making sure the data is in the right place when it’s needed for computations.\nSimilarly to before, we start by configuring an executor in parsl, and loading it. We’ll use multiprocessing by configuring the HighThroughputExecutor to use our local resources as a cluster, and we’ll activate our virtual environment to be sure we’re executing in a consistent environment.\n\n# Required packages\nimport parsl\nfrom parsl import python_app\nfrom parsl.config import Config\nfrom parsl.executors import HighThroughputExecutor\nfrom parsl.providers import LocalProvider\n\n# Configure the parsl executor\nactivate_env = 'workon scomp'\nhtex_local = Config(\n    executors=[\n        HighThroughputExecutor(\n            max_workers=15,\n            provider=LocalProvider(\n                worker_init=activate_env\n            )\n        )\n    ],\n)\nparsl.clear()\nparsl.load(htex_local)\n\n<parsl.dataflow.dflow.DataFlowKernel at 0x7f4297ebb580>\n\n\nWe now have a live parsl executor (htex_local) that is waiting to execute processes. We tell it to execute processes by annotating functions with decorators that indicate which tasks should be parallelized. Parsl then handles the scheduling and execution of those tasks based on the dependencies between them. In the simplest case, we’ll decorate our previous function for downloading a file with the @python_app decorator, which tells parsl that any function calls with this function should be run on the default executor (in this case, htex_local).\n\n# Decorators seem to be ignored as the first line of a cell, so print something first\nprint(\"Create decorated function\")\n\n@python_app\ndef download_file_parsl(row):\n    import urllib\n    service = \"https://arcticdata.io/metacat/d1/mn/v2/object/\"\n    pid = row[1]['identifier']\n    filename = row[1]['filename']\n    url = service + pid\n    print(\"Downloading: \" + filename)\n    msg = urllib.request.urlretrieve(url, filename)\n    return filename\n\nCreate decorated function\n\n\nNow we just write regular python code that calls that function, and parsl handles the scheduling. Parsl app executors return an AppFuture, and we can call the AppFuture.done() function to determine when the future result is ready without blocking. Or, we can just block on AppFuture.result() which waits for each of the executions to complete and then returns the result.\n\n#! eval: true\nprint(\"Define our download_futures function\")\n\n@timethis\ndef download_futures(table):\n    results = []\n    for row in table.iterrows():\n        result = download_file_parsl(row)\n        print(result)\n        results.append(result)\n    return(results)\n\n@timethis\ndef wait_for_futures(table):\n    results = download_futures(table)\n    done = [app_future.result() for app_future in results]\n    print(done)\n\nwait_for_futures(file_table[0:5])\n\nDefine our download_futures function\n<AppFuture at 0x7f42d84f33d0 state=pending>\n<AppFuture at 0x7f42963cb820 state=pending>\n<AppFuture at 0x7f42963cbd30 state=pending>\n<AppFuture at 0x7f42963d9280 state=pending>\n<AppFuture at 0x7f42963d9790 state=pending>\ndownload_futures: 21.121501922607422 ms\n\n\n['SWI_2007.tif', 'SWI_2019.tif', 'SWI_2021.tif', 'SWI_2020.tif', 'SWI_2001.tif']\nwait_for_futures: 25390.665531158447 ms\n\n\nWhen we’re done, be sure to clean up and shutdown the htex_local executor, or it will continue to persist in your environment and utilize resources. Generally, an executor should be created when setting up your environment, and then it can be used repeatedly for many different tasks.\n\nhtex_local.executors[0].shutdown()\nparsl.clear()"
  },
  {
    "objectID": "sections/parallel-programming.html#when-to-parallelize",
    "href": "sections/parallel-programming.html#when-to-parallelize",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.10 When to parallelize",
    "text": "4.10 When to parallelize\nIt’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!\nIn addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced. Some propose that this may follow Amdahl’s Law, where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines):\n\n\n\nAmdahl’s Law\n\n\nSo, it is important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done."
  },
  {
    "objectID": "sections/parallel-programming.html#parallel-pitfalls",
    "href": "sections/parallel-programming.html#parallel-pitfalls",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.11 Parallel Pitfalls",
    "text": "4.11 Parallel Pitfalls\nA set of tasks is considered ‘pleasingly parallel’ when large portions of the code can be executed indpendently of the other portions and have few or no dependencies on other parts of the execution. This situation is common, and we can frequently execute parallel tasks on independent subsets of our data. Nevertheless, dependencies among different parts of your computation can definitely create bottlenecks and slow down computations. Some of the challenges you may need to work around include:\n\nTask dependencies: occur when one task in the code depends on the results of another task or computation in the code.\nRace conditions: ooccur when two tasks execute in parallel, but produce different results based on which task finishes first. Ensuring that results are correct under different timing situations requires careful testing.\nDeadlocks: occur when two concurrent tasks block on the output of the other. Deadlocks cause parallel programs to lock up indefinitely, and can be difficult to track down.\n\nEven when tasks exhibit strong dependencies, it is frequently possible to still optimize that code by parallelizing explicit code sections, sometimes bringing other concurrency tools into the mix, such as the Message Passing Interface (MPI). But simply improving the efficiency of pleasingly parallel tasks can be liberating."
  },
  {
    "objectID": "sections/parallel-programming.html#summary",
    "href": "sections/parallel-programming.html#summary",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.12 Summary",
    "text": "4.12 Summary\nIn this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores. Next, we reviewed the way in which traditional sequential loops can be rewritten as functions that are applied to a list of inputs both serially and in parallel to utilize multiple cores to speed up computations. We reviewed the challenges of optimizing code, where one must constantly examine the bottlenecks that arise as we improve cpu-bound, I/O bound,and memory bound computations."
  },
  {
    "objectID": "sections/parallel-programming.html#further-reading",
    "href": "sections/parallel-programming.html#further-reading",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.13 Further Reading",
    "text": "4.13 Further Reading\nRyan Abernathey & Joe Hamman. 2020. Closed Platforms vs. Open Architectures for Cloud-Native Earth System Analytics. Medium."
  },
  {
    "objectID": "sections/data-structures-netcdf.html#learning-objectives",
    "href": "sections/data-structures-netcdf.html#learning-objectives",
    "title": "5  Data Structures and Formats for Large Data",
    "section": "5.1 Learning Objectives",
    "text": "5.1 Learning Objectives\n\nLearn about the NetCDF data format:\n\nCharacteristics: self-describing, scalable, portable, appendable, shareable, and archivable\nUnderstand the NetCDF data model: what are dimensions, variables, and attributes\nAdvantages and differences between NetCDF and tabular data formats\n\nLearn how to use the xarray Python package to work with NetCDF files:\n\nDescribe the core xarray data structures, the xarray.DataArray and the xarray.Dataset, and their components, including data variables, dimensions, coordinates, and attributes\nCreate xarray.DataArrays and xarra.DataSets out of raw numpy arrays and save them as netCDF files\nLoad xarray.DataSets from netCDF files and understand the attributes view\nPerform basic indexing, processing, and reduction of xarray.DataArrays\nConvert pandas.DataFrames into xarray.DataSets"
  },
  {
    "objectID": "sections/data-structures-netcdf.html#introduction",
    "href": "sections/data-structures-netcdf.html#introduction",
    "title": "5  Data Structures and Formats for Large Data",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nEfficient and reproducible data analysis begins with choosing a proper format to store our data, particularly when working with large, complex, multi-dimensional datasets. Consider, for example, the following Earth System Data Cube from Mahecha et al. 2020, which measures nine environmental variables at high resolution across space and time. We can consider this dataset large (high-resolution means we have a big file), complex (multiple variables), and multi-dimensional (each variable is measured along three dimensions: latitude, longitude, and time). Additionally, necessary metadata must accompany the dataset to make it functional, such as units of measurement for variables, information about the authors, and processing software used.\n\n\n\nMahecha et al. 2020 . Visualization of the implemented Earth system data cube. The figure shows from the top left to bottom right the variables sensible heat (H), latent heat (LE), gross primary production (GPP), surface moisture (SM), land surface temperature (LST), air temperature (Tair), cloudiness (C), precipitation (P), and water vapour (V). The resolution in space is 0.25° and 8 d in time, and we are inspecting the time from May 2008 to May 2010; the spatial range is from 15° S to 60° N, and 10° E to 65° W.\n\n\nKeeping complex datasets in a format that facilitates access, processing, sharing, and archiving can be at least as important as how we parallelize the code we use to analyze them. In practice, it is common to convert our data from less efficient formats into more efficient ones before we parallelize any processing. In this lesson, we will\n\nfamiliarize ourselves with the NetCDF data format, which enables us to store large, complex, multi-dimensional data efficiently, and\nlearn to use the xarray Python package to read, process, and create NetCDF files."
  },
  {
    "objectID": "sections/data-structures-netcdf.html#netcdf-data-format",
    "href": "sections/data-structures-netcdf.html#netcdf-data-format",
    "title": "5  Data Structures and Formats for Large Data",
    "section": "5.3 NetCDF Data Format",
    "text": "5.3 NetCDF Data Format\nNetCDF (Network Common Data Form) is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. NetCDF was initially developed at the Unidata Program Center, is supported on almost all platforms, and parsers exist for most scientific programming languages.\nThe NetCDF documentation outlines that this data format is desgined to be:\n\n\nSelf-describing: Information describing the data contents of the file is embedded within the data file itself. This means that there is a header describing the layout of the rest of the file and arbitrary file metadata.\nScalable: Small subsets of large datasets may be accessed efficiently through netCDF interfaces, even from remote servers.\nPortable: A NetCDF file is machine-independent i.e. it can be accessed by computers with different ways of storing integers, characters, and floating-point numbers.\nAppendable: Data may be appended to a properly structured NetCDF file without copying the dataset or redefining its structure.\nSharable: One writer and multiple readers may simultaneously access the same NetCDF file.\nArchivable: Access to all earlier forms of NetCDF data will be supported by current and future versions of the software.\n\n\n\n5.3.1 Data Model\nThe NetCDF data model is the way that NetCDF organizes data. This lesson will follow the Classic NetCDF Data Model, which is at the core of all netCDF files. \nThe model consists of three key components: variables, dimensions, and attributes.\n\nVariables are N-dimensional arrays of data. We can think of these as varying/measured/dependent quantities.\nDimensions describe the axes of the data arrays. A dimension has a name and a length. We can think of these as the constant/fixed/independent quantities at which we measure the variables.\nAttributes are small notes or supplementary metadata to annotate a variable or the file as a whole.\n\n\n\n\nClassic NetCDF Data Model (NetCDF documentation)\n\n\n\n\n5.3.2 Metadata Standards\nThe most commonly used metadata standard for geospatial data is the Climate and Forecast metadata standard, also called the CF conventions.\n\nThe CF conventions are specifically designed to promote the processing and sharing of files created with the NetCDF API. Principles of CF include self-describing data (no external tables needed for understanding), metadata equally readable by humans and software, minimum redundancy, and maximum simplicity. (CF conventions FAQ)\n\nThe CF conventions provide a unique standardized name and precise description of over 1,000 physical variables. To maximize the reusability of our data, it is best to include a variable’s standardized name as an attribute called standard_name. Variables should also include a units attribute. This attribute should be a string that can be recognized by UNIDATA’s UDUNITS package. In these links you can find:\n\na table with all of the CF convention’s standard names, and\na list of the units found in the UDUNITS database maintained by the North Carolina Institute for Climate Studies.\n\n\n\n5.3.3 Exercise\nLet’s do a short practice now that we have reviewed the classic NetCDF model and know a bit about metadata best practices.\n\nPart 1\n\nImagine the following scenario: we have a network of 25 weather stations. They are located in a square grid: starting at 30°0′N 60°0′E, there is a station every 10° North and every 10° East. Each station measures the air temperature at a set time for three days, starting on September 1st, 2022. On the first day, all stations record a temperature of 0°C. On the second day, all temperatures are 1°C, and on the third day, all temperatures are 2°C. What are the variables, dimensions and attributes for this data?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nVariables: There is a single variable being measured: temperature. The variable values can be represented as a 5x5x3 array, with constant values for each day.\nDimensions: This dataset has three dimensions: time, latitude, and longitude. Time indicates when the measurement happened, we can encode it as the dates 2022-09-01, 2022-09-02, and 2022-09-03. The pairs of latitude and longitude values indicate the positions of the weather stations. Latitude has values 30, 40, 50, 60, and 70, measured in degrees North. Longitude has values 60, 70, 80, 90, and 100, measured in degrees East.\n\nAttributes: Let’s divide these into attributes for the variable, the dimensions, and the whole dataset:\n\nVariable attributes:\n\nTemperature attributes:\n\nstandard_name: air_temperature\nunits: degree_C\n\n\nDimension attributes:\n\nTime attributes:\n\ndescription: date of measurement\n\nLatitude attributes:\n\nstandard_name: grid_latitude\nunits: degrees_N\n\nLongitude attributes:\n\nstandard_name: grid_longitude\nunits: degree_E\n\n\nDataset attributes:\n\ntitle: Temperature Measurements at Weather Stations\nsummary: an example of NetCDF data format\n\n\n\n\n\n\nPart 2\n\nNow imagine we calculate the average temperature over time at each weather station, and we wish to incorporate this data into the same dataset. How will adding the average temperature data change the dataset’s variables, attributes, and dimensions?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nVariables: Now we are measuring two variables: temperature and average temperature. The temperature data stays the same. We can represent the average temperature as a single 5x5 array with value 1 at each cell.\nDimensions: This dataset still has three dimensions: time, latitude, and longitude. The temperature variable uses all three dimensions, and the average temperature variable only uses two (latitude and longitude). This is ok! The dataset’s dimensions are the union of the dimensions of all the variables in the dataset. Variables in the same dataset may have all, some, or no dimensions in common.\n\nAttributes: To begin with, we need to keep all the previous attributes. Notice that the dataset’s title is general enough that we don’t need to update it. The only update we need to do is add the attributes for our new average temperature variable:\n\nAverage temperature attributes:\n\nstandard_name: average_air_temperature\ndescription: average temperature over three days\n\n\n\n\n\nOur next step is to see how we can translate all this information into something we can store and handle on our computers.\n\n\n\n\n\n\nGoing further with zarr\n\n\n\n\n\nZarr is a file storage format for chunked, compressed N-dimensional arrays based on an open source specification. It is very similar to NetCDF. The key is in the “chunked.” Zarr stores chunks of data in separate files, so that parallel operations (like reading and writing data) can be done much, much faster than working with NetCDF. This format is gaining lots of popularity for working with big data."
  },
  {
    "objectID": "sections/data-structures-netcdf.html#xarray",
    "href": "sections/data-structures-netcdf.html#xarray",
    "title": "5  Data Structures and Formats for Large Data",
    "section": "5.4 xarray",
    "text": "5.4 xarray\nxarray is an open source project and Python package that augments NumPy arrays by adding labeled dimensions, coordinates and attributes. xarray is based on the netCDF data model, making it the appropriate tool to open, process, and create datasets in netCDF format.\n\n\n\n\n\nxarray’s development portal\n\n\n\n5.4.1 xarray.DataArray\nThe xarray.DataArray is the primary data structure of the xarray package. It is an n-dimensional array with labeled dimensions. We can think of it as representing a single variable in the NetCDF data format: it holds the variable’s values, dimensions, and attributes.\nApart from variables, dimensions, and attributes, xarray introduces one more piece of information to keep track of a dataset’s content: in xarray each dimension has at least one set of coordinates. A dimension’s coordinates indicate the dimension’s values. We can think of the coordinate’s values as the tick labels along a dimension. For example, in our previous exercise about temperature measured in weather stations, latitude is a dimension, and the latitude’s coordinates are 30, 40, 50, 60, and 70 because those are the latitude values at which we are collecting temperature data. In that same exercise, time is a dimension, and its coordinates are 2022-09-1, 2022-09-02, and 2022-09-03.\nHere you can read more about the xarray terminology.\n\n5.4.1.1 Create an xarray.DataArray\nLet’s suppose we want to make an xarray.DataArray that includes the information from our previous exercise about measuring temperature across three days. First, we import all the necessary libraries.\n\nimport os              \nimport urllib \nimport pandas as pd\nimport numpy as np\n\nimport xarray as xr   # This is the package we'll explore\n\nVariable Values\nThe underlying data in the xarray.DataArray is a numpy.ndarray that holds the variable values. So we can start by making a numpy.ndarray with our mock temperature data:\n\n# values of a single variable at each point of the coords \ntemp_data = np.array([np.zeros((5,5)), \n                      np.ones((5,5)), \n                      np.ones((5,5))*2]).astype(int)\ntemp_data\n\narray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])\n\n\nWe could think this is “all” we need to represent our data. But if we stopped at this point, we would need to\n\nremember that the numbers in this array represent the temperature in degrees Celsius (doesn’t seem too bad),\nremember that the first dimension of the array represents time, the second latitude and the third longitude (maybe ok), and\nkeep track of the range of values that time, latitude, and longitude take (not so good).\n\nKeeping track of all this information separately could quickly get messy and could make it challenging to share our data and analyses with others. This is what the netCDF data model and xarray aim to simplify. We can get data and its descriptors together in an xarray.DataArray by adding the dimensions over which the variable is being measured and including attributes that appropriately describe dimensions and variables.\nDimensions and Coordinates\nTo specify the dimensions of our upcoming xarray.DataArray, we must examine how we’ve constructed the numpy.ndarray holding the temperature data. The diagram below shows how the dimensions of temp_data are ordered: the first dimension is time, the second is latitude, and the third is longitude.\n\nRemember that indexing in 2-dimensional numpy.ndarrays starts at the top-left corner of the array, and it is done by rows first and columns second (like matrices). This is why latitude is the second dimension and longitude the third. From the diagram, we can also see that the coordinates (values of each dimension) are as follow:\n\ndate coordinates are 2022-09-01, 2022-09-02, 2022-09-03\nlatitude coordinates are 70, 60, 50, 40, 30 (notice decreasing order)\nlongitude coordinates are 60, 70, 80, 90, 100 (notice increasing order)\n\nWe add the dimensions as a tuple of strings and coordinates as a dictionary:\n\n# names of the dimensions in the required order\ndims = ('time', 'lat', 'lon')\n\n# create coordinates to use for indexing along each dimension \ncoords = {'time' : pd.date_range(\"2022-09-01\", \"2022-09-03\"),\n          'lat' : np.arange(70, 20, -10),\n          'lon' : np.arange(60, 110, 10)}  \n\nAttributes\nNext, we add the attributes (metadata) for our temperature data as a dictionary:\n\n# attributes (metadata) of the data array \nattrs = { 'title' : 'temperature across weather stations',\n          'standard_name' : 'air_temperature',\n          'units' : 'degree_c'}\n\nPutting It All Together\nFinally, we put all these pieces together (data, dimensions, coordinates, and attributes) to create an xarray.DataArray:\n\n# initialize xarray.DataArray\ntemp = xr.DataArray(data = temp_data, \n                    dims = dims,\n                    coords = coords,\n                    attrs = attrs)\ntemp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 3, lat: 5, lon: 5)>\narray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])\nCoordinates:\n  * time     (time) datetime64[ns] 2022-09-01 2022-09-02 2022-09-03\n  * lat      (lat) int64 70 60 50 40 30\n  * lon      (lon) int64 60 70 80 90 100\nAttributes:\n    title:          temperature across weather stations\n    standard_name:  air_temperature\n    units:          degree_cxarray.DataArraytime: 3lat: 5lon: 50 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2array([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Coordinates: (3)time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03array(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')lat(lat)int6470 60 50 40 30array([70, 60, 50, 40, 30])lon(lon)int6460 70 80 90 100array([ 60,  70,  80,  90, 100])Attributes: (3)title :temperature across weather stationsstandard_name :air_temperatureunits :degree_c\n\n\nWe can also update the variable’s attributes after creating the object. Notice that each of the coordinates is also an xarray.DataArray, so we can add attributes to them.\n\n# update attributes\ntemp.attrs['description'] = 'simple example of an xarray.DataArray'\n\n# add attributes to coordinates \ntemp.time.attrs = {'description':'date of measurement'}\n\ntemp.lat.attrs['standard_name']= 'grid_latitude'\ntemp.lat.attrs['units'] = 'degree_N'\n\ntemp.lon.attrs['standard_name']= 'grid_longitude'\ntemp.lon.attrs['units'] = 'degree_E'\ntemp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (time: 3, lat: 5, lon: 5)>\narray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])\nCoordinates:\n  * time     (time) datetime64[ns] 2022-09-01 2022-09-02 2022-09-03\n  * lat      (lat) int64 70 60 50 40 30\n  * lon      (lon) int64 60 70 80 90 100\nAttributes:\n    title:          temperature across weather stations\n    standard_name:  air_temperature\n    units:          degree_c\n    description:    simple example of an xarray.DataArrayxarray.DataArraytime: 3lat: 5lon: 50 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2array([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Coordinates: (3)time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03description :date of measurementarray(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')lat(lat)int6470 60 50 40 30standard_name :grid_latitudeunits :degree_Narray([70, 60, 50, 40, 30])lon(lon)int6460 70 80 90 100standard_name :grid_longitudeunits :degree_Earray([ 60,  70,  80,  90, 100])Attributes: (4)title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArray\n\n\nAt this point, since we have a single variable, the dataset attributes and the variable attributes are the same.\n\n\n5.4.1.2 Indexing\nAn xarray.DataArray allows both positional indexing (like numpy) and label-based indexing (like pandas). Positional indexing is the most basic, and it’s done using Python’s [] syntax, as in array[i,j] with i and j both integers. Label-based indexing takes advantage of dimensions in the array having names and coordinate values that we can use to access data instead of remembering the positional order of each dimension.\nAs an example, suppose we want to know what was the temperature recorded by the weather station located at 40°0′N 80°0′E on September 1st, 2022. By recalling all the information about how the array is setup with respect to the dimensions and coordinates, we can access this data positionally:\n\ntemp[0,1,2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray ()>\narray(0)\nCoordinates:\n    time     datetime64[ns] 2022-09-01\n    lat      int64 60\n    lon      int64 80\nAttributes:\n    title:          temperature across weather stations\n    standard_name:  air_temperature\n    units:          degree_c\n    description:    simple example of an xarray.DataArrayxarray.DataArray0array(0)Coordinates: (3)time()datetime64[ns]2022-09-01description :date of measurementarray('2022-09-01T00:00:00.000000000', dtype='datetime64[ns]')lat()int6460standard_name :grid_latitudeunits :degree_Narray(60)lon()int6480standard_name :grid_longitudeunits :degree_Earray(80)Attributes: (4)title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArray\n\n\nOr, we can use the dimensions names and their coordinates to access the same value:\n\ntemp.sel(time='2022-09-01', lat=40, lon=80)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray ()>\narray(0)\nCoordinates:\n    time     datetime64[ns] 2022-09-01\n    lat      int64 40\n    lon      int64 80\nAttributes:\n    title:          temperature across weather stations\n    standard_name:  air_temperature\n    units:          degree_c\n    description:    simple example of an xarray.DataArrayxarray.DataArray0array(0)Coordinates: (3)time()datetime64[ns]2022-09-01description :date of measurementarray('2022-09-01T00:00:00.000000000', dtype='datetime64[ns]')lat()int6440standard_name :grid_latitudeunits :degree_Narray(40)lon()int6480standard_name :grid_longitudeunits :degree_Earray(80)Attributes: (4)title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArray\n\n\nNotice that the result of this indexing is a 1x1 xarray.DataArray. This is because operations on an xarray.DataArray (resp. xarray.DataSet) always return another xarray.DataArray (resp. xarray.DataSet). In particular, operations returning scalar values will also produce xarray objects, so we need to cast them as numbers manually. See xarray.DataArray.item.\nMore about xarray indexing.\n\n\n5.4.1.3 Reduction\nxarray has implemented several methods to reduce an xarray.DataArray along any number of dimensions. One of the advantages of xarray.DataArray is that, if we choose to, it can carry over attributes when doing calculations. For example, we can calculate the average temperature at each weather station over time and obtain a new xarray.DataArray.\n\navg_temp = temp.mean(dim = 'time') \n# to keep attributes add keep_attrs = True\n\navg_temp.attrs = {'title':'average temperature over three days'}\navg_temp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (lat: 5, lon: 5)>\narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])\nCoordinates:\n  * lat      (lat) int64 70 60 50 40 30\n  * lon      (lon) int64 60 70 80 90 100\nAttributes:\n    title:    average temperature over three daysxarray.DataArraylat: 5lon: 51.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0array([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])Coordinates: (2)lat(lat)int6470 60 50 40 30standard_name :grid_latitudeunits :degree_Narray([70, 60, 50, 40, 30])lon(lon)int6460 70 80 90 100standard_name :grid_longitudeunits :degree_Earray([ 60,  70,  80,  90, 100])Attributes: (1)title :average temperature over three days\n\n\nMore about xarray computations.\n\n\n\n5.4.2 xarray.DataSet\nAn xarray.DataSet resembles an in-memory representation of a NetCDF file and consists of multiple variables (each being an xarray.DataArray), with dimensions, coordinates, and attributes, forming a self-describing dataset. Attributes can be specific to each variable, each dimension, or they can describe the whole dataset. The variables in an xarray.DataSet can have the same dimensions, share some dimensions, or have no dimensions in common. Let’s see an example of this.\n\n5.4.2.1 Create an xarray.DataSet\nFollowing our previous example, we can create an xarray.DataSet by combining the temperature data with the average temperature data. We also add some attributes that now describe the whole dataset, not only each variable.\n\n# make dictionaries with variables and attributes\ndata_vars = {'avg_temp': avg_temp,\n            'temp': temp}\n\nattrs = {'title':'temperature data at weather stations: daily and and average',\n        'description':'simple example of an xarray.Dataset'}\n\n# create xarray.Dataset\ntemp_dataset = xr.Dataset( data_vars = data_vars,\n                        attrs = attrs)\n\nTake some time to click through the data viewer and read through the variables and metadata in the dataset. Notice the following:\n\ntemp_dataset is a dataset with three dimensions (time, latitude, and longitude),\ntemp is a variable that uses all three dimensions in the dataset, and\naveg_temp is a variable that only uses two dimensions (latitude and longitude).\n\n\ntemp_dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (lat: 5, lon: 5, time: 3)\nCoordinates:\n  * lat       (lat) int64 70 60 50 40 30\n  * lon       (lon) int64 60 70 80 90 100\n  * time      (time) datetime64[ns] 2022-09-01 2022-09-02 2022-09-03\nData variables:\n    avg_temp  (lat, lon) float64 1.0 1.0 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0 1.0\n    temp      (time, lat, lon) int64 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2\nAttributes:\n    title:        temperature data at weather stations: daily and and average\n    description:  simple example of an xarray.Datasetxarray.DatasetDimensions:lat: 5lon: 5time: 3Coordinates: (3)lat(lat)int6470 60 50 40 30standard_name :grid_latitudeunits :degree_Narray([70, 60, 50, 40, 30])lon(lon)int6460 70 80 90 100standard_name :grid_longitudeunits :degree_Earray([ 60,  70,  80,  90, 100])time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03description :date of measurementarray(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (2)avg_temp(lat, lon)float641.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0title :average temperature over three daysarray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])temp(time, lat, lon)int640 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArrayarray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Attributes: (2)title :temperature data at weather stations: daily and and averagedescription :simple example of an xarray.Dataset\n\n\n\n\n5.4.2.2 Save and Reopen\nFinally, we want to save our dataset as a NetCDF file. To do this, specify the file path and use the .nc extension for the file name. Then save the dataset using the to_netcdf method with your file path. Opening NetCDF is similarly straightforward using xarray.open_dataset().\n\n# specify file path: don't forget the .nc extension!\nfp = os.path.join(os.getcwd(),'temp_dataset.nc') \n# save file\ntemp_dataset.to_netcdf(fp)\n\n# open to check:\ncheck = xr.open_dataset(fp)\ncheck\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (lat: 5, lon: 5, time: 3)\nCoordinates:\n  * lat       (lat) int64 70 60 50 40 30\n  * lon       (lon) int64 60 70 80 90 100\n  * time      (time) datetime64[ns] 2022-09-01 2022-09-02 2022-09-03\nData variables:\n    avg_temp  (lat, lon) float64 1.0 1.0 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0 1.0\n    temp      (time, lat, lon) int64 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2\nAttributes:\n    title:        temperature data at weather stations: daily and and average\n    description:  simple example of an xarray.Datasetxarray.DatasetDimensions:lat: 5lon: 5time: 3Coordinates: (3)lat(lat)int6470 60 50 40 30standard_name :grid_latitudeunits :degree_Narray([70, 60, 50, 40, 30])lon(lon)int6460 70 80 90 100standard_name :grid_longitudeunits :degree_Earray([ 60,  70,  80,  90, 100])time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03description :date of measurementarray(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (2)avg_temp(lat, lon)float64...title :average temperature over three daysarray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])temp(time, lat, lon)int64...title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArrayarray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Attributes: (2)title :temperature data at weather stations: daily and and averagedescription :simple example of an xarray.Dataset\n\n\n\n\n\n5.4.3 Exercise\nFor this exercise, we will use a dataset including time series of annual Arctic freshwater fluxes and storage terms. The data was produced for the publication Jahn and Laiho, 2020 about changes in the Arctic freshwater budget and is archived at the Arctic Data Center doi:10.18739/A2280504J\n\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A792bfc37-416e-409e-80b1-fdef8ab60033'\n\nmsg = urllib.request.urlretrieve(url, \"FW_data_CESM_LW_2006_2100.nc\")\n\n\nfp = os.path.join(os.getcwd(),'FW_data_CESM_LW_2006_2100.nc')\nfw_data = xr.open_dataset(fp)\nfw_data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                          (time: 95, member: 11)\nCoordinates:\n  * time                             (time) float64 2.006e+03 ... 2.1e+03\n  * member                           (member) float64 1.0 2.0 3.0 ... 10.0 11.0\nData variables: (12/16)\n    FW_flux_Fram_annual_net          (time, member) float64 -1.26e+03 ... -2....\n    FW_flux_Barrow_annual_net        (time, member) float64 -600.7 ... -537.2\n    FW_flux_Nares_annual_net         (time, member) float64 -1.805e+03 ... -2...\n    FW_flux_Davis_annual_net         (time, member) float64 -2.313e+03 ... -3...\n    FW_flux_BSO_annual_net           (time, member) float64 -859.2 ... -993.2\n    FW_flux_Bering_annual_net        (time, member) float64 2.351e+03 ... 3.1...\n    ...                               ...\n    Solid_FW_flux_BSO_annual_net     (time, member) float64 -26.77 ... -35.43\n    Solid_FW_flux_Bering_annual_net  (time, member) float64 56.3 86.62 ... 22.87\n    runoff_annual                    (time, member) float64 3.39e+03 ... 3.97...\n    netPrec_annual                   (time, member) float64 2.019e+03 ... 2.1...\n    Liquid_FW_storage_Arctic_annual  (time, member) float64 8.125e+04 ... 9.7...\n    Solid_FW_storage_Arctic_annual   (time, member) float64 1.828e+04 ... 7.6...\nAttributes:\n    creation_date:   02-Jun-2020 15:38:31\n    author:          Alexandra Jahn, CU Boulder, alexandra.jahn@colorado.edu\n    title:           Annual timeseries of freshwater data from the CESM Low W...\n    description:     Annual mean Freshwater (FW) fluxes and storage relative ...\n    data_structure:  The data structure is |Ensemble member | Time (in years)...xarray.DatasetDimensions:time: 95member: 11Coordinates: (2)time(time)float642.006e+03 2.007e+03 ... 2.1e+03long_name :time in years, 1920-2100array([2006., 2007., 2008., 2009., 2010., 2011., 2012., 2013., 2014., 2015.,\n       2016., 2017., 2018., 2019., 2020., 2021., 2022., 2023., 2024., 2025.,\n       2026., 2027., 2028., 2029., 2030., 2031., 2032., 2033., 2034., 2035.,\n       2036., 2037., 2038., 2039., 2040., 2041., 2042., 2043., 2044., 2045.,\n       2046., 2047., 2048., 2049., 2050., 2051., 2052., 2053., 2054., 2055.,\n       2056., 2057., 2058., 2059., 2060., 2061., 2062., 2063., 2064., 2065.,\n       2066., 2067., 2068., 2069., 2070., 2071., 2072., 2073., 2074., 2075.,\n       2076., 2077., 2078., 2079., 2080., 2081., 2082., 2083., 2084., 2085.,\n       2086., 2087., 2088., 2089., 2090., 2091., 2092., 2093., 2094., 2095.,\n       2096., 2097., 2098., 2099., 2100.])member(member)float641.0 2.0 3.0 4.0 ... 9.0 10.0 11.0long_name :Ensemble member, 1-11array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])Data variables: (16)FW_flux_Fram_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Fram Strait, relative to 34.8array([[-1259.970567, -1264.007662, -1043.912842, ..., -1127.209397,\n        -1094.937548, -1068.404606],\n       [-1211.286894, -1479.531102, -1018.166211, ..., -1243.711036,\n        -1267.207286, -1114.552581],\n       [-1280.228564, -1385.553358,  -880.902707, ..., -1289.849198,\n        -1050.289977, -1041.592238],\n       ...,\n       [-1994.723559, -2192.810731, -1867.436333, ..., -1801.993229,\n        -2151.202886, -2616.74365 ],\n       [-2312.975878, -1882.595866, -1626.129161, ..., -2050.419246,\n        -2007.994165, -2369.86382 ],\n       [-1869.903721, -1941.767155, -1944.454371, ..., -1797.444957,\n        -2291.658043, -2133.186992]])FW_flux_Barrow_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Barrow Strait, relative to 34.8array([[-600.656931, -591.794563, -623.46547 , ..., -530.954288, -557.466782,\n        -540.397683],\n       [-545.381073, -651.15272 , -647.759741, ..., -554.62912 , -614.254495,\n        -681.241813],\n       [-676.367103, -597.249584, -526.308636, ..., -458.975897, -539.056379,\n        -600.418649],\n       ...,\n       [-495.781816, -343.416049, -572.034808, ..., -547.660529, -503.001068,\n        -575.476408],\n       [-508.571693, -385.524045, -505.368339, ..., -560.393935, -432.867012,\n        -501.710328],\n       [-478.525844, -380.776262, -525.255738, ..., -418.526205, -514.73683 ,\n        -537.165685]])FW_flux_Nares_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Nares Strait, relative to 34.8array([[-1805.094645, -1750.974399, -1736.428853, ..., -1790.293088,\n        -1630.279788, -1698.111863],\n       [-1661.738712, -1906.896808, -1756.856956, ..., -1846.616134,\n        -1754.973196, -1978.856594],\n       [-1956.029601, -1729.47722 , -1506.906015, ..., -1742.673838,\n        -1613.488436, -1857.85344 ],\n       ...,\n       [-2835.663916, -2685.863919, -3123.0961  , ..., -2930.78795 ,\n        -3075.907956, -2954.079147],\n       [-2767.255034, -2435.987932, -2852.725589, ..., -3132.779162,\n        -2879.811506, -2758.960293],\n       [-2801.006622, -2418.965807, -2907.765449, ..., -2699.143264,\n        -3153.186248, -2825.880317]])FW_flux_Davis_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Davis Strait, relative to 34.8array([[-2312.903562, -2206.193327, -2513.938315, ..., -2452.741479,\n        -2178.565682, -2496.419307],\n       [-2442.875007, -2302.612039, -2550.956543, ..., -2594.189567,\n        -2411.537129, -2491.906155],\n       [-2380.667757, -2460.765939, -2355.580677, ..., -2566.93496 ,\n        -2292.468786, -2443.485248],\n       ...,\n       [-3222.803208, -3277.824803, -3791.472809, ..., -3864.113295,\n        -3704.395263, -3775.769167],\n       [-3091.625499, -3293.624915, -3857.535966, ..., -3733.405415,\n        -3404.497911, -3538.640386],\n       [-3657.58114 , -3137.665787, -3346.385689, ..., -3778.231506,\n        -3182.004745, -3294.19718 ]])FW_flux_BSO_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through the Barents Sea Opening, relative to 34.8array([[ -859.243059,  -942.211781,  -723.703298, ..., -1036.046746,\n        -1055.237416,  -736.846205],\n       [ -878.919322,  -862.165165,  -760.842132, ...,  -981.459231,\n        -1017.759096,  -719.537064],\n       [ -867.125354,  -771.0794  ,  -883.947051, ...,  -823.744405,\n        -1084.024653,  -832.384503],\n       ...,\n       [-1201.528063,  -956.321947,  -989.990348, ...,  -928.170124,\n        -1522.813339, -1333.944058],\n       [-1175.496328,  -992.809814, -1042.238636, ..., -1027.30427 ,\n        -1012.655464, -1161.633369],\n       [-1108.045221, -1149.213528,  -974.059705, ...,  -944.651881,\n        -1351.228427,  -993.196776]])FW_flux_Bering_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Bering Strait, relative to 34.8array([[2351.369668, 2345.677436, 2046.439566, ..., 2298.623674, 2036.719667,\n        1907.280919],\n       [2481.513182, 2178.085803, 2490.061059, ..., 2760.976043, 2139.010713,\n        2320.373448],\n       [2514.857782, 2488.589714, 2864.883346, ..., 2666.687224, 2805.529561,\n        1831.068287],\n       ...,\n       [2212.93228 , 2305.747855, 3165.795471, ..., 2682.178162, 2137.858094,\n        2769.764568],\n       [2402.817047, 2871.272846, 2241.49669 , ..., 2899.779166, 2921.583788,\n        3124.232717],\n       [2553.009217, 2738.646198, 2402.378728, ..., 2832.505199, 2564.175895,\n        3110.720966]])Solid_FW_flux_Fram_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Fram Strait, relative to 34.8array([[-2355.201527, -3079.918585, -1900.235976, ..., -2379.143692,\n        -2263.860412, -1740.013499],\n       [-2072.011081, -2483.281102, -2397.287222, ..., -2044.468084,\n        -2757.085258, -2187.036132],\n       [-2495.74312 , -1468.060247, -2391.734318, ..., -2991.861612,\n        -2881.262317, -2262.942851],\n       ...,\n       [ -989.394195, -1285.473222,  -763.780509, ...,  -657.173252,\n         -940.635132, -2216.186576],\n       [-1176.377381,  -715.543546,  -763.626743, ...,  -876.188872,\n         -550.202924, -1673.306079],\n       [ -842.80755 , -1236.592837,  -685.109519, ...,  -585.170298,\n        -1213.069558, -1393.501459]])Solid_FW_flux_Barrow_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Barrow Strait, relative to 34.8array([[ -5.907839, -11.016486,   2.898278, ...,  -6.772   ,  -2.346885,\n         -4.972206],\n       [ -5.405102, -12.325443,  -2.837828, ...,  -1.54938 ,  -1.524433,\n         -3.239178],\n       [-13.615343,  -3.321843,   3.297263, ...,  17.846225,  -8.849867,\n         -1.40733 ],\n       ...,\n       [ -4.252253,  -4.74888 ,  -2.318929, ...,  -0.692743,  -3.516814,\n         -7.24111 ],\n       [ -2.473455,  -2.43267 ,  -4.119768, ...,  -3.041222,  -0.642326,\n         -3.151623],\n       [ -4.023888,  -4.630743,  -6.034809, ...,  -3.946639,  -2.759937,\n         -3.67783 ]])Solid_FW_flux_Nares_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Nares Strait, relative to 34.8array([[-443.523805, -369.69004 , -319.816739, ..., -359.434137, -331.597514,\n        -351.161639],\n       [-402.060722, -365.66951 , -455.727733, ..., -359.163976, -398.079664,\n        -312.769165],\n       [-309.118123, -407.329009, -518.392379, ..., -380.334965, -392.379044,\n        -384.005149],\n       ...,\n       [-322.681791, -285.266534, -360.379804, ..., -249.28949 , -291.504876,\n        -301.008009],\n       [-214.401449, -252.253729, -303.416623, ..., -295.322261, -282.239789,\n        -357.222407],\n       [-361.437905, -301.694338, -259.185245, ..., -248.62952 , -255.963657,\n        -396.696598]])Solid_FW_flux_Davis_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Davis Strait, relative to 34.8array([[-647.548166, -690.769077, -803.010762, ..., -637.388108, -669.678922,\n        -636.923716],\n       [-738.654959, -712.727442, -787.469841, ..., -741.271782, -734.0404  ,\n        -652.683568],\n       [-607.804625, -582.211375, -735.954593, ..., -633.625357, -651.638696,\n        -628.742061],\n       ...,\n       [-547.210914, -467.312263, -533.798556, ..., -565.575028, -573.092124,\n        -579.833192],\n       [-381.382969, -369.539971, -560.781192, ..., -423.884627, -600.9458  ,\n        -596.006766],\n       [-544.597115, -384.419366, -492.503528, ..., -482.520345, -479.724188,\n        -568.860609]])Solid_FW_flux_BSO_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through the Barents Sea Opening, relative to 34.8array([[-2.677222e+01, -2.198672e+02, -7.108708e+01, ..., -8.265375e+00,\n        -7.027574e+01, -2.478501e-01],\n       [-1.119710e+00, -1.611613e+01, -9.113674e+01, ..., -7.502441e+01,\n        -4.567288e+01, -8.217717e+01],\n       [-1.862617e+01, -4.320776e+00, -1.400749e+02, ..., -1.114686e+01,\n        -2.599572e+02, -2.824174e+01],\n       ...,\n       [-2.107002e+01, -1.972472e+01, -1.322291e+00, ..., -1.250803e+01,\n         0.000000e+00, -8.715960e+00],\n       [-1.129319e-01, -3.020100e+00, -9.972180e+00, ..., -6.321711e+00,\n        -1.357067e-01, -5.525681e-01],\n       [-1.324610e-02, -1.583726e+01, -2.125017e-01, ..., -2.465729e+00,\n        -3.163840e-01, -3.543094e+01]])Solid_FW_flux_Bering_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Bering Strait, relative to 34.8array([[ 56.300783,  86.619422,  21.629406, ...,  45.369409, 120.205643,\n          6.023335],\n       [ 52.854622,  68.339477, 139.181478, ...,  79.513795,  82.273825,\n        216.946026],\n       [ 64.470058,  16.432358,  33.79051 , ...,  36.5111  ,  51.494807,\n        -83.580959],\n       ...,\n       [ 58.127421,  22.637266,  79.579919, ...,  48.221805,  29.201449,\n        146.166502],\n       [ 46.223662,  44.331598,   9.747562, ...,  44.638335,  63.846993,\n         49.628882],\n       [  4.125923,  26.376146,  44.08245 , ...,  35.386517, 144.93525 ,\n         22.865989]])runoff_annual(time, member)float64...units :km3/yrlong_name :FW flux from river runoff into the Arctic Ocean domain, relative to 34.8array([[3389.793118, 3653.250059, 3255.904314, ..., 3484.324351, 3762.057224,\n        3233.230548],\n       [2991.152992, 3493.827835, 3384.707049, ..., 3571.643922, 3489.125416,\n        3641.709685],\n       [3534.397219, 3128.514068, 3354.798516, ..., 3304.537755, 3539.266607,\n        3084.447612],\n       ...,\n       [3665.258107, 3551.863606, 3809.485292, ..., 3493.525529, 3663.667382,\n        4321.328755],\n       [3743.83261 , 3378.368684, 3672.490382, ..., 3739.293615, 3739.029349,\n        3973.012085],\n       [3552.160713, 3480.78009 , 3710.884772, ..., 3580.675064, 4001.506981,\n        3973.911416]])netPrec_annual(time, member)float64...units :km3/yrlong_name :Net FW flux from precipitation minus evaporation over the Arctic Ocean domain, relative to 34.8array([[2019.436111, 2208.638737, 1936.136054, ..., 2049.989982, 1959.873063,\n        1750.749748],\n       [1945.582104, 2039.416637, 2085.04094 , ..., 1870.845855, 1965.883031,\n        2057.552438],\n       [2032.998228, 2047.057141, 1904.70621 , ..., 1894.083722, 1825.828287,\n        2026.627389],\n       ...,\n       [1858.080937, 1946.276296, 2100.604079, ..., 1994.116693, 2114.804976,\n        2423.484939],\n       [1915.662721, 1961.74668 , 1920.628415, ..., 1993.625762, 1744.708491,\n        2245.915281],\n       [1767.588737, 1904.765559, 1953.973994, ..., 1760.192971, 2384.175174,\n        2118.712754]])Liquid_FW_storage_Arctic_annual(time, member)float64...units :km3long_name :FW storage in the Arctic Ocean domain, , relative to 34.8. Ignoring any negative FW (water above reference salinity)array([[ 81250.414556,  85443.803218,  80863.782256, ...,  86120.03671 ,\n         81999.322618,  87066.701325],\n       [ 82567.489008,  85437.534745,  81016.233259, ...,  86801.295397,\n         81720.408045,  85405.906043],\n       [ 83435.689171,  84300.641386,  81701.536079, ...,  88202.733212,\n         82363.689791,  84802.874562],\n       ...,\n       [ 99179.037787, 100587.947588,  99713.659894, ..., 105329.776774,\n        101183.194676,  96731.099064],\n       [ 99022.701221, 100071.990872, 100428.151173, ..., 106023.385394,\n        101243.934883,  97353.008726],\n       [ 99884.660244, 101526.449779, 100904.197851, ..., 106283.670855,\n        101106.948969,  97309.828159]])Solid_FW_storage_Arctic_annual(time, member)float64...units :km3long_name :FW storage in sea ice and snow in the Arctic Ocean domain, relative to 34.8array([[18283.302344, 16436.957815, 18942.480543, ..., 19105.369531,\n        17193.142364, 16026.957959],\n       [17228.15868 , 16184.248407, 19493.271233, ..., 18736.727461,\n        17731.125586, 18033.048197],\n       [16582.390706, 17113.875851, 19081.541068, ..., 18073.074623,\n        16677.630513, 18713.913839],\n       ...,\n       [ 8286.152773,  7618.147979,  7159.460183, ...,  7615.242652,\n         7314.728945,  8332.238818],\n       [ 7925.266   ,  8366.732533,  7583.306716, ...,  7247.856691,\n         6969.390301,  7445.873087],\n       [ 6850.743786,  7492.329279,  7275.546906, ...,  7656.442802,\n         7476.367423,  7656.280716]])Attributes: (5)creation_date :02-Jun-2020 15:38:31author :Alexandra Jahn, CU Boulder, alexandra.jahn@colorado.edutitle :Annual timeseries of freshwater data from the CESM Low Warming Ensembledescription :Annual mean Freshwater (FW) fluxes and storage relative to 34.8 shown in Jahn and Laiho, GRL, 2020, calculated from the 11-member Community Earth System Model (CESM) Low Warming Ensemble output (Sanderson et al., 2017, Earth Syst. Dynam., 8, 827-847. doi: 10.5194/esd-8-827-2017). These 11 ensemble members were branched from the first 11 ensemble members of the CESM Large Ensemble (companion data file) at the end of 2005. Convention for the fluxes is that positive fluxes signify a source of FW to the Arctic and negative fluxes are a sink/export of FW for the Arctic. FW fluxes are the net fluxes through a strait over the full ocean depth, adding up any positive and negative fluxes. Liquid FW storage is calculated over the full depth of the ocean but ignoring any negative FW (resulting from salinties over 34.8). Solid FW storage includes FW stored in sea ice and FW stored in snow on sea ice. Surface fluxes and FW storage is calculated over the Arctic domain bounded by Barrow Strait, Nares Strait, Bering Strait, Fram Strait, and the Barents Sea Opeing (BSO). Davis Strait fluxes are included for reference only and are outside of the Arctic domain. A map showing the domain and the location of the straits can be found in Jahn and Laiho, GRL, 2020.data_structure :The data structure is |Ensemble member | Time (in years)|. All data are annual means. The data covers 2006-2100. There are 11 ensemble members.\n\n\n\nHow many dimensions does the runoff_annual variable have? What are the coordinates for the second dimension of this variable?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe can see in the object viewer that the runoff_annual variable has two dimensions: time and member, in that order. We can also access the dimensions by calling:\n\nfw_data.runoff_annual.dims\n\nThe second dimensions is member. Near the top of the object viewer, under coordinates, we can see that that member’s coordinates is an array from 1 to 11. We can directly see this array by calling:\n\nfw_data.member\n\n\n\n\n\nSelect the values for the second member of the netPrec_annual variable.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmember2 = fw_data.netPrec_annual.sel(member=2)\n\n\n\n\n\nWhat is the maximum value of the second member of the netPrec_annual variable in the time period 2022 to 2100? Hint.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBased on our previous answer, this maximum is:\n\nx_max = member2.loc[2022:2100].max()\nx_max.item()\n\nNotice we had to use item to transform the array into a number."
  },
  {
    "objectID": "sections/data-structures-netcdf.html#tabular-data-and-netcdf",
    "href": "sections/data-structures-netcdf.html#tabular-data-and-netcdf",
    "title": "5  Data Structures and Formats for Large Data",
    "section": "5.5 Tabular Data and NetCDF",
    "text": "5.5 Tabular Data and NetCDF\nUndoubtedly, tabular data is one of the most popular data formats. In this last section, we will discuss the relation between tabular data and the NetCDF data format and how to transform a pandas.DataFrame into an xarray.DataSet.\n\n5.5.1 Tabular to NetCDF\nWe assume our starting point is tabular data that meets the criteria for tidy data, which means:\n\nEach column holds a different variable.\nEach row holds a different observation.\n\nTake, for example, this tidy data subset from our exercise about weather stations measuring temperature: \nTo understand how this will transform into NetCDF format, we first need to identify which columns will act as dimensions and which as variables. We can also think of the values of the dimension columns as the coordinates in the xarray.DataSet. The diagram below shows how these columns transform into variables, dimensions, and coordinates.\n\nTabular formats like csv do not offer an intrinsic way to encode attributes for the dimensions or variables, this is why we don’t see any attributes in the resulting NetCDF data. One of the most significant advantages of NetCDF is its self-describing properties.\n\n\n5.5.2 pandas to xarray\nWhat does the previous example look like when working with pandas and xarray?\nLet’s work with a csv file containing the previous temperature measurements. Essentially, we need to read this file as a pandas.DataFrame and then use the pandas.DataFrame.to_xarray() method, taking into account that the dimensions of the resulting xarray.DataSet will be formed using the index column(s) of the pandas.DataFrame. In this case, we know the first three columns will be our dimension columns, so we need to group them as a multindex for the pandas.DataFrame. We can do this by using the index_col argument directly when we read in the csv file.\n\nfp = os.path.join(os.getcwd(),'netcdf_temp_data.csv') \n\n\n# specify columns representing dimensions\ndimension_columns = [0,1,2]\n\n# read file\ntemp = pd.read_csv(fp, index_col=dimension_columns)\ntemp\n\n\n\n\n\n  \n    \n      \n      \n      \n      temperature\n    \n    \n      time\n      longitude\n      latitude\n      \n    \n  \n  \n    \n      2022-09-01\n      30\n      60\n      0\n    \n    \n      70\n      0\n    \n    \n      40\n      60\n      0\n    \n    \n      70\n      0\n    \n    \n      2022-09-02\n      30\n      60\n      1\n    \n    \n      70\n      1\n    \n    \n      40\n      60\n      1\n    \n    \n      70\n      1\n    \n  \n\n\n\n\nAnd this is our resulting xarray.DataSet:\n\ntemp.to_xarray()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:      (time: 2, longitude: 2, latitude: 2)\nCoordinates:\n  * time         (time) object '2022-09-01' '2022-09-02'\n  * longitude    (longitude) int64 30 40\n  * latitude     (latitude) int64 60 70\nData variables:\n    temperature  (time, longitude, latitude) int64 0 0 0 0 1 1 1 1xarray.DatasetDimensions:time: 2longitude: 2latitude: 2Coordinates: (3)time(time)object'2022-09-01' '2022-09-02'array(['2022-09-01', '2022-09-02'], dtype=object)longitude(longitude)int6430 40array([30, 40])latitude(latitude)int6460 70array([60, 70])Data variables: (1)temperature(time, longitude, latitude)int640 0 0 0 1 1 1 1array([[[0, 0],\n        [0, 0]],\n\n       [[1, 1],\n        [1, 1]]])Attributes: (0)\n\n\nFor further reading and examples about switching between pandas and xarray you can visit the following:\n\nxarray’s Frequently Asked Questions\nxarray’s documentation about working with pandas\npandas.DataFrame.to_xarray documentation"
  },
  {
    "objectID": "sections/parallel-with-dask.html#learning-objectives",
    "href": "sections/parallel-with-dask.html#learning-objectives",
    "title": "6  Parallelization with Dask",
    "section": "6.1 Learning Objectives",
    "text": "6.1 Learning Objectives\n\nBecome familiar with the Dask processing workflow:\n\nWhat are the client, scheduler, workers, and cluster\nUnderstand delayed computations and “lazy” evaluation\nObtain information about computations via the Dask dashboard\n\nLearn to load data and specify partition/chunk sizes of dask.arrays/dask.dataframes \nIntegrate xarray and rioxarray with Dask for geospatial computations\nShare best practices and resources for further reading"
  },
  {
    "objectID": "sections/parallel-with-dask.html#introduction",
    "href": "sections/parallel-with-dask.html#introduction",
    "title": "6  Parallelization with Dask",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nDask is a library for parallel computing in Python. It can scale up code to use your personal computer’s full capacity or distribute work in a cloud cluster. By mirroring APIs of other commonly used Python libraries, such as Pandas and NumPy, Dask provides a familiar interface that makes it easier to parallelize your code. In this lesson, we will get acquainted with Dask’s way of distributing and evaluating computations and some of its most commonly used objects."
  },
  {
    "objectID": "sections/parallel-with-dask.html#dask-cluster",
    "href": "sections/parallel-with-dask.html#dask-cluster",
    "title": "6  Parallelization with Dask",
    "section": "6.3 Dask Cluster",
    "text": "6.3 Dask Cluster\nWe can deploy a Dask cluster on a single machine or an actual cluster with multiple machines. The cluster has three main components for processing computations in parallel. These are the client, the scheduler and the workers.\n\nWhen we code, we communicate directly with the client, which is responsible for submitting tasks to be executed to the scheduler.\nAfter receiving the tasks from the client, the scheduler determines how tasks will be distributed among the workers and coordinates them to process tasks in parallel.\nFinally, the workers compute tasks and store and return computations results. Workers can be threads, processes, or separate machines in a cluster. Here you can read more about what are threads and processes and some best practices for selecting one or the other.\n\nTo interact with the client and generate tasks that can be processed in parallel we need to use Dask objects to read and process our data. \n\n\n\nM. Schmitt, Understanding Dask Architecture: The Client, Scheduler and Workers"
  },
  {
    "objectID": "sections/parallel-with-dask.html#connect-to-an-existing-cluster",
    "href": "sections/parallel-with-dask.html#connect-to-an-existing-cluster",
    "title": "6  Parallelization with Dask",
    "section": "6.4 Connect to an existing cluster",
    "text": "6.4 Connect to an existing cluster\nThe instructor is going to start a local cluster on the server that everyone can connect to. This scenario is a realistic simulation of what it would look like for you to connect to a shared Dask resource. You can create your own local cluster running on your laptop using code at the end of this lesson.\nFirst, the instructor (and only the instructor!) will run:\n\nfrom dask.distributed import LocalCluster\ncluster = LocalCluster(n_workers=70, memory_limit='auto', processes=True)\n\nTo connect to it, we will use the address that the scheduler is listening on. The port is generated randomly, so first the instructor needs to get address for you to use in your code. In a ‘real world’ scenario, this address would be given to you by the administrator of the Dask cluster.\n\ncluster.scheduler_address\n\nNow, you can pass the address to the Client function, which sets up your session as a client of the Dask cluster,\n\nfrom dask.distributed import LocalCluster, Client\n\n# if you are copy pasting this from the book, make sure \n# the address is the same that the instructor gave in the lesson!\naddress = 'tcp://127.0.0.1:40869'\nclient = Client(address)\nclient\n\n\n6.4.1 Dask Dashboard\nWe chose to use the Dask cluster in this lesson instead of the default Dask scheduler to take advantage of the cluster dashboard, which offers live monitoring of the performance and progress of our computations. You can learn more about different Dask clusters here.\nAs seen in the images above, when we set up a cluster we can see the cluster dashboard address by looking at either the client or the cluster. In this example the dashboard address is http://128.111.85.28:8787/status.\nIn order to get access to the dashboard, click the “ports” tab net to terminal. Add the port 8787 so that it is forwarded to your localhost.\nWhen we go that address in a web browser we can see the dashboard’s main page. This page shows diagnostics about:\n\nthe cluster’s and individual worker’s memory usage,\nnumber of tasks being processed by each worker,\nindividual tasks being processed across workers, and\nprogress towards completion of individual tasks.\n\nThere’s much to say about interpreting the Dask dashboard’s diagnostics. We recommend this documentation to understand the basics of the dashboard diagnostics and this video as a deeper dive into the dashboard’s functions.\n\n\n\nA Dask dashboard."
  },
  {
    "objectID": "sections/parallel-with-dask.html#dask.dataframes",
    "href": "sections/parallel-with-dask.html#dask.dataframes",
    "title": "6  Parallelization with Dask",
    "section": "6.5 dask.dataframes",
    "text": "6.5 dask.dataframes\nWhen we analyze tabular data, we usually start our analysis by loading it into memory as a Pandas DataFrame. But what if this data does not fit in memory? Or maybe our analyzes crash because we run out of memory. These scenarios are typical entry points into parallel computing. In such cases, Dask’s scalable alternative to a Pandas DataFrame is the dask.dataframe. A dask.dataframe comprises many pd.DataFrames, each containing a subset of rows of the original dataset. We call each of these pandas pieces a partition of the dask.dataframe.\n\n\n\nDask Array design (dask documentation)\n\n\n\n6.5.1 Reading a csv\nTo get familiar with dask.dataframes, we will use tabular data of soil moisture measurements at six forest stands in northeastern Siberia. The data has been collected since 2014 and is archived at the Arctic Data Center (Loranty & Alexander, doi:10.18739/A24B2X59C). Just as we did in the previous lesson, we will download the data using the requests package and the data’s URL obtained from the Arctic Data Center.\n\nimport os              \nimport urllib \n\nimport dask.dataframe as dd\n\n\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A27e4043d-75eb-4c4f-9427-0d442526c154'\n\nmsg = urllib.request.urlretrieve(url, \"dg_soil_moisture.csv\")\n\nIn the Arctic Data Center metadata we can see this file is 115 MB. To import this file as a dask.dataframe with more than one partition, we need to specify the size of each partition with the blocksize parameter. In this example, we will split the data frame into six partitions, meaning a block size of approximately 20 MB.\n\nfp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')\ndf = dd.read_csv(fp, blocksize = '20MB' , encoding='ISO-8859-1')\ndf\n\n\n\n\n\n\n\nEncoding?\n\n\n\n\n\nAbout the encoding parameter: If we try to import the file directly, we will receive an UnicodeDecodeError. We can run the following code to find the file’s encoding and add the appropriate encoding to dask.dataframe.read_csv.\n\nimport chardet\nfp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')\nwith open(fp, 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(100000))\nresult\n\n\n\n\nNotice that we cannot see any values in the data frame. This is because Dask has not really loaded the data. It will wait until we explicitly ask it to print or compute something to do so.\nHowever, we can still do df.head(). It’s not costly for memory to access a few data frame rows.\n\ndf.head(3)\n\n\n\n6.5.2 Lazy Computations\nThe application programming interface (API) of a dask.dataframe is a subset of the pandas.DataFrame API. So if you are familiar with pandas, many of the core pandas.DataFrame methods directly translate to dask.dataframes. For example:\n\naverages = df.groupby('year').mean()\naverages\n\nNotice that we cannot see any values in the resulting data frame. A major difference between pandas.DataFrames and dask.dataframes is that dask.dataframes are “lazy”. This means an object will queue transformations and calculations without executing them until we explicitly ask for the result of that chain of computations using the compute method. Once we run compute, the scheduler can allocate memory and workers to execute the computations in parallel. This kind of lazy evaluation (or delayed computation) is how most Dask workloads work. This varies from eager evaluation methods and functions, which start computing results right when they are executed.\nBefore calling compute on an object, open the Dask dashboard to see how the parallel computation is happening.\n\naverages.compute()"
  },
  {
    "objectID": "sections/parallel-with-dask.html#dask.arrays",
    "href": "sections/parallel-with-dask.html#dask.arrays",
    "title": "6  Parallelization with Dask",
    "section": "6.6 dask.arrays",
    "text": "6.6 dask.arrays\nAnother common object we might want to parallelize is a NumPy array. The equivalent Dask object is the dask.array, which coordinates many NumPy arrays that may live on disk or other machines. Each of these NumPy arrays within the dask.array is called a chunk. Choosing how these chunks are arranged within the dask.array and their size can significantly affect the performance of our code. Here you can find more information about chunks.\n\n\n\nDask Array design (dask documentation)\n\n\nIn this short example we will create a 200x500 dask.array by specifying chunk sizes of 100x100.\n\nimport numpy as np\n\nimport dask.array as da\n\n\ndata = np.arange(100_000).reshape(200, 500)\na = da.from_array(data, chunks=(100, 100))\n\nComputations for dask.arrays also work lazily. We need to call compute to trigger computations and bring the result to memory.\n\na.mean()\n\n\na.mean().compute()"
  },
  {
    "objectID": "sections/parallel-with-dask.html#dask-and-xarray",
    "href": "sections/parallel-with-dask.html#dask-and-xarray",
    "title": "6  Parallelization with Dask",
    "section": "6.7 Dask and xarray",
    "text": "6.7 Dask and xarray\nIn the future, it might be more common having to read some big array-like dataset (like a high-resolution multiband raster) than creating one from scratch using NumPy. In this case, it can be useful to use the xarray module and its extender rioxarray together with Dask. In the previous lesson, Data Structures and Formats for Large Data, we explore how to use the xarray package to work with labelled arrays. rioxarray extends xarray with the rio accessor, which stands for “raster input and output”.\nIt is simple to wrap Dask around xarray objects. We only need to specify the number of chunks as an argument when we are reading in a dataset (see also [1]).\n\n6.7.1 Open .tif file\nAs an example, let’s do a Normalized Difference Vegetation Index (NDVI) calculation using remote sensing imagery collected by aerial vehicles over northeastern Siberia (Loranty, Forbath, Talucci, Alexander, DeMarco, et al. 2020. doi:10.18739/A2ZC7RV6H.). The NDVI is an index commonly used to check if an area has live green vegetation or not. It can also show the difference between water, plants, bare soil, and human-made structures, among other things.\nThe NDVI is calculated using the near-infrared and red bands of the satellite image. The formula is\n\\[NDVI = \\frac{NIR - Red}{NIR + Red}.\\]\nFirst, we download the data for the near-infrared (NIR) and red bands from the Arctic Data Center:\n\n# download red band\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3Aac25a399-b174-41c1-b6d3-09974b161e5a'\nmsg = urllib.request.urlretrieve(url, \"RU_ANS_TR2_FL005M_red.tif\")\n\n\n# download nir band\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A1762205e-c505-450d-90ed-d4f3e4c302a7'\n\nmsg = urllib.request.urlretrieve(url, \"RU_ANS_TR2_FL005M_nir.tif\")\n\nBecause these are .tif files and have geospatial metadata, we will use rioxarray to read them. You can find more information about rioxarray here.\nTo indicate we will open these .tif files with dask.arrays as the underlying object to the xarray.DataArray (instead of a numpy.array), we need to specify either a shape or the size in bytes for each chunk. Both files are 76 MB, so let’s have chunks of 15 MB to have roughly six chunks.\n\nimport rioxarray as rioxr\n\n\n# read in the file\nfp_red = os.path.join(os.getcwd(),\"RU_ANS_TR2_FL005M_red.tif\")\nred = rioxr.open_rasterio(fp_red, chunks = '15MB')\n\nWe can see a lot of useful information here:\n\nThere are eight chunks in the array. We were aiming for six, but this often happens with how Dask distributes the memory (76MB is not divisible by 6).\nThere is geospatial information (transformation, CRS, resolution) and no-data values.\nThere is an unnecessary dimension: a constant value for the band. So our next step is to squeeze the array to flatten it.\n\n\n# getting rid of unnecessary dimension\nred = red.squeeze()\n\nNext, we read in the NIR band and do the same pre-processing:\n\n# open data\nfp_nir = os.path.join(os.getcwd(),\"RU_ANS_TR2_FL005M_nir.tif\")\nnir = rioxr.open_rasterio(fp_nir, chunks = '15MB')\n\n#squeeze\nnir = nir.squeeze()\n\n\n\n6.7.2 Calculating NDVI\nNow we set up the NDVI calculation. This step is easy because we can handle xarrays and Dask arrays as NumPy arrays for arithmetic operations. Also, both bands have values of type float32, so we won’t have trouble with the division.\n\nndvi = (nir - red) / (nir + red)\n\nWhen we look at the NDVI we can see the result is another dask.array, nothing has been computed yet. Remember, Dask computations are lazy, so we need to call compute() to bring the results to memory.\n\nndvi_values = ndvi.compute()\n\nAnd finally, we can see what these look like. Notice that xarray uses the value of the dimensions as labels along the x and y axes. We use robust=True to ignore the no-data values when plotting.\n\nndvi_values.plot(robust=True)"
  },
  {
    "objectID": "sections/parallel-with-dask.html#setting-up-your-own-local-cluster",
    "href": "sections/parallel-with-dask.html#setting-up-your-own-local-cluster",
    "title": "6  Parallelization with Dask",
    "section": "6.8 Setting up your own local cluster",
    "text": "6.8 Setting up your own local cluster\n\n6.8.1 Setting up a Local Cluster\nWe can create a local cluster as follows:\n\nfrom dask.distributed import LocalCluster, Client\n\n\ncluster = LocalCluster(n_workers=4, memory_limit=0.1, processes=True)\ncluster\n\n\nAnd then we create a client to connect to our cluster, passing the Client function the cluster object.\n\nclient = Client(cluster)\nclient\n\n\nFrom here, you can continue to run Dask commands as normal."
  },
  {
    "objectID": "sections/parallel-with-dask.html#best-practices",
    "href": "sections/parallel-with-dask.html#best-practices",
    "title": "6  Parallelization with Dask",
    "section": "6.9 Best Practices",
    "text": "6.9 Best Practices\nDask is an exciting tool for parallel computing, but it may take a while to understand its nuances to make the most of it. There are many best practices and recommendations. These are some of the basic ones to take into consideration:\n\nFor data that fits into RAM, pandas, and NumPy can often be faster and easier to use than Dask workflows. The simplest solution can often be the best.\nWhile Dask may have similar APIs to pandas and NumPy, there are differences, and not all the methods for the pandas.DataFrames and numpy.arrays translate in the same way (or with the same efficiency) to Dask objects. When in doubt, always read the documentation.\nChoose appropriate chunk and partition sizes and layouts. This is crucial to best use how the scheduler distributes work. You can read here about best practices for chunking.\nAvoid calling compute repeatedly. It is best to group similar computations together and then compute once.\n\nFurther reading:\n\nA friendly article about common dask mistakes\nGeneral Dask best practices\ndask.dataframe best practices\ndask.array best practices"
  },
  {
    "objectID": "sections/group-project-1.html#learning-objectives",
    "href": "sections/group-project-1.html#learning-objectives",
    "title": "7  Group Project: Staging and Preprocessing",
    "section": "7.1 Learning Objectives",
    "text": "7.1 Learning Objectives\n\nGet familiarized with the overall group project workflow\nWrite a parsl app that will stage and tile the IWP example data in parallel"
  },
  {
    "objectID": "sections/group-project-1.html#introduction",
    "href": "sections/group-project-1.html#introduction",
    "title": "7  Group Project: Staging and Preprocessing",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\nThe Permafrost Discovery Gateway is an online platform for archiving, processing, analysis, and visualization of permafrost big imagery products to enable discovery and knowledge-generation. The PDG utilizes and makes available products derived from high resolution satellite imagery from the Polar Geospatial Center, Planet (3 meter resolution), Sentinel (10 meter resolution), Landsat (30 meter resolution), and MODIS (250 meter resolution). One of these products is a dataset showing Ice Wedge Polygons (IWP) that form in melting permafrost. Explore the Imagery Viewer, where you can visualize different data layers submitted by Arctic researchers.\nIce wedges form as a result of long-term melting and freezing cycles of permafrost. Very cold winters cause the frozen ground to crack, and these cracks fill with melted snow in the spring. The water in the cracks freezes and expands, and this repeats over thousands of years, until the ice wedges are several meters thick. Ice wedges can form very distinctive enclosed geometries that are clearly visible in high-resolution satellite images. Long-term gradual warming of the soil combined with extreme summer warmth causes the ice wedges to melt from the top, and eventually the ground subsides.\n\nIdentifying where these ice wedge features exist is important to understanding how the Arctic landscape is changing. Time series analysis will help researchers, community members, and policy-makers address threats to the biota, architecture, and communities throughout the region. To learn more about how this data was produced, see this publication.\nThe PDG team is using advanced analysis and computational tools to process high-resolution satellite imagery and automatically dectect where ice wedge polygons form. Below is an example of a satellite image (left) and the detected ice wedge polygons in geospatial vector format (right) of that same image.\n\nIn the group project, we are going to use a subset of the high-resolution dataset of these detected ice wedge polygons in order to learn some of the reproducible, scalable techniques that will allow us to process it. Our workflow will start with a set of GeoPackage files that contain the detected ice wedge polygons. These files all have irregular and sometimes overlapping extents due to the variation in satellite coverage, clouds, etc. Our first processing step will take these files and “tile” them into smaller files which have regular extents.\n\nIn step two of the workflow, we will take those regularly tiled GeoPackage files and rasterize them. The files will be regularly gridded, and a summary statistic will be calculated for each grid cell (such as the proportion of pixel area covered by polygons).\n\nIn the final step of the workflow, we will take the raster files and resample them to create a set of raster tiles at different resolutions. This last step is what will enable us to visualize our raster data dynamically, such that we see the lower resolution rasters when very zoomed out (and high resolution data would take too long to load), and higher resolution data when zoomed in and the extent is smaller. Then we convert each of these .tif files into PNG files that we can visualize on a basemap."
  },
  {
    "objectID": "sections/group-project-1.html#staging-and-tiling",
    "href": "sections/group-project-1.html#staging-and-tiling",
    "title": "7  Group Project: Staging and Preprocessing",
    "section": "7.3 Staging and Tiling",
    "text": "7.3 Staging and Tiling\nToday we will undertake the first step of the workflow, staging and tiling the data.\n\nIn your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-07.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills.\nOnly one person in the group should run the parallel code.\n\n7.3.1 Packages, Libraries, and Modules\n\nos 3.11.2\nparsl 2023.3.20\npdgstaging\n\npackage developed by Permafrost Discovery Gateway software developer and designer Robyn Thiessen-Bock\n\ngeopandas0.11\nrandom\nmatplotlib 3.5"
  },
  {
    "objectID": "sections/software-design-1.html#learning-objectives",
    "href": "sections/software-design-1.html#learning-objectives",
    "title": "8  Software Design I: Functions and Concurrency",
    "section": "8.1 Learning Objectives",
    "text": "8.1 Learning Objectives\nThe goal of this module is to gain some useful approaches to software design and modularity that will help with building scalable, portable, and reusable code. We will cover several key aspects of software design for concurrency:\n\nFunctions as objects\nGlobal variables\nPure functions\nTask dependencies\nLocks, deadlocks, and race conditions"
  },
  {
    "objectID": "sections/software-design-1.html#why-functions",
    "href": "sections/software-design-1.html#why-functions",
    "title": "8  Software Design I: Functions and Concurrency",
    "section": "8.2 Why functions?",
    "text": "8.2 Why functions?\n\n\n\n DRY: Don’t Repeat Yourself\n\n\n\n\n\n\nBy creating small functions that handle only one logical task and do it well, we quickly gain:\n\nImproved understanding\nReuse via decomposing tasks into bite-sized chunks\nImproved error testing\nImproved concurrency\n\nWhen writing functions that are to be used in concurrent programming, it is best to keep them short and focused on a single, well-defined task. This enables you test the function thoroughly, and reuse it in multiple contexts. It also enables you to more easily debug what is happening in the function when trying to understands parallel execution."
  },
  {
    "objectID": "sections/software-design-1.html#functions-as-objects",
    "href": "sections/software-design-1.html#functions-as-objects",
    "title": "8  Software Design I: Functions and Concurrency",
    "section": "8.3 Functions as objects",
    "text": "8.3 Functions as objects\nFunctions are first-class objects in Python (and many other languages). This has some real benefits for and implications for parallel programming. Because a function is an object, it means that it can be 1) stored in a variable, and 2) passed as an argument to another function. We saw that in the module on pleasingly parallel codes when we used ThreadPoolExecutor.map(), which takes a function and an iterable object as arguments. Let’s check out how you can use and manipulate functions as objects. First, let’s define a simple function, assign it to another variable, and then use both:\n\ndef double(x):\n    return 2*x\n\n# also assign the function to the `twotimes` variable\ntwotimes = double\ntype(twotimes)\n\nfunction\n\n\nNote that when we print it to screen, we see that prod is of type function, and when we use the two instances, we get identical results:\n\nprint(double(7))\nprint(twotimes(7))\nprint(double(5) == twotimes(5))\n\n14\n14\nTrue\n\n\nThis representation of a function as an object comes in handy when we want to invoke a function in multiple different contexts, such as in a parallel execution environment via a map() function.\n\nlist(map(twotimes, [2,3,4]))\n\n[4, 6, 8]\n\n\nThis works because the function twotimes can be passed to map and executed from within map. When you execute a function that is passed in via an argument, it is called function composition. We can easily illustrate this by creating some function and passing it to a wrapper function to be executed:\n\ndef some_function():\n    print(\"Ran some_function\")\n\ndef wrapper(func_to_run):\n    print(\"Ran wrapper\")\n    func_to_run()\n    print(\"Finished wrapper\")\n\nwrapper(some_function)\n\nRan wrapper\nRan some_function\nFinished wrapper\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote how we passed the some_function as a variable name without the parentheses.\n\n\n\n\n\n\n\n\nDecorators\n\n\n\nThis approach to function composition is exactly what is used by Python decorator functions."
  },
  {
    "objectID": "sections/software-design-1.html#global-variables",
    "href": "sections/software-design-1.html#global-variables",
    "title": "8  Software Design I: Functions and Concurrency",
    "section": "8.4 Global variables",
    "text": "8.4 Global variables\nWhen executing a function, the variables that are in scope to that function are local, meaning that the presence of another variable with the same name in another scope will not affect a calculation. For example:\n\ndef do_task():\n    x = 10\n\nx = 5\ndo_task()\nprint(x)\n\n5\n\n\nHowever, if that same variable is declared as global inside the function, then the assignment will have global impact on the value of the parent scope:\n\ndef do_task():\n    global x\n    x = 10\n\nx = 5\ndo_task()\nprint(x)\n\n10\n\n\nSo, you can see that writing a function that uses global variables can have effects outside of the scope of the function call. This can have drastic consequences on concurrent code, as the order in which function calls are made when operating concurrently are not deterministic, and so the impact of global variables will also not be deterministic.\nA related issue arises when code in a function depends on its enclosing namespace, such as when a function is defined inside of another function. When resolving a variable, python first looks in the Local namespace, and then in the Enclosing namespace, Global namespace, and Built-in namespace. So, even if a variable is not defined locally, it might still be resolved by one of the other namespaces in surprising ways.\n\na = 3\ndef do_stuff(b):\n    return a*b\n\ndo_stuff(6)\n\n18"
  },
  {
    "objectID": "sections/software-design-1.html#pure-functions",
    "href": "sections/software-design-1.html#pure-functions",
    "title": "8  Software Design I: Functions and Concurrency",
    "section": "8.5 Pure functions",
    "text": "8.5 Pure functions\nA pure function is a function that depends only on its input arguments, and it has no side effects. In other words, a pure function returns the same value if called repeatedly with the same arguments. Pure functions are particularly amenable to concurrency. For example, the double(x) function above is a pure function, because in all cases calling double(2) will always return 4.\nIn contrast, a non-pure function is a function in which the return value may change if the function is called repeatedly, typically because it depends on some particular state that affects the outcome but is not part of the input arguments. For example, the time.time() function returns different values based on the current state of the system clock.\nUsing a global variable in a function creates a side-effect that makes it an impure function, as would other operations that modify an external state variable.\n\n\n\n\n\n\nImportant\n\n\n\nPure functions make writing concurrent code much easier."
  },
  {
    "objectID": "sections/software-design-1.html#task-dependencies",
    "href": "sections/software-design-1.html#task-dependencies",
    "title": "8  Software Design I: Functions and Concurrency",
    "section": "8.6 Task dependencies",
    "text": "8.6 Task dependencies\nTask dependencies occur when one task in the code depends on the results of another task or computation in the code."
  },
  {
    "objectID": "sections/software-design-1.html#locks",
    "href": "sections/software-design-1.html#locks",
    "title": "8  Software Design I: Functions and Concurrency",
    "section": "8.7 Locks",
    "text": "8.7 Locks\nLocks are a mechanism to manage access to a resource so that multiple threads can access the resource. By adding locks to an otherwise parallel process, we introduce a degree of serial execution to the locked portion of the process. Basically, each thread can only access the resource when it has the lock, and only one lock is given out at a time. Take this example of what happens without locking:\n\nfrom concurrent.futures import ProcessPoolExecutor\nimport time\n\ndef hello(i):\n    print(i, 'Hello')\n    print(i, 'world')\n\nexecutor = ProcessPoolExecutor()\nfutures = [executor.submit(hello, i) for i in range(3)]\nfor future in futures:\n    future.result()\n\n0\n\n\n1\n\n\n \n\n\nHello\n\n\n\n\n\n0\n\n\n \n\n\nworld\n\n\n \n\n\n\n\n\n2\n\n\nHello\n\n\n \n\n\nHello\n\n\n\n\n\n1\n\n\n \n\n\n\n\n\n2\n\n\nworld\n\n\n\n\n\n \n\n\nworld\n\n\n\n\n\nYou can see that the results come back in a semi-random order, and the call to sleep creates a delay between printing the two words, which means that the three messages get jumbled when printed. To fix this, we can introduce a lock from the multiprocessing package.\n\nfrom concurrent.futures import ProcessPoolExecutor\nimport time\nimport multiprocessing\n\ndef hello(i, lock):\n    with lock:\n        print(i, 'Hello')\n        print(i, 'world')\n\nlock = multiprocessing.Manager().Lock()\nexecutor = ProcessPoolExecutor()\nfutures = [executor.submit(hello, i, lock) for i in range(3)]\nfor future in futures:\n    future.result()\n\n0\n\n\n \n\n\nHello\n\n\n\n\n\n0\n\n\n \n\n\nworld\n\n\n\n\n\n1\n\n\n \n\n\nHello\n\n\n\n\n\n1\n\n\n \n\n\nworld\n\n\n\n\n\n2\n\n\n \n\n\nHello\n\n\n\n\n\n2\n\n\n \n\n\nworld\n\n\n\n\n\nThe lock is generated and then passed to each invocation of the hello function. Using with triggers the use of the context manager for the lock, which allows the manager to synchronize use of the lock. This ensures that only one process can be printing at the same time, ensuring that the outputs are properly ordered.\n\n\n\n\n\n\nWarning\n\n\n\nSynchronizing with a Lock turns a parallel process back into a serial process, at least while the lock is in use. So use with care lest you lose all benefits of concurrency."
  },
  {
    "objectID": "sections/software-design-1.html#race-conditions",
    "href": "sections/software-design-1.html#race-conditions",
    "title": "8  Software Design I: Functions and Concurrency",
    "section": "8.8 Race conditions",
    "text": "8.8 Race conditions\nRace conditions occur when two tasks execute in parallel, but produce different results based on which task finishes first. Ensuring that results are correct under different timing situations requires careful testing."
  },
  {
    "objectID": "sections/software-design-1.html#deadlocks",
    "href": "sections/software-design-1.html#deadlocks",
    "title": "8  Software Design I: Functions and Concurrency",
    "section": "8.9 Deadlocks",
    "text": "8.9 Deadlocks\nDeadlocks occur when two concurrent tasks block on the output of the other. Deadlocks cause parallel programs to lock up indefinitely, can be difficult to track down, and will often require the program to be killed."
  },
  {
    "objectID": "sections/software-design-1.html#further-reading",
    "href": "sections/software-design-1.html#further-reading",
    "title": "8  Software Design I: Functions and Concurrency",
    "section": "8.10 Further reading",
    "text": "8.10 Further reading\n\nWith Statement Context Managers"
  },
  {
    "objectID": "sections/geopandas.html#learning-objectives",
    "href": "sections/geopandas.html#learning-objectives",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.1 Learning Objectives",
    "text": "9.1 Learning Objectives\n\nManipulating raster data with rasterio\nManipulating vector data with geopandas\nWorking with raster and vector data together"
  },
  {
    "objectID": "sections/geopandas.html#introduction",
    "href": "sections/geopandas.html#introduction",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.2 Introduction",
    "text": "9.2 Introduction\nIn this lesson, we’ll be working with geospatial raster and vector data to do an analysis on vessel traffic in south central Alaska. If you aren’t already familiar, geospatial vector data consists of points, lines, and/or polygons, which represent locations on the Earth. Geospatial vector data can have differing geometries, depending on what it is representing (eg: points for cities, lines for rivers, polygons for states.) Raster data uses a set of regularly gridded cells (or pixels) to represent geographic features.\nBoth geospatial vector and raster data have a coordinate reference system, which describes how the points in the dataset relate to the 3-dimensional sphereoid of Earth. A coordinate reference system contains both a datum and a projection. The datum is how you georeference your points (in 3 dimensions!) onto a spheroid. The projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. All coordinate reference systems require a datum. However, some coordinate reference systems are “unprojected” (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984.\nCoordinate reference systems are often referenced using a shorthand 4 digit code called an EPSG code. We’ll be working with two coordinate reference systems in this lesson with the following codes:\n\n3338: Alaska Albers\n4326: WGS84 (World Geodetic System 1984), used in GPS\n\nIn this lesson, we are going to take two datasets:\n\nAlaskan commercial salmon fishing statisical areas\nNorth Pacific and Arctic Marine Vessel Traffic Dataset\n\nand use them to calculate the total distance travelled by ships within each fishing area.\nThe high level steps will be\n\nread in the datasets\nreproject them so they are in the same projection\nextract a subset of the raster and vector data using a bounding box\nturn each polygon in the vector data into a raster mask\nuse the masks to calculate the total distance travelled (sum of pixels) for each fishing area"
  },
  {
    "objectID": "sections/geopandas.html#pre-processing-raster-data",
    "href": "sections/geopandas.html#pre-processing-raster-data",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.3 Pre-processing raster data",
    "text": "9.3 Pre-processing raster data\nFirst we need to load in our libraries. We’ll use geopandas for vector manipulation, rasterio for raster maniupulation.\nFirst, we’ll use requests to download the ship traffic raster from Kapsar et al.. We grab a one month slice from August, 2020 of a coastal subset of data with 1km resolution. To get the URL in the code chunk below, you can right click the download button for the file of interest and select “copy link address.”\n\nimport urllib\n\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A6b847ab0-9a3d-4534-bf28-3a96c5fa8d72'\n\nmsg = urllib.request.urlretrieve(url, \"Coastal_2020_08.tif\")\n\nUsing rasterio, open the raster file, plot it, and look at the metadata. We use the with here as a context manager. This ensures that the connection to the raster file is closed and cleaned up when we are done with it.\n\nimport rasterio\nimport matplotlib.pyplot as plt\n\nwith rasterio.open(\"Coastal_2020_08.tif\") as ship_con:\n    # read in raster (1st band)\n    ships = ship_con.read(1)\n    ships_meta = ship_con.profile\n\nplt.imshow(ships)\nprint(ships_meta)\n\n{'driver': 'GTiff', 'dtype': 'float32', 'nodata': -3.3999999521443642e+38, 'width': 3087, 'height': 2308, 'count': 1, 'crs': CRS.from_epsg(3338), 'transform': Affine(999.7994153462766, 0.0, -2550153.29233849,\n       0.0, -999.9687691991521, 2711703.104608573), 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n\n\n\n\n\nYou’ll notice that we are saving two objects here, ships and ships_meta. Looking at the types of these two objects is useful to understand what rasterio is doing.\n\ntype(ships)\n\nnumpy.ndarray\n\n\n\ntype(ships_meta)\n\nrasterio.profiles.Profile\n\n\nThe ships object is a numpy array, while the ships_meta is a special rasterio class called Profile. To understand why the raster data is represented as an array, and what that profile object is, let’s look into what raster data are, exactly.\n\nSource: Leah A. Wasser, Megan A. Jones, Zack Brym, Kristina Riemer, Jason Williams, Jeff Hollister, Mike Smorul. Raster 00: Intro to Raster Data in R. National Evologial Observatory Network (NEON).\nThe upper left panel of the figure above shows some satellite imagery data. These data are in raster format, which when you zoom in, you can see consist of regularly gridded pixels, each of which contails a value. When we plot these data, we can assign a color map to the pixel values, which generates the image we see. The data themselves, though, are just an n-dimensional grid of numbers. Another way we might describe this is…an array! So, this is why raster data is represented in python using a numpy array.\nThis is all great, and the array of values is a lot of information, but there are some key items that are missing. This array isn’t imaginary, it represents a physical space on this earth, so where is all of that contextual information? The answer is in the rasterio profile object. This object contains all of the metadata needed to interpret the raster array. Here is what our ships_meta contains:\n'driver': 'GTiff',\n'dtype': 'float32',\n'nodata': -3.3999999521443642e+38,\n'width': 3087,\n'height': 2308,\n'count': 1,\n'crs': CRS.from_epsg(3338),\n'transform': Affine(999.7994153462766, 0.0, -2550153.29233849, 0.0, -999.9687691991521, 2711703.104608573),\n'tiled': False,\n'compress': 'lzw',\n'interleave': 'band'}\nThis object gives us critical information, like the CRS of the data, the no data value, and the transform. The transform is what allows us to move from image pixel (row, column) coordinates to and from geographic/projected (x, y) coordinates. The transform and the CRS are crititally important, and related. If the CRS are instructions for how the coordinates can be represented in space and on a flat surface (in the case of projected coordinate systems), then the transform describes how to locate the raster array positions in the correct coordinates given by the CRS.\nNote that since the array and the profile are in separate objects it is easy to lose track of one of them, accidentally overwrite it, etc. Try to adopt a naming convention that works for you because they usually need to work together in geospatial operations."
  },
  {
    "objectID": "sections/geopandas.html#pre-processing-vector-data",
    "href": "sections/geopandas.html#pre-processing-vector-data",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.4 Pre-processing vector data",
    "text": "9.4 Pre-processing vector data\nNow download a vector shapefile of commercial fishing districts in Alaska.\n\nurl = 'https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7c942c45-1539-4d47-b429-205499f0f3e4'\n\nmsg = urllib.request.urlretrieve(url, \"Alaska_Commercial_Salmon_Boundaries.gpkg\")\n\nRead in the data using geopandas.\n\nimport geopandas as gpd\n\ncomm = gpd.read_file(\"Alaska_Commercial_Salmon_Boundaries.gpkg\")\n\nNote the “pandas” in the library name “geopandas.” Our comm object is really just a special type of pandas data frame called a geodataframe. This means that in addition to any geospatial stuff we need to do, we can also just do regular pandas things on this data frame.\nFor example, we can get a list of column names (there are a lot!)\n\ncomm.columns.values\n\narray(['OBJECTID', 'GEOMETRY_START_DATE', 'GEOMETRY_END_DATE',\n       'STAT_AREA', 'STAT_AREA_NAME', 'FISHERY_GROUP_CODE',\n       'GIS_SERIES_NAME', 'GIS_SERIES_CODE', 'REGION_CODE',\n       'REGISTRATION_AREA_NAME', 'REGISTRATION_AREA_CODE',\n       'REGISTRATION_AREA_ID', 'REGISTRATION_LOCATION_ABBR',\n       'MANAGEMENT_AREA_NAME', 'MANAGEMENT_AREA_CODE', 'DISTRICT_NAME',\n       'DISTRICT_CODE', 'DISTRICT_ID', 'SUBDISTRICT_NAME',\n       'SUBDISTRICT_CODE', 'SUBDISTRICT_ID', 'SECTION_NAME',\n       'SECTION_CODE', 'SECTION_ID', 'SUBSECTION_NAME', 'SUBSECTION_CODE',\n       'SUBSECTION_ID', 'COAR_AREA_CODE', 'CREATOR', 'CREATE_DATE',\n       'EDITOR', 'EDIT_DATE', 'COMMENTS', 'STAT_AREA_VERSION_ID',\n       'Shape_Length', 'Shape_Area', 'geometry'], dtype=object)\n\n\nWe can also look at the head of the data frame:\n\ncomm.head()\n\n\n\n\n\n  \n    \n      \n      OBJECTID\n      GEOMETRY_START_DATE\n      GEOMETRY_END_DATE\n      STAT_AREA\n      STAT_AREA_NAME\n      FISHERY_GROUP_CODE\n      GIS_SERIES_NAME\n      GIS_SERIES_CODE\n      REGION_CODE\n      REGISTRATION_AREA_NAME\n      ...\n      COAR_AREA_CODE\n      CREATOR\n      CREATE_DATE\n      EDITOR\n      EDIT_DATE\n      COMMENTS\n      STAT_AREA_VERSION_ID\n      Shape_Length\n      Shape_Area\n      geometry\n    \n  \n  \n    \n      0\n      12\n      1975-01-01 00:00:00+00:00\n      NaT\n      33461\n      Tanana River mouth to Kantishna River\n      B\n      Salmon\n      B\n      3\n      Yukon Area\n      ...\n      YU\n      Evelyn Russel\n      2006-03-26 00:00:00+00:00\n      Sabrina Larsen\n      2017-02-02 00:00:00+00:00\n      Yukon District, 6 Subdistrict and 6-A Section ...\n      NaN\n      4.610183\n      0.381977\n      MULTIPOLYGON (((-151.32805 64.96913, -151.3150...\n    \n    \n      1\n      13\n      1975-01-01 00:00:00+00:00\n      NaT\n      33462\n      Kantishna River to Wood River\n      B\n      Salmon\n      B\n      3\n      Yukon Area\n      ...\n      YU\n      Evelyn Russel\n      2006-03-26 00:00:00+00:00\n      Sabrina Larsen\n      2017-02-02 00:00:00+00:00\n      Yukon District, 6 Subdistrict and 6-B Section ...\n      NaN\n      3.682421\n      0.321943\n      MULTIPOLYGON (((-149.96255 64.70518, -149.9666...\n    \n    \n      2\n      18\n      1978-01-01 00:00:00+00:00\n      NaT\n      33431\n      Toklik to Cottonwood Point\n      B\n      Salmon\n      B\n      3\n      Yukon Area\n      ...\n      YL\n      Evelyn Russel\n      2006-03-26 00:00:00+00:00\n      Sabrina Larsen\n      2017-02-02 00:00:00+00:00\n      Yukon District and 3 Subdistrict until 1/1/1980\n      NaN\n      2.215641\n      0.198740\n      MULTIPOLYGON (((-161.39853 61.55463, -161.4171...\n    \n    \n      3\n      19\n      1980-01-01 00:00:00+00:00\n      NaT\n      33442\n      Right Bank, Bishop Rock to Illinois Creek\n      B\n      Salmon\n      B\n      3\n      Yukon Area\n      ...\n      YU\n      Evelyn Russel\n      2006-03-26 00:00:00+00:00\n      Sabrina Larsen\n      2017-02-02 00:00:00+00:00\n      NaN\n      NaN\n      9.179852\n      0.382788\n      MULTIPOLYGON (((-153.15234 65.24944, -153.0761...\n    \n    \n      4\n      20\n      1980-01-01 00:00:00+00:00\n      NaT\n      33443\n      Left Bank, Cone Point to Illinois Creek\n      B\n      Salmon\n      B\n      3\n      Yukon Area\n      ...\n      YU\n      Evelyn Russel\n      2006-03-26 00:00:00+00:00\n      Sabrina Larsen\n      2017-02-02 00:00:00+00:00\n      NaN\n      NaN\n      9.500826\n      0.378262\n      MULTIPOLYGON (((-152.99905 65.17027, -152.9897...\n    \n  \n\n5 rows × 37 columns\n\n\n\nNote the existence of the geometry column. This is where the actual geospatial points that comprise the vector data are stored, and this brings up the important difference between raster and vector data - while raster data is regularly gridded at a specific resolution, vector data are just points in space.\n\nSource: Leah A. Wasser, Megan A. Jones, Zack Brym, Kristina Riemer, Jason Williams, Jeff Hollister, Mike Smorul. Raster 00: Intro to Raster Data in R. National Evologial Observatory Network (NEON).\nThe diagram above shows the three different types of geometries that geospatial vector data can take, points, lines or polygons. Whatever the geometry type, the geometry information (the x,y points) is stored in the column named geometry in the geopandas data frame. In this example, we have a dataset containing polygons of fishing districts. Each row in the dataset corresonds to a district, with unique attributes (the other columns in the dataset), and it’s own set of points defining the boundaries of the district, contained in the geometry column.\n\ncomm['geometry'][:5]\n\n0    MULTIPOLYGON (((-151.32805 64.96913, -151.3150...\n1    MULTIPOLYGON (((-149.96255 64.70518, -149.9666...\n2    MULTIPOLYGON (((-161.39853 61.55463, -161.4171...\n3    MULTIPOLYGON (((-153.15234 65.24944, -153.0761...\n4    MULTIPOLYGON (((-152.99905 65.17027, -152.9897...\nName: geometry, dtype: geometry\n\n\nSo, now we know where our x,y points are, where is all of the other information like the crs? With vector data, all of this information is contained within the geodataframe. We can access the crs attribute on the data frame and print it like so:\n\ncomm.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nWe can also plot one geometry cell by simply calling it:\n\ncomm['geometry'][8]\n\n\n\n\nNow that we know a little about what we are working with, let’s get this data ready to analyze. First, we can make a plot of it just to see what we have.\n\ncomm.plot(figsize=(9,9))\n\n<AxesSubplot:>\n\n\n\n\n\nThis plot doesn’t look so good. Turns out, these data are in WGS 84 (EPSG 4326), as opposed to Alaska Albers (EPSG 3338), which is what our raster data are in. To make pretty plots, and allow our raster data and vector data to be analyzed together, we’ll need to reproject the vector data into 3338. To to this, we’ll use the to_crs method on our comm object, and specify as an argument the projection we want to transform to.\n\ncomm_3338 = comm.to_crs(\"EPSG:3338\")\n\ncomm_3338.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nMuch better!"
  },
  {
    "objectID": "sections/geopandas.html#crop-data-to-area-of-interest",
    "href": "sections/geopandas.html#crop-data-to-area-of-interest",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.5 Crop data to area of interest",
    "text": "9.5 Crop data to area of interest\nFor this example, we are only interested in south central Alaska, encompassing Prince William Sound, Cook Inlet, and Kodiak. Our raster data is significantly larger than that, and the vector data is statewide. So, as a first step we might want to crop our data to the area of interest.\nFirst, we’ll need to create a bounding box. We use the box function from shapely to create the bounding box, then create a geoDataFrame from the points, and finally convert the WGS 84 coordinates to the Alaska Albers projection.\n\nfrom shapely.geometry import box\n\ncoord_box = box(-159.5, 55, -144.5, 62)\n\ncoord_box_df = gpd.GeoDataFrame(\n    crs = 'EPSG:4326',\n    geometry = [coord_box]).to_crs(\"EPSG:3338\")\n\nNow, we can read in raster again cropped to bounding box. We use the mask function from rasterio.mask. Note that we apply this to the connection to the raster file (with rasterio.open(...)), then update the metadata associated with the raster, because the mask function requires as its first dataset argument a dataset object opened in r mode.\n\nimport rasterio.mask\nimport numpy as np\n\nwith rasterio.open(\"Coastal_2020_08.tif\") as ship_con:\n    shipc_arr, shipc_transform = rasterio.mask.mask(ship_con,\n                                                    coord_box_df[\"geometry\"],\n                                                    crop=True)\n    shipc_meta = ship_con.meta\n    # select just the 2-D array (by default a 3-D array is returned even though we only have one band)\n    shipc_arr = shipc_arr[0,:,:]\n    # turn the no-data values into NaNs.\n    shipc_arr[shipc_arr == ship_con.nodata] = np.nan\n\n\nshipc_meta.update({\"driver\": \"GTiff\",\n                 \"height\": shipc_arr.shape[0],\n                 \"width\": shipc_arr.shape[1],\n                 \"transform\": shipc_transform,\n                 \"compress\": \"lzw\"})\n\nNow we’ll do a similar task with the vector data. Tin this case, we use a spatial join. The join will be an inner join, and select only rows from the left side (our fishing districts) that are within the right side (our bounding box). I chose this method as opposed to a clipping type operation because it ensures that we don’t end up with any open polygons at the boundaries of our box, which could cause problems for us down the road.\n\ncomm_clip = gpd.sjoin(comm_3338,\n                      coord_box_df,\n                      how='inner',\n                      predicate='within')\n\n\n9.5.1 Check extents\nNow let’s look at the two cropped datasets overlayed on each other to ensure that the extents look right.\n\nimport rasterio.plot\n\n# set up plot\nfig, ax = plt.subplots(figsize=(7, 7))\n# plot the raster\nrasterio.plot.show(shipc_arr,\n                   ax=ax,\n                   vmin = 0,\n                   vmax = 50000,\n                   transform = shipc_transform)\n# plot the vector\ncomm_clip.plot(ax=ax, facecolor='none', edgecolor='red')\n\n<AxesSubplot:>"
  },
  {
    "objectID": "sections/geopandas.html#calculate-total-distance-per-fishing-area",
    "href": "sections/geopandas.html#calculate-total-distance-per-fishing-area",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.6 Calculate total distance per fishing area",
    "text": "9.6 Calculate total distance per fishing area\nIn this step, we rasterize each polygon in the shapefile, such that pixels in or touching the polygon get a value of 1, and pixels not touching it get a value of 0. Then, for each polygon, we extract the indices of the raster array that are equal to 1. We then extract the values of these indicies from the original ship traffic raster data, and calculate the sum of the values over all of those pixels.\nHere is a simplified diagram of the process:\n\n\n9.6.0.1 Zonal statistics over one polygon\nLet’s look at how this works over just one fishing area first. We use the rasterize method from the features module in rasterio. This takes as arguments the data to rasterize (in this case the 40th row of our dataset), and the shape and transform the output raster will take. We alo set the all_touched argument to true, which means any pixel that touches a boundary of our vector will be burned into the mask.\n\nfrom rasterio import features\n\nr40 = features.rasterize(comm_clip['geometry'][40].geoms,\n                                    out_shape=shipc_arr.shape,\n                                    transform=shipc_meta['transform'],\n                                    all_touched=True)\n\nIf we have a look at a plot of our rasterized version of the single fishing district, we can see that instead of a vector, we now have a raster showing the rasterized district (with pixel values of 1) and any area not in the district has a pixel value of 0.\n\n# set up plot\nfig, ax = plt.subplots(figsize=(7, 7))\n# plot the raster\nrasterio.plot.show(r40,\n                   ax=ax,\n                   vmin = 0,\n                   vmax = 1,\n                   transform = shipc_meta['transform'])\n# plot the vector\ncomm_clip.plot(ax=ax, facecolor='none', edgecolor='red')\n\n<AxesSubplot:>\n\n\n\n\n\nA quick call to np.unique shows our unique values are 0 or 1, which is what we expect.\n\nnp.unique(r40)\n\narray([0, 1], dtype=uint8)\n\n\nFinally, we need to know is the indices of the original raster where the fishing district is. We can use np.where to extract this information\n\nr40_index = np.where(r40 == 1)\nprint(r40_index)\n\n(array([108, 108, 108, 108, 108, 109, 109, 109, 109, 109, 109, 109, 109,\n       109, 109, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110,\n       110, 110, 110, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111,\n       111, 111, 111, 111, 112, 112, 112, 112, 112, 112, 112, 112, 112,\n       112, 112, 112, 112, 112, 112, 113, 113, 113, 113, 113, 113, 113,\n       113, 113, 113, 113, 113, 113, 113, 113, 114, 114, 114, 114, 114,\n       114, 114, 114, 114, 114, 114, 115, 115, 115, 115, 115, 115, 116]), array([759, 760, 762, 763, 764, 755, 756, 757, 758, 759, 761, 762, 763,\n       764, 765, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763,\n       764, 765, 766, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762,\n       763, 764, 765, 766, 753, 754, 755, 756, 757, 758, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 753, 754, 755, 756, 757, 758, 759,\n       760, 761, 762, 763, 764, 765, 766, 767, 753, 754, 755, 756, 757,\n       758, 759, 760, 761, 762, 763, 753, 754, 755, 756, 757, 758, 754]))\n\n\nIn the last step, we’ll using these indices to extract the values of the data from the fishing raster, and sum them to get a total distance travelled.\n\nnp.nansum(shipc_arr[r40_index])\n\n14369028.0\n\n\nNow that we know the individual steps, let’s run this over all of the districts. First we’ll create an id column in the vector data frame. This will help us track unique fishing districts later.\n\n\n9.6.0.2 Zonal statistics over all polygons\n\ncomm_clip['id'] = range(0,len(comm_clip))\n\nFor each district (with geometry and id), we run the features.rasterize function. Then we calculate the sum of the values of the shipping raster r_array based on the indicies in the raster where the district is located.\n\ndistance_dict = {}\nfor geom, idx in zip(comm_clip['geometry'], comm_clip['id']):\n    rasterized = features.rasterize(geom.geoms,\n                                    out_shape=shipc_arr.shape,\n                                    transform=shipc_meta['transform'],\n                                    all_touched=True)\n\n    r_index = np.where(rasterized == 1)\n    distance_dict[idx] = np.nansum(shipc_arr[r_index])\n\nNow we just create a data frame from that dictionary, and join it to the vector data using pandas operations.\n\nimport pandas as pd\n\n# create a data frame from the result\ndistance_df = pd.DataFrame.from_dict(distance_dict,\n                                     orient='index',\n                                     columns=['distance'])\n\n# extract the index of the data frame as a column to use in a join and convert distance to kilometers\ndistance_df['id'] = distance_df.index\ndistance_df['distance'] = distance_df['distance']/1000\n\nNow we join the result to the original geodataframe.\n\n# join the sums to the original data frame\nres_full = comm_clip.merge(distance_df,\n                           on = \"id\",\n                           how = 'inner')\n\nFinally, we can plot our result!\n\nimport matplotlib.ticker\nfig, ax = plt.subplots(figsize=(7, 7))\n\nax = res_full.plot(column = \"distance\", legend = True, ax = ax)\nfig = ax.figure\nlabel_format = '{:,.0f}'\ncb_ax = fig.axes[1]\nticks_loc = cb_ax.get_yticks().tolist()\ncb_ax.yaxis.set_major_locator(matplotlib.ticker.FixedLocator(ticks_loc))\ncb_ax.set_yticklabels([label_format.format(x) for x in ticks_loc])\nax.set_axis_off()\nax.set_title(\"Distance Traveled by Ships in Kilometers\")\nplt.show()\n\n\n\n\nFrom here we can do any additional geopandas operations we might be interested in. For example, what if we want to calculate the total distance by registration area (a superset of fishing district). We can do that using dissolve from geopandas.\n\nreg_area = res_full.dissolve(by = \"REGISTRATION_AREA_NAME\", \n                             aggfunc = 'sum')\n\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/geopandas/geodataframe.py:1686: FutureWarning:\n\nThe default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n\n\n\nLet’s have a look at the same plot as before, but this time over our aggregated data.\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\nax = reg_area.plot(column = \"distance\", legend = True, ax = ax)\nfig = ax.figure\nlabel_format = '{:,.0f}'\ncb_ax = fig.axes[1]\nticks_loc = cb_ax.get_yticks().tolist()\ncb_ax.yaxis.set_major_locator(matplotlib.ticker.FixedLocator(ticks_loc))\ncb_ax.set_yticklabels([label_format.format(x) for x in ticks_loc])\nax.set_axis_off()\nax.set_title(\"Distance Traveled by Ships in Kilometers\")\nplt.show()"
  },
  {
    "objectID": "sections/geopandas.html#summary",
    "href": "sections/geopandas.html#summary",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.7 Summary",
    "text": "9.7 Summary\nWe covered a lot of ground here, so let’s recap some of the high level points:\n\nRaster data consist of regularly gridded values, and can be represented in python as an array\nVector data consist of any number of points, that might be connected, and is represented in python as a geodataframe\nWe can do geospatial operations like changing the projection or cropping the data to a particular extent on both raster and vector data\nYou can use vector data to help analyze raster data (and vice versa!) by rasterizing the vector data and using numpy operations on the resulting array."
  },
  {
    "objectID": "sections/ice-wedge-polygons.html#introduction",
    "href": "sections/ice-wedge-polygons.html#introduction",
    "title": "10  Billions of Ice Wedge Polygons",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nThe Permafrost Discovery Gateway (PDG) is an online platform for creation, archiving, processing, analysis, and visualization of permafrost big imagery products to enable discovery and knowledge-generation. The new online scientific gateway will make information of changing permafrost conditions available throughout the Arctic by providing access to very high resolution satellite data products and new visualization tools that will allow exploration and discovery for researchers, educators, and the public at large.\nKnowledge discovery through AI, big imagery, high-performance computing (HPC) is just starting to be realized in Arctic permafrost science. Chandi Witharana and the PDG team have developed a novel high- performance image analysis framework – Mapping application for Arctic Permafrost Land Environment (MAPLE) that enables the integration of operational-scale GeoAI capabilities into Arctic permafrost modeling. Interoperability across heterogeneous HPC systems, optimal usage of computational resource, and extensibility of mapping workflows are some of the key design goals of MAPLE. MAPLE was recently deployed across the Arctic tundra to map ice-wedge polygons from thousands of Maxar imagery. This mapping exercise has produced the first pan-Arctic ice-wedge polygon map, which consists of more than one billion individual ice-wedge polygons."
  },
  {
    "objectID": "sections/ice-wedge-polygons.html#billions-of-ice-wedge-polygons",
    "href": "sections/ice-wedge-polygons.html#billions-of-ice-wedge-polygons",
    "title": "10  Billions of Ice Wedge Polygons",
    "section": "10.2 Billions of Ice Wedge Polygons",
    "text": "10.2 Billions of Ice Wedge Polygons\nChandi’s slides are available to download here"
  },
  {
    "objectID": "sections/ice-wedge-polygons.html#developing-science-gateways-with-clowder-for-the-permafrost-discover-gateway",
    "href": "sections/ice-wedge-polygons.html#developing-science-gateways-with-clowder-for-the-permafrost-discover-gateway",
    "title": "10  Billions of Ice Wedge Polygons",
    "section": "10.3 Developing Science Gateways with Clowder for the Permafrost Discover Gateway",
    "text": "10.3 Developing Science Gateways with Clowder for the Permafrost Discover Gateway\nLuigi’s slides are available to download here"
  },
  {
    "objectID": "sections/group-project-2.html#setup",
    "href": "sections/group-project-2.html#setup",
    "title": "11  Group Project: Data Processing",
    "section": "11.1 Setup",
    "text": "11.1 Setup\nIn your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-11.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills.\nOnly one person in the group should run the parallel code."
  },
  {
    "objectID": "sections/group-project-2.html#rasterizing-geopackages",
    "href": "sections/group-project-2.html#rasterizing-geopackages",
    "title": "11  Group Project: Data Processing",
    "section": "11.2 Rasterizing GeoPackages",
    "text": "11.2 Rasterizing GeoPackages\nIn the last lession, we staged the input files into GeoPackages. Now we will import those .gpkg files and write each as a raster (.tif). The resulting rasters will have 2 bands, one for each statistic we calculate based on the vector geometries in each pixel.\nWe need to create the highest zoom level rasters before we create the lower zoom levels in the next lesson. This brings us one step closer to visualizing the ice wedge polygons on a basemap, with the ability to zoom in and out!\n\n11.2.1 Packages, Libraries, and Modules\n\nos 3.11.2\nparsl 2023.3.20\npdgstaging\n\npackage developed by Permafrost Discovery Gateway software developer and designer Robyn Thiessen-Bock\n\npdgraster\n\npackage developed by Permafrost Discovery Gateway software developer and designer Robyn Thiessen-Bock\n\ngeopandas0.11\nrandom\nmatplotlib 3.5"
  },
  {
    "objectID": "sections/data-ethics.html#learning-objectives",
    "href": "sections/data-ethics.html#learning-objectives",
    "title": "12  Data Ethics for Scalable Computing",
    "section": "12.1 Learning objectives",
    "text": "12.1 Learning objectives\n\nReview FAIR and CARE Principles, and their relevance to data ethics\nExamine how ethical considerations are shared and considered at the Arctic Data Center\nDiscuss ethical considerations in machine learning"
  },
  {
    "objectID": "sections/data-ethics.html#intro-to-data-ethics",
    "href": "sections/data-ethics.html#intro-to-data-ethics",
    "title": "12  Data Ethics for Scalable Computing",
    "section": "12.2 Intro to Data Ethics",
    "text": "12.2 Intro to Data Ethics\n\nTo recap, the Arctic Data Center is an openly-accessible data repository and the data published through the repository is open for anyone to reuse, subject to conditions of the license (at the Arctic Data Center, data is released under one of two licenses: CC-0 Public Domain and CC-By Attribution 4.0). In facilitating use of data resources, the data stewardship community has converged on principles surrounding best practices for open data management.\nTwo principles that the Arctic Data Center explicitly adopts are FAIR Principles (Findable, Accessible, Interoperable, and Reproducible) and CARE Principles for Indigenous Governance (Collective Benefit, Authority to Control, Responsibility, Ethics).\nFAIR and CARE principles are relevant in the context of data ethics for multiple reasons. FAIR speaks to how metadata is managed, stored, and shared.\n\nFAIR principles and open science are overlapping concepts, but are distinctive from one another. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. The FAIR principles place emphasis on machine readability, “distinct from peer initiatives that focus on the human scholar” (Wilkinson et al 2016) and as such, do not fully engage with sensitive data considerations and with Indigenous rights and interests (Research Data Alliance International Indigenous Data Sovereignty Interest Group, 2019). Metadata can be FAIR but not open. For example, sensitive data (data that contains personal information) may not be appropriate to share, however sharing the anonymized metadata that is easily understandable will reduce research redundancy.\n\nResearch has historically perpetuated colonialism and represented extractive practices, meaning that the research results were not mutually beneficial. These issues also related to how data was owned, shared, and used. To address issues like these, the Global Indigenous Data Alliance (GIDA) introduced CARE Principles for Indigenous Data Governance to support Indigenous data sovereignty. CARE Principles speak directly to how the data is stored and shared in the context of Indigenous data sovereignty. CARE Principles stand for:\n\nCollective Benefit - Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data\nAuthority to Control - Indigenous Peoples’ rights and interests in Indigenous data must be recognized and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data.\nResponsibility - Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples.\nEthics - Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem. To many, the FAIR and CARE principles are viewed by many as complementary: CARE aligns with FAIR by outlining guidelines for publishing data that contributes to open-science and at the same time, accounts for Indigenous’ Peoples rights and interests."
  },
  {
    "objectID": "sections/data-ethics.html#ethics-at-the-arctic-data-center",
    "href": "sections/data-ethics.html#ethics-at-the-arctic-data-center",
    "title": "12  Data Ethics for Scalable Computing",
    "section": "12.3 Ethics at the Arctic Data Center",
    "text": "12.3 Ethics at the Arctic Data Center\nTransparency in data ethics is a vital part of open science. Regardless of discipline, various ethical concerns are always present, including professional ethics such as plagiarism, false authorship, or falsification of data, to ethics regarding the handling of animals, to concerns relevant to human subjects research. As the primary repository for the Arctic program of the National Science Foundation, the Arctic Data Center accepts Arctic data from all disciplines. Recently, a new submission feature was released which asks researchers to describe the ethical considerations that are apparent in their research. This question is asked to all researchers, regardless of disciplines.\nSharing ethical practices openly, similar in the way that data is shared, enables deeper discussion about data management practices, data reuse, sensitivity, sovereignty and other considerations. Further, such transparency promotes awareness and adoption of ethical practices.\nInspired by CARE Principles for Indigenous Data Governance (Collective Benefit, Authority to Control, Responsibility, Ethics) and FAIR Principles (Findable, Accessible, Interoperable, Reproducible), we include a space in the data submission process for researchers to describe their ethical research practices. These statements are published with each dataset, and the purpose of these statements is to promote greater transparency in data collection and to guide other researchers. For more information about the ethical research practices statement, check out this blog.\nTo help guide researchers as they write their ethical research statements, we have listed the following ethical considerations that are available on our website. The concerns are organized first by concerns that should be addressed by all researchers, and then by discipline.\nConsider the following ethical considerations that are relevant for your field of research.\n\n12.3.1 Ethical Considerations for all Arctic Researchers\n\nResearch Planning\n\nWere any permits required for your research?\nWas there a code of conduct for the research team decided upon prior to beginning data collection?\nWas institutional or local permission required for sampling?\nWhat impact will your research have on local communities or nearby communities (meaning the nearest community within a 100 mile radius)?\n\nData Collection\n\nWere any local community members involved at any point of the research process, including study site identification, sampling, camp setup, consultation or synthesis?\nWere the sample sites near or on Indigenous land or communities?\n\nData Sharing and Publication\n\nHow were the following concerns accounted for: misrepresentation of results, misrepresentation of experience, plagiarism, improper authorship, or the falsification or data?\nIf this data is intended for publication, are authorship expectations clear for everyone involved? Other professional ethics can be found here\n\n\n\n12.3.2 Archaeological and Paleontological Research\n\nResearch Planning\n\nWere there any cultural practices relevant to the study site? If yes, how were these practices accounted for by the research methodologies.\n\nData Collection\n\nDid your research include the removal or artifacts?\nWere there any contingencies made for the excavation and return of samples after cleaning, processing, and analysis?\n\nData Sharing and Publication 4. Were the samples deposited to a physical repository? 5. Were there any steps taken to account for looting threats? Please explain why or why not?\n\n\n12.3.3 Human Participation and Sensitive Data\n\nResearch Planning\n\nPlease describe the institutional IRB approval that was required for this research.\nWas any knowledge provided by community members?\n\nData Collection 3. Did participants receive compensation for their participation? 4. Were decolonization methods used?\nData Sharing and Publication\n\nHave you shared this data with the community or participants involved?\n\n\n\n12.3.4 Marines Sciences (e.g. Marine Biology Research)\n\nResearch Planning\n\nWere any of the study sites or species under federal or local protection?\n\nData Collection\n\nWere endangered, threatened, or otherwise special-status species collected?\nHow were samples collected? Please describe any handling practices used to collect data.\nWhat safety measures were in place to keep researchers, research assistants, technicians, etc., out of harms way during research?\nHow were animal care procedures evaluated, and do they follow community norms for organismal care?\n\nData Sharing and Publication 6. Did the species or study area represent any cultural importance to local communities, or include culturally sensitive information? Please explain how you came to this conclusion and how any cultural sensitivity was accounted for.\n\n\n12.3.5 Physical Sciences (e.g. Geology, Glaciology, and Ice Research)\n\nResearch Planning 1. Was any knowledge provided by community members, including information regarding the study site?\nData Collection\n\nWhat safety measures were in place to keep researchers, research assistants, technicians, etc., out of harms way during research?\nWere there any impacts to the environment/habitat before, during or after data collection?\n\nData Sharing and Publication\n\nIs there any sensitive information including information on sensitive sites, valuable samples, or culturally sensitive information?\n\n\n\n12.3.6 Plant and Soil Research\n\nResearch Planning\n\nWere any of the study sites protected under local or federal regulation?\nWas any knowledge provided by nearby community members, including information regarding the study site?\n\nData Collection\n\nDid sample collection result in erosion of soil or other physical damage? If so, how was this addressed?\nWhat safety measures were in place to keep researchers, research assistants, technicians, etc., out of harms way during research?\n\nData Sharing and Publication\n\nDo any of the study sites or specimens represent culturally sensitive areas or species? Please explain how you came to this conclusion, and if yes, how was this accounted for?\n\n\n\n12.3.7 Spatial Data\n\nResearch Planning\n\nWere any land permits required for this research?\n\nData Collection\n\nWere any data collected using citizen science or community participation?\nIf yes, were community members compensated for their time and made aware of their data being used and for what purpose?\n\nData Sharing and Publication\n\nIf data were ground-truthed, was institutional or local permissions required and/or obtained for land/property access?\nHave you shared this data with the community or participants involved?\nIf location sensitive data was obtained (endangered/threatened flora & fauna location, archaeological and historical sites, identifiable ships, sensitive spatial information), how were the data desensitized?\n\n\n\n12.3.8 Wildlife Sciences (e.g. Ecology and Biology Research)\n\nResearch Planning\n\nWere any permits required for data sampling?\n\nData Collection\n\nWere endangered, threatened, or otherwise special-status plants or animal species collected?\nHow were samples collected? Please describe any handling practices used to collect data.\nWhat safety measures were in place to keep researchers, research assistants, technicians, etc., out of harms way during research?\nHow were animal care procedures evaluated, and do they follow community norms for organism care?\n\nData Sharing and Publication\n\nDo any of the study sites or specimens represent culturally sensitive areas or species? Please explain how you came to this conclusion, and if yes, how was this accounted for?\n\nMenti question:\n\nHave you thought about any of the ethical considerations listed above before?\nWere any of the considerations new or surprising?\nAre there any for your relevant discipline that are missing?"
  },
  {
    "objectID": "sections/data-ethics.html#ethics-in-machine-learning",
    "href": "sections/data-ethics.html#ethics-in-machine-learning",
    "title": "12  Data Ethics for Scalable Computing",
    "section": "12.4 Ethics in Machine Learning",
    "text": "12.4 Ethics in Machine Learning\nMenti poll\n\nWhat is your level of familiarity with machine learning\nHave you thought about ethics in machine learning prior to this lesson?\nCan anyone list potential ethical considerations in machine learning?\n\nWhat comes to mind when considering ethics in machine learning?\nThe stories that hit the news are often of privacy breaches or biases seeping into the training data. Bias can enter at any point of the research project, from preparing the training data, designing the algorithms, to collecting and interpreting the data. When working with sensitive data, a question to also consider is how to deanonymize, anonymized data. A unique aspect to machine learning is how personal bias can influence the analysis and outcomes. A great example of this is the case of ImageNet.\n\n12.4.1 ImageNet: A case study of ethics and bias in machine learning\n Image source: Kate Crawford and Trevor Paglen, “Excavating AI: The Politics of Training Sets for Machine Learning” (September 19, 2019).\nImageNet is a great example of how personal bias can enter machine learning through the training data. ImageNet was a training data set of photos that was used to train image classifiers. The data set was initially created as a large collection of pictures, which were mainly used to identify objects, but some included images of people. The creators of the data set created labels to categorize the images, and through crowdsourcing, people from the internet labeled these images. (This example is from Kate Crawford and Trevor Paglen, “Excavating AI: The Politics of Training Sets for Machine Learning”, September 19, 2019).\n\n12.4.1.1 Discussion:\n\nWhere are the two areas bias could enter this scenario?\nAre there any ways that this bias could be avoided?\nWhile this example is specific to images, can you think of any room for bias in your research?"
  },
  {
    "objectID": "sections/data-ethics.html#references-and-further-reading",
    "href": "sections/data-ethics.html#references-and-further-reading",
    "title": "12  Data Ethics for Scalable Computing",
    "section": "12.5 References and Further Reading",
    "text": "12.5 References and Further Reading\nCarroll, S.R., Herczog, E., Hudson, M. et al. (2021) Operationalizing the CARE and FAIR Principles for Indigenous data futures. Sci Data 8, 108 https://doi.org/10.1038/s41597-021-00892-0\nChen, W., & Quan-Haase, A. (2020) Big Data Ethics and Politics: Towards New Understandings. Social Science Computer Review. https://journals.sagepub.com/doi/10.1177/0894439318810734\nCrawford, K., & Paglen, T. (2019) Excavating AI: The Politics of Training Sets for Machine Learning. https://excavating.ai/\nGray, J., & Witt, A. (2021) A feminist data ethics of care framework for machine learning: The what, why, who and how. First Monday, 26(12), Article number: 11833\nPuebla, I., & Lowenberg, D. (2021) Recommendations for the Handling for Ethical Concerns Relating to the Publication of Research Data. FORCE 11. https://force11.org/post/recommendations-for-the-handling-of-ethical-concerns-relating-to-the-publication-of-research-data/\nResearch Data Alliance International Indigenous Data Sovereignty Interest Group. (2019). “CARE Principles for Indigenous Data Governance.” The Global Indigenous Data Alliance. GIDA-global.org\nWilkinson, M., Dumontier, M., Aalbersberg, I. et al. (2016) The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018. https://doi.org/10.1038/sdata.2016.18\nZwitter, A., Big Data ethics. (2014) Big Data and Society. DOI: 10.1177/2053951714559253"
  },
  {
    "objectID": "sections/google-earth-engine.html#learning-objectives",
    "href": "sections/google-earth-engine.html#learning-objectives",
    "title": "13  Google Earth Engine",
    "section": "13.1 Learning Objectives",
    "text": "13.1 Learning Objectives\n\nUnderstand what Google Earth Engine provides and its applications\nLearn how to search for, import, manipulate, and visualize Google Earth Engine Data\nLearn about some real-world applications of Google Earth Engine in the geosciences"
  },
  {
    "objectID": "sections/google-earth-engine.html#introduction",
    "href": "sections/google-earth-engine.html#introduction",
    "title": "13  Google Earth Engine",
    "section": "13.2 Introduction",
    "text": "13.2 Introduction\nGoogle Earth Engine (GEE) is a geospatial processing platform powered by Google Cloud Platform. It contains over 30 years (and multiple petabytes) of satellite imagery and geospatial datasets that are continually updated and available instantly. Users can process data using Google Cloud Platform and built-in algorithms or by using the Earth Engine API, which is available in Python (and JavaScript) for anyone with an account (Earth Engine is free to use for research, education, and nonprofit use). \n\n\nGEE is just one of a number of cloud platform solutions developed for climate and geoscience research. Others include Microsoft Planetary Computer, Pangeo, & Amazon Sustainability Data Initiative (ADSI)\n\n\n\n\nImage Source: Earth Engine Data Catalog\n\n So what’s so exciting about platforms like GEE? Ryan Abernathey frames this nicely in his blogpost Closed Platform vs. Open Architectures for Cloud-Native Earth System Analytics…\n\nas Earth System data have gotten larger, the typical download-data-work-locally workflow is no longer always feasible\nthose data are also produced and distributed by lots of different organizations (e.g. NASA, NOAA, Copernicus)\nresearchers often need to apply a wide range of analytical methods to those data, ranging from simple stats to machine learning approaches\n\nGEE offers web access (i.e. no need to download data to your computer) to an extensive catalog of analysis-ready geospatial data (from many different organizations) and scalable computing power via their cloud service, making global-scale analyses and visualizations possible for anyone with an account (sign up here!). Explore the public Earth Engine Data Catalog which includes a variety of standard Earth science raster datasets. Browse by dataset tags or by satellite (Landsat, MODIS, Sentinel).\nIn this lesson, we’ll first get some hands-on practice connecting to and using Google Earth Engine to visualize global precicpation data. We’ll then walk through a demonstration using GEE to visualize and analyze fire dynamics in the Arctic."
  },
  {
    "objectID": "sections/google-earth-engine.html#exercise-1-an-introductory-lesson-on-using-google-earth-engine",
    "href": "sections/google-earth-engine.html#exercise-1-an-introductory-lesson-on-using-google-earth-engine",
    "title": "13  Google Earth Engine",
    "section": "13.3 Exercise 1: An introductory lesson on using Google Earth Engine",
    "text": "13.3 Exercise 1: An introductory lesson on using Google Earth Engine\n\n13.3.1 Part i. Setup\n\nCreate a Google Earth Engine account (if you haven’t already done so)\n\n\nPlease refer back to the Preface to find instructions on creating a GEE account.\n\n\nLoad libraries\n\n\nimport ee\nimport geemap\nimport pandas as pd\n\n\nAuthenticate your GEE account\n\n\nIn order to begin using GEE, you’ll need to connect your environment (scomp) to the authentication credentials associated with your Google account. This will need to be done each time you connect to GEE, (but only be done once per session).\n\n\nee.Authenticate() # triggers the authentication process\n\n\nThis should launch a browser window where you can login with your Google account to the Google Earth Engine Authenticator. Following the prompts will generate a code, which you’ll then need to copy and paste into the VS Code command palette (at the top of the IDE). This will be saved as an authentication token so you won’t need to go through this process again until the next time you start a new session. The browser-based authentication steps will look something like this:\n\n\nNotebook Authenticator: choose an active Google account and Cloud Project (you may have to create one if this is your first time authenticating) and click “Generate Token”\n\nChoose an account: if prompted, select the same Google account as above\nGoogle hasn’t verified this app: You may be temped to click the blue “Back to safety” button, but don’t! Click “Continue”\n\nSelect what Earth Engine Notebook Client can access: click both check boxes, then “Continue”\n\nCopy your authorization code to your clipboard to paste into the VS Code command palette\n\n\nLastly, intialize. This verifies that valid credentials have been created and populates the Python client library with methods that the backend server supports.\n\n\nee.Initialize() \n\nIf successful, you’re now ready to begin working with Earth Engine data!\n\n\n13.3.2 Part ii. Explore the ERA5 Daily Aggregates Data\nWe’ll be using the ERA5 daily aggregates reanalysis dataset, produced by the European Centre for Medium-Range Weather Forecasts (ECMWF), found here, which models atmospheric weather observations.\n\n\nReanalysis combines observation data with model data to provide the most complete picture of past weather and climate. To read more about reanalyses, check out the EWCMWF website.\n\n\n\n\nERA5 Daily Aggregates dataset, available via the Earth Engine Data Catelog\n\n Take a few moments to explore the metadata record for this dataset. You’ll notice that it includes a bunch of important information, including:\n\nDataset Availability: the date range\nDataset Provider: where the data come from\nEarth Engine Snippet: a code snippet used for loading the dataset\nDescription (tab): get to know a bit about the data\nBands (tab): the variables present in the dataset; each band has its own name, data type, scale, mask and projection\nImage Properties: metadata available for each image band\nExample Code: a script to load and visualize ERA5 climate reanalysis parameters in Google Earth Engine (JavaScript)\n\n\n\n13.3.3 Part iii. Visualize global precipitation using ERA5 Daily Aggregate data\nContent for this section was adapted from Dr. Sam Stevenson’s Visualizing global precipitation using Google Earth Engine lesson, given in her EDS 220 course in Fall 2021.\n\nCreate an interactive basemap\n\n\nThe default basemap is (you guessed it) Google Maps. The following code displays an empty Google Map that you can manipulate just like you would in the typical Google Maps interface. Do this using the Map method from the geemap library. We’ll also center the map at a specified latitude and longitude (here, 50N, 151E), set a zoom level, and save our map as an object called myMap.\n\n\nmyMap = geemap.Map(center = [60, -151], zoom = 4)\nmyMap\n\n\nLoad the ERA5 Image Collection from GEE\n\n\nNext, we need to tell GEE what data we want to layer on top of our basemap. The ImageCollection method extracts a set of individual images that satisfies some criterion that you pass to GEE through the ee package. This is stored as an ImageCollection object which can be filtered and processed in various ways. We can pass the ImageCollction method agruments to tell GEE which data we want to retrieve. Below, we retrieve all daily ERA5 data.\n\n\n\n\n\n\n\nEarth Engine Snippets make importing ImageCollections easy!\n\n\n\nTo import an ImageCollection, copy and paste the Earth Engine Snippet for your dataset of interest. For example, the Earth Enginge Snippet to import the ERA5 daily aggregates data can be found on the dataset page.\n\n\n\nweatherData = ee.ImageCollection('ECMWF/ERA5/DAILY')\n\n\nSelect an image to plot\n\n\nTo plot a map over our Google Maps basemap, we need an Image rather than an ImageCollection. ERA5 contains many different climate variables – explore which variables the dataset contains under the Bands tab. We’ll use the select method to choose the parameter(s) we’re interested in from our weatherData object. Let’s select the total_precipitation band.\n\n\n\nPlotting an Image will produce a static visualization (e.g. total precipitation at a particular point in time, or the average precipitation over a specified date range), while an ImageCollection can be visualized as either an animation or as a series of thumbnails (aka a “filmstrip”), such as this animation showing a three-day progression of Atlantic hurricanes in September, 2017 (source: Google Earth Engine).\n\n\n\n\n\n# select desired bands (total_preciptation)\nprecip = weatherData.select(\"total_precipitation\")\n\n\nWe can look at our precip object metadata using the print method to see that we’ve isolated the total_precipitation band, but it’s still an ImageCollection.\n\n\nprint(precip)\n\n\n\n\n\n\n\nNote\n\n\n\nYou may see a message in that says, “Output exceeds the size limit. Open the full output data in a text editor” when printing your image object metadata. Click here to see the entire output, which includes date range information.\n\n\n\nLet’s say that we want to look at data for a particular time of interest – e.g. January 1, 2019 - December 31, 2019. We can apply the filterDate method to our selected total_precipitation parameter to filter for data from our chosen date range. We can also apply the mean method, which takes whatever precedes it and calculates the average – this step reduces our ImageCollection to a single Image.\n\n\n# initial date of interest (inclusive)\ni_date = '2019-01-01'\n\n# final data of interest (exclusive)\nf_date = '2020-01-01'\n\n# select desired bands (total_preciptation), dates, and calculate total precipitation across that date range\nprecip = weatherData.select(\"total_precipitation\").filterDate(i_date, f_date).sum()\n\n\nUse the print method again to check out your new precip object – notice that it’s now an ee.Image (rather than ee.ImageCollection) and the start and end date values over which the average is taken are as we specified.\n\n\nprint(precip)\n\n\nAdd the precipitation Image to the basemap\n\n\nFirst, set a color palette to use when plotting the data layer. The following is a palette specified for ERA5 precipitation data (scroll down to the example code, available on the landing page for the ERA5 metadata in the Earth Engine Data Catelog). Here, we adjusted the max value to change the range of pixel values to which the palette should be applied – this will make our colors stand out a bit more when we layer our precipitation data on our basemap, below.\n\n\n\nLearn more about GEE color palettes and Image visualization here.\n\nprecip_palette = {\n    'min':0,\n    'max':5,\n    'palette': ['#FFFFFF', '#00FFFF', '#0080FF', '#DA00FF', '#FFA400', '#FF0000']\n}\n\n\n\n\n\n\n\nNote\n\n\n\nGEE has lots of pre-defined color palettes to choose from based on the type of data you want to visualize. Also check out Crameri et al. 2020 for recommended best practices when choosing color gradients that accurately represent data and are readable by those with color vision deficiencies.\n\n\n\nFinally, plot our filtered data, precip, on top of our basemap using the addLayer method. We’ll also pass it our visualization parameters (colors and ranges stored in precip_palette, the name of the data field, total precipitation, and opacity (so that we can see the basemap underneath).\n\n\nmyMap.addLayer(precip, precip_palette, 'total precipitation', opacity = 0.7)\nmyMap\n\n\n\n\n\n\n\n13.3.4 Part iv. Extract annual sum of precipitation data from points\nWe probably don’t want to just plot the data, extracting it at points of interest to use in analysis would be helpful too. I’ll show a simple example here of how to extract raster values from a set of point geometries.\nFirst, we’ll create a pandas data frame with some locations in Alaska.\n\n# initialize data frame of points\ndata = [['anc', 61.2, -150.1 ], ['fai', 64.8, -147.6], ['jun', 58.3, -134.4]]\n  \n# Create the pandas DataFrame\ndf = pd.DataFrame(data, columns=['loc', 'lat', 'lon'])\n\nNext we need to convert the rows in the data frame to a GEE FeatureCollection. To do this, we will run a loop over every row in the data frame which:\n\ncreates a point geometry from the lat/lon coordinates\ncreates an attribute dictionary from the row\nuses the attribute dictionary and geom in the Feature function\n\n\n# convert data frame to list of GEE features\nfeatures=[]\n\nfor index, row in df.iterrows():\n    p_geom = ee.Geometry.Point([row['lon'], row['lat']])\n    # construct attributes for each row\n    p_props = dict(row)\n    p_feature = ee.Feature(p_geom, p_props)\n    features.append(p_feature)\n\nWe then pass that features list into the FeatureCollection function\n\nee_fc = ee.FeatureCollection(features)\n\nNow, we can use the sampleRegions method on our precip image, passing the feature collection we just created to the collection argument.\n\nres = precip.sampleRegions(collection = ee_fc, scale = 30)\n\nThis returns another feature collection, which we can convert back to a pandas data frame with just one function call.\n\nres_df = geemap.ee_to_pandas(res)\n\nOf note, by default GEE data is in maps mercator (EPSG:3857). The documentation has many more details on how to handle projections, but in this case it is okay for us to use the default value since our input data were also in EPSG:3857.\nThis was a simple example but you can do much more with these methods, including extracing timeseries data, extracting data from regions with polygon geometries (as opposed to point geometries), buffering features, and more. The GEE documentation is excellent, and examples abound.\n\n\n13.3.5 Part v. Takeaways\nIn just about five lines of code (and mere seconds of execution time), we’ve applied and visualized a global precipitation model (that’s pretty cool, right??). We can zoom in/out across our interactive map while Google Earth Engine recalculates and revisualizes our model in near real-time.\nGEE does have some limitations. To name a few:\n\nit’s a closed platform service, so GEE software can only be run on Google’s infrastructure, as opposed to other open source options, like Pangeo\nit’s only free for non-commercial use\nthere are memory and storage limits\n\nDespite these limitations, GEE’s data catelog and cloud computing resources can vastly streamline and expedite analyzing and visualizing large geospatial datasets."
  },
  {
    "objectID": "sections/google-earth-engine.html#exercise-2-visualize-fire-dynamics-in-the-arctic-using-gee",
    "href": "sections/google-earth-engine.html#exercise-2-visualize-fire-dynamics-in-the-arctic-using-gee",
    "title": "13  Google Earth Engine",
    "section": "13.4 Exercise 2: Visualize fire dynamics in the Arctic using GEE",
    "text": "13.4 Exercise 2: Visualize fire dynamics in the Arctic using GEE\nIn this exercise, use the Google Earth Engine datasets listing to find a dataset showing burn areas, that includes a day burned layer.\nCreate a map of your dataset using methods similar to above. Look to the dataset landing page on GEE for palette help, or create your own!\nBelow is one solution. Don’t peek unless you are stuck!\n\n\nShow the code\nburn = ee.ImageCollection(\"ESA/CCI/FireCCI/5_1\")\nburn_2020 = burn.select('BurnDate').filterDate('2019-01-01', '2019-12-31').max()\nfire_pal = {\n  'min': 1,\n  'max': 366,\n  'palette': [\n    'ff0000', 'fd4100', 'fb8200', 'f9c400', 'f2ff00', 'b6ff05',\n    '7aff0a', '3eff0f', '02ff15', '00ff55', '00ff99', '00ffdd',\n    '00ddff', '0098ff', '0052ff', '0210ff', '3a0dfb', '7209f6',\n    'a905f1', 'e102ed', 'ff00cc', 'ff0089', 'ff0047', 'ff0004'\n  ]\n}\n\n\nmyMap2 = geemap.Map(center = [66,-145], zoom = 7)\n\nmyMap2.addLayer(burn_2020, fire_pal, \"Burned area with date burned\", opacity = 1)\nlegend_dict = {\n    \"January 1\" : '#ff0000',\n    \"March 1\" : '#7aff0a',\n    \"June 1\" : '#00ddff',\n    \"Sept 1\" : '#a905f1'\n}\n\nmyMap2.add_legend(legend_title=\"Burn Day\", legend_dict=legend_dict)"
  },
  {
    "objectID": "sections/google-earth-engine.html#additional-resources",
    "href": "sections/google-earth-engine.html#additional-resources",
    "title": "13  Google Earth Engine",
    "section": "13.5 Additional Resources",
    "text": "13.5 Additional Resources\nTextbook:\n\nCloud-Based Remote Sensing with Google Earth Engine Book\n\nTools:\n\nGEE Code Editor, a web-based IDE for using GEE (JavaScript)\n\nData:\n\nEarth Engine Data Catalog, the main resource for “official” GEE Datasets\n\nawesome-gee-community-datasets, the main resource for “community” GEE Datasets\n\nDocumentation, Tutorials, & Help:\n\nearthengine-api installation instructions\n\nCreating and managing Google Cloud projects\n\nTroubleshooting authentication issues\n\nAn Intro to the Earth Engine Python API\n\ngeemap documentation\n\nQiusheng Wu’s YouTube channel for GEE & geemap Python tutorials\nGEE on StackExhange\n\nOther:\n\nClimate Engine App, a no-code user interface to Google Earth Engine for quickly and easily visualizing various Earth observation processes and variables\nQiusheng Wu"
  },
  {
    "objectID": "sections/adc-data-publishing.html#learning-objectives",
    "href": "sections/adc-data-publishing.html#learning-objectives",
    "title": "14  Documenting and Publishing Data",
    "section": "14.1 Learning Objectives",
    "text": "14.1 Learning Objectives\n\nBecome familiar with the submission process\nUnderstand what constitutes as “large data”\n\nKnow when to reach out to support team for help\n\nLean how data & code can be documented and published in open data archives"
  },
  {
    "objectID": "sections/adc-data-publishing.html#introduction",
    "href": "sections/adc-data-publishing.html#introduction",
    "title": "14  Documenting and Publishing Data",
    "section": "14.2 Introduction",
    "text": "14.2 Introduction\nA data repository is a database infrastructure that collects, manages, and stores data. In addition to the Arctic Data Center, there are many other repositories dedicated to archiving data, code, and creating rich metadata. The Knowledge Network for Biocomplexity (KNB), the Digital Archaeological Record (tDAR), Environmental Data Initiative (EDI), and Zenodo are all examples of dedicated data repositories."
  },
  {
    "objectID": "sections/adc-data-publishing.html#metadata",
    "href": "sections/adc-data-publishing.html#metadata",
    "title": "14  Documenting and Publishing Data",
    "section": "14.3 Metadata",
    "text": "14.3 Metadata\nMetadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where they were collected, and why they were collected.\nFor consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the machine-readable version of the metadata for a sockeye salmon dataset:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<eml:eml packageId=\"df35d.442.6\" system=\"knb\" \n    xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.1\">\n    <dataset>\n        <title>Improving Preseason Forecasts of Sockeye Salmon Runs through \n            Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007</title>\n        <creator id=\"1385594069457\">\n            <individualName>\n                <givenName>Mark</givenName>\n                <surName>Willette</surName>\n            </individualName>\n            <organizationName>Alaska Department of Fish and Game</organizationName>\n            <positionName>Fishery Biologist</positionName>\n            <address>\n                <city>Soldotna</city>\n                <administrativeArea>Alaska</administrativeArea>\n                <country>USA</country>\n            </address>\n            <phone phonetype=\"voice\">(907)260-2911</phone>\n            <electronicMailAddress>mark.willette@alaska.gov</electronicMailAddress>\n        </creator>\n        ...\n    </dataset>\n</eml:eml>\nAlternatively, the same metadata document can be converted to HTML format and displayed in a more readable form on the web:\n\nAs you can see from the picture above, users can download either the whole dataset or its individual components. This makes the dataset and its associated data resuable.\nAdditionally, the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data."
  },
  {
    "objectID": "sections/adc-data-publishing.html#data-package-structure",
    "href": "sections/adc-data-publishing.html#data-package-structure",
    "title": "14  Documenting and Publishing Data",
    "section": "14.4 Data Package Structure",
    "text": "14.4 Data Package Structure\nNote that the dataset above lists a collection of files that are contained within the dataset. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\n\nThese data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI (digital object identifier) identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID (universally unique identifier) that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4, and each of the individual files have their own identifiers as well."
  },
  {
    "objectID": "sections/adc-data-publishing.html#archiving-data-the-large-data-perspective",
    "href": "sections/adc-data-publishing.html#archiving-data-the-large-data-perspective",
    "title": "14  Documenting and Publishing Data",
    "section": "14.5 Archiving Data: The Large Data Perspective",
    "text": "14.5 Archiving Data: The Large Data Perspective\nThere are two components to any data package archived with the Arctic Data Center: the metadata & the data themselves. Data can be images, plain text documents, tabular data, spatial data, scripts used to analyze the data, a readme file, and more. To the best of your ability, please make sure that the data uploaded are in an open format, rather than proprietary format. We strongly recommend using open, self-documenting binary formats for large data archival. NetCDF, HDF, .mat (v7.3) and Parquet files are all examples of “self-documenting” files. In the case of a NetCDF file, users can input the attribute name, attribute description, missing value codes, units, and more into the file itself. When these data are well-documented within themselves, it can save the time when users submit their data to us, since the documentation for variable level information is already mostly complete. We’ll discuss NetCDF and metadata more in Session 8. For geospatial data, we recommend using geotiff for raster files, and geopackage files for vector files.\nThis section provides an overview of some highlights within the data submission process, and will specifically address issues related to datasets with large amounts of data, whether that be in number of files or cumulative file size.\nFirst we’ll go over the metadata submission; then learn how to upload the data using a secure File Transfer Protocol; and finally how to add attribute information to the data.\n\n14.5.1 Step 1: The Narrative Metadata Submission\n\n14.5.1.1 ORCiDs\nIn order to archive data with the Arctic Data Center, you must log in with your ORCID account. If you do not have one, you can create at https://orcid.org/. ORCID is a non-profit organization made up of research institutions, funders, publishers and other stakeholders in the research space. ORCID stands for Open Researcher and Contributor ID. The purpose of ORCID is to give researchers a unique identifier which then helps highlight and give credit to researchers for their work. If you click on someone’s ORCID, their work and research contributions will show up (as long as the researcher used ORCID to publish or post their work).\nOnce you’re logged into the Arctic Data Center with your ORCID, you can access the data submission form by clicking “Submit Data” in the navigation bar. For most dataset submissions, you would submit your data and metadata at the same using the “Add Files” buttons seen in the image below. However, when you know you have a large quantity of files or large cumulative file size, you should focus only on submitting metadata through the web form. We’ll discuss how to submit large quantities of data in the next section.\n\n\n\n14.5.1.2 Overview Section\nIn the overview section, you will include a descriptive title of your data set, select the appropriate data sensitivity tag, an abstract of the data set, keywords, funding information, and a license.\nIn general, if your data has been anonymized or de-identified in any way, your submission is no longer considered to have “Non-sensitive data”. If you have not had to de-identify your data or through an Instituional Review Board process, you should select the “Non-sensitive data” tag. You can find a more in-depth review of the data sensitivity tag in Chapter 12 of our Fundamentals in Data Management coursebook.\n\nYou also must enter a funding award number and choose a license. The funding field will search for an NSF award identifier based on words in its title or the number itself. When including funding information not from an NSF award, please make sure to add an award number, title, and organization if possible.\nThe licensing options are CC-0 and CC-BY, both of which allow your data to be downloaded and re-used by other researchers.\n\nCC-0 Public Domain Dedication: “…can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.”\nCC-BY: Attribution 4.0 International License: “…free to…copy,…redistribute,…remix, transform, and build upon the material for any purpose, even commercially,…[but] must give appropriate credit, provide a link to the license, and indicate if changes were made.”\n\n\n\n\n14.5.1.3 People Information\nInformation about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people:\n\nCreators - all the people who should be in the citation for the dataset\nContacts - one is required, but defaults to the dataset submitter if omitted\nPrincipal Investigators\nAny others that are relevant\n\nFor each, please provide their ORCID identifier, which helps link this dataset to their other scholarly works.\n\n\n\n14.5.1.4 Temporal Information\nAdd the temporal coverage of the data, which represents the time period when data was collected. If your sampling over time was discontinuous and you require multiple date ranges to represent your data, please email us the date ranges and we will add that to the submission on the back-end.\n\n\n\n\n14.5.1.5 Location Information\nThe geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that spatial search will be more precise.\n\nNote that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global:\n\nMission Canyon, Santa Barbara, California, USA\n\n\n\n\n14.5.1.6 Methods\nMethods are critical to the accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data. Ideally, enough detail should be provided such that a reasonable scientist could interpret the study and data for reuse without needing to consult the researchers or any other resources.\nIncluded in the methods section is a question that asks users about the ethical research practices that may or may not have been considered throughout the research process. You will learn more about this in Chapter 14, and can find more in-depth information on our website’s data ethics page.\nThe ethical research practices response box must be filled out in order to save your dataset. If users feel as though this question is not applicable to their research, we encourage them to discuss why that is rather than simply stating “Not Applicable,” or some variation thereof. For example, say a researcher has compiled satellite imagery of weather patterns over the open ocean. Rather than respond with “N/A”, a user should instead include something to the effect of “Dataset contains satellite imagery of weather patterns over the Atlantic Ocean. Imagery was downloaded from NASA and NOAA, and does not contain any individual’s identifiable information.” When in doubt, you can always email the support team at support@arcticdata.io.\n\n\n\n14.5.1.7 Save Metadata Submission\nWhen you’re finished editing the narrative metadata, click the Save Dataset button at the bottom right of your screen.\nIf there are errors or missing fields, they will be highlighted with a red banner as seen earlier. Correct those, and then try submitting again. If the save button disappears after making corrections, add a space in the abstract and the save button should reappear. If not, please reach out to the support team for assistance.\nWhen you are successful, you should see a large green banner with a link to the current dataset view. Click the X to close that banner if you want to continue editing metadata.\n\n\n\n\n14.5.2 Step 2: Adding File & Variable Level Metadata\nThe final major section of metadata concerns the structure and content of your data files. Assuming there are many files (and not a few very large ones), it would be unreasonable for users to input file and variable level metadata for each file. When this situation occurs, we encourage users to fill out as much information as possible for each unique type of file. Once that is completed, usually with some assistance from the Data Team, we will then programmatically carry over the information to other relevant files.\nWhen you’re data are associated with your metadata submission, they will appear in the data section at the top of the page when you go to edit your dataset. Choose which file you would like to begin editing by selecting the “Describe” button to the right of the file name.\n\nOnce there, you will see the following screen. In the Overview section, we recommend not editing the file name, and instead add a descriptive overview of the file. Once done, click the Attributes tab.\n\nThe Attributes tab is where you enter variable (aka attribute) information, including:\n\nattribute name (for programs)\nattribute label (for display)\n\n\n\nvariable definition (be specific)\ntype of measurement\n\n\n\nunits & code definitions\n\n\nUsers will need to add these definitions for every variable (column) in the file. When done, click Done. Now, the list of data files will show a green checkbox indicating that you have fully described that file’s internal structure. Proceed with documenting other unique file types, and then click Save Dataset to save all of these changes.\n\n\n\n\n\n\nTip\n\n\n\nWhen dealing with large datasets, the Data Team can help users fill out metadata for similar file types with identical internal structures. An example of this will be discussed in the Best Practices section.\n\n\n\nAfter you get the big green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes.\n\n\n14.5.3 Step 3: Uploading Large Data\nIn order to submit your large data files to the Arctic Data Center repository, we encourage users to directly upload their data to the Data Team’s servers using a secure file transfer protocol (SFTP). There are a number of GUI driven and command line programs out there that all work well. For a GUI program, our team uses and recommends the free program Cyberduck. We will discuss command line programs in Session 18 in more detail, including rsync and Globus.\nBefore we begin, let’s answer the following question: Why would a user want to upload their data through a separate process, rather than the web form when they submit their metadata?\nDepending on your internet connection, the number of files you have, and the cumulative size of the data, users may experience difficulty uploading their data through the submission form. These difficulties are most often significantly reduced upload speeds and submission malfunctions. As such, it is best for all parties for large quantities of data to be uploaded directly to our server through an FTP.\nThe second question is, under what circumstances should I consider uploading data directly to the server, versus the webform? Although server uploads are more efficient for large datasets, for smaller datasets it is much more efficient to go through the webform. So, what constitutes a large dataset and what tools should you consider? Here is a helpful diagram:\n\n\nWeb Editor: The web editor is most often used by those with less than a hundred files and a small cumulative file size (0-5 GBs). Overall, this option is best for those who have less than 250 files with a small cumulative file size.\nSFTP: For users that expect to upload more than 250 files and have a medium cumulative file size (10-100 GBs), uploading data to our servers via SFTP is the recommended method. This can be done through the command line, or a program like Cyberduck. If you find yourself considering uploading a zip file through the web editor, you should instead upload your files using this method.\nGridFTP: For users that expect to upload hundreds or thousands of files, with a cumulative file size of hundreds of GB to TBs, you will likely want to make use of GridFTP through Globus. Jeanette will be talking about data transfers in more depth on Thursday.\n\nBefore you can upload your data to the Data Team’s server, make sure to email us at support@arcticdata.io to retrieve the login password. Once you have that, you can proceed through the following steps.\n\n\n\n\n\n\nTip\n\n\n\nIf you know that you will need to use this process for more than one dataset, we suggest creating folders with the same name as the associated dataset’s title. This way, it will be clear as to which submission the data should be associated with.\n\n\nOnce you have finished uploading your data to our servers, please let the Data Team know via email so that we can continue associate your uploaded data with your metadata submission.\nAs mentioned in Step 1: The Narrative Metadata Submission section above, when the data package is finalized and made public, there will be a sentence in the abstract that directs users to a separate page where your data will live. The following image is an example of where the data from this dataset live."
  },
  {
    "objectID": "sections/adc-data-publishing.html#best-practices-for-submitting-large-data",
    "href": "sections/adc-data-publishing.html#best-practices-for-submitting-large-data",
    "title": "14  Documenting and Publishing Data",
    "section": "14.6 Best Practices for Submitting Large Data",
    "text": "14.6 Best Practices for Submitting Large Data\n\n14.6.1 Data Organization\nThe data archival process is much smoother when users already have a well structured and organized file management system. We strongly recommend starting to work with the Arctic Data Center towards the beginning of your project, instead of the very end. Our curation team can help you publish your data faster, and more smoothly, the earlier you talk to us. Regardless of where you are in your project, however, considering your organization strategy is best done before data upload.\nTake a look at this recently made data portal. Originally, this user had only submitted one dataset with all of her files (over 700, at varying resolutions). The data contain rasters and shapefiles depicting vessel paths in the North Pacific at many different resolutions. Upon review, the submitter and curation team together determined that it would be best if other users could download the data at specific resolution scales. As a solution, we created separate datasets according to resolution and data type. With the addition of the data portal, the user was able to house their related datasets at a single URL, which allows for search and discovery within only the datasets for that project.\n\n\n\n\n\n\nNote\n\n\n\nTo find out more about portals, visit https://arcticdata.io/data-portals/. You can view our catalog of portals here. We recommend reviewing the Distributed Biological Observatory’s portal to see what a more fleshed out portal can look like.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis example also illustrates that users can create multiple datasets for a single NSF award. It’s understandable that you may think you have to submit all your data for a given award under one dataset. However, we think breaking up data into separate datasets (in logical, structured ways) increases the discoverability and usability of the data. This is especially true when users also create a data portal for their project.\nWhen uploading your data files using SFTP, either through the command line or a GUI, we recommend creating folders with the same name as your dataset’s title so that it is easier to associate the data with their associated metadata submission.\nFor more complex folder structures, it is wise to include a README file that explicitly walks users through the structure and provides a basic understanding of what is available. Generally speaking, we recommend structuring your data in an easy to understand way such that a README isn’t completely necessary."
  },
  {
    "objectID": "sections/adc-data-publishing.html#data-transfer-tools",
    "href": "sections/adc-data-publishing.html#data-transfer-tools",
    "title": "14  Documenting and Publishing Data",
    "section": "14.7 Data transfer tools",
    "text": "14.7 Data transfer tools\nNow that we’ve talked about what types of large datasets you might have that need to get published on the Arctic Data Center, let’s discuss how to actually get the data there. If you have even on the order of only 50GB, or more than 500 files, it will likely be more expedient for you to transfer your files via a command line tool than uploading them via our webform. So you know that you need to move a lot of data, how are you going to do it? More importantly, how can you do it in an efficient way?\nThere are three key elements to data transfer efficiency:\n\nendpoints\nnetwork\ntransfer tool\n\n\nEndpoints\nThe from and to locations of the transfer, an endpoint is a remote computing device that can communicate back and forth with the network to which it is connected. The speed with which an endpoint can communicate with the network varies depending on how it is configured. Performance depends on the CPU, RAM, OS, and disk configuration. One key factor that affects data transfer speed is how quickly that machine can write data to disk. Slow write speeds will throttle a data transfer on even the fastest internet connection with the most streamlined transfer tool. Examples of endpoints could be:\n\nNCEAS included-crab server\nYour standard laptop\nA cloud service like AWS\n\n\n\nNetwork speed\nNetwork speed determines how quickly information can be sent between endpoints. It is largely, but not enitrely, dependent on what you pay for. Importantly, not all networks are created equal, even if they nominally have the same speed capability. Wired networks get significantly more speed than wireless. Networks with lots of “stuff” along the pipe (like switches or firewalls) can perform worse than those that don’t. Even the length and type of network cabling used can matter.\n\n\nTransfer tools\nPoll: what data transfer tools do you use regularly?\nFinally, the tool or software that you use to transfer data can also significantly affect your transfer speed. There are a lot of tools out there that can move data around, both GUI driven and command line. We’ll discuss a few here, and their pros and cons.\n\nscp\nscp or secure copy uses ssh for authentication and transfer, and it is included with both unix and linux. It requires no setup (unless you are on a Windows machine and need to install), and if you can ssh to a server, you can probably use scp to move files without any other setup. scp copies all files linearly and simply. If a transfer fails in the middle, it is difficult to know exactly what files didn’t make it, so you might have to start the whole thing over and re-transfer all the files. This, obviously, would not be ideal for large data transfers. For a file or two, scp is a fine tool to use.\n\n\nrsync\nrsync is similar to scp, but syncs files/directories as opposed to copying. This means that rsync checks the destination to see if that file (with the same size and modified date) already exists. If it does, rsync will skip the file. This means that if an rsync transfer fails, it can be restarted again and will pick up where it left off, essentially. Neat!\n\n\nGlobus\nGlobus is a software that uses multiple network sockets simultaneously on endpoints, such that data transfers can run in parallel. As you can imagine, that parallelization can dramatically speed up data transfers. Globus, like rsync can also fail gracefully, and even restart itself. Globus does require that each endpoint be configured as a Globus node, which is more setup than is required of either scp or rsync. Many instituions computing resources may have endpoints already configured as Globus endpoints, so it is always worth checking in with any existing resources that might already be set up before setting up your own. Although Globus is a free software, there are paid options which provide support for configuring your local workstation as a Globus node. Globus is a fantastic tool, but remember the other two factors controlling data transfer, it can only help so much in overcoming slow network or write speeds.\n\n\n\n14.7.0.1 AWS sync\nAmazon Web Services (AWS) has a Command Line Interface (CLI) that includes a sync utility. This works much like rsync does in that it only copies new or updated files to the destination. The difference, of course, is that AWS sync is specifically built to work with interacting with the AWS cloud, and is compatible with S3 buckets.\n\n\n14.7.0.2 nc\nnc (or netcat) is a low level file transfer utility that is extremely efficient when moving files around on nodes in a cluster. It is not the easiest of these tools to use, however, in certain situations it might be the best option because it has the least overhead, and therefore can run extremely efficiently."
  },
  {
    "objectID": "sections/adc-data-publishing.html#summary",
    "href": "sections/adc-data-publishing.html#summary",
    "title": "14  Documenting and Publishing Data",
    "section": "14.8 Summary",
    "text": "14.8 Summary\nIn this lesson we learned about metadata and the structure of a data package; how to submit narrative metadata; how to upload data and when to use different services; how to document file and attribute-level metadata; and best practices for data organization and management. As mentioned earlier, more in-depth information on uploading data to our servers using Globus and RSync can be found in Session 18."
  },
  {
    "objectID": "sections/group-project-3.html#setup",
    "href": "sections/group-project-3.html#setup",
    "title": "15  Group Project: Visualization",
    "section": "15.1 Setup",
    "text": "15.1 Setup\nIn your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-15.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills.\nOnly one person in the group should run the parallel code."
  },
  {
    "objectID": "sections/group-project-3.html#resampling-rasters",
    "href": "sections/group-project-3.html#resampling-rasters",
    "title": "15  Group Project: Visualization",
    "section": "15.2 Resampling rasters",
    "text": "15.2 Resampling rasters\nIn this portion of the group project, we will further process the raster files by resampling them to lower zoom levels. This means we will take the average of clusters of pixels, and that value will be assigned to the single pixel that encompasses the entire area that is represented by the original cluster of pixels. Here is an example of creating a zoom level 10 pixel from 4 zoom level 11 pixels:\n\n\n\nDrawing of downsampling a raster, created by Robyn Thiessen-Bock.\n\n\nAs we aggregate cells to produce rasters at lower zoom levels, the cell size increases, but the extent of the raster remains the same. This is called “downsampling”.\nAfter we do this for each zoom level, you can imagine they are organzied as a pyramid, with the lowest zoom levels (and lowest resolution rasters) at the top, like so:\n\n\n\nA pyramid of rasters at different resolutions (zoom levels). Souce: maptiler\n\n\nThe highest zoom level has the most rasters. As we create the lower zoom levels, we aggregate the rasters only from the zoom level directly above. As the zoom level decreases, our computation is faster.\nCheck out the Permafrost Discovery Gateway Imagery Viewer and zoom in and out. You can see for yourself how the resolution increases and the extent that you are trying to view decreases.\n\n\n\nIce wedge polygons at high resolution, visible when zoomed-in to the Permafrost Discovery Gateway Imagery Viewer.\n\n\nThe higher resolution tiles are the “child” tiles, and the lower resolution tiles are the “parent” tiles.\n\n15.2.1 Packages, Libraries, and Modules\n\nos 3.11.2\nparsl 2023.3.20\npdgstaging\n\ndeveloped by Permafrost Discovery Gateway software developer and designer Robyn Thiessen-Bock\n\npdgraster\n\ndeveloped by Permafrost Discovery Gateway software developer and designer Robyn Thiessen-Bock\n\ngeopandas0.11\nrandom\nmatplotlib 3.5\nipyleaflet 0.17"
  },
  {
    "objectID": "sections/software-design-2.html#learning-objectives",
    "href": "sections/software-design-2.html#learning-objectives",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.1 Learning objectives",
    "text": "16.1 Learning objectives\n\nPython modules and packages\nManaging packages with poetry\nTesting: why, how, and when"
  },
  {
    "objectID": "sections/software-design-2.html#python-modules-and-packages",
    "href": "sections/software-design-2.html#python-modules-and-packages",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.2 Python modules and packages",
    "text": "16.2 Python modules and packages\nThe Python Package Index currently houses over 400,000 software projects that you can install, reuse, and extend to accelerate your work. If you’ve worked in python for long at all, you have certainly used the import statement to to load modules that you need in your code into a namespace for use.\n\nimport numpy as np\nnp.pi*np.power(3, 2)\n\n28.274333882308138\n\n\nThis lesson is a first primer on building your own modules and packages in python, and provides some core tools to get started building your own packages. For a deeper dive, check out the Python Packages book, which is freely available online. First, a few definitions:\n\nModule\n\nA python module is a file containing commands that create functions, variables, and other definitions that can be reused in other code that you write. Each module in python creates names in its own namespace. Using import in python allows you to take a name from one module and be able to use it in another namespace.\n\nPackage\n\nA python package represents a way to bundle python modules, documentation, and related resources as reusable and sharable code, both for your own use and to collaborate with others.\n\n\nBuilding reusable code though modules and packages is key to the scientific reproducibility theme that weaves throughout this course. Creating your own modules and bundling them into packages will help you (and future you) in so many ways:\n\nto keep you organized, making it easier for you and others to understand and use your code\nto provide consistent approaches for documentation\nto easily import your code into your own projects in a portable way that promotes code reuse\nto formally document and manage software dependencies\nto share your software with colleagues aand the community"
  },
  {
    "objectID": "sections/software-design-2.html#anatomy-of-modules-and-packages",
    "href": "sections/software-design-2.html#anatomy-of-modules-and-packages",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.3 Anatomy of modules and packages",
    "text": "16.3 Anatomy of modules and packages\nModules in python are loaded from the definitions stored in files of the same name with a .py file extension. Within each file are definitions of objects which can be accessed when the module is imported. For example, imagine you have a file named hello.py in your current directory that contains a function called helloworld with the following contents:\n\ndef helloworld():\n  \"\"\"\n  Print a hello message\n  \"\"\"\n  print(\"Hello world!\")\n\nWith that in place, you can then import and use that function in other code you write:\n\nimport helloworld\nhelloworld()\n\nPackages help organize these modules into a directory structure so that they can be predictably named and utilized. Packages can contain subpackages to keep modules organized into logical groups. Packages also provide structured metadata so that we can document the package and its dependencies so that it can be reliably installed and correctly used. The structure of a package is a simple directory tree containing the package modules and subpackages. Two common structures are supported. First, with modules in the root directory:\nadcmodel\n  ├── README.md\n  ├── adcmodel\n  │   ├── __init__.py\n  │   ├── affine.py\n  │   ├── data.py\n  │   └── reports\n  │       ├── __init__.py\n  │       └── pdf.py\n  └── tests\nAlternatively, the module code is often put in it’s own src directory, like:\nadcmodel\n  ├── README.md\n  ├── src\n  │   └── adcmodel\n  │       ├── __init__.py\n  │       ├── affine.py\n  │       ├── data.py\n  │       └── reports\n  │           ├── __init__.py\n  │           └── pdf.py\n  └── tests\nThe presence of the file __init__.py in each module directory indicates that that directory is a module. We won’t get into the details of this special file, but it is often used to load code to initialize a package."
  },
  {
    "objectID": "sections/software-design-2.html#managing-packages-with-poetry",
    "href": "sections/software-design-2.html#managing-packages-with-poetry",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.4 Managing packages with poetry",
    "text": "16.4 Managing packages with poetry\n\n\n\n\n\nWhile this structure and organization can be created manually, tools like Python Poetry provide a simple way to create and manage packages and their dependencies. Poetry is but one of many tools for managing packages in the python ecosystem, but we think it is useful and straightforward to use, and it tames some of the complexity of the python packaging ecosystem.\nIn the rest of this tutorial we will:\n\nCreate a new virutal environment for our package\nSet up a new VS Code workspace to develop the package\nGet Poetry installed and create a skeleton package\nAdd some code to the package and run it\nTalk through testing strategies for the package\n\n\n\n\n\n\n\nSidebar on other package managers\n\n\n\n\n\nPoetry is a great packaging tool, especially for pure python packages that don’t include libraries in other languages or require more complex build configurations. In those situations, there are many other tools in the python ecosystem that are useful, including tools like Hatch and PDM. The pyOpenSci initiative has created a great flow chart to help guide the choice of packaging tools, especially if you need to create more complex artifacts like wheels.\n\n\n\nImage credit: pyOpenSci\n\n\n\n\n\n\n16.4.1 Get poetry in a new virtual environment\nTo get started, we’re using virtualenvwrapper to manage virtual environments in the course, so let’s be sure it is installed in your environment. We’re starting from the scalable-computing-examples workspace, so you should be in the scomp virtual environment in your terminal. So, make sure virtualenvwrapper is installed, and install it if it is not in scomp, using:\n$ pip install virtualenvwrapper\nThe example package we will create is called adcmodel, and we will create a virtual environment of the same name to manage its dependencies. We’ll also install poetry into that adcmodel virtual environment.\n$ cd ~\n$ mkvirtualenv -p python3.9 adcmodel\n$ pip install poetry\nOnce that is done, we’re going to return our current workspace to the scomp virtual environment (so that future lessons continue to work), and create a new workspace for our project work. To reset the venv, run the following from your terminal:\n$ workon scomp\n$ cd ~/scalable-computing-examples\nNow we have a virtual environment. Next we’ll use it to create our new package."
  },
  {
    "objectID": "sections/software-design-2.html#setup-package-workspace",
    "href": "sections/software-design-2.html#setup-package-workspace",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.5 Setup package workspace",
    "text": "16.5 Setup package workspace\n\n16.5.1 Open VS Code window\nNow that we havea virtual environment set up, let’s create a new VS Code window, connect to `included-crab, and get our workspace set up.\n\nStart by opening a new VS Code window by connecting to included-crab using Command-Shift-P and then select “Connect to Host…” and choose included-crab.nceas.ucsb.edu\nOnce the window opens, open a new Terminal if one doesn’t open, and switch to our new adcmodel venv using:\n\nworkon adcmodel\nFinally, choose the Python version to use with Command-Shift-P, then choose Python: Select Interpreter, and choose the python version that lists your adcmodel virtual environment.\nWhen that is complete, your VS Code window should look like:\n\n\n\nVS Code remote with a virtual environment enabled.\n\n\n\n\n16.5.2 Create a new package\nNow we will create a skeleton for our package with poetry new, and take a look at what it produces:\n(adcmodel) jones@included-crab:~$ poetry new adcmodel\nCreated package adcmodel in adcmodel\n(adcmodel) jones@included-crab:~$ cd adcmodel/\n(adcmodel) jones@included-crab:~/adcmodel$ tree\n.\n├── README.md\n├── adcmodel\n│   └── __init__.py\n├── pyproject.toml\n└── tests\n    └── __init__.py\n\n2 directories, 4 files\nYou’ll see it created a README, an adcmodel directory for our source code (with the accompanying __init__.py to indicate this is a module), a metadata file defining our project metadata and dependencies, and a tests directory that we’ll come back to later when we explore testing.\n\n\n16.5.3 Setup the workspace\nTo make it easy to open up the project in VS Code, it is helpful to setup a workspace file, and configure VS Code to automatically activate our adcmodel virtual environment. First, let’s set up the workspace by opening the project folder and saving a workspace file:\n\nClick “Open Folder” to open the adcmodel folder and display it in the VS Code file exporer\nSelect “File | Save Workspace As…” to save the workspace configuration file\nNext, open the Terminal window and type workon adcmodel if it hasn’t already been done\nFinally, save a new settings file for our virtual environment:\n\ncreate a directory called .vscode using mkdir .vscode\nSave a new settings file as .vscode/settings.json with the following content\n\n\n{\n    \"python.terminal.activateEnvInCurrentTerminal\": true,\n    \"python.defaultInterpreterPath\": \"~/.virtualenvs/adcmodel/bin/python\"\n}\n\nClose the window, and open “New Window” under VS Code, then using Ctrl-R shortcut, open the recent adcmodel workspace on included-crab. If this all goes well, your session should be set to open up this adcmodel virtualenv each time that you open the workspace, and you should be able to see the contents of the skeleton package that you created.\n\nLet’s do some coding!"
  },
  {
    "objectID": "sections/software-design-2.html#the-adcmodel-example-package",
    "href": "sections/software-design-2.html#the-adcmodel-example-package",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.6 The adcmodel example package",
    "text": "16.6 The adcmodel example package\nSo, imagine this adcmodel package is intended to hold the code for one of our research projects in which we need to load data from a variety of sources, handle quality analysis and data cleaning, and then integrate, analyze and visualize those data. While there are many useful and general functions you might want to build into a package such as this, we’re going to keep it simple and show how to build a single function that consistenly loads and caches CSV-formatted data from the Arctic Data Center based on the identifier of the data object.\nFor this, we’ll create a module called adcmodel.data that contains our data handling functions, one of which will be a load_adc_csv() function that takes an Arctic Data Center Identifier as input.\n\n16.6.1 Load software dependencies\nTo create this functionality, we’ll make use of existing data loading functions and data retrieval functions from the pandas package, and operating system utilities from the os package. Almost all packages and functions you write will make use of the existing ecosystem of python packages, and we need to keep track of those dependencies in our package.\nLuckily, Poetry provides simple command line utilities for adding, removing, and installing dependent packages into your virtual environment. For each package that you need, use poetry add to register that package in your dependencies list and install it in your adcmodel virtual environment. We’ll do this for pandas, but don’t need to register urllib or os because these base packages ship with python.\n\n\n\n\n\n\n\nThe pyproject.toml configuration file\n\n\n\nDependency metadata that is added to the project and other information about the project is found in the pyproject.toml configuration file that is managed by poetry. In this file you’ll find key metadata about the project like it’s name and description and authors, as well as the list of specific dependencies that the package needs installed to work correctly. You should open this file and edit the key metadata, but poetry add and poetry remove are probably better for adding and removing dependencies.\n[tool.poetry]\nname = \"adcmodel\"\nversion = \"0.1.0\"\ndescription = \"Arctic Data Center example package for scalable computing course\"\nauthors = [\"Matt Jones <gitcode@magisa.org>\"]\nreadme = \"README.md\"\n\n[tool.poetry.dependencies]\npython = \"^3.9\"\npandas = \"^1.5.0\"\n\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n\n\n\n16.6.2 Adding code to modules\nLet’s write some module code! In this case, we want to implement the adcmodel.data.load_adc_csv(identifier) function that we’ll use for data loading and caching. We do this by creating a new file called data.py in the adcmodel directory, and then implementing the function. Create a new python file called data.py and save it to the adcmodel module directory, with the following function implementation:\n\nimport pandas as pd\nimport urllib.request\nimport hashlib\nimport os\n\n\ndef load_adc_csv(identifier=None):\n    \"\"\"\n    Load and cache a CSV data file from the Arctic Data Center as a pandas.DataFrame\n    \"\"\"\n    hashval = hashlib.sha256(bytes(identifier, 'utf-8')).hexdigest()\n    path = \"data/\" + hashval\n    if not os.path.exists(\"data/\"):\n        os.mkdir(\"data/\")\n    if not os.path.exists(path):\n        service = \"https://arcticdata.io/metacat/d1/mn/v2/object/\"\n        url = service + identifier\n        msg = urllib.request.urlretrieve(url, path)\n    df = pd.read_csv(path)\n    return df\n\nOnce you save the file, we can use poetry install to install the package and try running the function in an interactive session. Check it out by opening a new Jupyter notebook and running the following in the first cell (after you install the needed jupyter packages):\n\nfrom adcmodel.data import load_adc_csv\n\ndf = load_adc_csv(\"urn:uuid:e248467d-e1f9-4a32-9e38-a9b4fb17cefb\")\ndf.head()\n\n🎉 Congratulations, you’ve created your first package! 🎉\n\n\n16.6.3 Add development packages\nUtilities like python linters and code formatters can make coding faster, more consistent, and more reliable. Poetry supports the use of development packages that are available in the virtual environment during the development cycle but are not required for runtime use of the package. Like regular dependencies, development packages can be added with poetry add, but we also pass in a qualifier to add it to --group dev, which marks it as only for use during development.\npoetry add --group dev pytest black\nYou can now use black to format your code (via Command-Shift-P Format Document) and pytest to run tests. Next, let’s get into testing."
  },
  {
    "objectID": "sections/software-design-2.html#testing",
    "href": "sections/software-design-2.html#testing",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.7 Testing",
    "text": "16.7 Testing\nA major advantage of writing packages is that you can thoroughly test each of the functions that you write, thereby ensuring that they operate correctly both with good data and bad data. While it’s always helpful to confirm that a function returns the correct value within the bounds that it is normally meant to operate, it is also important to pass it data that it wouldn’t normally see. For example, does your linear model produce the correct results when it is passed a normal dataset? How about if it is passed a dataset with only one observation, or with all the same value, or with 0 or negative numbers? Does it produce an understandable error if some of the values are character values when they should be numeric? Does it continue operating in the face of bad data? Should it, or should it not produce an exception? These and others are the questions you need to ask when testing your packages.\nPython provides a number of robust testing frameworks, but we will work with pytest, one of the common ones used for unit testing. The poetry new command already set up our package to be ready for testing, so now all we need to do is to add some test files in the tests directory.\nIn pytest and other frameworks, the common pattern is to use the python assert function to determine whether a value produced from your code matches expectations. For example, given a function that produces a product of two numbers, we could assert that product(5, 6) == 30 and that product(-3, -2) == 6.\nGenerally, it is good practice to produce at least one test file for each of your package source files, and their names are prefixed with test_. So, for our data.py source file, we might expect to build tests in a file called test_data.py. Let’s create that file in the tests directory and walk through populating it with some simple tests. The functions in the test file are also named with test_ as the prefix on the function name, and generally each test function atomically tests one aspect of the code. That way, when a test function fails, it should be quickly clear what type of an error would produce that kind of failure, and how to locate it in the code.\nTo get started, create tests/test_data.py and populate it with our first test function:\n\nfrom adcmodel.data import load_adc_csv\n\ndef test_load_adc_csv():\n    df = load_adc_csv(\"urn:uuid:e248467d-e1f9-4a32-9e38-a9b4fb17cefb\")\n    assert df is not None\n\nYou can run the tests after saving the file by running pytest on the commandline, which will produce output indicating which tests passed and failed.\nWe can add other tests that test multiple conditions, and we can use any of the features of python to creatively test that our code will produce correct results. Here’s another test to check that the data file we loaded has the right number of rows and columns:\n\ndef test_load_adc_csv_shape():\n    df = load_adc_csv(\"urn:uuid:e248467d-e1f9-4a32-9e38-a9b4fb17cefb\")\n    assert df is not None\n    assert (df.shape[0] == 17856) & (df.shape[1] == 6)\n\n\nFinally, what happens when tests are run that fail? Try adding the following test, and determine if the failure is due to the code, the test, or the data? What would you change to make this test reliably pass, or fail more gracefully?\n\ndef test_load_adc_csv_inputs():\n    df = load_adc_csv()\n    assert df is not None\n\nThe answer to this and other more detailed questions on testing are covered in the excellent overview of testing provided by the “Chapter 5: Testing” of “Python Packages”."
  },
  {
    "objectID": "sections/software-design-2.html#summary-and-next-steps",
    "href": "sections/software-design-2.html#summary-and-next-steps",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.8 Summary and next steps",
    "text": "16.8 Summary and next steps\nPython packages are an excellent mechanism to organize your modules for reuse both within your local code projects and with your colleagues and the broader science community. Adopting a structured approach to organizing, documenting, and testing your code can ultimately save you a lot of time and also increase the quality of your work. Scientific software is also a first-class product of the research enterprise, and is critical for fully understanding the lineage of derived data and research results. The emerging consensus is that publishing open source software packages to GitHub or other source code control systems, and then archiving them in repositories like Software Heritage, the Arctic Data Center, Zenodo, or the Python Package Index increases the robustness, transparency, and reproducibility of research results. And it also provides a robust starting point for research teams to build upon and amplify each other’s work. Using tools like Poetry and pytest to organize and build packages makes the process quick and efficient."
  },
  {
    "objectID": "sections/software-design-2.html#further-reading",
    "href": "sections/software-design-2.html#further-reading",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.9 Further reading",
    "text": "16.9 Further reading\n\nNamespacing with Python\npyOpenSci Python Package Guide\nPython Packages by Tomas Beuzen & Tiffany Timbers\nPython Poetry"
  },
  {
    "objectID": "sections/cloud-computing.html#learning-objectives",
    "href": "sections/cloud-computing.html#learning-objectives",
    "title": "17  What is Cloud Computing Anyways?",
    "section": "17.1 Learning Objectives",
    "text": "17.1 Learning Objectives\n\nLearn different uses of the term cloud computing\nUnderstand the services of cloud providers\nGain familiarity with containerized computing\nWork hands-on running a docker container, and building your own and running it"
  },
  {
    "objectID": "sections/cloud-computing.html#fa-cloud-what-is-cloud-computing-anyways",
    "href": "sections/cloud-computing.html#fa-cloud-what-is-cloud-computing-anyways",
    "title": "17  What is Cloud Computing Anyways?",
    "section": "17.2  What is cloud computing anyways?",
    "text": "17.2  What is cloud computing anyways?\nThe buzzword we all hear, but maybe don’t quite understand. ..\nCloud computing is a lot of things… but generally speaking:\n\n\n\nCloud computing is the delivery of on-demand computer resources over the Internet. Or just “using someone else’s computer”.\n\n\n“The Cloud” is powered by a global network of data centers which house the hardware (servers), power, and backup systems, etc. These data centers and infrastructure are managed by cloud providers\n\n\n\n\nCloud computing services are typically offered using a “pay-as-you-go” pricing model, which in some scenarios may reduce capital expenses.\n\n\nCloud computing is a technology approach to using lightweight virtualization services to share large physical computing clusters across many users\n\n\n\nCheck out this article by Ingrid Burrington in The Atlantic, Why Amazon’s Data Centers are Hidden in Spy Country, for some interesting insight into one of the largest cloud provider’s data centers."
  },
  {
    "objectID": "sections/cloud-computing.html#commercial-clouds",
    "href": "sections/cloud-computing.html#commercial-clouds",
    "title": "17  What is Cloud Computing Anyways?",
    "section": "17.3 Commercial clouds",
    "text": "17.3 Commercial clouds\nThere are a lots of different cloud computing platforms, but the big ones are:\n Amazon Web Services (AWS)  Google Cloud Platform (GCP)  Microsoft Azure\n\nThere are many other cloud service providers that offer varying degrees of infrastructure, ease of use, and cost. Check out DigitalOcean, Kamatera, and Vultr to start."
  },
  {
    "objectID": "sections/cloud-computing.html#academic-clouds",
    "href": "sections/cloud-computing.html#academic-clouds",
    "title": "17  What is Cloud Computing Anyways?",
    "section": "17.4 Academic clouds",
    "text": "17.4 Academic clouds\n\n\n\n   \n\n\nFederal agencies in the US and other institutions also support massive computing facilities supporting cloud computing. While there are too many to fully list, programs such as the National Science Foundation’s ACCESS program, the Department of Energy’s National Energy Research Scientific Computing Center (NERSC), and the CyVerse platform provide massive cloud computing resources to academic and agency researchers. The huge advantage is that these resources are generally free-for-use for affiliated researchers. When you need access to massive CPU and GPU hours, an application for access to these facilities can be extremely effective.\n\n\n\nAnd the Pangeo project is creating an open community focused on maintaining, supporting, and deploying open infrastructure for cloud computing. They support key scientific software packages used throughout the cloud community, including xarray and dask, and generally are broadening capacity for large-scale, impactful research.\n\n17.4.1 Cloud deployment options\nDifferent cloud service and deployment models offer a suite of options to fit client needs\n Service Models: When you work in “the cloud” you’re using resources – including servers, storage, networks, applications, services, (and more!) – from a very large resource pool that is managed by you or the cloud service provider. Three cloud service models describe to what extent your resources are managed by yourself or by your cloud service providers.\n\n\n\n Infrastructure as a Service (IaaS)\n\n\n\n\n Platform as a Service (PaaS)\n\n\n\n\n Software as a Service (SaaS)\n\n\n\n Deployment Models: Cloud deployment models describe the type of cloud environment based on ownership, scale, and access.\n\n\n\n Private Cloud\n\n\n\n\n Public Cloud\n\n\n\n\n Hybrid Cloud"
  },
  {
    "objectID": "sections/cloud-computing.html#service-models",
    "href": "sections/cloud-computing.html#service-models",
    "title": "17  What is Cloud Computing Anyways?",
    "section": "17.5 Service Models",
    "text": "17.5 Service Models\n Infrastructure as a Service (IaaS) provides users with computing resources like processing power, data storage capacity, and networking. IaaS platforms offer an alternative to on-premise physical infrastructure, which can be costly and labor-intensive. In comparison, IaaS platforms are more cost-effective (pay-as-you-go), flexible, and scalable.\nOne example of IaaS is Amazon EC2, which allows users to rent virtual computers on which to run their own computer applications (e.g. R/RStudio).\n Platform as a Service (PaaS) provides developers with a framework and tools for creating unique applications and software. A benefit of SaaS is that developers don’t need to worry about managing servers and underlying infrastructure (e.g. managing software updates or security patches). Rather, they can focus on the development, testing, and deploying of their application/software.\nOne example of SaaS is AWS Lambda, a serverless, event-driven compute service that lets you run code for virtually any type of application or backend service without provisioning or managing servers.\n Software as a Service (SaaS) makes software available to users via the internet. With SaaS, users don’t need to install and run software on their computers. Rather, they can access everything they need over the internet by logging into their account(s). The software/application owner does not have any control over the backend except for application-related management tasks.\nSome examples of SaaS applications include Dropbox, Slack, and DocuSign."
  },
  {
    "objectID": "sections/cloud-computing.html#fa-pizza-slice-an-analogy-pizza-as-a-service",
    "href": "sections/cloud-computing.html#fa-pizza-slice-an-analogy-pizza-as-a-service",
    "title": "17  What is Cloud Computing Anyways?",
    "section": "17.6  An Analogy: Pizza as a Service",
    "text": "17.6  An Analogy: Pizza as a Service\n\nImage Source: David Ng, Oursky"
  },
  {
    "objectID": "sections/cloud-computing.html#virtual-machines-and-containers",
    "href": "sections/cloud-computing.html#virtual-machines-and-containers",
    "title": "17  What is Cloud Computing Anyways?",
    "section": "17.7 Virtual Machines and Containers",
    "text": "17.7 Virtual Machines and Containers\nAs servers grow in size, we have increasing amounts of power and resources, but also a larger space to manage. Traditional operating systems use a common memory and process management model that is shared by all users and applications, which can cause some issues if one of the users consumes all of the memory, fills the disk, or causes a kernel panic. When running on a bare server, all of the processes from all users are mixed together and are not isolated, so the actions of one process can have large consequences for all of the others.\n\n\n\n\nVirtual machines Virtualization is an approach to isolate the environments of the various users and services of a system so that we can make better use of the resource, and protect processes. In a virtualized environment, the host server still runs a host operating system, which includes a hypervisor process that can mediate between guest hosts on the machine and the underlying host operating system and hardware. This is effective at creating multiple virtual machines (VMs) running side-by-side on the same hardware. From the outside, and to most users, virtual machines appear to be a regular host on the network, when in fact they are virtual hosts that share the same underlying physical resources. But it also results in a fair amount of redundancy, in that each virtual machine must have its own operating system, libraries, and other resources. And calls pass through the guest operating system through the hypervisor to the physical layer, which can impose a performance penalty.\n\n\n\n\nContainers A further step down the isolation road is to use a Container Runtime such as containerd or Docker Engine. Like virtual machines, containers provide mechanisms to create images that can be executed by a container runtime, and which provide stronger isolation among deployments. But they are also more lightweight, as the container only contains the libraries and executables needed to execute a target application, and not an entire guest operating system. They also are built using a layered file system, which allows multiple images to be layered together to create a composite that provides rich services withot as much duplication. This means that applications run with fewer resources, start up and shut down more quickly, and can be migrated easily to other hosts in a network."
  },
  {
    "objectID": "sections/reproducibility-containers.html#learning-objectives",
    "href": "sections/reproducibility-containers.html#learning-objectives",
    "title": "18  Reproducibility and Containers",
    "section": "18.1 Learning Objectives",
    "text": "18.1 Learning Objectives\n\nThink about dependency management, reproducibility, and software\nBecome familiar with containers as a tool to improve computational reproducibility\nDiscuss how the techniques from this class can improve reproducibility\nSlides: Accelerating synthesis science through reproducible science"
  },
  {
    "objectID": "sections/reproducibility-containers.html#software-collapse",
    "href": "sections/reproducibility-containers.html#software-collapse",
    "title": "18  Reproducibility and Containers",
    "section": "18.2 Software collapse",
    "text": "18.2 Software collapse\n\n\n\nIn his blog post on software collapse, Konrad Hinsen outlines the fragile state of scientific software today. While we like to think that our software is robust, in reality it is being constantly revised, expanded, broken, and abandoned, such that our use of any software today is almost certainly doomed to disfunction shortly. Hinsen describes the science software ecosystem as having 4 layers, from totally general operating systems at the bottom, to scientific libraries, discipline-specific libraries, and finally project-specific software at the top. Hinsen states:\n\n\n\n\n\nAdapted from Hinsen’s software stack\n\n\n\n\n\n\nThere are essentially two tasks that need to be organized and financed: preventing collapse due to changes in layers 1 and 2, and implementing new models and methods as the scientific state of the art advances. These two tasks go on in parallel and are often executed by the same people, but in principle they are separate and one could concentrate on just one or the other.\nThe common problem of both communities is collapse, and the common enemy is changes in the foundations that scientists and developers build on.\n\nWhile most reproducibility work has focused at the project-specific software level, Hinsen argues that there is tremendous risk deep down in the software ecosystem. There are many examples of compatibility decisions in scientific software that create problems for long-term stability, even if they are the right decision at the time. For example, see the compatbility dicussions from numpy and pandas to get a sense fo the challenges faced when maintaining science software. This issue with abandoned software and lonely maintainers has become a prevalent meme in the open source software world.\n\n\n\nDependency"
  },
  {
    "objectID": "sections/reproducibility-containers.html#delaying-the-inevitable",
    "href": "sections/reproducibility-containers.html#delaying-the-inevitable",
    "title": "18  Reproducibility and Containers",
    "section": "18.3 Delaying the inevitable",
    "text": "18.3 Delaying the inevitable\nWhile software is destined to continue to rot, projects like WholeTale, repo2docker, and Binder are working towards tools that can help capture more of the software stack needed to reproduce computations, thereby enabling us to freeze older versions of software and replay them in compatible combinations. Or, barring full re-execution, at least to understand them. The idea is to take advantage of containerized computing environments, which can be described as ‘software as code’ because containers can be declaratively described in a configuration file. While project-specific software and possibly disciplinary software dependencies might be captured in a requirements.txt file for python, we also need to know all of the details of lower layers, including system libraries and kernel versions. Plus data dependencies, processing workflows, and provenance. And it turns out that these very details can be captured quite effectively in a virtualized container. For example, WholeTale uses container images to record not just project-specific software dependencies, but also operating system details, data dependencies, and runtime provenance.\n\nThis is accompished by first configuring a base image that contains core software from layers 1 and 2, and then researchers work within common tools like Jupyter and RStudio to create their scientific products at layers 3 and 4. These can then be captured in virtual environments, which get added to the container configuration, along with key details such as external data dependencies and process metadata. All of this can be serialized as a “Tale”, which is basically an archival package bundled around a container image definition.\n\n\n\nThis approach combines the evolving approach to using a Dockerfile to precisely configure a container environment with the use of structured metadata and provenance information to create an archival-quality research artifact. These WholeTale tales can be downloaded to any machine with a container runtime and executed under the same conditions that were used when the original tale was archived."
  },
  {
    "objectID": "sections/reproducibility-containers.html#hands-off-with-containers-and-docker",
    "href": "sections/reproducibility-containers.html#hands-off-with-containers-and-docker",
    "title": "18  Reproducibility and Containers",
    "section": "18.4 Hands-off with Containers and Docker",
    "text": "18.4 Hands-off with Containers and Docker\nWorking with docker or containers requires a container runtime. One of the nicest lately is Rancher Desktop. Install the binary for you platform, and then after it starts, enter the configuration Preferences, and then deselect “Kubernetes” to disable the kubernetes distribution, which takes up a lot of resources.\n\nThere are many different tools you can use with docker, including the Docker engine system from Docker.com using the docker client tool, and the containerd ecosystem using nerdctl as a client tool. Both the docker command and the nerdctl command share the same commands. The basics we will practice will be:\n\nnerdctl pull python:3.9: to grab an existing python image from the DockerHub repository\nnerdctl run -it python:3.9 -- python: to start a standard python interpreter\nnerdctl build: to build a new image from a Dockerfile configuration file"
  },
  {
    "objectID": "sections/reproducibility-containers.html#discussion",
    "href": "sections/reproducibility-containers.html#discussion",
    "title": "18  Reproducibility and Containers",
    "section": "18.5 Discussion",
    "text": "18.5 Discussion\nTo wrap up this week, let’s kick off a discussion with a couple of key questions.\n\nAs we learn new tools for scalable and reproducible computing, what can we as software creators do to improve robustness and ease maintenance of our packages?\nGiven the fragility of software ecosystems, are you worried about investing a lot of time in learning and building code for proprietary cloud systems? Can we compel vendors to keep their systems open?\nHow much can technological solutions such as containers truly address the issues around depencies, maintainability, and sustainability of scientific software?\nWhat relative fraction of the research budget should funders invest in software infrastructure, data infrastructure, and research outcomes?"
  },
  {
    "objectID": "sections/parquet-arrow.html#learning-objectives",
    "href": "sections/parquet-arrow.html#learning-objectives",
    "title": "19  Parquet and Arrow",
    "section": "19.1 Learning Objectives",
    "text": "19.1 Learning Objectives\n\nThe difference between column major and row major data\nSpeed advantages to columnnar data storage\nHow arrow enables faster processing"
  },
  {
    "objectID": "sections/parquet-arrow.html#introduction",
    "href": "sections/parquet-arrow.html#introduction",
    "title": "19  Parquet and Arrow",
    "section": "19.2 Introduction",
    "text": "19.2 Introduction\nParalleization is great, and can greatly help you in working with large data. However, it might not help you with every processing problem. Like we talked about with Dask, sometimes your data are too large to be read into memory, or you have I/O limitations. Parquet and pyarrow are newer, powerful tools that are designed to help overcome some of these problems. pyarrow and Parquet are newer technologies, and are a bit on the ‘bleeding edge’, but there is a lot of excitement about the possibility these tools provide.\nBefore jumping into those tools, however, first let’s discuss system calls. These are calls that are run by the operating system within their own process. There are several that are relevant to reading and writing data: open, read, write, seek, and close. Open establishes a connection with a file for reading, writing, or both. On open, a file offset points to the beginning of the file. After reading or writing n bytes, the offset will move n bytes forward to prepare for the next opration. Read will read data from the file into a memory buffer, and write will write data from a memory buffer to a file. Seek is used to change the location of the offset pointer, for either reading or writing purposes. Finally, close closes the connection to the file.\nIf you’ve worked with even moderately sized datasets, you may have encounted an “out of memory” error. Memory is where a computer stores the information needed immediately for processes. This is in contrast to storage, which is typically slower to access than memory, but has a much larger capacity. When you open a file, you are establishing a connection between your processor and the information in storage. On read, the data is read into memory that is then available to your python process, for example.\nSo what happens if the data you need to read in are larger than your memory? My brand new M1 MacBook Pro has 16 GB of memory, but this would be considered a modestly sized dataset by this courses’s standards. There are a number of solutions to this problem, which don’t involve just buying a computer with more memory. In this lesson we’ll discuss the difference between row major and column major file formats, and how leveraging column major formats can increase memory efficiency. We’ll also learn about another python library called pyarrow, which has a memory format that allows for “zero copy” read."
  },
  {
    "objectID": "sections/parquet-arrow.html#row-major-vs-column-major",
    "href": "sections/parquet-arrow.html#row-major-vs-column-major",
    "title": "19  Parquet and Arrow",
    "section": "19.3 Row major vs column major",
    "text": "19.3 Row major vs column major\nThe difference between row major and column major is in the ordering of items in the array when they are read into memory.\nTake the array:\na11 a12 a13\n\na21 a22 a23\nThis array in a row-major order would be read in as:\na11, a12, a13, a21, a22, a23\nYou could also read it in column-major order as:\na11, a21, a12, a22, a13, a33\nBy default, C and SAS use row major order for arrays, and column major is used by Fortran, MATLAB, R, and Julia.\nPython uses neither, instead representing arrays as lists of lists, though numpy uses row-major order.\n\n19.3.1 Row major versus column major files\nThe same concept can be applied to file formats as the example with arrays above. In row-major file formats, the values (bytes) of each record are read sequentially.\n\n\n\nName\nLocation\nAge\n\n\n\n\nJohn\nWashington\n40\n\n\nMariah\nTexas\n21\n\n\nAllison\nOregon\n57\n\n\n\nIn the above row major example, data are read in the order: John, Washingon, 40 \\n Mariah, Texas, 21.\nThis means that getting a subset of rows with all the columns would be easy; you can specify to read in only the first X rows (utilizing the seek system call). However, if we are only interested in Name and Location, we would still have to read in all of the rows before discarding the Age column.\nIf these data were organized in a column major format, they might look like this:\nName: John, Mariah, Allison\nLocation: Washington, Texas, Oregon\nAge: 40, 21, 57\nAnd the read order would first be the names, then the locations, then the age. This means that selecting all values from a set of columns is quite easy (all of the Names and Ages, or all Names and Locations), but reading in only the first few records from each column would require reading in the entire dataset. Another advantage to column major formats is that compression is more efficient since compression can be done across each column, where the data type is uniform, as opposed to across rows with many data types."
  },
  {
    "objectID": "sections/parquet-arrow.html#parquet",
    "href": "sections/parquet-arrow.html#parquet",
    "title": "19  Parquet and Arrow",
    "section": "19.4 Parquet",
    "text": "19.4 Parquet\nParquet is an open-source binary file format that stores data in a column-major format. The format contains several key components:\n\nrow group\ncolumn\npage\nfooter\n\n\nRow groups are blocks of data over a set number of rows that contain data from the same columns. Within each row group, data are organized in column-major format, and within each column are pages that are typically 1MB. The footer of the file contains metadata like the schema, encodings, unique values in each column, etc.\nThe parquet format has many tricks to to increase storage efficiency, and is increasingly being used to handle large datasets."
  },
  {
    "objectID": "sections/parquet-arrow.html#arrow",
    "href": "sections/parquet-arrow.html#arrow",
    "title": "19  Parquet and Arrow",
    "section": "19.5 Arrow",
    "text": "19.5 Arrow\nSo far, we have discussed the difference between organizing information in row-major and column-major format, how that applies to arrays, and how it applies to data storage on disk using Parquet.\nArrow is a language-agnostic specification that enables representation of column-major information in memory without having to serialize data from disk. The Arrow project provides implementation of this specification in a number of languages, including Python.\nLet’s say that you have utilized the Parquet data format for more efficient storage of your data on disk. At some point, you’ll need to read that data into memory in order to do analysis on it. Arrow enables data transfer between the on disk Parquet files and in-memory Python computations, via the pyarrow library.\npyarrow is great, but relatively low level. It supports basic group by and aggregate functions, as well as table and dataset joins, but it does not support the full operations that pandas does."
  },
  {
    "objectID": "sections/parquet-arrow.html#example",
    "href": "sections/parquet-arrow.html#example",
    "title": "19  Parquet and Arrow",
    "section": "19.6 Example",
    "text": "19.6 Example\nIn this example, we’ll read in a dataset of fish abundance in the San Francisco Estuary, which is published in csv format on the Environmental Data Initiative. This dataset isn’t huge, but it is big enough (3 GB) that working with it locally can be fairly taxing on memory. Motivated by user difficulties in actually working with the data, the deltafish R package was written using the R implementation of arrow. It works by downloading the EDI repository data, writing it to a local cache in parquet format, and using arrow to query it. In this example, I’ve put the Parquet files in a sharable location so we can explore them using pyarrow.\nFirst, we’ll load the modules we need.\n\nimport pyarrow.dataset as ds\nimport numpy as np\nimport pandas as pd\n\nNext we can read in the data using ds.dataset(), passing it the path to the parquet directory and how the data are partitioned.\n\ndeltafish = ds.dataset(\"/home/shares/deltafish/fish\",\n                       format=\"parquet\",\n                       partitioning='hive')\n\nYou can check out a file listing using the files method. Another great feature of parquet files is that they allow you to partition the data accross variables of the dataset. These partitions mean that, in this case, data from each species of fish is written to it’s own file. This allows for even faster operations down the road, since we know that users will commonly need to filter on the species variable. Even though the data are partitioned into different files, pyarrow knows that this is a single dataset, and you still work with it by referencing just the directory in which all of the partitioned files live.\n\ndeltafish.files\n\n['/home/shares/deltafish/fish/Taxa=Acanthogobius flavimanus/part-0.parquet',\n '/home/shares/deltafish/fish/Taxa=Acipenser medirostris/part-0.parquet',\n '/home/shares/deltafish/fish/Taxa=Acipenser transmontanus/part-0.parquet',\n '/home/shares/deltafish/fish/Taxa=Acipenser/part-0.parquet'...\nYou can view the columns of a dataset using schema.to_string()\n\ndeltafish.schema.to_string()\n\nSampleID: string\nLength: double\nCount: double\nNotes_catch: string\nSpecies: string\nIf we are only interested in a few species, we can do a filter:\n\nexpr = ((ds.field(\"Taxa\")==\"Dorosoma petenense\")| \n        (ds.field(\"Taxa\")==\"Morone saxatilis\") |\n        (ds.field(\"Taxa\")== \"Spirinchus thaleichthys\"))\n\nfishf = deltafish.to_table(filter = expr,\n                           columns =['SampleID', 'Length', 'Count', 'Taxa'])\n\nThere is another dataset included, the survey information. To do a join, we can just use the join method on the arrow dataset.\nFirst read in the survey dataset.\n\nsurvey = ds.dataset(\"/home/jclark/deltafish/survey\",\n                    format=\"parquet\",\n                    partitioning='hive')\n\nTake a look at the columns again:\n\nsurvey.schema.to_string()\n\nLet’s pick out only the ones we are interested in.\n\nsurvey_s = survey.to_table(columns=['SampleID','Datetime', 'Station', 'Longitude', 'Latitude'])\n\nThen do the join, and convert to a pandas data.frame.\n\nfish_j = fishf.join(survey_s, \"SampleID\").to_pandas()\n\nNote that when we did our first manipulation of this dataset, we went from working with a FileSystemDataset, which is a representation of a dataset on disk without reading it into memory, to a Table, which is read into memory. pyarrow has a number of functions that do computations on datasets without reading them into memory. However these are evaluated “eagerly,” as opposed to “lazily.” These are useful in some cases, like above, where we want to take a larger than memory dataset and generate a smaller dataset (via filter, or group by/summarize), but are not as useful if we need to do a join before our summarization/filter.\nMore functionality for lazy evaluation is on the horizon for pyarrow though, by leveraging Ibis."
  },
  {
    "objectID": "sections/parquet-arrow.html#synopsis",
    "href": "sections/parquet-arrow.html#synopsis",
    "title": "19  Parquet and Arrow",
    "section": "19.7 Synopsis",
    "text": "19.7 Synopsis\nIn this lesson we learned:\n\nthe difference between row major and column major formats\nunder what circumstances a column major data format can improve memory efficiency\nhow pyarrow can interact with Parquet files to analyze data"
  }
]