[["index.html", "Fundamentals in Data Management for Qualitative and Quantitative Arctic Research Fundamentals in Data Management for Qualitative and Quantitative Arctic Research About", " Fundamentals in Data Management for Qualitative and Quantitative Arctic Research April 18 - 22, 2022 Fundamentals in Data Management for Qualitative and Quantitative Arctic Research About This 5-day in-person workshop will provide researchers with an overview of reproducible and ethical research practices, steps and methods for more easily documenting and preserving their data at the Arctic Data Center, and an introduction to programming in R. Special attention will be paid to qualitative data management, including practices working with sensitive data. Example datasets will draw from natural and social sciences, and methods for conducting reproducible research will be discussed in the context of both qualitative and quantitative data. Responsible and reproducible data management practices will be discussed as they apply to all aspects of the data life cycle. This includes ethical data collection and data sharing, data sovereignty, and the CARE principles. The CARE principles are guidelines that help ensure open data practices (like the FAIR principles) appropriately engage with Indigenous Peoples’ rights and interests. Schedule Code of Conduct Please note that by participating in this activity you agree to abide by the NCEAS Code of Conduct. About this book These written materials reflect the continuous development of learning materials at the Arctic Data Center and NCEAS to support individuals to understand, adopt, and apply ethical open science practices. In bringing these materials together we recognize that many individuals have contributed to their development. The primary authors are listed alphabetically in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials. This work is licensed under a Creative Commons Attribution 4.0 International License. Citation: Amber E. Budden, S. Jeanette Clark, Natasha Haycock-Chavez, Noor Johnson, Matthew B. Jones. 2022. Fundamentals in Data Management for Qualitative and Quantitative Arctic Research. Additional contributors: Stephanie Hampton, Jim Regetz, Bryce Mecum, Julien Brun, Julie Lowndes, Erin McLean, Andrew Barrett, David LeBauer, Jessica Guo. "],["welcome-and-introductions.html", "1 Welcome and Introductions 1.1 Introduction to the Arctic Data Center and NSF Standards and Policies", " 1 Welcome and Introductions This course is one of three that we are currently offering, covering fundamentals of open data sharing, reproducible research, ethical data use and reuse, and scalable computing for reusing large data sets. 1.1 Introduction to the Arctic Data Center and NSF Standards and Policies 1.1.1 Learning Objectives In this lesson, we will discuss: The mission and structure of the Arctic Data Center How the Arctic Data Center supports the research community About data policies from the NSF Arctic program 1.1.2 Arctic Data Center - History and Introduction The Arctic Data Center is the primary data and software repository for the Arctic section of National Science Foundation’s Office of Polar Programs (NSF OPP). We’re best known in the research community as a data archive – researchers upload their data to preserve it for the future and make it available for re-use. This isn’t the end of that data’s life, though. These data can then be downloaded for different analyses or synthesis projects. In addition to being a data discovery portal, we also offer top-notch tools, support services, and training opportunities. We also provide data rescue services. NSF has long had a commitment to data reuse and sharing. Since our start in 2016, we’ve grown substantially – from that original 4 TB of data from ACADIS to now over 65 TB. In 2021 alone, we saw 16% growth in dataset count, and about 30% growth in data volume. This increase has come from advances in tools – both ours and of the scientific community, plus active community outreach and a strong culture of data preservation from NSF and from researchers. We plan to add more storage capacity in the coming months, as researchers are coming to us with datasets in the terabytes, and we’re excited to preserve these research products in our archive. We’re projecting our growth to be around several hundred TB this year, which has a big impact on processing time. Give us a heads up if you’re planning on having larger submissions so that we can work with you and be prepared for a large influx of data. The data that we have in the Arctic Data Center comes from a wide variety of disciplines. These different programs within NSF all have different focuses – the Arctic Observing Network supports scientific and community-based observations of biodiversity, ecosystems, human societies, land, ice, marine and freshwater systems, and the atmosphere as well as their social, natural, and/or physical environments, so that encompasses a lot right there in just that one program. We’re also working on a way right now to classify the datasets by discipline, so keep an eye out for that coming soon. Along with that diversity of disciplines comes a diversity of file types. The most common file type we have are image files in four different file types. Probably less than 200-300 of the datasets have the majority of those images – we have some large datasets that have image and/or audio files from drones. Most of those 6600+ datasets are tabular datasets. There’s a large diversity of data files, though, whether you want to look at remote sensing images, listen to passive acoustic audio files, or run applications – or something else entirely. We also cover a long period of time, at least by human standards. The data represented in our repository spans across centuries. We also have data that spans the entire Arctic, as well as the sub-Arctic, regions. 1.1.3 Data Discovery Portal To browse the data catalog, navigate to arcticdata.io. Go to the top of the page and under data, go to search. Right now, you’re looking at the whole catalog. You can narrow your search down by the map area, a general search, or searching by an attribute. Clicking on a dataset brings you to this page. You have the option to download all the files by clicking the green “Download All” button, which will zip together all the files in the dataset to your Downloads folder. You can also pick and choose to download just specific files. All the raw data is in open formats to make it easily accessible and compliant with FAIR principles – for example, tabular documents are in .csv (comma separated values) rather than Excel documents. The metrics at the top give info about the number of citations with this data, the number of downloads, and the number of views. This is what it looks like when you click on the Downloads tab for more information. Scroll down for more info about the dataset – abstract, keywords. Then you’ll see more info about the data itself. This shows the data with a description, as well as info about the attributes (or variables or parameters) that were measured. The green check mark indicates that those attributes have been annotated, which means the measurements have a precise definition to them. Scrolling further, we also see who collected the data, where they collected it, and when they collected it, as well as any funding information like a grant number. For biological data, there is the option to add taxa. 1.1.4 Tools and Infrastructure Across all our services and partnership, we are strongly aligned with the community principles of making data FAIR (Findable, Accesible, Interoperable and Reusable). We have a number of tools available to submitters and researchers who are there to download data. We also partner with other organizations, like Make Data Count and DataONE, and leverage those partnerships to create a better data experience. One of those tools is provenance tracking. With provenance tracking, users of the Arctic Data Center can see exactly what datasets led to what product, using the particular script that the researcher ran. Another tool are our Metadata Quality Checks. We know that data quality is important for researchers to find datasets and to have trust in them to use them for another analysis. For every submitted dataset, the metadata is run through a quality check to increase the completeness of submitted metadata records. These checks are seen by the submitter as well as are available to those that view the data, which helps to increase knowledge of how complete their metadata is before submission. That way, the metadata that is uploaded to the Arctic Data Center is as complete as possible, and close to following the guideline of being understandable to any reasonable scientist. 1.1.5 Support Services Metadata quality checks are the automatic way that we ensure quality of data in the repository, but the real quality and curation support is done by our curation team. The process by which data gets into the Arctic Data Center is iterative, meaning that our team works with the submitter to ensure good quality and completeness of data. When a submitter submits data, our team gets a notification and beings to evaluate the data for upload. They then go in and format it for input into the catalog, communicating back and forth with the researcher if anything is incomplete or not communicated well. This process can take anywhere from a few days or a few weeks, depending on the size of the dataset and how quickly the researcher gets back to us. Once that process has been completed, the dataset is published with a DOI (digital object identifier). 1.1.6 Training and Outreach In addition to the tools and support services, we also interact with the community via trainings like this one and outreach events. We run workshops at conferences like the American Geophysical Union, Arctic Science Summit Week and others. We also run an intern and fellows program, and webinars with different organizations. We’re invested in helping the Arctic science community learn reproducible techniques, since it facilitates a more open culture of data sharing and reuse. We strive to keep our fingers on the pulse of what researchers like yourselves are looking for in terms of support. We’re active on Twitter to share Arctic updates, data science updates, and specifically Arctic Data Center updates, but we’re also happy to feature new papers or successes that you all have had with working with the data. We can also take data science questions if you’re running into those in the course of your research, or how to make a quality data management plan. Follow us on Twitter and interact with us – we love to be involved in your research as it’s happening as well as after it’s completed. 1.1.7 Data Rescue We also run data rescue operations. We digitiazed Autin Post’s collection of glacier photos that were taken from 1964 to 1997. There were 100,000+ files and almost 5 TB of data to ingest, and we reconstructed flight paths, digitized the images of his notes, and documented image metadata, including the camera specifications. 1.1.8 Who Must Submit Projects that have to submit their data include all Arctic Research Opportunities through the NSF Office of Polar Programs. That data has to be uploaded within two years of collection. The Arctic Observing Network has a shorter timeline – their data products must be uploaded within 6 months of collection. Additionally, we have social science data, though that data often has special exceptions due to sensitive human subjects data. At the very least, the metadata has to be deposited with us. Arctic Research Opportunities (ARC) Complete metadata and all appropriate data and derived products Within 2 years of collection or before the end of the award, whichever comes first ARC Arctic Observation Network (AON) Complete metadata and all data Real-time data made public immediately Within 6 months of collection Arctic Social Sciences Program (ASSP) NSF policies include special exceptions for ASSP and other awards that contain sensitive data Human subjects, governed by an Institutional Review Board, ethically or legally sensitive, at risk of decontextualization Metadata record that documents non-sensitive aspects of the project and data Title, Contact information, Abstract, Methods For more complete information see our “Who Must Submit” webpage Recognizing the importance of sensitive data handling and of ethical treatment of all data, the Arctic Data Center submission system provides the opportunity for researchers to document the ethical treatment of data and how collection is aligned with community principles (such as the CARE principles). Submitters may also tag the metadata according to community develop data sensitivity tags. We will go over these features in more detail shortly. 1.1.9 Summary All the above informtion can be found on our website or if you need help, ask our support team at support@arcticdata.io or tweet us @arcticdatactr! "],["open-data-and-reproducibility.html", "2 Open Data and Reproducibility 2.1 Introduction to Reproducible Research 2.2 Related resources and references", " 2 Open Data and Reproducibility 2.1 Introduction to Reproducible Research 2.1.1 Learning Objectives Shared understanding of open and research reproducibility Introduction to computational reproducibility Introduction to FAIR and CARE as they relate to open data Familiarity with metadata best practices 2.1.2 What do we mean by open science and research reproducibility? Throughout this course you will hear us emphasize open practices and research reproducibility. Indeed, as a data repository charged with both preserving and making accessible the products of NSF funded research, it is no surprise that these topics are central to our mission. However, these terms, while readily parsed, can mean different things to different individuals depending on their frame of reference and research activities. Further, even with a shared definition variation exists in the degree to which openness and research reproducibility can (or should) be implemented. Before we begin, we are going to take the time to discuss and possibly develop a shared understanding of the terms ‘open’ and ‘reproducible’ as they pertain to research. Discuss In small groups answer the following questions: What does open research mean to you? Do you consider your own research practices open? If not, why not? If so, how so? What does reproducible research mean to you? Who should be able to reproduce research? Is your own research reproducible? If not, why not? If so, how so? Identify an individual to summarize and report back to the full group. 2.1.3 What is research reproducibility and how does it relate to open science? Reproducibility is a hallmark of the scientific research process, which is based on empirical observations coupled with explanatory models. Whether integrating data from across multiple studies and sources, or working with your own data, the data life cycle typically involves some degree of data collection/integration, quality assurance practices, analysis and synthesis. Operating in a reproducible fashion means that each of these steps can be easily re-executed to achieve the same result, ideally as part of a single workflow. Reproducibility means different things to different researchers. For our purposes, practical reproducibility looks like: Preserving the data Preserving the software workflow Documenting what you did Describing how to interpret it all Reproducibility does not, by definition, require openness. Reproducibility can be achieved within a single research activity or across a research program with a closed group of collaborators. However, when working in an OPEN and REPRODUCIBLE manner, we are better able to transform knowledge into benefits for society. In this section we will expand on the benefits of reproducible research and open science before highlighting some best practices. 2.1.4 Open Science To enable full reproducibility by the broader community; researchers, practitioners, policy makers etc, all products of the research activity need to be accessible - open data, open code, and open publications. Further, full research transparency also requires open peer review. There are, of course, data sensitivities and ethical considerations regarding open everything and these will be discussed later. At its core, the aims of Open Science are to: Increase transparency of the research process Enable reproducibility of results and conclusions Accelerate discovery Enhance and facilitate collaboration Increase diversity, equity and inclusion Transform knowledge into benefits for society So, why is reproducible research important? Working in a reproducible manner builds efficiencies into your own research practices. The ability to automate processes and rerun analyses as you collect more data, or share your full workflow (including data, code and products) with colleagues, will accelerate the pace of your research and collaborations. However, beyond these direct benefits, reproducible research builds trust in science with the public, policy makers and others. What data were used in this study? What methods applied? What were the parameter settings? What documentation or code are available to us to evaluate the results? Can we trust these data and methods? Are the results reproducible? Ionnidis (2005) contends that “Most research findings are false for most research designs and for most fields”, and a study of replicability in psychology experiments found that “Most replication effects were smaller than the original results” (Open Science Collaboration, 2015). In the case of ‘climategate’, it took three years, and over 300 personnel, to gather the necessary provenance information in order to document how results, figures and other outputs were derived from input sources. Time and effort that could have been significantly reduced with appropriate documentation and reproducible practices. Moving forward, through reproducible research training, practices, and infrastructure, the need to manually chase this information will be reduced enabling replication studies and great trust in science. Computational reproducibility Computational reproducibility is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions. To be able to evaluate the data, analyses, and models on which conclusions are drawn, computational reproducibility requires open science approaches, including straightforward steps for archiving data and code openly along with the scientific workflows describing the provenance of scientific results (e.g., Hampton et al. (2015), Munafò et al. (2017)). Scientific workflows encapsulate all of the steps from data acquisition, cleaning, transformation, integration, analysis, and visualization. Workflows can range in detail from simple flowcharts to fully executable scripts. R scripts and python scripts are a textual form of a workflow, and when researchers publish specific versions of the scripts and data used in an analysis, it becomes far easier to repeat their computations and understand the provenance of their conclusions. Preserving computational workflows enables understanding, evaluation, and reuse for the benefit of future you and your collaborators and colleagues across disciplines. 2.1.5 A Note on Community Principles: CARE and FAIR In facilitating use of data resources, the community have converged on principles surrounding best practices for open data management. One set of these principles is the FAIR principles: Findable, Accessible, Interoperable, and Reproducible. FAIR principles and open science are overlapping, but distinct concepts. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. Note, that accessibility in FAIR does not require the data to be open. The guiding principles put forward by FORCE 11 for accessibility are that: (meta)data are retrievable by their identifier using a standard communicaitons protocol the protocol is open, free, and universally implementable the protocol allows for an authentication and authorization procedure, where neccesary metadata are accessible, even when the data are no longer available The CARE Principles for Indigenous Data Governance complement the more data-centric approach of the FAIR principles, introducing social responsibility to open data management practices. The CARE Principles stand for: Collective Benefit Authority to Control Responsibility Ethics These will be discussed more comprehensively during the data ethics section of the course 2.1.6 Metadata Best Practices Well structured and described data unpin the FAIR principles and we’ll visit how best to organize your data in a tidy and effective manner (in both theory and practice) shortly. Here we focus on metadata best practices. Metadata (data about data) is an important part of the data life cycle because it enables data reuse long after the original collection. Imagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files? The goal is to have enough information for the researcher to understand the data, interpret the data, and then re-use the data in another study. Another way to think about it is to answer the following questions with the documentation: What was measured? Who measured it? When was it measured? Where was it measured? How was it measured? How is the data structured? Why was the data collected? Who should get credit for this data (researcher AND funding agency)? How can this data be reused (licensing)? Bibliographic Details The details that will help your data be cited correctly are: a global identifier like a digital object identifier (DOI); a descriptive title that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data a descriptive abstract that serves as a brief overview off the specific contents and purpose of the data package funding information like the award number and the sponsor; the people and organizations like the creator of the dataset (ie who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset Discovery Details The details that will help your data be discovered correctly are: the geospatial coverage of the data, including the field and laboratory sampling locations, place names and precise coordinates; the temporal coverage of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to; the taxonomic coverage of the data, including what species were measured and what taxonomy standards and procedures were followed; as well as any other contextual information as needed. Interpretation Details The details that will help your data be interpreted correctly are: the collection methods for both field and laboratory data; the full experimental and project design as well as how the data in the dataset fits into the overall project; the processing methods for both field and laboratory samples IN FULL; all sample quality control procedures; the provenance information to support your analysis and modelling methods; information about the hardware and software used to process your data, including the make, model, and version; and the computing quality control procedures like any testing or code review. Data Structure and Contents Well constructed metadata also includes information about the data structure and contents. Everything needs a description: the data model, the data objects (like tables, images, matricies, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation. Variable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (such as 0 = not collected), and any missing values (such as 999 = NA). Not only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable). Using an example from the natural sciences, if you were to search for the character string carbon dioxide flux in the general search box at the Arctic Data Center, not all relevant results will be shown due to varying vocabulary conventions (ie, CO2 flux instead of carbon dioxide flux) across disciplines — only datasets containing the exact words carbon dioxide flux are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search. Above left demonstrates a typical search for “carbon dioxide flux”, yielding 20 datasets. Above right illustrates an annotated search for “carbon dioxide flux”, yielding 29 datasets. Note that if you were to interact with the site and explore the results of the figure on the right, the dataset in red of Figure 3 will not appear in the typical search for “carbon dioxide flux.” Rights and Attribution Correctly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like: the citation format to be used when giving credit for the data; the attribution expectations for the dataset; the reuse rights, which describe who may use the data and for what purpose; the redistribution rights, which describe who may copy and redistribute the metadata and the data; and the legal terms and conditions like how the data are licensed for reuse. Ethical Data Practices Additionally, at the Arctic Data Center, we now require data submissions to include information on the level of data sensitivity and to provide a statement of ethical research practice. As the primary repository for the NSF Office of Polar Programs Arctic Section, the Arctic Data Center accepts data from all disciplines. This includes data from social science research that may include sensitive data. Sharing sensitive data can pose challenges to researchers, however sharing metadata or anonymized data contributes to discovery, supports open science principles, and helps reduce duplicate research efforts. To help mitigate the challenges of sharing sensitive data, researchers submitting data now have the option to choose between varying levels of sensitivity that best represent their dataset. Within the research methods section, submitters are now asked to describe the ethical data practices used throughout their research. The information provided will be visible as part of the metadata record and this field has been added to encourage transparency in data ethics. Transparency in data ethics is a vital part of open science and sharing ethical practices encourages deeper discussion about data reuse and ethics. So, how do you organize all this information? There are a number of metadata standards (think, templates) that you could use, including the Ecological Metadata Language (EML), Geospatial Metadata Standards like ISO 19115 and ISO 19139, the Biological Data Profile (BDP), Dublin Core, Darwin Core, PREMIS, the Metadata Encoding and Transmission Standard (METS), and the list goes on and on. The Arctic Data Center runs on EML and has a simple to use interface that guides you through the process of creating your metadata record. 2.1.7 Data Identifiers Many journals require a DOI - a digital object identifier - be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to. At the Arctic Data Center, we assign a DOI to each published dataset. But, sometimes datasets need to be updated. Each version of a dataset published with the Arctic Data Center has a unique identifier associated with it. Researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available. When there is a newer version available, that will be clearly marked on the original dataset page with a yellow banner indicating as such. Having the data identified in this manner allows us to accurately track the dataset usage metrics. The Arctic Data Center tracks the number of citations, the number of downloads, and the number of views of each dataset in the catalog. 2.1.8 Data Citation Data citation best practices are focused on providing credit where credit is due and indexing and exposing data citations across international repository networks. In 2014, Force 11 established a Joint Declaration of Data Citation Principles that includes: Importance of data citation Credit and Attribution Evidence Unique Identification Access Persistence Specificity and Verifiability Interoperability and Flexibility Transitive Credit We want to move towards a model such that when a user cites a research publication we will also know: Which data produced it What software produced it What was derived from it Who to credit down the attribution stack This is transitive credit. And it changes the way in which we think about science communication and traditional publications. 2.2 Related resources and references [1] J. et al. Spies. The reproducibility of psychological science. Tech. rep. Report of the Open Science Collaboration, 2012. [2] O. B. Amaral and K. Neves. “Reproducibility: expect less of the scientific paper”. En. In: Nature 597.7876 (Sep. 2021). Bandiera_ abtest: a Cg_ type: Comment Number: 7876 Publisher: Nature Publishing Group Subject_ term: Publishing, Research data, Research management, pp. 329-331. DOI: 10.1038/d41586-021-02486-7. &lt;URL: https://www.nature.com/articles/d41586-021-02486-7&gt; (visited on 09/19/2021). [3] S. G. Baker, A. K. Darke, P. Pinsky, et al. “Transparency and reproducibility in data analysis: the Prostate Cancer Prevention Trial”. In: Biostatistics Oxford England 11.3 (2010), pp. 413-418. &lt;URL: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2883301&amp;tool=pmcentrez&amp;rendertype=abstract&gt;. [4] P. Cassey and T. M. Blackburn. “Reproducibility and Repeatability in Ecology”. In: BioScience 56.12 (2006), pp. 958-959. ISSN: 00063568. DOI: 10.1641/0006-3568(2006)56[958:RARIE]2.0.CO;2. &lt;URL: http://caliber.ucpress.net/doi/full/10.1641/0006-3568%282006%2956%5B958%3ARARIE%5D2.0.CO%3B2}.&gt; [5] R. Gentleman. “Reproducible research: a bioinformatics case study.” In: Statistical applications in genetics and molecular biology 4.1 (2005), p. Article2. &lt;URL: http://www.bepress.com/sagmb/vol4/iss1/art2&gt;. [6] C. Goble. What is reproducibility? The R* Brouhaha. Science. Feb. 25, 2019. &lt;URL: https://www.slideshare.net/carolegoble/what-is-reproducibility-gobleclean&gt; (visited on 02/25/2019). [7] B. Marwick. “Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation”. En. In: Journal of Archaeological Method and Theory 24.2 (Jun. 2017), pp. 424-450. ISSN: 1072-5369, 1573-7764. DOI: 10.1007/s10816-015-9272-9. &lt;URL: http://link.springer.com/10.1007/s10816-015-9272-9&gt; (visited on 08/28/2017). [8] B. Marwick, C. Boettiger, and L. Mullen. “Packaging Data Analytical Work Reproducibly Using R (and Friends)”. In: The American Statistician 72.1 (Jan. 2018), pp. 80-88. ISSN: 0003-1305. DOI: 10.1080/00031305.2017.1375986. &lt;URL: https://doi.org/10.1080/00031305.2017.1375986&gt; (visited on 02/14/2019). [9] D. Nüst and E. Pebesma. “Practical Reproducibility in Geography and Geosciences”. In: Annals of the American Association of Geographers 0.0 (Oct. 2020). Publisher: Taylor &amp; Francis _ eprint: https://doi.org/10.1080/24694452.2020.1806028, pp. 1-11. ISSN: 2469-4452. DOI: 10.1080/24694452.2020.1806028. &lt;URL: https://doi.org/10.1080/24694452.2020.1806028&gt; (visited on 06/14/2021). [10] D. Nüst, V. Sochat, B. Marwick, et al. “Ten simple rules for writing Dockerfiles for reproducible data science”. En. In: PLOS Computational Biology 16.11 (Nov. 2020). Publisher: Public Library of Science, p. e1008316. ISSN: 1553-7358. DOI: 10.1371/journal.pcbi.1008316. &lt;URL: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316&gt; (visited on 11/18/2020). [11] S. M. Powers and S. E. Hampton. “Open science, reproducibility, and transparency in ecology”. En. In: Ecological Applications 29.1 (2019). _ eprint: https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/eap.1822, p. e01822. ISSN: 1939-5582. DOI: 10.1002/eap.1822. &lt;URL: https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/eap.1822&gt; (visited on 07/01/2021). [12] G. K. Sandve, A. Nekrutenko, J. Taylor, et al. “Ten Simple Rules for Reproducible Computational Research”. In: PLoS Computational Biology 9.10 (Oct. 2013). Ed. by P. E. Bourne, p. e1003285. ISSN: 1553-7358. DOI: 10.1371/journal.pcbi.1003285. &lt;URL: http://dx.plos.org/10.1371/journal.pcbi.1003285&gt; (visited on 10/28/2013). [13] M. Schwab, M. Karrenbach, and J. Claerbout. “Making Scientific Computations Reproducible”. In: Computing in Science Engineering 2.6 (2000), pp. 61-67. ISSN: 15219615. DOI: 10.1109/5992.881708. &lt;URL: http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=881708&gt;. [14] V. Stodden. “The Legal Framework for Reproducible Scientific Research: Licensing and Copyright”. In: Computing in Science &amp; Engineering 11.1 (Jan. 2009), pp. 35-40. ISSN: 1521-9615. DOI: 10.1109/MCSE.2009.19. &lt;URL: http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4720221&gt;. [15] J. P. Wilson, K. Butler, S. Gao, et al. “A Five-Star Guide for Achieving Replicability and Reproducibility When Working with GIS Software and Algorithms”. In: Annals of the American Association of Geographers 111.5 (Jul. 2021). Publisher: Taylor &amp; Francis _ eprint: https://doi.org/10.1080/24694452.2020.1806026, pp. 1311-1317. ISSN: 2469-4452. DOI: 10.1080/24694452.2020.1806026. &lt;URL: https://doi.org/10.1080/24694452.2020.1806026&gt; (visited on 09/10/2021). "],["thinking-preferences.html", "3 Thinking Preferences 3.1 Thinking preferences", " 3 Thinking Preferences 3.1 Thinking preferences 3.1.1 Learning Objectives An activity and discussion that will provide: Opportunity to get to know fellow participants and trainers An introduction to variation in thinking preferences 3.1.2 Thinking Preferences Activity Step 1: Don’t read ahead!! We’re headed to the patio. 3.1.3 About the Whole Brain Thinking System Everyone thinks differently. The way individuals think guides the way they work, and the way groups of individuals think guides how teams work. Understanding thinking preferences facilitates effective collaboration and team work. The Whole Brain Model, developed by Ned Herrmann, builds upon our understanding of brain functioning. For example, the left and right hemispheres are associated with different types of information processing and our neocortex and limbic system regulate different functions and behaviours. Horse illusion The Herrmann Brain Dominance Instrument (HBDI) provides insight into dominant characteristics based on thinking preferences. There are four major thinking styles that reflect the left cerebral, left limbic, right cerebral and right limbic. Analytical (Blue) Practical (Green) Relational (Red) Experimental (Yellow) These four thinking styles are characterized by different traits. Those in the BLUE quadrant have a strong logical and rational side. They analyze information and may be technical in their approach to problems. They are interested in the ‘what’ of a situation. Those in the GREEN quadrant have a strong organizational and sequential side. They like to plan details and are methodical in their approach. They are interested in the ‘when’ of a situation. The RED quadrant includes those that are feelings-based in their apporach. They have strong interpersonal skills and are good communicators. They are interested in the ‘who’ of a situation. Those in the YELLOW quadrant are ideas people. They are imaginative, conceptual thinkers that explore outside the box. Yellows are interested in the ‘why’ of a situation. Undertsanding how people think and process information helps us understand not only our own approach to problem solving, but also how individuals within a team can contribute. There is great value in diversity of thinking styles within collaborative teams, each type bringing stengths to different aspects of project development. Of course, most of us identify with thinking styles in more than one quadrant and these different thinking preferences reflect a complex self made up of our rational, theoretical self; our ordered, safekeeping self; our emotional, interpersonal self; and our imaginitive, experimental self. 3.1.3.1 Bonus Activity: Your Complex Self Using the statements contrained within this document, plot the quadrilateral representing your complex self. "],["writing-data-management-plans.html", "4 Writing Data Management Plans 4.1 Writing Good Data Management Plans", " 4 Writing Data Management Plans 4.1 Writing Good Data Management Plans 4.1.1 Learning Objectives In this lesson, you will learn: Why create data management plans The major components of data management plans Tools that can help create a data management plan Features and functionality of the DMPTool 4.1.2 When to Plan: The Data Life Cycle Shown below is one version of the Data Life Cycle that was developed by DataONE. The data life cycle provides a high level overview of the stages involved in successful management and preservation of data for use and reuse. Multiple versions of a data life cycle exist with differences attributable to variation in practices across domains or communities. It is not neccesary for researchers to move through the data life cycle in a cylical fashion and some research activities might use only part of the life cycle. For instance, a project involving meta-analysis might focus on the Discover, Integrate, and Analyze steps, while a project focused on primary data collection and analysis might bypass the Discover and Integrate steps. However, ‘Plan’ is at the top of the data life cycle as it is advisable to initiate your data management planning at the beginning of your research process, before any data has been collected. 4.1.3 Why Plan? Planning data management in advance povides a number of benefits to the researcher. Saves time and increases efficiency; Data management planning requires that a researcher think about data handling in advance of data collection, potentially raising any challenges before they are encountered. Engages your team; Being able to plan effectively will require conversation with multiple parties, engaging project participants from the outset. Allows you to stay organized; It will be easier to organize your data for analysis and reuse. Meet funder requirements; Funders require a data management plan as part of the proposal process. Share data; Information in the DMP is the foundation for archiving and sharing data with community. 4.1.4 How to Plan As indicated above, engaging your team is a benefit of data management planning. Collaborators involved in the data collection and processing of your research data bring diverse expertise. Therefore, plan in collaboration with these individuals. Make sure to plan from the start to avoid confusion, data loss, and increase efficiency. Given DMPs are a requirement of funding agencies, it is nearly always neccesary to plan from the start. However, the same should apply to research that is being undertaken outside of a specific funded proposal. Make sure to utilize resources that are available to assist you in helping to write a good DMP. These might include your institutional library or organization data manager, online resources or education materials such as these. Use tools available to you; you don’t have to reinvent the wheel. Revise your plan as situations change and you potentially adapt/alter your project. Like your research projects, data management plans are not static, they require changes and updates throughout the research project process. 4.1.5 What to include in a DMP If you are writing a data management plan as part of a solicitation proposal, the funding agency will have guidelines for the information they want to be provided in the plan. A good plan will provide information on the study design; data to be collected; metadata; policies for access, sharing &amp; reuse; long-term storage &amp; data management; and budget. A note on Metadata: Both basic metadata (such as title and researcher contact information) and comprehensive metadata (such as complete methods of data collection) are critical for accurate interpretation and understanding. The full definitions of variables, especially units, inside each dataset are also critical as they relate to the methods used for creation. Knowing certain blocking or grouping methods, for example, would be necessary to understand studies for proper comparisons and synthesis. The article Ten Simple Rules for Creating a Good Data Management Plan is a great resource for thinking about writing a data management plan and the information you should include within the plan. The ten simple rules are: 1. Determine the research sponsor requirements If you are writing your DMP in association with a proposal submission, your funding body will likely have specific requirements for your DMP. If you are creating a DMP for your own research purposes, it may be useful to refer to a generic plan or one from a funder aligned with your domain. 2. Identify the data to be collected Consider the types, sources, volume, and data and file formats. 3. Define how the data will be organized Will you store you data in excel spreadsheets? csv format? Are your data in a database structure? 4. Explain how the data will be documented Metadata!! What standard will your be using? How will you create your metadata? 5. Describe how quality data will be assured What methods or approaches will be taken to assure data quality? Training activities, instrument calibration, verification tests, double-blind data entry, statistical and visual interpretation are all approaches to error detection. 6. Present a sound storage &amp; preservation strategy How long will the data be accessible? How will data be stored and protected during the project? How will data be preserved and made available for future use? This is where you would include information on the repository that will preserve your data. 7. Define the project’s data policies What are your licensing and data sharing arrangements? Do you have any human subject or other sensitive data that requires special consideration? 8. Describe how the data will be disseminated More active, robust and preferred approaches include: (1) publishing the data in an open repository or archive; (2) publishing the data, metadata, and relevant code as a “data paper”. 9. Assign roles and responsibilities Roles may include data collection, data entry, QA/QC, metadata creation and management, backup, data preparation and submission to an archive, and systems administration. 10. Prepare a realistic budget Review your plan and make sure that there are lines in the budget to support the people that manage the data as well as pay for the requisite hardware, software etc. 4.1.6 NSF DMP requirements In the 2014 Proposal Preparation Instructions, Section J ‘Special Information and Supplementary Documentation’ NSF put foward the baseline requirements for a data management plan. In addition, there are specific divison and program requirements that provide additional detail. If you are working on a research project with funding that does not require a data management plan, or are developing a plan for unfunded research, the NSF generic requirements are a good set of guidelines to follow. Five Sections of the NSF DMP Requirements 1. Products of research Types of data, samples, physical collections, software, curriculum materials, other materials produced during project 2. Data formats and standards Standards to be used for data and metadata format and content (for initial data collection, as well as subsequent storageand processing) 3. Policies for access and sharing Provisions for appropriate protection of privacy, confidentiality, security, intellectual property, or other rights or requirements 4. Policies and provisions for re-use Including re-distribution and the production of derivatives 5. Archiving of data Plans for archiving data, samples, research products and for preservation of access 4.1.7 Tools in Support of Creating a DMP The DMP Tool and DMP Online are both easy to use web based tools that support the development of a DMP. The tools are partnered and share a code base; the DMPTool incorporates templates from US funding agencies and the DMP Online is focussed on EU requirements. 4.1.8 Hands-On: Creating a DMP Go to https://dmptool.org Click ‘get Started’ to login. You will have three options. Options 1 and 2 apply if your organization is partnered with the DMP Tool or if you already have an account, option 3 is in order to set up an account. Under Option 1 you will be prompted to search for your organization and can then log-in using your institutional ID. Once logged in you will be taken to your DMP dashboard. Here you will find a list of all the plans that are affiliated with your account. Before getting started, it is worth taking a look at a couple of resources within the DMPTool that are helpful. These can be found under ‘Learn’ at the top right. The first is the list of Funder Requirements. This details the full set of funder / division / program DMP requirements that have been converted into templates within the tool. For each set of requirements you can download the template to use outside of the tool, review the date of the most recent update, refer to the oringal guidance directly from the funder website and review sample plans. Another place to discover example plans is under the ‘Public Plans’ section (Learn&gt;Public Plans). Any plan submitted by a user that was marked as public can be found here. No information is provided on whether these plans were associated with a funded proposal, nor any evaluation of the plan quality. However, they are useful to review if writing a DMP for the first time. OK, back to your dashboard. To create a new plan, simply click the ‘Create Plan’ button on the right. You may also click on the words “Create Plan’ aover the horizontal line, they go to the same location. You are now in the DMP Tool editor which guides you through a series of questions in order to complete the plan. The first questions connect your plan to your institution and ensure the correct template is being used. For the purposes of this workshop, when completing the title, also check the box next to it indicating that this plan is a test. This ensures that the plan does not get included in the DMPTool reporting metrics. The boxes for research organization and funding agency will prompt you for affiliations. If there are multiple plan templates for a given funding agency, another box will pop up asking you to select a template from the drop-down list. For this workshop, we are going to use the NSF: Generic template. Click ‘Create Plan’. This will take you into the template and you will see five tabs - Project Details, Plan Overview, Write Plan, Share and Download. We will work through these now. Project Details: Here we can provide more information about the project. For example, an abstract or funder grant number. As with all other fields throughout the tool, required answers are indicated by an asterisk. Note that you can add your ORCiD here. Since you would have logged in with your institutional ID, or created a specific DMPTool account, this field has not been filled automatically. You will also notice on the right hand side that you have the option to include guidance from up to six organizaitons. By default, the DMPTool guidance and your institutional affiliation guidance is included. If you have collaborators at other institutions you may choose to add those by selecting ‘See the full list’. Plan Overview: This page provides a synopsis of the funder template. It’s a quick way to view what is going to be required when writing the plan. Note that in this example, there are five sections as we are using the NSF Generic template. You do not need to enter any information on this page and clicking ‘Write Plan’ will simply take you to the next tab. Write Plan: We now see those same five sections as expandable options. These are titled as before and the numbers in parentheses indicate how many questions are contained within each section and how many of those questions have been answered. Because we are just beginning, the first number in each case is 0. I want to highlight that this page will look different according to the template that you are using. Different funding agencies / divisions / programs have different requirements for the data management plan. Therefore the template within the DMPTool may have more sections and more questions within each section. We are working through a simple, generic example. Below is an example for creating a DMP using the NSF Polar Programs Arctic Section template. Back to the NSF Generic template. Clicking on any + symbol within the boxes will expand to show the editing pane. You will see the full question with a text box below. On the right hand side is a box containing guidance. The tabs here represent the organizations that were selected under the Project Details step. They provide links to useful information and guidance. Depending on the template that has been selected, there might also be an ‘example answer’ under the text box. It is not intended that you copy and paste this verbatim. Rather, this is example prose that you can refer to for answering the quesiton. The image below shows one such example when using the NSF Polar Programs Arctic Section template. This is one advantage of signing in through your institution and for institutions to partner with the DMPTool, they can provide specific example language and guidance for their researchers. You will also notice a box for comments on the right hand side. In a moment you will see how collaboration can be managed. However, if you have shared your plan with others, this is where you will be able to see their comments. There is no requirement for you to answer all questions in one sitting. Completing the plan can require information gathering from multiple sources. Saving the plan at this point does not submit the plan, it simply saves your edits and you can move between sections in any order by expanding the relevant pane. Share: Whether you have completed the first draft or not, you may choose to share your plan with others. The ‘Share’ tab lets you set up visibility preferences and manage collaboration. Under visibility you may opt for: Public: Your plan will be viewable by anyone visiting the site under Learn&gt;Public Plans Organization: If you have logged in through an institutional ID, others logging in through the same institution can also view your plan Private: The plan can only be viewed by you and those you permit. The institutional administrator of the DMPtool account will also be able to view your plan. Typically this is a data librarian and their contact information will be at the top right of the webpage, under your institutional logo. Note: You will notice that the above options are not available to you as you step through this exercise. This is because your plan is a ‘test’ plan and so will not be posted to the website or managed in the same way. You can go back to your dashboard and unselect ‘test’ at any point if your test evolves into a real plan. For a private plan you can assign collaborators to three different roles: Co-owner: The collaborator can edit project details, change visibility, and add collaborators. They have the same privilidges as you. Editor: The collaborator can comment and make changes. Comments will appear in the tab indicated above. Read only: The collaborator can view and comment, but not make changes. The advantage of collaborating through the tool vs sending emails back and forth is that the tool reflects the most current version at all times, it increases efficiency and keeps all commentary together. Download: Here you can set your preferences for downloading your plan. The DMPTool does not submit your plan to your funding agency and so many researchers choose to download in text or docx formats so that they can make formatting edits to align with the rest of their proposal and meet page length requirements. You can do some of this formatting in advance using the options in this tab. Any plans that are shared publicly are shared in PDF format and include the project details coversheet by default. Below shows the base template for downloading your plan (with no content included to date). You can choose to omit unanswered quesitons. Finally, you can make changes to your plan at any point by returning to the dashboard. Here you can remove your plan from test status by unchecking the green box. You can also Edit, Share, Download, Copy or Remove your plan using the options in the drop down under ‘Actions’. "],["introduction-to-r.html", "5 Introduction to R 5.1 Introduction to R", " 5 Introduction to R 5.1 Introduction to R 5.1.1 Learning Objectives In this lesson we will: get oriented to the RStudio interface work with R in the console be introduced to built-in R functions learn to use the help pages 5.1.2 Introduction and Motivation There is a vibrant community out there that is collectively developing increasingly easy to use and powerful open source programming tools. The changing landscape of programming is making learning how to code easier than it ever has been. Incorporating programming into analysis workflows not only makes science more efficient, but also more computationally reproducible. In this course, we will use the programming language R, and the accompanying integrated development environment (IDE) RStudio. R is a great language to learn for data-oriented programming because it is widely adopted, user-friendly, and (most importantly) open source! So what is the difference between R and RStudio? Here is an analogy to start us off. If you were a chef, R is a knife. You have food to prepare, and the knife is one of the tools that you’ll use to accomplish your task. And if R were a knife, RStudio is the kitchen. RStudio provides a place to do your work! Other tools, communication, community, it makes your life as a chef easier. RStudio makes your life as a researcher easier by bringing together other tools you need to do your work efficiently - like a file browser, data viewer, help pages, terminal, community, support, the list goes on. So it’s not just the infrastructure (the user interface or IDE), although it is a great way to learn and interact with your variables, files, and interact directly with git. It’s also data science philosophy, R packages, community, and more. So although you can prepare food without a kitchen and we could learn R without RStudio, that’s not what we’re going to do. We are going to take advantage of the great RStudio support, and learn R and RStudio together. Something else to start us off is to mention that you are learning a new language here. It’s an ongoing process, it takes time, you’ll make mistakes, it can be frustrating, but it will be overwhelmingly awesome in the long run. We all speak at least one language; it’s a similar process, really. And no matter how fluent you are, you’ll always be learning, you’ll be trying things in new contexts, learning words that mean the same as others, etc, just like everybody else. And just like any form of communication, there will be miscommunications that can be frustrating, but hands down we are all better off because of it. While language is a familiar concept, programming languages are in a different context from spoken languages, but you will get to know this context with time. For example: you have a concept that there is a first meal of the day, and there is a name for that: in English it’s “breakfast”. So if you’re learning Spanish, you could expect there is a word for this concept of a first meal. (And you’d be right: ‘desayuno’). We will get you to expect that programming languages also have words (called functions in R) for concepts as well. You’ll soon expect that there is a way to order values numerically. Or alphabetically. Or search for patterns in text. Or calculate the median. Or reorganize columns to rows. Or subset exactly what you want. We will get you increase your expectations and learn to ask and find what you’re looking for. 5.1.2.1 Resources This lesson is a combination of excellent lessons by others. Huge thanks to Julie Lowndes for writing most of this content and letting us build on her material, which in turn was built on Jenny Bryan’s materials. I definitely recommend reading through the original lessons and using them as reference: Julie Lowndes’ Data Science Training for the Ocean Health Index R, RStudio, RMarkdown Programming in R Here are some other resources that we like for learning R: Learn R in the console with swirl The Introduction to R lesson in Data Carpentry’s R for data analysis course Jenny Bryan’s Stat 545 course materials Other resources: LaTeX Equation Formatting Base R Cheatsheet MATLAB/R Translation Cheat Sheet 5.1.3 R at the console Launch RStudio/R. Notice the default panes: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) FYI: you can change the default location of the panes, among many other things: Customizing RStudio. An important first question: where are we? If you’ve just opened RStudio for the first time, you’ll be in your Home directory. This is noted by the ~/ at the top of the console. You can see too that the Files pane in the lower right shows what is in the Home directory where you are. You can navigate around within that Files pane and explore, but note that you won’t change where you are: even as you click through you’ll still be Home: ~/. OK let’s go into the Console, where we interact with the live R process. We use R to calculate things for us, so let’s do some simple math. 3*4 ## [1] 12 The console is not a great place to run most code, however, since you can’t save what you ran. A better way is to create an R script, and write your code from there. Then you run your code from the script, and save it when you are done. Before we open a new R script, we are going to create a project to save it into. In this workshop, we are going to be using R projects to organize our work. An R project is tied to a directory (folder) on your local computer, and makes organizing your work and collaborating with others easier. Setup In the file menu, select “new project” In the wizard, select “new directory” then “new project” Name your project “training” and save it in a place that makes sense on your computer. Click “ok”. A new R window will launch Now, create an R script. Select file &gt; new file &gt; R script Now we will continue learning R code, while writing in our R script. You can assign the value of that mathematic operation to a variable, or object, in R. You do this using the assignment operator, &lt;-. Make an assignment and then inspect the object you just created. x &lt;- 3 * 4 x ## [1] 12 In my head I hear, e.g., “x gets 12”. All R statements where you create objects – “assignments” – have this form: objectName &lt;- value. I’ll write it in the console with a hash #, which is the way R comments so it won’t be evaluated. ## objectName &lt;- value ## This is also how you write notes in your code to explain what you are doing. Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a convention for demarcating words in names. # i_use_snake_case # other.people.use.periods # evenOthersUseCamelCase Make an assignment this_is_a_really_long_name &lt;- 2.5 To inspect this variable, instead of typing it, we can press the up arrow key and call your command history, with the most recent commands first. Let’s do that, and then delete the assignment: this_is_a_really_long_name ## [1] 2.5 Another way to inspect this variable is to begin typing this_…and RStudio will automagically have suggested completions for you that you can select by hitting the tab key, then press return. One more: science_rocks &lt;- &quot;yes it does!&quot; You can see that we can assign an object to be a word, not a number. In R, this is called a “string”, and R knows it’s a word and not a number because it has quotes \" \". You can work with strings in your data in R pretty easily, thanks to the stringr and tidytext packages. Strings and numbers lead us to an important concept in programming: that there are different “classes” or types of objects. An object is a variable, function, data structure, or method that you have written to your environment. You can see what objects you have loaded by looking in the “environment” pane in RStudio. The operations you can do with an object will depend on what type of object it is. This makes sense! Just like you wouldn’t do certain things with your car (like use it to eat soup), you won’t do certain operations with character objects (strings), for example. Try running the following line in your console: &quot;Hello world!&quot; * 3 What happened? Why? You may have noticed that when assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name: weight_kg &lt;- 55 # doesn&#39;t print anything (weight_kg &lt;- 55) # but putting parenthesis around the call prints the value of `weight_kg` ## [1] 55 weight_kg # and so does typing the name of the object ## [1] 55 Now that R has weight_kg in memory, we can do arithmetic with it. For instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg): 2.2 * weight_kg ## [1] 121 We can also change a variable’s value by assigning it a new one: weight_kg &lt;- 57.5 2.2 * weight_kg ## [1] 126.5 This means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a new variable, weight_lb: weight_lb &lt;- 2.2 * weight_kg and then change weight_kg to 100. weight_kg &lt;- 100 What do you think is the current content of the object weight_lb? 126.5 or 220? Why? You can also store more than one value in a single object. Storing a series of weights in a single object is a convenient way to perform the same operation on multiple values at the same time. One way to create such an object is the function c(), which stands for combine or concatenate. Here we will create a vector of weights in kilograms, and convert them to pounds, saving the weight in pounds as a new object. weight_kg &lt;- c(55, 25, 12) weight_kg ## [1] 55 25 12 weight_lb &lt;- weight_kg * 2.2 weight_lb ## [1] 121.0 55.0 26.4 5.1.3.1 Error messages are your friends Implicit contract with the computer/scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Pay attention to how you type. Remember that this is a language, not unsimilar to English! There are times you aren’t understood – it’s going to happen. There are different ways this can happen. Sometimes you’ll get an error. This is like someone saying ‘What?’ or ‘Pardon’? Error messages can also be more useful, like when they say ‘I didn’t understand this specific part of what you said, I was expecting something else’. That is a great type of error message. Error messages are your friend. Google them (copy-and-paste!) to figure out what they mean. And also know that there are errors that can creep in more subtly, without an error message right away, when you are giving information that is understood, but not in the way you meant. Like if I’m telling a story about tables and you’re picturing where you eat breakfast and I’m talking about data. This can leave me thinking I’ve gotten something across that the listener (or R) interpreted very differently. And as I continue telling my story you get more and more confused… So write clean code and check your work as you go to minimize these circumstances! 5.1.3.2 Logical operators and expressions A moment about logical operators and expressions. We can ask questions about the objects we just made. == means ‘is equal to’ != means ‘is not equal to’ &lt; means ` is less than’ &gt; means ` is greater than’ &lt;= means ` is less than or equal to’ &gt;= means ` is greater than or equal to’ weight_kg == 2 ## [1] FALSE FALSE FALSE weight_kg &gt;= 30 ## [1] TRUE FALSE FALSE weight_kg != 5 ## [1] TRUE TRUE TRUE Shortcuts You will make lots of assignments and the operator &lt;- is a pain to type. Don’t be lazy and use =, although it would work, because it will just sow confusion later. Instead, utilize RStudio’s keyboard shortcut: Alt + - (the minus sign). Notice that RStudio automagically surrounds &lt;- with spaces, which demonstrates a useful code formatting practice. Code is miserable to read on a good day. Give your eyes a break and use spaces. RStudio offers many handy keyboard shortcuts. Also, Alt+Shift+K brings up a keyboard shortcut reference card. 5.1.3.3 Clearing the environment Now look at the objects in your environment (workspace) – in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with a few different R commands: objects() ## [1] &quot;science_rocks&quot; &quot;this_is_a_really_long_name&quot; ## [3] &quot;weight_kg&quot; &quot;weight_lb&quot; ## [5] &quot;x&quot; ls() ## [1] &quot;science_rocks&quot; &quot;this_is_a_really_long_name&quot; ## [3] &quot;weight_kg&quot; &quot;weight_lb&quot; ## [5] &quot;x&quot; If you want to remove the object named weight_kg, you can do this: rm(weight_kg) To remove everything: rm(list = ls()) or click the broom in RStudio’s Environment pane. 5.1.4 R functions, help pages So far we’ve learned some of the basic syntax and concepts of R programming, and how to navigate RStudio, but we haven’t done any complicated or interesting programming processes yet. This is where functions come in! A function is a way to group a set of commands together to undertake a task in a reusable way. When a function is executed, it produces a return value. We often say that we are “calling” a function when it is executed. Functions can be user defined and saved to an object using the assignment operator, so you can write whatever functions you need, but R also has a mind-blowing collection of built-in functions ready to use. To start, we will be using some built in R functions. All functions are called using the same syntax: function name with parentheses around what the function needs in order to do what it was built to do. The pieces of information that the function needs to do its job are called arguments. So the syntax will look something like: result_value &lt;- function_name(argument1 = value1, argument2 = value2, ...). 5.1.4.1 A simple example To take a very simple example, let’s look at the mean() function. As you might expect, this is a function that will take the mean of a set of numbers. Very convenient! Let’s create our vector of weights again: weight_kg &lt;- c(55, 25, 12) and use the mean function to calculate the mean weight. mean(weight_kg) ## [1] 30.66667 5.1.4.2 Getting help What if you know the name of the function that you want to use, but don’t know exactly how to use it? Thankfully RStudio provides an easy way to access the help documentation for functions. To access the help page for mean, enter the following into your console: ?mean The help pane will show up in the lower right hand corner of your RStudio. The help page is broken down into sections: Description: An extended description of what the function does. Usage: The arguments of the function(s) and their default values. Arguments: An explanation of the data each argument is expecting. Details: Any important details to be aware of. Value: The data the function returns. See Also: Any related functions you might find useful. Examples: Some examples for how to use the function. 5.1.4.3 Your turn Exercise: Talk to your neighbor(s) and look up the help file for a function that you know or expect to exist. Here are some ideas: ?getwd(), ?plot(), min(), max(), ?log()). And there’s also help for when you only sort of remember the function name: double-questionmark: ??install Not all functions have (or require) arguments: date() ## [1] &quot;Wed Apr 20 15:26:05 2022&quot; 5.1.4.4 Use a function to read a file into R So far we have learned how to assign values to objects in R, and what a function is, but we haven’t quite put it all together yet with real data yet. To do this, we will introduce the function read.csv, which will be in the first lines of many of your future scripts. It does exactly what it says, it reads in a csv file to R. Since this is our first time using this function, first access the help page for read.csv. This has a lot of information in it, as this function has a lot of arguments, and the first one is especially important - we have to tell it what file to look for. Let’s get a file! 5.1.4.4.1 Download a file from the Arctic Data Center Follow these steps to get set up for the next exercise: Navigate to this dataset by Craig Tweedie that is published on the Arctic Data Center. Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X. Download the first csv file called “BGchem2008data.csv” by clicking the “download” button next to the file. Move this file from your Downloads folder into a place you can more easily find it. Eg: a folder called data in your previously-created directory arctic_training_files. Now we have to tell read.csv how to find the file. We do this using the file argument which you can see in usage section in the help page. In R, you can either use absolute paths (which will start with your home directory ~/) or paths relative to your current working directory. RStudio has some great autocomplete capabilities when using relative paths, so we will go that route. Assuming you have moved your file to a folder within training called data, and your working directory is your home directory (~/) your read.csv call will look like this: bg_chem &lt;- read.csv(&quot;Documents/training/data/BGchem2008data.csv&quot;) You should now have an object of the class data.frame in your environment called bg_chem. Check your environment pane to ensure this is true. Note that in the help page there are a whole bunch of arguments that we didn’t use in the call above. Some of the arguments in function calls are optional, and some are required. Optional arguments will be shown in the usage section with a name = value pair, with the default value shown. If you do not specify a name = value pair for that argument in your function call, the function will assume the default value (example: header = TRUE for read.csv). Required arguments will only show the name of the argument, without a value. Note that the only required argument for read.csv is file. You can always specify arguments in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want file = \"data/BGchem2008data.csv\", since file is the first argument. If we wanted to add another argument, say stringsAsFactors, we need to specify it explicitly using the name = value pair, since the second argument is header. For functions I call often, I use this resolve by position for the first argument or maybe the first two. After that, I always use name = value. Many R users (including myself) will override the default stringsAsFactors argument using the following call: bg_chem &lt;- read.csv(&quot;Documents/arctic_training_files/data/BGchem2008data.csv&quot;, stringsAsFactors = FALSE) 5.1.5 Using data.frames A data.frame is a two dimensional data structure in R that mimics spreadsheet behavior. It is a collection of rows and columns of data, where each column has a name and represents a variable, and each row represents a measurement of that variable. When we ran read.csv, the object bg_chem that we created is a data.frame. There are a a bunch of ways R and RStudio help you explore data frames. Here are a few, give them each a try: click on the word bg_chem in the environment pane click on the arrow next to bg_chem in the environment pane execute head(bg_chem) in the console execute View(bg_chem) in the console Usually we will want to run functions on individual columns in a data.frame. To call a specific column, we use the list subset operator $. Say you want to look at the first few rows of the Date column only. This would do the trick: head(bg_chem$Date) ## [1] &quot;2008-03-21&quot; &quot;2008-03-21&quot; &quot;2008-03-21&quot; &quot;2008-03-21&quot; &quot;2008-03-21&quot; ## [6] &quot;2008-03-22&quot; How about calculating the mean temperature of all the CTD samples? mean(bg_chem$CTD_Temperature) ## [1] -0.9646915 Or, if we want to save this to a variable to use later: mean_temp &lt;- mean(bg_chem$CTD_Temperature) You can also create basic plots using the list subset operator. plot(bg_chem$CTD_Depth, bg_chem$CTD_Temperature) There are many more advancted tools and functions in R that will enable you to make better plots using cleaner syntax, we will cover some of these later in the course. 5.1.5.1 Your Turn Exercise: Spend a few minutes exploring this dataset. Try out different functions on columns using the list subset operator and experiment with different plots. 5.1.6 R Packages R packages are the building blocks of computational reproducibility in R. Each package contains a set of related functions that enable you to more easily do a task or set of tasks in R. There are thousands of community-maintained packages out there for just about every imaginable use of R - including many that you have probably never thought of! To install a package, we use the syntax install.packages('packge_name'). A package only needs to be installed once, so this code can be run directly in the console if needed. Generally, you don’t want to save your install package calls in a script, because when you run the script it will re-install the package, which you only need to do once, or if you need to update the package. Use the chunk below to check to make sure you have all of the packages installed that you need for the course: packages &lt;- c(&quot;readr&quot;, &quot;dplyr&quot;, &quot;tidyr&quot;, &quot;googlesheets4&quot;, &quot;tidytext&quot;, &quot;wordcloud&quot;, &quot;reshape2&quot;, &quot;ggplot2&quot;, &quot;viridis&quot;, &quot;scales&quot;, &quot;leaflet&quot;, &quot;sf&quot;, &quot;ggmap&quot;, &quot;DT&quot;, &quot;rmarkdown&quot;, &quot;knitr&quot;) for (package in packages) { if (!(package %in% installed.packages())) { install.packages(package) } } rm(packages) #remove variable from workspace 5.1.6.1 I just entered a command and nothing is happening It may be because you didn’t complete a command: is there a little + in your console? R is saying that it is waiting for you to finish. In the example below, I need to close that parenthesis. &gt; x &lt;- seq(1, 10 + You can either just type the closing parentheses here and push return, or push the esc button twice. 5.1.6.2 R says my object is not found New users will frequently see errors that look like this: Error in mean(myobject) : object 'myobject' not found This means that you do not have an object called myobject saved in your environment. The common reasons for this are: typo: make sure your object name is spelled exactly like what shows up in the console. Remember R is case sensitive. not writing to a variable: note that the object is only saved in the environment if you use the assignment operator, eg: myobject &lt;- read.csv(...) not executing the line in your script: remember that writing a line of code in a script or RMarkdown document is not the same as writing in the console, you have to execute the line of code using command + enter or using one of the several ways in the RStudio graphical user interface. "],["introduction-to-rmarkdown.html", "6 Introduction to RMarkdown 6.1 Literate Analysis with RMarkdown", " 6 Introduction to RMarkdown 6.1 Literate Analysis with RMarkdown 6.1.1 Learning Objectives In this lesson we will: explore an example of RMarkdown as literate analysis learn markdown syntax write and run R code in RMarkdown build and knit an example document 6.1.2 Introduction and motivation The concept of literate analysis dates to a 1984 article by Donald Knuth. In this article, Knuth proposes a reversal of the programming paradigm. Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do. If our aim is to make scientific research more transparent, the appeal of this paradigm reversal is immediately apparent. All too often, computational methods are written in such a way as to be borderline incomprehensible - even to the person who originally wrote the code! The reason for this is obvious, computers interpret information very differently than people do. By switching to a literate analysis model, you help enable human understanding of what the computer is doing. As Knuth describes, in the literate analysis model, the author is an “essayist” who chooses variable names carefully, explains what they mean, and introduces concepts in the analysis in a way that facilitates understanding. RMarkdown is an excellent way to generate literate analysis, and a reproducible workflow. RMarkdown is a combination of two things - R, the programming language, and markdown, a set of text formatting directives. In R, the language assumes that you are writing R code, unless you specify that you are writing prose (using a comment, designated by #). The paradigm shift of literate analysis comes in the switch to RMarkdown, where instead of assuming you are writing code, Rmarkdown assumes that you are writing prose unless you specify that you are writing code. This, along with the formatting provided by markdown, encourages the “essayist” to write understandable prose to accompany the code that explains to the human-beings reading the document what the author told the computer to do. This is in contrast to writing just R code, where the author telling to the computer what to do with maybe a smattering of terse comments explaining the code to a reader. Before we dive in deeper, let’s look at an example of what literate analysis with RMarkdown can look like using a real example. Here is an example of a real analysis workflow written using RMarkdown. There are a few things to notice about this document, which assembles a set of similar data sources on salmon brood tables with different formatting into a single data source. It introduces the data sources using in-line images, links, interactive tables, and interactive maps. An example of data formatting from one source using R is shown. The document executes a set of formatting scripts in a directory to generate a single merged file. Some simple quality checks are performed (and their output shown) on the merged data. Simple analysis and plots are shown. In addition to achieving literate analysis, this document also represents a reproducible analysis. Because the entire merging and quality control of the data is done using the R code in the RMarkdown, if a new data source and formatting script are added, the document can be run all at once with a single click to re-generate the quality control, plots, and analysis of the updated data. RMarkdown is an amazing tool to use for collaborative research, so we will spend some time learning it well now, and use it through the rest of the course. Setup Open a new RMarkdown file using the following prompts: File -&gt; New File -&gt; RMarkdown A popup window will appear. You can just click the OK button here, or give your file a new title if you wish. Leave the output format as HTML. 6.1.3 Basic RMarkdown syntax The first thing to notice is that by opening a file, we are seeing the 4th pane of the RStudio console, which is essentially a text editor. Let’s have a look at this file — it’s not blank; there is some initial text already provided for you. Notice a few things about it: There are white and grey sections. R code is in grey sections, and other text is in white. Let’s go ahead and “Knit” the document by clicking the blue yarn at the top of the RMarkdown file. When you first click this button, RStudio will prompt you to save this file. Save it in the top level of your home directory on the server, and name it something that you will remember (like rmarkdown-intro.Rmd). What do you notice between the two? First, the knit process produced a second file (an HTML file) that popped up in a second window. You’ll also see this file in your directory with the same name as your Rmd, but with the html extension. In it’s simplest format, RMarkdown files come in pairs - the RMarkdown file, and its rendered version. In this case, we are knitting, or rendering, the file into HTML. You can also knit to PDF or Word files. Notice how the grey R code chunks are surrounded by 3 backticks and {r LABEL}. These are evaluated and return the output text in the case of summary(cars) and the output plot in the case of plot(pressure). The label next to the letter r in the code chunk syntax is a chunk label - this can help you navigate your RMarkdown document using the dropdown menu at the bottom of the editor pane. Notice how the code plot(pressure) is not shown in the HTML output because of the R code chunk option echo = FALSE. RMarkdown has lots of chunk options, including ones that allow for code to be run but not shown (echo = FALSE), code to be shown but not run (eval = FALSE), code to be run, but results not shown (results = 'hide'), or any combination of those. Before we get too deeply into the R side, let’s talk about Markdown. Markdown is a formatting language for plain text, and there are only around 15 rules to know. Notice the syntax in the document we just knitted: headers get rendered at multiple levels: #, ## bold: **word** There are some good cheatsheets to get you started, and here is one built into RStudio: Go to Help &gt; Markdown Quick Reference . Important: note that the hash symbol # is used differently in Markdown and in R: in R, a hash indicates a comment that will not be evaluated. You can use as many as you want: # is equivalent to ######. It’s just a matter of style. in Markdown, a hash indicates a level of a header. And the number you use matters: # is a “level one header”, meaning the biggest font and the top of the hierarchy. ### is a level three header, and will show up nested below the # and ## headers. Challenge In Markdown, Write some italic text, make a numbered list, and add a few sub-headers. Use the Markdown Quick Reference (in the menu bar: Help &gt; Markdown Quick Reference). Re-knit your html file and observe your edits. 6.1.3.1 New Rmarkdown editing tools The new version of RStudio (1.4) has a ‘what you see is what you get’ (wysiwyg) editor, which can be a nice way to write markdown without remembering all of the markdown rules. Since there aren’t many rules for markdown, I recommend just learning them - especially since markdown is used in many, many other contexts besides RMarkdown (formatting GitHub comments, for example). To access the editor, click the compass icon in the upper right hand corner of your editor pane. You’ll notice that your document is now formatted as you type, and you can change elements of the formatting using the row of icons in the top of the editor pane. Although I don’t really recommend doing all of your markdown composition in this format, there are two features to this editor that I find immensely helpful - adding citations, and adding tables. From the insert drop down, select “citation.” In the window that appears, there are several options in the left hand panel for the source of your citation. If you had a citation manager, such as Zotero, installed, this would be included in that list. For now, select “From DOI”, and in the search bar enter a DOI of your choice (eg: 10.1038/s41467-020-17726-z), then select “Insert.” After selecting insert, a couple of things happen. First, the citation reference is inserted into your markdown text as [@oke2020]. Second, a file called references.bib containing the BibTex format of the citation is created. Third, that file is added to the YAML header of your RMarkdown document (bibliography: references.bib). Adding another citation will automatically update your references.bib file. So easy! The second task that the markdown editor is convenient for is generating tables. Markdown tables are a bit finicky and annoying to type, and there are a number of formatting options that are difficult to remember if you don’t use them often. In the top icon bar, the “table” drop down gives several options for inserting, editing, and formatting tables. Experiment with this menu to insert a small table. 6.1.4 Code chunks Next, do what I do every time I open a new RMarkdown: delete everything below the “setup chunk” (line 10). The setup chunk is the one that looks like this: knitr::opts_chunk$set(echo = TRUE) This is a very useful chunk that will set the default R chunk options for your entire document. I like keeping it in my document so that I can easily modify default chunk options based on the audience for my RMarkdown. For example, if I know my document is going to be a report for a non-technical audience, I might set echo = FALSE in my setup chunk, that way all of the text, plots, and tables appear in the knitted document. The code, on the other hand, is still run, but doesn’t display in the final document. Now let’s practice with some R chunks. You can Create a new chunk in your RMarkdown in one of these ways: click “Insert &gt; R” at the top of the editor pane type by hand ```{r} ``` use the keyboard shortcut Command + Option + i (for windows, Ctrl + Alt + i) Now, let’s write some R code. x &lt;- 4*3 x Hitting return does not execute this command; remember, it’s just a text file. To execute it, we need to get what we typed in the the R chunk (the grey R code) down into the console. How do we do it? There are several ways (let’s do each of them): copy-paste this line into the console (generally not recommended as a primary method) select the line (or simply put the cursor there), and click ‘Run’. This is available from the bar above the file (green arrow) the menu bar: Code &gt; Run Selected Line(s) keyboard shortcut: command-return click the green arrow at the right of the code chunk Challenge Add a few more commands to your code chunk. Execute them by trying the three ways above. Question: What is the difference between running code using the green arrow in the chunk and the command-return keyboard shortcut? 6.1.5 Literate analysis practice Now that we have gone over the basics, let’s go a little deeper by building a simple, small RMarkdown document that represents a literate analysis using real data. Setup Navigate to the following dataset: https://doi.org/10.18739/A25T3FZ8X Download the file “BGchem2008data.csv” Click the “Upload” button in your RStudio server file browser. In the dialog box, make sure the destination directory is the data directory in your R project, click “choose file,” and locate the BGchem2008data.csv file. Press “ok” to upload the file. 6.1.5.1 Developing code in RMarkdown Experienced R users who have never used RMarkdown often struggle a bit in the transition to developing analysis in RMarkdown - which makes sense! It is switching the code paradigm to a new way of thinking. Rather than starting an R chunk and putting all of your code in that single chunk, here I describe what I think is a better way. Open a document and block out the high-level sections you know you’ll need to include using top level headers. Add bullet points for some high level pseudo-code steps you know you’ll need to take. Start filling in under each bullet point the code that accomplishes each step. As you write your code, transform your bullet points into prose, and add new bullet points or sections as needed. For this mini-analysis, we will just have the following sections and code steps: Introduction read in data Analysis calculate summary statistics calculate mean Redfield ratio plot Redfield ratio Challenge Create the ‘outline’ of your document with the information above. Top level bullet points should be top level sections. The second level points should be a list within each section. Next, write a sentence saying where your dataset came from, including a hyperlink, in the introduction section. Hint: Navigate to Help &gt; Markdown Quick Reference to lookup the hyperlink syntax. 6.1.5.2 Read in the data Now that we have outlined our document, we can start writing code! To read the data into our environment, we will use a function from the readr package. To use a package in our analysis, we need to load it into our environment using library(package_name). Even though we have installed it, we haven’t yet told our R session to access it. Because there are so many packages (many with conflicting namespaces) R cannot automatically load every single package you have installed. Instead, you load only the ones you need for a particular analysis. Loading the package is a key part of the reproducible aspect of our Rmarkdown, so we will include it as an R chunk. It is generally good practice to include all of your library calls in a single, dedicated R chunk near the top of your document. This lets collaborators know what packages they might need to install before they start running your code. You should have already installed readr as part of the setup for this course, so add a new R chunk below your setup chunk that calls the readr library, and run it. It should look like this: library(readr) Now, below the introduction that you wrote, add a chunk that uses the read_csv function to read in your data file. About RMarkdown paths In computing, a path specifies the unique location of a file on the filesystem. A path can come in one of two forms: absolute or relative. Absolute paths start at the very top of your file system, and work their way down the directory tree to the file. Relative paths start at an arbitrary point in the file system. In R, this point is set by your working directory. RMarkdown has a special way of handling relative paths that can be very handy. When working in an RMarkdown document, R will set all paths relative to the location of the RMarkdown file. This way, you don’t have to worry about setting a working directory, or changing your colleagues absolute path structure with the correct user name, etc. If your RMarkdown is stored near where the data it analyses are stored (good practice, generally), setting paths becomes much easier! If you saved your “BGchem2008data.csv” data file in the same location as your Rmd, you can just write the following to read it in. The help page (?read_csv, in the console) for this function tells you that the first argument should be a pointer to the file. Rstudio has some nice helpers to help you navigate paths. If you open quotes and press ‘tab’ with your cursor between the quotes, a popup menu will appear showing you some options. bg_chem &lt;- read_csv(&quot;../data/BGchem2008data.csv&quot;) Parsed with column specification: cols( Date = col_date(format = &quot;&quot;), Time = col_datetime(format = &quot;&quot;), Station = col_character(), Latitude = col_double(), Longitude = col_double(), Target_Depth = col_double(), CTD_Depth = col_double(), CTD_Salinity = col_double(), CTD_Temperature = col_double(), Bottle_Salinity = col_double(), d18O = col_double(), Ba = col_double(), Si = col_double(), NO3 = col_double(), NO2 = col_double(), NH4 = col_double(), P = col_double(), TA = col_double(), O2 = col_double() ) Warning messages: 1: In get_engine(options$engine) : Unknown language engine &#39;markdown&#39; (must be registered via knit_engines$set()). 2: Problem with `mutate()` input `Lower`. ℹ NAs introduced by coercion ℹ Input `Lower` is `as.integer(Lower)`. 3: In mask$eval_all_mutate(dots[[i]]) : NAs introduced by coercion If you run this line in your RMarkdown document, you should see the bg_chem object populate in your environment pane. It also spits out lots of text explaining what types the function parsed each column into. This text is important, and should be examined, but we might not want it in our final document. Challenge Use one of two methods to figure out how to suppress warning and message text in your chunk output: The gear icon in the chunk, next to the play button The RMarkdown reference guide (also under Help &gt; Cheatsheets) Aside Why not use read.csv from base R? We chose to show read_csv from the readr package for a few reasons. One is to introduce the concept of packages and showing how to load them, but read_csv has several advantages over read.csv. more reasonable function defaults (no stringsAsFactors!) smarter column type parsing, especially for dates it is much faster than read.csv, which is helpful for large files 6.1.5.3 Calculate Summary Statistics As our “analysis” we are going to calculate some very simple summary statistics and generate a single plot. In this dataset of oceanographic water samples, we will be examining the ratio of nitrogen to phosphate to see how closely the data match the Redfield ratio, which is the consistent 16:1 ratio of nitrogen to phosphorous atoms found in marine phytoplankton. Under the appropriate bullet point in your analysis section, create a new R chunk, and use it to calculate the mean nitrate (NO3), nitrite (NO2), ammonium (NH4), and phosphorous (P) measured. Save these mean values as new variables with easily understandable names, and write a (brief) description of your operation using markdown above the chunk. nitrate &lt;- mean(bg_chem$NO3) nitrite &lt;- mean(bg_chem$NO2) amm &lt;- mean(bg_chem$NH4) phos &lt;- mean(bg_chem$P) In another chunk, use those variables to calculate the nitrogen:phosphate ratio (Redfield ratio). ratio &lt;- (nitrate + nitrite + amm)/phos You can access this variable in your Markdown text by using R in-line in your text. The syntax to call R in-line (as opposed to as a chunk) is a single backtick `, the letter “r”, whatever your simple R command is - here we will use round(ratio) to print the calculated ratio, and a closing backtick `. So: ` 6 `. This allows us to access the value stored in this variable in our explanatory text without resorting to the evaluate-copy-paste method so commonly used for this type of task. The text as it looks in your RMrakdown will look like this: The Redfield ratio for this dataset is approximately `r round(ratio)`. And the rendered text like this: The Redfield ratio for this dataset is approximately 6. Finally, create a simple plot using base R that plots the ratio of the individual measurements, as opposed to looking at mean ratio. plot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4) Challenge Decide whether or not you want the plotting code above to show up in your knitted document along with the plot, and implement your decision as a chunk option. “Knit” your RMarkdown document (by pressing the Knit button) to observe the results. Aside How do I decide when to make a new chunk? Like many of life’s great questions, there is no clear cut answer. My preference is to have one chunk per functional unit of analysis. This functional unit could be 50 lines of code or it could be 1 line, but typically it only does one “thing.” This could be reading in data, making a plot, or defining a function. It could also mean calculating a series of related summary statistics (as above). Ultimately the choice is one related to personal preference and style, but generally you should ensure that code is divided up such that it is easily explainable in a literate analysis as the code is run. 6.1.6 RMarkdown and environments Let’s walk through an exercise with the document you built together to demonstrate how RMarkdown handles environments. We will be deliberately inducing some errors here for demonstration purposes. First, follow these steps: Restart your R session (Session &gt; Restart R) Run the last chunk in your Rmarkdown by pressing the play button on the chunk Perhaps not surprisingly, we get an error: Error in plot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4) : object &#39;bg_chem&#39; not found This is because we have not run the chunk of code that reads in the bg_chem data. The R part of Rmarkdown works just like a regular R script. You have to execute the code, and the order that you run it in matters. It is relatively easy to get mixed up in a large RMarkdown document - running chunks out of order, or forgetting to run chunks. To resolve this, follow the next step: Select from the “Run” menu (top right of Rmarkdown editor) “Restart R and run all chunks” Observe the bg_chem variable in your environment. This is one of my favorite ways to reset and re-run my code when things seem to have gone sideways. This is great practice to do periodically since it helps ensure you are writing code that actually runs. For the next demonstration: Restart your R session (Session &gt; Restart R) Press Knit to run all of the code in your document Observe the state of your environment pane Assuming your document knitted and produced an html page, your code ran. Yet the environment pane is empty. What happened? The Knit button is rather special - it doesn’t just run all of the code in your document. It actually spins up a fresh R environment separate from the one you have been working in, runs all of the code in your document, generates the output, and then closes the environment. This is one of the best ways RMarkdown helps ensure you have built a reproducible workflow. If, while you were developing your code, you ran a line in the console as opposed to adding it to your RMarkdown document, the code you develop while working actively in your environment will still work. However, when you knit your document, the environment RStudio spins up doesn’t know anything about that working environment you were in. Thus, your code may error because it doesn’t have that extra piece of information. Commonly, library calls are the source of this kind of frustration when the author runs it in the console, but forgets to add it to the script. To further clarify the point on environments, perform the following steps: Select from the “Run” menu (top right of Rmarkdown editor) “Run All” Observe all of the variables in your environment. Aside What about all my R scripts? Some pieces of R code are better suited for R scripts than RMarkdown. A function you wrote yourself that you use in many different analyses is probably better to define in an R script than repeated across many RMarkdown documents. Some analyses have mundane or repetitive tasks that don’t need to be explained very much. For example, in the document shown in the beginning of this lesson, 15 different excel files needed to be reformatted in slightly different, mundane ways, like renaming columns and removing header text. Instead of including these tasks in the primary markdown, I instead chose to write one R script per file and stored them all in a directory. I took the contents of one script and included it in my literate analysis, using it as an example to explain what the scripts did, and then used the source function to run them all from within my RMarkdown. So, just because you know RMarkdown now, doesn’t mean you won’t be using R scripts anymore. Both .R and .Rmd have their roles to play in analysis. With practice, it will become more clear what works well in RMarkdown, and what belongs in a regular R script. 6.1.7 Go Further Create an RMarkdown document with some of your own data. If you don’t have a good dataset handy, use the example dataset here: Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X. Your document might contain the following sections: Introduction to your dataset Include an external link Simple analysis Presentation of a result A table An in-line R command 6.1.8 Resources RMarkdown Reference Guide RMarkdown Home Page RMarkdown Cheat Sheet 6.1.9 Troubleshooting 6.1.9.1 My RMarkdown won’t knit to PDF If you get an error when trying to knit to PDF that says your computer doesn’t have a LaTeX installation, one of two things is likely happening: Your computer doesn’t have LaTeX installed You have an installation of LaTeX but RStudio cannot find it (it is not on the path) If you already use LaTeX (like to write papers), you fall in the second category. Solving this requires directing RStudio to your installation - and isn’t covered here. If you fall in the first category - you are sure you don’t have LaTeX installed - can use the R package tinytex to easily get an installation recognized by RStudio, as long as you have administrative rights to your computer. To install tinytex run: install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() If you get an error that looks like destination /usr/local/bin not writable, you need to give yourself permission to write to this directory (again, only possible if you have administrative rights). To do this, run this command in the terminal: sudo chown -R `whoami`:admin /usr/local/bin and then try the above install instructions again. More information about tinytex can be found here "],["collaboration-authorship-and-data-policies.html", "7 Collaboration, authorship and data policies 7.1 Developing a Code of Conduct 7.2 Authorship and Credit Policies 7.3 Data Sharing and Reuse Policies 7.4 Research Data Publishing Ethics 7.5 Extra Reading", " 7 Collaboration, authorship and data policies 7.1 Developing a Code of Conduct Whether you are joining a lab group or establishing a new collaboration, articulating a set of shared agreements about how people in the group will treat each other will help create the conditions for successful collaboration. If agreements or a code of conduct do not yet exist, invite a conversation among all members to create them. Co-creation of a code of conduct will foster collaboration and engagement as a process in and of itself, and is important to ensure all voices heard such that your code of conduct represents the perspectives of your community. If a code of conduct already exists, and your community will be a long-acting collaboration, you might consider revising the code of conduct. Having your group ‘sign off’ on the code of conduct, whether revised or not, supports adoption of the principles. When creating a code of conduct, consider both the behaviors you want to encourage and those that will not be tolerated. For example, the Openscapes code of conduct includes Be respectful, honest, inclusive, accommodating, appreciative, and open to learning from everyone else. Do not attack, demean, disrupt, harass, or threaten others or encourage such behavior. Below are other example codes of conduct: NCEAS Code of Conduct Carpentries Code of Conduct Arctic Data Center Code of Conduct Mozilla Community Participation Guidelines Ecological Society of America Code of Conduct 7.2 Authorship and Credit Policies Navigating issues of intellectual property and credit can be a challenge, particularly for early career researchers. Open communication is critical to avoiding misunderstandings and conflicts. Talk to your coauthors and collaborators about authorship, credit, and data sharing early and often. This is particularly important when working with new collaborators and across lab groups or disciplines which may have divergent views on authorship and data sharing. If you feel uncomfortable talking about issues surrounding credit or intellectual property, seek the advice or assistance of a mentor to support you in having these important conversations. The “Publication” section of the Ecological Society of America’s Code of Ethics is a useful starting point for discussions about co-authorship, as are the International Committee of Medical Journal Editors guidelines for authorship and contribution. You should also check guidelines published by the journal(s) to which you anticipate submitting your work. For collaborative research projects, develop an authorship agreement for your group early in the project and refer to it for each product. This example authorship agreement from the Arctic Data Center provides a useful template. It builds from information contained within Weltzin et al (2006) and provides a rubric for inclusion of individuals as authors. Your collaborative team may not choose to adopt the agreement in the current form, however it will prompt thought and discussion in advance of developing a consensus. Some key questions to consider as you are working with your team to develop the agreement: What roles do we anticipate contributors will play? e.g., the NISO Contributor Roles Taxonomy (CRediT) identifies 14 distinct roles: Conceptualization Data curation Formal Analysis Funding acquisition Investigation Methodology Project administration Resources Software Supervision Validation Visualization Writing – original draft Writing – review &amp; editing What are our criteria for authorship? (See the ICMJE guidelines for potential criteria) Will we extend the opportunity for authorship to all group members on every paper or product? Do we want to have an opt in or opt out policy? (In an opt out policy, all group members are considered authors from the outset and must request removal from the paper if they don’t want think they meet the criteria for authorship) Who has the authority to make decisions about authorship? Lead author? PI? Group? How will we decide authorship order? In what other ways will we acknowledge contributions and extend credit to collaborators? How will we resolve conflicts if they arise? 7.3 Data Sharing and Reuse Policies As with authorship agreements, it is valuable to establish a shared agreement around handling of data when embarking on collaborative projects. Data collected as part of a funded research activity will typically have been managed as part of the Data Management Plan (DMP) associated with that project. However, collaborative research brings together data from across research projects with different data management plans and can include publicly accessible data from repositories where no management plan is available. For these reasons, a discussion and agreement around the handling of data brought into and resulting from the collaboration is warranted and management of this new data may benefit from going through a data management planning process. Below we discuss example data agreements. The example data policy template provided by the Arctic Data Center addresses three categories of data. Individual data not in the public domain Individual data with public access Derived data resulting from the project For the first category, the agreement considers conditions under which those data may be used and permissions associated with use. It also addresses access and sharing. In the case of individual, publicly accessible data, the agreement stipulates that the team will abide by the attribution and usage policies that the data were published under, noting how those requirements we met. In the case of derived data, the agreement reads similar to a DMP with consideration of making the data public; management, documentation and archiving; pre-publication sharing; and public sharing and attribution. As research data objects receive a persistent identifier (PID), often a DOI, there are citable objects and consideration should be given to authorship of data, as with articles. The following example lab policy from the Wolkovich Lab combines data management practices with authorship guidelines and data sharing agreements. It provides a lot of detail about how this lab approaches data use, attribution and authorship. For example: Section 6: Co-authorship &amp; data If you agree to take on existing data you cannot offer co-authorship for use of the data unless four criteria are met: The co-author agrees to (and does) make substantial intellectual contribution to the work, which includes the reading and editing of all manuscripts on which you are a co-author through the submission-for-publication stage. This includes helping with interpretation of the data, system, study questions. Agreement of co-authorship is made at the start of the project. Agreement is approved of by Lizzie. All data-sharers are given an equal opportunity at authorship. It is not allowed to offer or give authorship to one data-sharer unless all other data-sharers are offered an equal opportunity at authorship—this includes data that are publicly-available, meaning if you offer authorship to one data-sharer and were planning to use publicly-available data you must reach out to the owner of the publicly-available data and strongly offer equivalent authorship as offered to the other data-sharer. As an example, if five people share data freely with you for a meta-analysis and and a sixth wants authorship you either must strongly offer equivalent authorship to all five or deny authorship to the sixth person. Note that the above requirements must also be met in this situation. If one or more datasets are more central or critical to a paper to warrant selective authorship this must be discussed and approved by Lizzie (and has not, to date, occurred within the lab). 7.3.0.1 Policy Preview This policy is communicated with all incoming lab members, from undergraduate to postdocs and visiting scholars, and is shared here with permission from Dr Elizabeth Wolkovich. 7.3.1 Community Principles: CARE and FAIR The CARE and FAIR Principles were introduced previously in the context of introducing the Arctic Data Center and our data submission and documentation process. In this section we will dive a little deeper. To recap, the Arctic Data Center is an openly-accessible data repository and the data published through the repository is open for anyone to reuse, subject to conditions of the license (at the Arctic Data Center, data is released under one of two licenses: CC-0 Public Domain and CC-By Attribution 4.0). In facilitating use of data resources, the data stewardship community have converged on principles surrounding best practices for open data management One set of these principles is the FAIR principles. FAIR stands for Findable, Accessible, Interoperable, and Reproducible. The “Fostering FAIR Data Practices in Europe” project found that it is more monetarily and timely expensive when FAIR principles are not used, and it was estimated that 10.2 billion dollars per years are spent through “storage and license costs to more qualitative costs related to the time spent by researchers on creation, collection and management of data, and the risks of research duplication.” FAIR principles and open science are overlapping concepts, but are distinctive concepts. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. Another set of community developed principles surrounding open data are the CARE Principles. The CARE principles for Indigenous Data Governance complement the more data-centric approach of the FAIR principles, introducing social responsibility to open data management practices. The CARE Principles stand for: Collective Benefit - Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data Authority to Control - Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data. Responsibility - Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples. Ethics - Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem. The CARE principles align with the FAIR principles by outlining guidelines for publishing data that is findable, accessible, interoperable, and reproducible while at the same time, accounts for Indigenous’ Peoples rights and interests. Initially designed to support Indigenous data sovereignty, CARE principles are now being adopted across domains, and many researchers argue they are relevant for both Indigenous Knowledge and data, as well as data from all disciplines (Carroll et al., 2021). These principles introduce a “game changing perspective” that encourages transparency in data ethics, and encourages data reuse that is purposeful and intentional that aligns with human well-being aligns with human well-being (Carroll et al., 2021). 7.4 Research Data Publishing Ethics For over 20 years, the Committee on Publication Ethics (COPE) has provided trusted guidance on ethical practices for scholarly publishing. The COPE guidelines have been broadly adopted by academic publishers across disciplines, and represent a common approach to identify, classify, and adjudicate potential breaches of ethics in publication such as authorship conflicts, peer review manipulation, and falsified findings, among many other areas. Despite these guidelines, there has been a lack of ethics standards, guidelines, or recommendations for data publications, even while some groups have begun to evaluate and act upon reported issues in data publication ethics. Data retractions To address this gap, the Force 11 Working Group on Research Data Publishing Ethics was formed as a collaboration among research data professionals and the Committee on Publication Ethics (COPE) “to develop industry-leading guidance and recommended best practices to support repositories, journal publishers, and institutions in handling the ethical responsibilities associated with publishing research data.” The group released the “Joint FORCE11 &amp; COPE Research Data Publishing Ethics Working Group Recommendations” (Puebla, Lowenberg, and WG 2021), which outlines recommendations for four categories of potential data ethics issues: Force11/COPE Authorship and Contribution Conflicts Authorship omissions Authorship ordering changes / conflicts Institutional investigation of author finds misconduct Legal/regulatory restrictions Copyright violation Insufficient rights for deposit Breaches of national privacy laws (GPDR, CCPA) Breaches of biosafety and biosecurity protocols Breaches of contract law governing data redistribution Risks of publication or release Risks to human subjects Lack of consent Breaches of himan rights Release of personally identifiable information (PII) Risks to species, ecosystems, historical sites Locations of endangered species or historical sites Risks to communities or societies Data harvested for profit or surveillance Breaches of data sovereignty Rigor of published data Unintentional errors in collection, calculation, display Un-interpretable data due to lack of adequate documentation Errors of of study design and inference Data manipulation or fabrication Guidelines cover what actions need to be taken, depending on whether the data are already published or not, as well as who should be involved in decisions, who should be notified of actions, and when the public should be notified. The group has also published templates for use by publishers and repositories to announce the extent to which they plan to conform to the data ethics guidelines. Discussion: Data publishing policies At the Arctic Data Center, we need to develop policies and procedures governing how we react to potential breaches of data publication ethics. In this exercise, break into groups to provide advice on how the Arctic Data Center should respond to reports of data ethics issues, and whether we should adopt the Joint FORCE11 &amp; COPE Research Data Publishing Ethics Working Group Policy Templates for repositories. In your discussion, consider: Should the repository adopt the repository policy templates from Force11? Who should be involved in evaluation of the merits of ethical cases reported to ADC? Who should be involved in deciding the actions to take? What are the range of responses that the repository should consider for ethical breaches? Who should be notified when a determination has been made that a breach has occurred? You might consider a hypothetical scenario such as the following in considering your response. The data coordinator at the Arctic Data Center receives an email in 2022 from a prior postdoctoral fellow who was employed as part of an NSF-funded project on microbial diversity in Alaskan tundra ecosystems. The email states that a dataset from 2014 in the Arctic Data Center was published with the project PI as author, but omits two people, the postdoc and an undergraduate student, as co-authors on the dataset. The PI retired in 2019, and the postdoc asks that they be added to the author list of the dataset to correct the historical record and provide credit. 7.5 Extra Reading Cheruvelil, K. S., Soranno, P. A., Weathers, K. C., Hanson, P. C., Goring, S. J., Filstrup, C. T., &amp; Read, E. K. (2014). Creating and maintaining high-performing collaborative research teams: The importance of diversity and interpersonal skills. Frontiers in Ecology and the Environment, 12(1), 31-38. DOI: 10.1890/130001 Carroll, S. R., Garba, I., Figueroa-Rodríguez, O. L., Holbrook, J., Lovett, R., Materechera, S., … Hudson, M. (2020). The CARE Principles for Indigenous Data Governance. Data Science Journal, 19(1), 43. DOI: http://doi.org/10.5334/dsj-2020-043 "],["data-ethics.html", "8 Data Ethics 8.1 Open Data Ethics 8.2 Discussion questions: 8.3 Data Ethics Resources", " 8 Data Ethics 8.1 Open Data Ethics Developed in collaboration with the Exchange for Local Observations and Knowledge of the Arctic (ELOKA) and Navigating the New Arctic Community Office (NNA-CO). 8.1.1 Learning Objectives In this lesson, we will: Discuss Indigenous Data Sovereignty in the context of open science Discuss FAIR and CARE Principles Examine case studies of relationships between Arctic Indigenous communities and natural science researchers 8.1.2 Ethical Data Considerations in Your Research As the primary repository for the Arctic program of the National Science Foundation, the Arctic Data Center accepts Arctic data from all disciplines. Recently, a new submission feature was released which asks researchers to describe the ethical considerations that are apparent in their research. This question is asked to all researchers, regardless of disciplines. 8.1.2.1 Discussion Questions Before diving into the world of ethics, what ethical concerns do you consider in your research? Please fill out this form to briefly describe any ethical concerns apparent in the research you conduct: https://forms.gle/VcogEPwcgVfnEj9f7 8.1.3 FAIR and Open Science To recap, the Arctic Data Center is an openly-accessible data repository and the data published through the repository is open for anyone to reuse, subject to conditions of the license (at the Arctic Data Center, data is released under one of two licenses: CC-0 Public Domain and CC-By Attribution 4.0). In facilitating use of data resources, the data stewardship community have converged on principles surrounding best practices for open data management One set of these principles is the FAIR principles. FAIR stands for Findable, Accessible, Interoperable, and Reproducible. The FAIR (Findable, Accessible, Interoperable, Reproducible) principles for data management are widely known and broadly endorsed. FAIR Principles and open science are not synonymous, however they often go hand in hand. However, data can be open and not FAIR, and vice verse. The “Fostering FAIR Data Practices in Europe” project found that it is more monetarily and timely expensive when FAIR principles are not used, and it was estimated that 10.2 billion dollars per years are spent through “storage and license costs to more qualitative costs related to the time spent by researchers on creation, collection and management of data, and the risks of research duplication.” FAIR principles and open science are overlapping concepts, but are distinctive concepts. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. The FAIR principles place emphasis on machine readability, “distinct from peer initiatives that focus on the human scholar” (Wilkinson et al 2016) and as such, do not fully engage with sensitive data considerations and with Indigenous rights and interests (Research Data Alliance International Indigenous Data Sovereignty Interest Group, 2019). Research has historically perpetuated colonialism and represented extractive practices, meaning that the research results were not mutually beneficial. These issues also related to how data was owned, shared, and used. To address issues like these, the Global Indigenous Data Alliance (GIDA) introduced CARE Principles for Indigenous Data Governance to support Indigenous data sovereignty. CARE Principles for Indigenous Data Governance stand for Collective Benefit, Authority to Control, Responsibility, Ethics. The CARE principles for Indigenous Data Governance complement the more data-centric approach of the FAIR principles, introducing social responsibility to open data management practices. These principles ask researchers to put human well-being at the forefront of open-science and data sharing (Carroll et al., 2021; Research Data Alliance International Indigenous Data Sovereignty Interest Group, September 2019). Indigenous data sovereignty and considerations related to working with Indigenous communities are particularly relevant to the Arctic. The CARE Principles stand for: Collective Benefit - Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data Authority to Control - Indigenous Peoples’ rights and interests in Indigenous data must be recognized and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data. Responsibility - Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples. Ethics - Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem. To many, the FAIR and CARE principles are viewed by many as complementary: CARE aligns with FAIR by outlining guidelines for publishing data that contributes to open-science and at the same time, accounts for Indigenous’ Peoples rights and interests. Sharing sensitive data introduces unique ethical considerations, and FAIR and CARE principles speak to this by recommending sharing anonymized metadata to encourage discover ability and reduce duplicate research efforts, following consent of rights holders (Puebla &amp; Lowenberg, 2021). While initially designed to support Indigenous data sovereignty, CARE principles are being adopted more broadly and researchers argue they are relevant across all disciplines (Carroll et al., 2021). As such, these principles introduce a “game changing perspective” for all researchers that encourages transparency in data ethics, and encourages data reuse that is both purposeful and intentional and that aligns with human well-being (Carroll et al., 2021). Hence, to enable the research community to articulate and document the degree of data sensitivity, and ethical research practices, the Arctic Data Center has introduced new submission requirements. 8.1.4 Discussion: Case Studies from “Challenges in Community-Research Relationships: Learning from Natural Science in Nunavut” (Gearheard &amp; Shirley, 2006) https://journalhosting.ucalgary.ca/index.php/arctic/article/view/63328/47265 In 2006, Gearheard &amp; Shirley (2006) published “Challenges in Community-Research Relationship: Learning from Natural Science in Nunavut” (see the full paper here: https://journalhosting.ucalgary.ca/index.php/arctic/article/view/63328/47265). In this paper, case studies of illustrated to demonstrate examples relationships between researchers and communities. These case studies illustrate some challenges faced by communities and researchers, and the importance of communicating with a community from the beginning of the research to ensure that the project is mutually beneficial. 8.1.4.1 Case Study One For two years, a natural science project in Nunavut, Canada was conducted at a seasonal camp about 90 miles from the nearest community. The camp: never had more than 6 people contained typical scientific equipment (skidoos, a generator, four wheeler, etc) did not hire any local help for fieldwork Upon license renewal of the project’s third year, the hamlet council wrote a letter to the Nunavut Research Institute (NRI) expressing concerns over the conduct and impact of the research. The NRI helped coordinate meetings between the Principle Investigator and the appropriate community leaders and organizations. During these meetings, community representatives brought up the following concerns: Potential disturbance of fish in nearby lakes with the community fished Garbage or permanent structures left on the land after the research was completed Hiring of community members Prospecting ie the research was part of unauthorized mineral exploration These concerns had been apparent since the beginning of the research projects and the community wanted to know why the researcher was still carrying out the research. The researcher assured the community that the research would not disturb fishing areas, and that they would continue to remove all garbage and evidence from the camp. The researcher was not able to hire community members for that coming season but indicated this could be done in the future. The researcher also invited community members to see the camp and observe the fieldwork, and promised that this research was not related to mining exploration in any form. As for the final concern, the researcher stated that they did not realized these concerns were so strong, and that similar research was being conducted in other areas in Nunavut where communities were supportive. In the end, the community decided not to support this research, and indicated this to the NRI and regional authorities. Because the plans were well underway, the researcher was permitted to carry out the research but to cancel research planned for the following years. The community noted their disappointment in the fact that the research license was still given to the researchers even though they were not in support. They expressed their desire to be more extensively involved in the research licensing process of future studies and have greater authority in regards to the research conducted on their land. The research expressed disappointment and confusion. The research was moved to another location in Nunavut after the third season. 8.1.4.2 Case Study Two This case is about an ongoing, long-term physical science study that was carried out at various locations in Nunavut. Research activities were on a small scale, the team consisted of 2-4 people, and none of the research team spent more than three months there at a time. The research equipment was minimal, including a small tent, wooden hunt and skidoo. The research received Nunavut research licenses for several years and the study never encountered concerns or opposition from the community. The principal investigator also developed long-term personal relationships with members of a community near one of the study sites. Before each visit to a field site, the researcher spent a few days in the nearby community to visit informational with local authorities. The researcher also: Found that face to face communication was better than email, phone or fax Presented at schools about the ongoing research Created a collaborative scientific sampling project between the local school and a school from the researcher’s home province Created a “proxy project” near the community where the research team could demonstrate the field research techniques Hired community members to help with camp setup and cleanup This research continued to receive community approval throughout the years. 8.2 Discussion questions: What main differences between the two case studies stand out to you? What data ethics do you consider in your research? Why or why not? How do you see CARE and FAIR principles working together? How do you see Indigenous data sovereignty working in the context of open science? 8.3 Data Ethics Resources 8.3.0.1 Resources Trainings: Fundamentals of OCAP (online training - for working with First Nations in Canada): https://fnigc.ca/ocap-training/take-the-course/ Native Nations Institute trainings on Indigenous Data Sovereignty and Indigenous Data Governance: https://igp.arizona.edu/jit The Alaska Indigenous Research Program, is a collaboration between the Alaska Native Tribal Health Consortium (ANTHC) and Alaska Pacific University (APU) to increase capacity for conducting culturally responsive and respectful health research that addresses the unique settings and health needs of Alaska Native and American Indian People. The 2022 program runs for three weeks (May 2 - May 20), with specific topics covered each week. Week two (Research Ethics) may be of particular interest. Registration is free. The r-ETHICS training (Ethics Training for Health in Indigenous Communities Study) is starting to become an acceptable, recognizable CITI addition for IRB training by tribal entities. Kawerak, Inc and First Alaskans Institute have offered trainings in research ethics and Indigenous Data Sovereignty. Keep an eye out for further opportunities from these Alaska-based organizations. On open science and ethics: https://www.nature.com/articles/d41586-022-00724-0?WT.ec_id=NATURE-20220317&amp;utm_source=nature_etoc&amp;utm_medium=email&amp;utm_campaign=20220317&amp;sap-outbound-id=B4EAD742973291804C3AEF5A15DD13806C9F2C30 ON-MERRIT recommendations for maximizing equity in open and responsible research https://zenodo.org/record/6276753#.YjjgC3XMLCI https://link.springer.com/article/10.1007/s10677-019-10053-3 https://sagebionetworks.org/in-the-news/on-the-ethics-of-open-science-2/ Arctic social science and data management: Arctic Horizons report: Anderson, S., Strawhacker, C., Presnall, A., et al. (2018). Arctic Horizons: Final Report. Washington D.C.: Jefferson Institute. https://www.jeffersoninst.org/sites/default/files/Arctic%20Horizons%20Final%20Report%281%29.pdf Arctic Data Center workshop report: https://arcticdata.io/social-scientific-data-workshop/ Arctic Indigenous research and knowledge sovereignty frameworks, strategies and reports: Kawerak, Inc. (2021) Knowledge &amp; Research Sovereignty Workshop May 18-21, 2021 Workshop Report. Prepared by Sandhill.Culture. Craft and Kawerak Inc. Social Science Program. Nome, Alaska. Inuit Circumpolar Council. 2021. Ethical and Equitable Engagement Synthesis Report: A collection of Inuit rules, guidelines, protocols, and values for the engagement of Inuit Communities and Indigenous Knowledge from Across Inuit Nunaat. Synthesis Report. International. Inuit Tapiriit Kanatami. 2018. National Inuit Strategy on Research. Accessed at: https://www.inuitcircumpolar.com/project/icc-ethical-and-equitable-engagement-synthesis-report/ Indigenous Data Governance and Sovereignty: McBride, K. Data Resources and Challenges for First Nations Communities. Document Review and Position Paper. Prepared for the Alberta First Nations Information Governance Centre. Carroll, S.R., Garba, I., Figueroa-Rodríguez, O.L., Holbrook, J., Lovett, R., Materechera, S., Parsons, M., Raseroka, K., Rodriguez-Lonebear, D., Rowe, R., Sara, R., Walker, J.D., Anderson, J. and Hudson, M., 2020. The CARE Principles for Indigenous Data Governance. Data Science Journal, 19(1), p.43. DOI: http://doi.org/10.5334/dsj-2020-043 Kornei, K. (2021), Academic citations evolve to include Indigenous oral teachings, Eos, 102, https://doi.org/10.1029/2021EO210595. Published on 9 November 2021. Kukutai, T. &amp; Taylor, J. (Eds.). (2016). Indigenous data sovereignty: Toward an agenda. Canberra: Australian National University Press. See the editors’ Introduction and Chapter 7. Kukutai, T. &amp; Walter, M. (2015). Indigenising statistics: Meeting in the recognition space. Statistical Journal of the IAOS, 31(2), 317–326. Miaim nayri Wingara Indigenous Data Sovereignty Collective and the Australian Indigenous Governance Institute. (2018). Indigenous data sovereignty communique. Indigenous Data Sovereignty Summit, 20 June 2018, Canberra. http://www.aigi.com.au/wp-content/uploads/2018/07/Communique-Indigenous-Data-Sovereignty-Summit.pdf National Congress of American Indians. (2018). Resolution KAN-18-011: Support of US Indigenous data sovereignty and inclusion of tribes in the development of tribal data governance principles. http://www.ncai.org/attachments/Resolution_gbuJbEHWpkOgcwCICRtgMJHMsUNofqYvuMSnzLFzOdxBlMlRjij_KAN-18-011%20Final.pdf Rainie, S., Kukutai, T., Walter, M., Figueroa-Rodriguez, O., Walker, J., &amp; Axelsson, P. (2019) Issues in Open Data - Indigenous Data Sovereignty. In T. Davies, S. Walker, M. Rubinstein, &amp; F. Perini (Eds.), The State of Open Data: Histories and Horizons. Cape Town and Ottawa: African Minds and International Development Research Centre. https://zenodo.org/record/2677801#.YjqOFDfMLPY Schultz, Jennifer Lee, and Stephanie Carroll Rainie. 2014. “The Strategic Power of Data : A Key Aspect of Sovereignty.” 5(4). Trudgett, Skye, Kalinda Griffiths, Sara Farnbach, and Anthony Shakeshaft. 2022. “A Framework for Operationalising Aboriginal and Torres Strait Islander Data Sovereignty in Australia: Results of a Systematic Literature Review of Published Studies.” eClinicalMedicine 45: 1–23. IRBs/Tribal IRBs: Around Him D, Aguilar TA, Frederick A, Larsen H, Seiber M, Angal J. Tribal IRBs: A Framework for Understanding Research Oversight in American Indian and Alaska Native Communities. Am Indian Alsk Native Ment Health Res. 2019;26(2):71-95. doi: 10.5820/aian.2602.2019.71. PMID: 31550379. Kuhn NS, Parker M, Lefthand-Begay C. Indigenous Research Ethics Requirements: An Examination of Six Tribal Institutional Review Board Applications and Processes in the United States. Journal of Empirical Research on Human Research Ethics. 2020;15(4):279-291. doi:10.1177/1556264620912103 Marley TL. Indigenous Data Sovereignty: University Institutional Review Board Policies and Guidelines and Research with American Indian and Alaska Native Communities. American Behavioral Scientist. 2019;63(6):722-742. doi:10.1177/0002764218799130 Marley TL. Indigenous Data Sovereignty: University Institutional Review Board Policies and Guidelines and Research with American Indian and Alaska Native Communities. American Behavioral Scientist. 2019;63(6):722-742. doi:10.1177/0002764218799130 Ethical research with Sami communities: Eriksen, H., Rautio, A., Johnson, R. et al. Ethical considerations for community-based participatory research with Sami communities in North Finland. Ambio 50, 1222–1236 (2021). https://doi.org/10.1007/s13280-020-01459-w Jonsson, Å.N. Ethical guidelines for the documentation of árbediehtu, Sami traditional knowledge. In Working with Traditional Knowledge: Communities, Institutions, Information Systems, Law and Ethics. Writings from the Árbediehtu Pilot Project on Documentation and Protection of Sami Traditional Knowledge. Dieđut 1/2011. Sámi allaskuvla / Sámi University College 2011: 97–125. https://samas.brage.unit.no/samas-xmlui/bitstream/handle/11250/177065/Diedut-1-2011_AasaNordinJonsson.pdf?sequence=8&amp;isAllowed=y "],["data-modeling-essentials.html", "9 Data Modeling Essentials", " 9 Data Modeling Essentials 9.0.1 Learning Objectives Understand basics of relational data models aka tidy data Learn how to design and create effective data tables 9.0.2 Introduction In this lesson we are going to learn what relational data models are, and how they can be used to manage and analyze data efficiently. Relational data models are what relational databases use to organize tables. However, you don’t have to be using a relational database (like mySQL, MariaDB, Oracle, or Microsoft Access) to enjoy the benefits of using a relational data model. Additionally, your data don’t have to be large or complex for you to benefit. Here are a few of the benefits of using a relational data model: Powerful search and filtering Handle large, complex data sets Enforce data integrity Decrease errors from redundant updates Simple guidelines for data management A great paper called ‘Some Simple Guidelines for Effective Data Management’ (Borer et al. 2009) lays out exactly that - guidelines that make your data management, and your reproducible research, more effective. Use a scripted program (like R!) A scripted program helps to make sure your work is reproducible. Typically, point-and-click actions, such as clicking on a cell in a spreadsheet program and modifying the value, are not reproducible or easily explained. Programming allows you to both reproduce what you did, and explain it if you use a tool like Rmarkdown. Non-proprietary file formats are preferred (eg: csv, txt) Using a file that can be opened using free and open software greatly increases the longevity and accessibility of your data, since your data do not rely on having any particular software license to open the data file. Keep a raw version of data In conjunction with using a scripted language, keeping a raw version of your data is definitely a requirement to generate a reproducible workflow. When you keep your raw data, your scripts can read from that raw data and create as many derived data products as you need, and you will always be able to re-run your scripts and know that you will get the same output. Use descriptive file and variable names (without spaces!) When you use a scripted language, you will be using file and variable names as arguments to various functions. Programming languages are quite sensitive with what they are able to interpret as values, and they are particularly sensitive to spaces. So, if you are building reproducible workflows around scripting, or plan to in the future, saving your files without spaces or special characters will help you read those files and variables more easily. Additionally, making file and variables descriptive will help your future self and others more quickly understand what type of data they contain. Include a header line in your tabular data files Using a single header line of column names as the first row of your data table is the most common and easiest way to achieve consistency among files. Use plain ASCII text ASCII (sometimes just called plain text) is a very commonly used standard for character encoding, and is far more likely to persist very far into the future than proprietary binary formats such as Excel. The next three are a little more complex, but all are characteristics of the relational data model: Design tables to add rows, not columns Each column should contain only one type of information Record a single piece of data only once; separate information collected at different scales into different tables. 9.0.3 Recognizing untidy data Before we learn how to create a relational data model, let’s look at how to recognize data that does not conform to the model. Data Organization This is a screenshot of an actual dataset that came across NCEAS. We have all seen spreadsheets that look like this - and it is fairly obvious that whatever this is, it isn’t very tidy. Let’s dive deeper in to exactly why we wouldn’t consider it tidy. Multiple tables Your human brain can see from the way this sheet is laid out that it has three tables within it. Although it is easy for us to see and interpret this, it is extremely difficult to get a computer to see it this way, which will create headaches down the road should you try to read in this information to R or another programming language. Inconsistent observations Rows correspond to observations. If you look across a single row, and you notice that there are clearly multiple observations in one row, the data are likely not tidy. Inconsistent variables Columns correspond to variables. If you look down a column, and see that multiple variables exist in the table, the data are not tidy. A good test for this can be to see if you think the column consists of only one unit type. Marginal sums and statistics Marginal sums and statistics also are not considered tidy, and they are not the same type of observation as the other rows. Instead, they are a combination of observations. ### Good enough data modeling Denormalized data When data are “denormalized” it means that observations about different entities are combined. In the above example, each row has measurements about both the community in which observations occurred, as well as observations of two individuals surveyed in that community. This is not normalized data. People often refer to this as wide format, because the observations are spread across a wide number of columns. Note that, should one survey another individual in either community, we would have to add new columns to the table. This is difficult to analyze, understand, and maintain. Tabular data Observations. A better way to model data is to organize the observations about each type of entity in its own table. This results in: Separate tables for each type of entity measured Each row represents a single observation within that entity Observations (rows) are all unique This is normalized data (aka tidy data) Variables. In addition, for normalized data, we expect the variables to be organized such that: All values in a column are of the same type All columns pertain to the same observed entity (e.g., row) Each column represents either an identifying variable or a measured variable Challenge Try to answer the following questions: What are the observed entities in the example above? What are the measured variables associated with those observations? Answer: If we use these questions to tidy our data, we should end up with: one table for each entity observed one column for each measured variable additional columns for identifying variables (such as community) Here is what our tidy data look like: Note that this normalized version of the data meets the three guidelines set by (Borer et al. 2009): Design tables to add rows, not columns Each column should contain only one type of information Record a single piece of data only once; separate information collected at different scales into different tables. 9.0.4 Using normalized data Normalizing data by separating it into multiple tables often makes researchers really uncomfortable. This is understandable! The person who designed this study collected all of this information for a reason - so that they could analyze it together. Now that our community and survey information are in separate tables, how would we use population as a predictor variable for language spoken, for example? The answer is keys - and they are the cornerstone of relational data models. When one has normalized data, we often use unique identifiers to reference particular observations, which allows us to link across tables. Two types of identifiers are common within relational data: Primary Key: unique identifier for each observed entity, one per row Foreign Key: reference to a primary key in another table (linkage) Challenge In our normalized tables above, identify the following: the primary key for each table any foreign keys that exist Answer The primary key of the top table is community. The primary key of the bottom table is id. The community column is the primary key of that table because it uniquely identifies each row of the table as a unique observation of a community. In the second table, however, the community column is a foreign key that references the primary key from the first table. Entity-Relationship Model (ER) An Entity-Relationship model allows us to compactly draw the structure of the tables in a relational database, including the primary and foreign keys in the tables. In the above model, one can see that each community in the community observations table must have one or more survey participants in the survey table, whereas each survey response has one and only one community. Here is a more complicated ER Model showing examples of other types of relationships. Merging data Frequently, analysis of data will require merging these separately managed tables back together. There are multiple ways to join the observations in two tables, based on how the rows of one table are merged with the rows of the other. When conceptualizing merges, one can think of two tables, one on the left and one on the right. The most common (and often useful) join is when you merge the subset of rows that have matches in both the left table and the right table: this is called an INNER JOIN. Other types of join are possible as well. A LEFT JOIN takes all of the rows from the left table, and merges on the data from matching rows in the right table. Keys that don’t match from the left table are still provided with a missing value (na) from the right table. A RIGHT JOIN is the same, except that all of the rows from the right table are included with matching data from the left, or a missing value. Finally, a FULL OUTER JOIN includes all data from all rows in both tables, and includes missing values wherever necessary. Sometimes people represent these as Venn diagrams showing which parts of the left and right tables are included in the results for each join. These however, miss part of the story related to where the missing value come from in each result. In the figure above, the blue regions show the set of rows that are included in the result. For the INNER join, the rows returned are all rows in A that have a matching row in B. 9.0.5 Data modeling exercise Break into groups Our funding agency requires that we take surveys of individuals who complete our training courses so that we can report on the demographics of our trainees and how effective they find our courses to be. In your small groups, design a set of tables that will capture information collected in a participant survey that would apply to many courses. Don’t focus on designing a comprehensive set of questions for the survey, one or two simple stand ins (eg: “Did the course meet your expectations?”, “What could be improved?”, “To what degree did your knowledge increase?”) would be sufficient. Include as variables (columns) a basic set of information not only from the surveys, but about the courses, such as the date of the course and name of the course. Draw your entity-relationship model for your tables. 9.0.6 Resources Borer et al. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America. White et al. 2013. Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution 6. Software Carpentry SQL tutorial Tidy Data "],["cleaning-and-manipulating-data.html", "10 Cleaning and Manipulating Data 10.1 Data Cleaning and Manipulation", " 10 Cleaning and Manipulating Data 10.1 Data Cleaning and Manipulation 10.1.1 Learning Objectives In this lesson, you will learn: What the Split-Apply-Combine strategy is and how it applies to data The difference between wide vs. tall table formats and how to convert between them How to use dplyr and tidyr to clean and manipulate data for analysis How to join multiple data.frames together using dplyr 10.1.2 Introduction The data we get to work with are rarely, if ever, in the format we need to do our analyses. It’s often the case that one package requires data in one format, while another package requires the data to be in another format. To be efficient analysts, we should have good tools for reformatting data for our needs so we can do our actual work like making plots and fitting models. The dplyr and tidyr R packages provide a fairly complete and extremely powerful set of functions for us to do this reformatting quickly and learning these tools well will greatly increase your efficiency as an analyst. Analyses take many shapes, but they often conform to what is known as the Split-Apply-Combine strategy. This strategy follows a usual set of steps: Split: Split the data into logical groups (e.g., area, stock, year) Apply: Calculate some summary statistic on each group (e.g. mean total length by year) Combine: Combine the groups back together into a single table Figure 1: diagram of the split apply combine strategy As shown above (Figure 1), our original table is split into groups by year, we calculate the mean length for each group, and finally combine the per-year means into a single table. dplyr provides a fast and powerful way to express this. Another exceedingly common thing we need to do is “reshape” our data. Let’s look at an example table that is in what we will call “wide” format: site 1990 1991 … 1993 gold 100 118 … 112 lake 100 118 … 112 … … … … … dredge 100 118 … 112 You are probably quite familiar with data in the above format, where values of the variable being observed are spread out across columns (Here: columns for each year). Another way of describing this is that there is more than one measurement per row. This wide format works well for data entry and sometimes works well for analysis but we quickly outgrow it when using R. For example, how would you fit a model with year as a predictor variable? In an ideal world, we’d be able to just run: lm(length ~ year) But this won’t work on our wide data because lm needs length and year to be columns in our table. Or how would we make a separate plot for each year? We could call plot one time for each year but this is tedious if we have many years of data and hard to maintain as we add more years of data to our dataset. The tidyr package allows us to quickly switch between wide format and what is called tall format using the pivot_longer function: site_data %&gt;% pivot_longer(-site, names_to = &quot;year&quot;, values_to = &quot;length&quot;) site year length gold 1990 101 lake 1990 104 dredge 1990 144 … … … dredge 1993 145 In this lesson we’re going to walk through the functions you’ll most commonly use from the dplyr and tidyr packages: dplyr mutate() group_by() summarise() select() filter() arrange() left_join() rename() tidyr pivot_longer() pivot_wider() unite() separate() 10.1.3 Data Cleaning Basics To demonstrate how to use these functions, we are going to be using a modified version of some real survey data we have gathered for feedback on our courses (like the one you are taking now!) I’ve made some changes to the original data, including removing references to instructor names, untidying the responses with controlled vocabularies, and scrambling the word order of the free text answers. These changes will help illustrate some of the common issues seen with survey data. They also help ensure that only general conclusions can be gathered from the free text responses, since scrambling the text inhibits our interpretative abilities. 10.1.4 Read in survey data First, open a new RMarkdown document. Delete everything below the setup chunk, and add a library chunk that calls dplyr, tidyr, and readr: library(dplyr) library(tidyr) library(readr) Aside A note on loading packages. You may have noticed the following warning messages pop up when you ran your library chunk. Attaching package: ‘dplyr’ The following objects are masked from ‘package:stats’: filter, lag The following objects are masked from ‘package:base’: intersect, setdiff, setequal, union These are important warnings. They are letting you know that certain functions from the stats and base packages (which are loaded by default when you start R) are masked by different functions with the same name in the dplyr package. It turns out, the order that you load the packages in matters. Since we loaded dplyr after stats, R will assume that if you call filter, you mean the dplyr version unless you specify otherwise. Being specific about which version of filter, for example, you call is easy. To explicitly call a function by its unambiguous name, you use the syntax package_name::function_name(...). So, if I wanted to call the stats version of filter in this Rmarkdown document, I would use the syntax stats::filter(...). Challenge The warnings above are important, but we might not want them in our final document. After you have read them, adjust the chunk settings on your library chunk to suppress warnings and messages. Now that we have learned a little mini-lesson on functions, let’s get the data that we are going to use for this lesson. Setup Navigate to the demo dataset Right click the “download” button for the file “survey_data.csv” Select “copy link address” from the dropdown menu Paste the URL into a read_csv call like below The code chunk you use to read in the data should look something like this: survey_raw &lt;- read_csv(&quot;https://dev.nceas.ucsb.edu/knb/d1/mn/v2/object/urn%3Auuid%3A71cb8d0d-70d5-4752-abcd-e3bcf7f14783&quot;, show_col_types = FALSE) This dataset is already relatively clean, but as we’ll see there are a few issues that we need to resolve, and a couple of things we can do to make our analysis easier. We can examine the dataset in a few different ways, one easy way is the glimpse function, which shows the first few items of every column in the data.frame. glimpse(survey_raw) ## Rows: 69 ## Columns: 7 ## $ ResponseId &lt;chr&gt; &quot;R_1pXb0f3ruY1IJbR&quot;, &quot;R_28B834pFGa72Z4q&quot;, &quot;R_1OkjoEUKFhA3NJ… ## $ StartDate &lt;date&gt; 2022-02-18, 2022-02-18, 2022-02-18, 2022-02-18, 2022-02-18… ## $ Q1 &lt;chr&gt; &quot;exceeded expectations&quot;, &quot;just above expectations&quot;, &quot;just a… ## $ Q2 &lt;chr&gt; &quot;substantial increase&quot;, &quot;increase&quot;, &quot;increase&quot;, &quot;increase&quot;,… ## $ Q3 &lt;chr&gt; &quot;from reproducible section. the with I learned topics wasn&#39;… ## $ Q4 &lt;chr&gt; &quot;If that general. am I in amount so evil tired think we I t… ## $ notes &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… For some context, here is a table showing what each of the questions are asking: id question Q1 To what degree did the workshop met your expectations? Q2 How has your skill level changed in understanding the value of reproducible research? Q3 What is the most valuable thing you learned during the workshop? Q4 What do you think should be changed for future offerings? Q1 and Q2 have responses with a controlled vocabulary (choice answers), while Q3 and Q4 are free text. About the pipe (%&gt;%) operator Before we jump into learning tidyr and dplyr, we first need to explain the %&gt;%. Both the tidyr and the dplyr packages use the pipe operator - %&gt;%, which may look unfamiliar. The pipe is a powerful way to efficiently chain together operations. The pipe will take the output of a previous statement, and use it as the input to the next statement. Say you want to both filter out rows of a dataset, and select certain columns. Instead of writing df_filtered &lt;- filter(df, ...) df_selected &lt;- select(df_filtered, ...) You can write df_cleaned &lt;- df %&gt;% filter(...) %&gt;% select(...) If you think of the assignment operator (&lt;-) as reading like “gets”, then the pipe operator would read like “then.” So you might think of the above chunk being translated as: The cleaned dataframe gets the original data, and then a filter (of the original data), and then a select (of the filtered data). The benefits to using pipes are that you don’t have to keep track of (or overwrite) intermediate data frames. The drawbacks are that it can be more difficult to explain the reasoning behind each step, especially when many operations are chained together. It is good to strike a balance between writing efficient code (chaining operations), while ensuring that you are still clearly explaining, both to your future self and others, what you are doing and why you are doing it. RStudio has a keyboard shortcut for %&gt;% : Ctrl + Shift + M (Windows), Cmd + Shift + M (Mac). Selecting/removing columns: select() One of the first things we might want to do to this dataset is to remove the notes column. In this case, the notes column is completely empty, so it doesn’t serve us much purpose. Let’s use the select function to select the columns we want to keep. survey_clean &lt;- survey_raw %&gt;% select(ResponseId, StartDate, Q1, Q2, Q3, Q4) select also allows you to say which columns you don’t want, by passing unquoted column names preceded by minus (-) signs: survey_clean &lt;- survey_raw %&gt;% select(-notes) Split-Apply-Combine Let’s say we want to see the number of responses for each choice given for question 1. We’ll use the split-apply-combine strategy to split that column into groups for each unique response, and then apply a function to count the number of times each group appears. group_by splits the rows of your data.frame according to the unique values within the grouping column. Running group_by on its own won’t look like anything happened, since that information is stored behind the scenes in your data.frame which otherwise looks the same. summarize applies the function you pass to it, creating a new column with the function applied over each unique value in your grouping column. q1_response &lt;- survey_raw %&gt;% group_by(Q1) %&gt;% summarise(n = n()) Q1 n 1 5 exceeded expectations 38 just above expectations 14 just below expectations 1 met expectations 11 What does that 1 mean? Turns out our kind of messy data didn’t have a consistent coding system. After doing some research, we figure out that 1 actually means “below expectations”. Let’s fix that in our original data so the data are consistent. Changing column content: mutate() We can use the mutate function to change a column, or to create a new column. Here we change the column Q1, using the if_else function. This function takes three arguments, a conditional statement, what to return if the condition is TRUE, and what to return if the condition is FALSE. In this case, our implementation says if Q1 is equal to (==) 1, change the value to “below expectations.” Otherwise, keep the value of Q1. survey_clean &lt;- survey_raw %&gt;% select(-notes) %&gt;% mutate(Q1 = if_else(Q1 == &quot;1&quot;, &quot;below expectations&quot;, Q1)) Now we can do our group by and summarize again: q1_response &lt;- survey_clean %&gt;% group_by(Q1) %&gt;% summarise(n = n()) Q1 n below expectations 5 exceeded expectations 38 just above expectations 14 just below expectations 1 met expectations 11 If we want to arrange our rows from highest to lowest counts, we can use arrange. The minus sign sorts from high to low. q1_response &lt;- survey_clean %&gt;% group_by(Q1) %&gt;% summarise(n = n()) %&gt;% arrange(-n) Q1 n exceeded expectations 38 just above expectations 14 met expectations 11 below expectations 5 just below expectations 1 Let’s look at the results from question 2 now. q2_response &lt;- survey_clean %&gt;% group_by(Q2) %&gt;% summarise(n = n()) Q2 n increase 19 Increase 2 no change 3 slight increase 4 substantial increase 38 substantial Increase 3 It also has issues with the unique values, with some stray capitalization errors. We will use mutate again here to change the existing Q2 column. On the right hand of the expression in the mutate function, we include another function, called to_lower. This function will turn all of the characters in the column into lower case. We can add this to our existing data cleaning pipeline as below: survey_clean &lt;- survey_raw %&gt;% select(-notes) %&gt;% mutate(Q1 = if_else(Q1 == &quot;1&quot;, &quot;below expectations&quot;, Q1)) %&gt;% mutate(Q2 = tolower(Q2)) Looking at the aggregated responses again: q2_response &lt;- survey_clean %&gt;% group_by(Q2) %&gt;% summarise(n = n()) %&gt;% arrange(-n) Q2 n substantial increase 41 increase 21 slight increase 4 no change 3 Changing shape: pivot_longer() and pivot_wider() Although the format of this table is tidy, we may have good reasons to change shape to where the question is in one column, and the response in another. This may make certain types of plots or tables easier to make. To change from a wide format to a long format, we use pivot_longer. The key arguments to this function are: cols, the columns you are pivoting over, names_to, the column name you are creating from the column names covered by cols, and values_to, the column name you are creating from the values in those columns. Note in the cols argument, we use a little helper function called starts_with to select the columns starting with “Q” (Q1, Q2, Q3, Q4) as our pivoting columns. Our new column of old column names will be called “id” to indicate that it represents the question id. The new column of values within those columns we will call “answer.” survey_long &lt;- survey_clean %&gt;% pivot_longer(cols = starts_with(&quot;Q&quot;), names_to = &quot;id&quot;, values_to = &quot;answer&quot;) This kind of pivoting allows us to quickly create a table with responses to both question 1 and question 2, summarized. First we use the filter function to filter for those questions, then the group_by and summarize we have been using. comb &lt;- survey_long %&gt;% filter(id %in% c(&quot;Q1&quot;, &quot;Q2&quot;)) %&gt;% group_by(id, answer) %&gt;% summarise(n = n()) %&gt;% arrange(id, -n) ## `summarise()` has grouped output by &#39;id&#39;. You can override using the `.groups` ## argument. id answer n Q1 exceeded expectations 38 Q1 just above expectations 14 Q1 met expectations 11 Q1 below expectations 5 Q1 just below expectations 1 Q2 substantial increase 41 Q2 increase 21 Q2 slight increase 4 Q2 no change 3 10.1.5 Joins So now that we’re awesome at manipulating a single data.frame, where do we go from here? Manipulating more than one data.frame. If you’ve ever used a database, you may have heard of or used what’s called a “join”, which allows us to to intelligently merge two tables together into a single table based upon a shared column between the two. We’ve already covered joins in [Data Modeling] so let’s see how it’s done with dplyr. The dataset we’re working with contains an additional CSV which has information about each individual training event. This is a really common way of storing auxiliary information about our dataset of interest (survey responses) but, for analytical purposes, we often want them in the same data.frame. Joins let us do that easily. Let’s look at a preview of what our join will do by looking at a simplified version of our data: First, we read in the events data just as we did previously with the survey data. events &lt;- read_csv(&quot;https://dev.nceas.ucsb.edu/knb/d1/mn/v2/object/urn%3Auuid%3A0a1dd2d8-e8db-4089-a176-1b557d6e2786&quot;, show_col_types = FALSE) Now, we can use our left_join function to join the two tables together by the StartDate column. Notice that I have deviated from our usual pipe syntax (although it does work here!) because I prefer to see the data.frames that I am joining side by side in the syntax. survey_joined &lt;- left_join(survey_long, events, by = &quot;StartDate&quot;) Now we have a joined data frame! 10.1.5.1 Review That was a lot of exposition for very little code. Here is a summary of the main data cleaning steps we used: survey_raw &lt;- read_csv(&quot;https://dev.nceas.ucsb.edu/knb/d1/mn/v2/object/urn%3Auuid%3A71cb8d0d-70d5-4752-abcd-e3bcf7f14783&quot;, show_col_types = FALSE) survey_clean &lt;- survey_raw %&gt;% select(-notes) %&gt;% mutate(Q1 = if_else(Q1 == &quot;1&quot;, &quot;below expectations&quot;, Q1)) %&gt;% mutate(Q2 = tolower(Q2)) events &lt;- read_csv(&quot;https://dev.nceas.ucsb.edu/knb/d1/mn/v2/object/urn%3Auuid%3A0a1dd2d8-e8db-4089-a176-1b557d6e2786&quot;, show_col_types = FALSE) survey_joined &lt;- left_join(survey_clean, events, by = &quot;StartDate&quot;) As well as the split-apply-combine for questions 1 and 2: q1_response &lt;- survey_clean %&gt;% group_by(Q1) %&gt;% summarise(n = n()) %&gt;% arrange(-n) q2_response &lt;- survey_clean %&gt;% group_by(Q2) %&gt;% summarise(n = n()) %&gt;% arrange(-n) And finally the pivot long: survey_long &lt;- survey_clean %&gt;% pivot_longer(cols = starts_with(&quot;Q&quot;), names_to = &quot;id&quot;, values_to = &quot;answer&quot;) separate() and unite() separate() and its complement, unite() allow us to easily split a single column into numerous (or numerous into a single). This can come in really handle when we need to split a column into two pieces by a consistent separator (like a dash). Let’s make a new data.frame with fake data to illustrate this. Here we have a set of site identification codes. with information about the island where the site is (the first 3 letters) and a site number (the 3 numbers). If we want to group and summarize by island, we need a column with just the island information. sites_df &lt;- data.frame(site = c(&quot;HAW-101&quot;, &quot;HAW-103&quot;, &quot;OAH-320&quot;, &quot;OAH-219&quot;, &quot;MAI-039&quot;)) sites_df %&gt;% separate(site, c(&quot;island&quot;, &quot;site_number&quot;), &quot;-&quot;) ## island site_number ## 1 HAW 101 ## 2 HAW 103 ## 3 OAH 320 ## 4 OAH 219 ## 5 MAI 039 Exercise: Split the city column in the following data.frame into city and state_code columns: cities_df &lt;- data.frame(city = c(&quot;Juneau AK&quot;, &quot;Sitka AK&quot;, &quot;Anchorage AK&quot;)) # Write your solution here unite() does just the reverse of separate(). If we have a data.frame that contains columns for year, month, and day, we might want to unite these into a single date column. dates_df &lt;- data.frame(year = c(&quot;1930&quot;, &quot;1930&quot;, &quot;1930&quot;), month = c(&quot;12&quot;, &quot;12&quot;, &quot;12&quot;), day = c(&quot;14&quot;, &quot;15&quot;, &quot;16&quot;)) dates_df %&gt;% unite(date, year, month, day, sep = &quot;-&quot;) ## date ## 1 1930-12-14 ## 2 1930-12-15 ## 3 1930-12-16 Exercise: Use unite() on your solution above to combine the cities_df back to its original form with just one column, city: # Write your solution here "],["human-subjects-research-considerations.html", "11 Human Subjects Research Considerations 11.1 Human Subjects Considerations", " 11 Human Subjects Research Considerations 11.1 Human Subjects Considerations “Content developed by Noor Johnson, Exchange for Local Observations and Knowledge of the Arctic, and Andy Barrett, Navigating the New Arctic Community Office” 11.1.1 Learning Objectives In this lesson, we will: Discuss research protocols including Institutional Review Boards Review principles and protocols relating with working with Indigenous communuities Examine case studies involving human subjects 11.1.2 Introduction This part of the course offers an introduction to research involving human subjects. It was developed with input from ELOKA and the NNA-CO, and is a work-in-progress – this is the first time we are offering this training. The training introduces ethics issues in a broad way and includes discussion of social science data and open science, but the majority of the section focuses on issues related to research with, by, and for Indigenous communities. We recognize that there is a need for more in-depth training and focus on open science for social scientists and others who are not engaging with Indigenous Knowledge holders and Indigenous communities, and hope to develop further resources in this area in the future. Many of the data stewardship practices that have been identified as good practices through Indigenous Data Sovereignty framework development are also relevant for those working with Arctic communities that are not Indigenous, although the rights frameworks and collective ownership is specific to the Indigenous context. The examples we include in this training are primarily drawn from the North American research context. In future trainings, we plan to expand and include examples from other Indigenous Arctic contexts. We welcome suggestions and resources that would strengthen this training for audiences outside of North America. We also recognize the importance of trainings on Indigenous data sovereignty and ethics that are being developed and facilitated by Indigenous organizations and facilitators. In this training we offer some introductory material but there is much more depth offered in IDS specific trainings. We include some suggestions of organizations offering further training in the “resources” section and encourage participants to seek out these opportunities to deepen their understanding. And there are several sessions this week with some of the authors of the frameworks and protocols that will be discussed, including: "],["social-science-and-open-data.html", "12 Social Science and Open Data", " 12 Social Science and Open Data Open science is ostensibly about making access to data and knowledge more equitable and open and enabling reproducible research. These are laudable goals and on the surface, could provide a counterbalance to traditional approaches that limited Arctic community members’ access to research data and knowledge. Because it is being implemented within a social and institutional context that continues to perpetuate inequities in research, however, there are significant challenges remaining to ethically and equitably implementing open science practices. Here are some examples of impediments to ethical and equitable implementation of open science: Many Open Science tools and practices (e.g. GitHub, non-proprietary software) have been adopted from software development, and are not familiar to many, even physical, scientists. This makes barriers to implementation of open science and participation in open science practices higher for some than for others. Some of the main focus areas of open science (making sure data is archived in a clean and reusable format, open access publication) are still not accessible for Arctic residents and others who are not already situated within an academic environment. Open science assumes access to tools and infrastructure, such as computers that have reliable and low-cost or subsidized internet access. These structural inequalities extend even within the academy. Researchers and academics based in lower-income countries have fewer provided resources and sources of support to pay for open access fees and face greater challenges in adopting open science practices. The emphasis of open science on stakeholder and rights holder engagement and knowledge co-production also creates unintended challenges for Arctic communities. For example, when the National Science Foundation’s Navigating the New Arctic initiative, which brought millions of additional funds to Arctic research, suggested that research projects incorporate co-production and collaboration with Arctic communities, community representatives reported being inundated with requests from researchers. Perhaps because NNA was designed, in part, to bring new researchers and disciplines into Arctic research, many of these requests were made without adequate regard for recognized good practices for community engagement (such as the importance of building relationships, reaching out very early for input; communicating “early and often;” and incorporating significant resources for community participants’ time and knowledge into project budgets, among other things). As a letter to NSF written by Kawerak, the Association of Village Council Presidents, the Aleut Community of St. Paul Island, and the Bering Sea Elders Group emphasized, without adequate attention to these practices as well as an emphasis on topics that communities have identified as important, research will not serve community partners and will fail to achieve its broader impact goals. (See also the 2021 update letter from the same organizations). In addition to equity related challenges in promoting open access, there are also different disciplinary norms and requirements that can create challenges for the adoption of open science practices. For example, traditional social science methods training and IRB processes emphasize confidentiality and privacy. Social scientists generally lack access to training about benefits of sharing data. On the other hand, social scientists are often trained to be attuned to issues of equity and access, including issues related to information equity. Those who lack this background may not give adequate attention or time to the process of partnering with communities. The Arctic Horizons Report reviewed challenges around social science data management. In 2020, a workshop organized by the Arctic Data Center reviewed some of the challenges for sharing and reusing social science data, including: Data heterogeneity, including “unstructured” data that is not always well supported by data repositories; A dearth of metadata support for social sciences, with repositories not always offering relevant metadata fields; A lack of “formal vocabularies” that limits findability of social science data in searches; Limited training and support in data management practices for social scientists; limited examples using social science data in interdisciplinary trainings. In addition to these challenges, a number of broader concerns about sharing data were identified, including: Concerns about proper handling of sensitive data; importance of upholding IRB and ethics requirements, data sharing/use agreements; Importance of context - researchers may feel that data reuse is too difficult given lack of contextual knowledge; Concerns about upholding Indigenous data sovereignty. 12.0.1 The role of IRBs, funding agencies, and Indigenous data In the United States, Institutional Review Boards (IRBs; in Canada they are referred to as Research Ethics Boards or REBs) focus on ensuring ethical treatment and protection of research subjects with a particular focus on vulnerable populations and ethical management of data. For projects proposing research that involves Indigenous peoples in the United States, IRBs will often refer the review to Tribal IRBs, which are research review boards established and implemented directly by Tribal Nations. Tribal IRBs reflect and respond to community needs, changes in research, and revisions to research policy (Around Him et al. 2019). Oversight mechanisms range from federally registered review bodies and policy development to community-specific frameworks and approaches (see Around Him et al. 2019 for differentiation, justification and authority). In addition to IRB review, universities require ethics training for researchers who are doing research with human subjects, including Indigenous Peoples. Many universities use the web-based, third-party CITI training program, which offers different short courses. A basic training course for social and behavioral science researchers covers the history of ethical misconduct in research, how human subjects research is defined, federal regulations that govern research practice, assessing risk, informed consent, privacy and confidentiality, and ethics requirements for different categories of vulnerable populations including prisoners and children. While the CITI human subjects trainings touch on topics related to Indigenous peoples, they are not at all comprehensive. A CITI webinar, “Research with Native American Communities: Important Considerations when Applying Federal Regulations” introduces more topics. The r-ETHICS training (Ethics Training for Health in Indigenous Communities Study) is starting to become an acceptable, recognizable CITI addition for IRB training by tribal entities. Specific universities have adopted tribal consultation policies (such as the Arizona Board of Regents’ (ABOR) Tribal Consultation policy (1-118) adopted in 2016; University of Wisconsin System Board of Regents tribal consultation policy adopted in 2021; Washington State University EP 41 adopted in 2021). These policies highlight where consultation is expected/required and what the process should be (Marley 2019). In the United States, the Inter-Agency Arctic Research Policy Committee has established the Principles for Conducting Research in the Arctic (2018) with input from the Arctic research community, which are: Be Accountable Establish Effective Communication Respect Indigenous Knowledge and Cultures Build and Sustain Relationships Pursue Responsible Environmental Stewardship Along with the Principles for Conducting Research in the Arctic, the Arctic Research Consortium of the United States (ARCUS) has also published a list of resources: Conducting Research with Northern Communities - Documented Practices for Productive, Respectful Relationships Between Researchers and Community Members (https://www.arcus.org/resources/northern-communities). IRBs are not necessarily aware of these principles but national funding agencies may use them to inform proposal reviewers about expectations for research projects that work with Arctic communities. ## Working with Arctic Communities Arctic communities (defined as a place and the people who live there, based on geographic location in the Arctic/sub-Arctic) are involved in research in diverse ways - as hosts to visiting or non-local researchers, as well as “home” to community researchers who are leading or collaborating on research projects. Over the past decades, community voices of discontent with standard research practices that are often exclusive and perpetuate inequities have grown stronger. The Arctic research community (defined more broadly as the range of institutions, organizations, researchers and local communities involved in research) is in the midst of a complex conversation about equity in research aimed at transforming research practice to make it more equitable and inclusive. 12.0.2 IDS Frameworks There has been increasing emphasis on development of frameworks to support ethical research and data stewardship grounded in Indigenous understandings and world views. The emergence of national and global networks focusing on Indigenous data sovereignty has supported the development of some of these frameworks. For example, the Global Indigenous Data Alliance (GIDA) developed the CARE principles in response to the emergence of the FAIR principles. CARE is a set of high-level principles that are broad enough to encompass more specific frameworks and principles developed by International and national Indigenous networks, organizations, and Tribes. This alliance is supported by three national networks, the United States Indigenous Data Sovereignty Network (USIDSN), Te Mana Raraunga Maori Data Sovereignty Network, and the Maiam nayri Wingara Aboriginal and Torres Strait Islander Data Sovereignty Collective. The latter networks each have worked within their respective national contexts to develop IDS principles at a “mid-level.” These frameworks are being developed at different governance levels, from “high level” frameworks that are global in scale and therefore more general to mid-level frameworks that are developed at a national scale or by a subset of Indigenous Peoples/Nations (such as circumpolar Inuit or Inuit within the national level in Canada), to foundational level frameworks that are developed at the Tribal or community scale. It is important for researchers to be aware of the different frameworks that can inform and guide ethical research practice and data management. Frameworks developed at the high or mid-level do not replace foundational frameworks. In the absence of a written framework at the tribal/community scale, the practices of strong consultation and engagement outlined in mid-level frameworks can help inform the development of an ethical approach. One of the drivers of community concerns is the colonial practice of extracting knowledge from a place or group of people without respect for local norms of relationship with people and place, and without an ethical commitment to sharing and making benefits of knowledge accessible and accountable to that place. Extractive research can be defined as research that is not mutually beneficial, and the results primarily benefit the researcher. Such approaches to knowledge and data extraction follow hundreds of years of exploration and research that viewed science as a tool of “Enlightenment” yet focused exclusively on benefits to White, European (or “southern” from an Arctic community perspective) researchers and scientists. This prioritization of non-local perspectives and needs (to Arctic communities) continues in Arctic research. One result of this approach to research has been a lack of access for Arctic residents to the data and knowledge that have resulted from research conducted in their own communities. Much of this data was stored in the personal files or hard drives of researchers, or in archives located in urban centers far from the Arctic. 12.0.3 Indigenous data governance and sovereignty All governing entities, whether national, state, local, or tribal, need access to good, current, relevant data in order to make policy, planning, and programmatic decisions. Indigenous nations and organizations have had to push for data about their peoples and communities to be collected and shared in ethical and culturally appropriate ways, and they have also had to fight for resources and capacity to develop and lead their own research programs. 12.0.3.1 Indigenous data definitions: Indigenous data sovereignty “…refers to the right of Indigenous peoples to govern the collection, ownership, and application of data about Indigenous communities, peoples, lands, and resources (Rainie et al. 2019). These governance rights apply “regardless of where/by whom data is held (Rainie et al. 2019). Some Indigenous individuals and communities have expressed dissatisfaction with the term “data” as being too narrowly focused and abstract to represent the embedded and holistic nature of knowledge in Indigenous communities. Knowledge sovereignty is a related term that has a similar meaning but is framed more broadly, and has been defined as: “Tribal communities having control over the documentation and production of knowledge (such as through research activities) which relate to Alaska Native people and the resources they steward and depend on” (Kawerak 2021). Indigenous data is “data in a wide variety of formats inclusive of digital data and data as knowledge and information. It encompasses data, information, and knowledge about Indigenous individuals, collectives, entities, lifeways, cultures, lands, and resources.” (Rainie et al. 2019) Indigenous data governance is “The entitlement to determine how Indigenous data is governed and stewarded” (Rainie et al. 2019) High level frameworks in support of Indigenous Data Sovereignty: The UN Declaration on the Rights of Indigenous Peoples, which recognizes the collective rights of Indigenous peoples. Article 18 of UNDRIP recognizes the right of Indigenous Peoples to participate in decision-making about matters that affect their rights; Article 19 recognizes the requirement for states to consult and cooperate with Indigenous Peoples to gain their Free Prior and Informed Consent on legislation that affects them. Other international protocols such as Nagoya Protocol on access and benefit sharing and the Cartagena Protocol on Biosafety (UNCBD) also recognize Indigenous rights with reference to intellectual property and the right to benefit from Indigenous knowledge and data as the owners/stewards of that data. Mid-level frameworks: The First Nations Principles of OCAP® (ownership, control, access, possession). Developed by the First Nations Information Governance Centre, which has developed good educational materials explaining the principles and how to implement them, as well as a training course (available on their website; the class has a registration fee). Inuit Circumpolar Council’s Ethical and Equitable Engagement Synthesis. This synthesis summarizes the process and approach that ICC took to develop principles of ethical and equitable engagement for Inuit, which involved broad engagement and input from Inuit in Alaska, Canada, Greenland, and Chukotka. ICC is working on a full set of principles, which will be released after they have gone through review. National Inuit Strategy on Research. Developed by Inuit Tapiriit Kanatami, the National Inuit Organization of Canada, with input from the Inuit regions of northern Canada. The research strategy discusses Indigenous data sovereignty. In Alaska, the Alaska Native Knowledge Center developed Guidelines for Respecting Cultural Knowledge. These include specific guidelines for authors and illustrators, curriculum developers and administrators, educators, editors and publishers, document reviewers, researchers, native language specialists, native community organizers, and the general public. Principles of Māori Data Sovereignty (Te Mana Rararunga) - Developed by the Maori Data Sovereignty Network (Maui Hudson is a founding member of this network). Maiam nayri Wingara key principles developed by the Maiam nayri Wingara Aboriginal and Torres Strait Islander Data Sovereignty Collective. Foundational level frameworks: Tribal or community-level expectations/frameworks - such as Native Village of Kotzebue’s Research Protocol (Whiting 2022), which requests that researchers follow ethical research practices pertaining to informed consent and: Inform the Tribe of plan to research and continue to inform them after permission has been granted; Consult with the Tribe in project development, implementation and planning. Explain the purposes, goals, time frame, and methodology of the research, including the sponsoring institutions and affiliations of the research project and identify the person in charge, as well as all investigators involved in the research, and the need for consultants, guides, or interpreters and proposed compensation rates for same Share results with the Tribe in non-technical language Give credit to those contributing to the research project by acknowledging the Intellectual Property Rights of individual Tribal citizens taking part in the research (unless there are requirements for anonymity) Recognize that all information belongs to the Tribe and divulgence of such information is expressly forbidden without permission of the Tribe; Compensate Indigenous Knowledge holders fairly for sharing their knowledge. 12.0.4 Co-production of knowledge Here, we define co-production of knowledge as the contribution of various knowledge sources, and capacity sharing from different rights holders and stakeholders in the context of co-creating knowledge and information to inform research projects and environmental decision-making (Lemos &amp; Morehouse, 2005). Many argue that co-production of knowledge will increase the usability and relevance of science for society (Lemos &amp; Morehouse, 2005; Meadow et al., 2015; Wall et al., 2017). Based on a literature review from Djenontin &amp; Meadow (2018) of successful examples of co-production, here are several themes of successful co-production. Development, design and implementation components Rights holders and stakeholders’ needs should drive the project. Several approaches to help ensure this include: co-defining research questions co-conceptualizing and co-designing the research project co-leadership In the current research framework that involves timely deadlines for finishing and producing research, there are many challenges that include time and resources. Many of these challenges stem from research that is still founded on a western timeline. Navigating these challenges is a work in progress, however the foundation starts at building long-term relationships. 12.0.5 Disucssion What do you see as the benefits to your research (or to Arctic research more generally) from applying the frameworks discussed today? What do you see as the major impediments to adopting these frameworks? How might you navigate these conflicts within your own research project(s)? "],["human-subject-case-examples-and-discussion.html", "13 Human Subject Case Examples and Discussion", " 13 Human Subject Case Examples and Discussion 13.0.0.1 Case One ** Discussion ** This case study illustrates the important issue of explicit consent. Are there any other ethical considerations are apparent? Does consent play a role in your research, and if so, how does consent inform and influence your research methodologies? 13.0.0.2 Case Two This is a case study of research conducted in Alaska. The research occurred in the 1950s and the purpose of the study was to understand how the thyroid gland helps humans adapt to the Arctic climate. Research subjects were given a radioactive medical tracer - idoine-131. There were 102 Native American research subjects, and 19 Air Force and Army service men. Most of the Native American subjects did not realize they were part of research - they thought they were receiving medical attention. Furthermore, none of the research subjects knew that they were given a radioactive medical tracer. Read the full article here: https://www8.nationalacademies.org/onpinews/newsitem.aspx?RecordID=5106 Discussion While this is an extreme example of lack of consent and extractive research practices, it illustrates the problematic history of between researchers and Indigenous Peoples. What elements of this example relate to any of the policies or principles we have discussed? What are the main takeaways from our discussion on research and data ethics so far? 13.0.0.3 Case Three The following case is from an NSF funded project with the Association for Practical and Professional Ethics, titled “Do the Ends Justify the Means? The Ethics of Deception in Social Science Research”. While these cases are not directly related to the Arctic, they bring up interesting points that are relevant to interdisciplinary and social science research beyond a specific geographic scope. For more information, visit the site: https://onlineethics.org/cases/graduate-research-ethics-cases-and-commentaries-volume-1-1997/do-ends-justify-means-ethics Ann Smith is a social psychologist who wants to study attitude change. She submits a proposal to her institution outlining details of a study that will examine the attitude change of participants following a workshop on environmental issues. Smith plans to identify attitude change by administering a pretest and a post-test. She is worried, however, that the participants will recognize that she is looking for changes in their attitudes and that this knowledge will influence their answers on the pos-ttest. To address this problem, she plans to disguise the issues she is most interested in; when she administers the tests, she will give a very broad explanation that does not fully disclose the nature of the study. Her proposal includes these procedures and an explanation of why she believes they are necessary; she also includes a plan to “debrief” the subjects (tell them the real purpose of the study) after they finish taking the second test. Discussion Questions What might be the benefits of this research, if any? What risks to subjects, if any, do you identify? What issues should members of the Institutional Review Board (IRB)(1) raise regarding Smith’s proposal? If you were a member of the IRB, how would you weigh the benefits of the research with the risks to subjects in this case? Based on your assessment of the benefits and risks, would you approve Smith’s proposal as submitted? If not, what changes would you suggest? Is deception of subjects ever justifiable? If so, under what conditions? How might conducting experiments that involve deception of subjects affect the researcher? Is there any way in which such experiments could reflect upon science itself? If so, how? "],["data-publishing.html", "14 Data Publishing 14.1 Data Documentation and Publishing", " 14 Data Publishing 14.1 Data Documentation and Publishing 14.1.1 Learning Objectives In this lesson, you will learn: About open data archives, especially the Arctic Data Center What science metadata are and how they can be used How data and code can be documented and published in open data archives Web-based submission 14.1.2 Data sharing and preservation 14.1.3 Data repositories: built for data (and code) GitHub is not an archival location Examples of dedicated data repositories: KNB, Arctic Data Center, tDAR, EDI, Zenodo Rich metadata Archival in their mission Certification for repositories: https://www.coretrustseal.org/ Data papers, e.g., Scientific Data List of data repositories: http://re3data.org Repository finder tool: https://repositoryfinder.datacite.org/ 14.1.4 Metadata Metadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where they were collected, and why they were collected. For consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the metadata for a sockeye salmon dataset: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; That same metadata document can be converted to HTML format and displayed in a much more readable form on the web: https://knb.ecoinformatics.org/#view/doi:10.5063/F1F18WN4 And, as you can see, the whole dataset or its components can be downloaded and reused. Also note that the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data. 14.1.5 Structure of a data package Note that the dataset above lists a collection of files that are contained within the dataset. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document. These data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4,and each of the individual files have their own identifiers as well. 14.1.6 DataONE Federation DataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses. DataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the (currently 44!) repositories one at a time. 14.1.7 Publishing data from the web Each data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the Arctic Data Center, we provide some easy to use web forms for editing and submitting a data package. This section provides a brief overview of some highlights within the data submission process, in advance of a more comprehensive hands-on activity. ORCiDs We will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button. ORCID is a non-profit organization made up of research institutions, funders, publishers and other stakeholders in the research space. ORCID stands for Open Researcher and Contributor ID. The purpose of ORCID is to give researchers a unique identifier which then helps highlight and give credit to researchers for their work. If you click on someone’s ORCID, their work and research contributions will show up (as long as the researcher used ORCID to publish or post their work). After signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. Sensitive Data Handling Underneath the Title field, you will see a section titled “Data Sensitivity”. As the primary repository for the NSF Office of Polar Programs Arctic Section, the Arctic Data Center accepts data from all disciplines. This includes data from social science research that may include sensitive data, meaning data that contains personal or identifiable information. Sharing sensitive data can pose challenges to researchers, however sharing metadata or anonymized data contributes to discovery, supports open science principles, and helps reduce duplicate research efforts. To help mitigate the challenges of sharing sensitive data, the Arctic Data Center has added new features to the data submission process influenced by the CARE Principles for Indigenous Data Governance (Collective benefit, Authority to control, Responsibility, Ethics). Researchers submitting data now have the option to choose between varying levels of sensitivity that best represent their dataset. Data submitters can select one of three sensitivity level data tags that best fit their data and/or metadata. Based on the level of sensitivity, guidelines for submission are provided. The data tags range from non-confidential information to maximally sensitive information. The purpose of these tags is to ethically contribute to open science by making the richest set of data available for future research. The first tag, “non-sensitive data”, represents data that does not contain potentially harmful information, and can be submitted without further precaution. Data or metadata that is “sensitive with minimal risk” means that either the sensitive data has been anonymized and shared with consent, or that publishing it will not cause any harm. The third option, “some or all data is sensitive with significant risk” represents data that contains potentially harmful or identifiable information, and the data submitter will be asked to hold off submitting the data until further notice. In the case where sharing anonymized sensitive data is not possible due to ethical considerations, sharing anonymized metadata still aligns with FAIR (Findable, Accessible, Interoperable, Reproducible) principles because it increases the visibility of the research which helps reduce duplicate research efforts. Hence, it is important to share metadata, and to publish or share sensitive data only when consent from participants is given, in alignment with the CARE principles and any IRB requirements. You will continue to be prompted to enter information about your research, and in doing so, create your metadata record. We recommend taking your time because the richer your metadata is, the more easily reproducible and usable your data and research will be for both your future self and other researchers. Detailed instructions are provided below for the hands-on activity. Research Methods Methods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software. As part of a recent update, researchers are now asked to describe the ethical data practices used throughout their research. The information provided will be visible as part of the metadata record. This feature was added to the data submission process to encourage transparency in data ethics. Transparency in data ethics is a vital part of open science and sharing ethical practices encourages deeper discussion about data reuse and ethics. We encourage you to think about the ethical data and research practices that were utilized during your research, even if they don’t seem obvious at first. File and Variable Level Metadata In addition to providing information about, (or a description of) your dataset, you can also provide information about each file and the variables within the file. By clicking the “Describe” button you can add comprehensive information about each of your measurements, such as the name, measurement type, standard units etc. Provenance The data submission system also provides the opportunity for you to provide provenance information, describe the relationship between package elements. When viewing your dataset followinng submission, After completing your data description and submitting your dataset you will see the option to add source data and code, and derived data and code. These are just some of the features and functionality of the Arctic Data Center submission system and we will go through them in more detail below as part of a hands-on activity. 14.1.7.1 Download the data to be used for the tutorial I’ve already uploaded the test data package, and so you can access the data here: https://demo.arcticdata.io/#view/urn:uuid:0702cc63-4483-4af4-a218-531ccc59069f Grab both CSV files, and the R script, and store them in a convenient folder. 14.1.7.2 Login via ORCID We will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button. When you sign in, you will be redirected to orcid.org, where you can either provide your existing ORCID credentials or create a new account. ORCID provides multiple ways to login, including using your email address, an institutional login from many universities, and/or a login from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login. 14.1.7.3 Create and submit the dataset After signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. 14.1.7.3.1 Click Add Files to choose the data files for your package You can select multiple files at a time to efficiently upload many files. The files will upload showing a progress indicator. You can continue editing metadata while they upload. 14.1.7.3.2 Enter Overview information This includes a descriptive title, abstract, and keywords. You also must enter a funding award number and choose a license. The funding field will search for an NSF award identifier based on words in its title or the number itself. The licensing options are CC-0 and CC-BY, which both allow your data to be downloaded and re-used by other researchers. CC-0 Public Domain Dedication: “…can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.” CC-BY: Attribution 4.0 International License: “…free to…copy,…redistribute,…remix, transform, and build upon the material for any purpose, even commercially,…[but] must give appropriate credit, provide a link to the license, and indicate if changes were made.” 14.1.7.3.3 People Information Information about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people: Creators - all the people who should be in the citation for the dataset Contacts - one is required, but defaults to the first Creator if omitted Principal Investigators Any others that are relevant For each, please provide their ORCID identifier, which helps link this dataset to their other scholarly works. 14.1.7.3.4 Location Information The geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that spatial search will be more precise. Note that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global: Mission Canyon, Santa Barbara, California, USA 14.1.7.3.5 Temporal Information Add the temporal coverage of the data, which represents the time period to which data apply. Again, use multiple date ranges if your sampling was discontinuous. 14.1.7.3.6 Methods Methods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data. 14.1.7.3.7 Save a first version with Submit When finished, click the Submit Dataset button at the bottom. If there are errors or missing fields, they will be highlighted. Correct those, and then try submitting again. When you are successful, you should see a large green banner with a link to the current dataset view. Click the X to close that banner if you want to continue editing metadata. Success! 14.1.7.4 File and variable level metadata The final major section of metadata concerns the structure and content of your data files. In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure. For example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information. The Attributes tab is where you enter variable (aka attribute) information, including: variable name (for programs) variable label (for display) - variable definition (be specific) - type of measurement - units &amp; code definitions You’ll need to add these definitions for every variable (column) in the file. When done, click Done. Now, the list of data files will show a green checkbox indicating that you have fully described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes. After you get the big green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes. 14.1.7.5 Add workflow provenance Understanding the relationships between files (aka provenance) in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, which are often then used in analysis and visualization code to produce final outputs. In the DataONE network, we support structured descriptions of these relationships, so researchers can see the flow of data from raw data to derived to outputs. You add provenance by navigating to the data table descriptions and selecting the Add buttons to link the data and scripts that were used in your computational workflow. On the left side, select the Add circle to add an input data source to the filteredSpecies.csv file. This starts building the provenance graph to explain the origin and history of each data object. The linkage to the source dataset should appear. Then you can add the link to the source code that handled the conversion between the data files by clicking on Add arrow and selecting the R script: The diagram now shows the relationships among the data files and the R script, so click Submit to save another version of the package. Et voilà! A beautifully preserved data package! "],["reproducible-survey-workflows.html", "15 Reproducible Survey Workflows 15.1 Reproducible Survey Workflows", " 15 Reproducible Survey Workflows 15.1 Reproducible Survey Workflows 15.1.1 Learning Objectives Overview of survey instruments Best practices for survey data management Overview of data integration packages Practice in google forms integration through R 15.1.2 Survey instruments and methods Surveys and questionnaires are commonly used research methods within social science and other fields. For example, understanding regional and national population demographics, income, and education as part of the National Census activity, assessing audience perspectives on specific topics of research interest (e.g. the work by Tenopir and colleagues on Data Sharing by Scientists), evaluation of learning deliverable and outcomes, and consumer feedback on new and upcoming products. These are distinct from the use of the term survey within natural sciences, which might include geographical surveys (“the making of measurement in the field from which maps are drawn”), ecological surveys (“the process whereby a proposed development site is assess to establish any environmental impact the development may have”) or biodiversity surveys (“provide detailed information about biodiversity and community structure”) among others. In social science, a survey can be defined as: a research method involving the use of standardised questionnaires or interviews to collect data about people and their preferences, thoughts, and behaviours in a systematic manner. - Social Science Research: Principles, Methods and Practices There are a large number of platforms that provide opportunity for structured feedback and have specific use cases, outside of formalized research methods. For example, participant registration (e.g. Google forms, Aventri), presentation feedback (e.g. Mentimeter), scheduling (e.g. Doodle). In this section we will focus on software designed or commonly used for research activity. 15.1.3 Survey data best practices 15.1.4 Building workflows: Options for data integration 15.1.4.1 Qualtrics and qualtRics qualtRics (note the capital R) is an R package that enables retrieval of Qualtrics data directly from the Qualtrics platform and into R, reducing pre-processing time associated with data manipulation and import. Although Qualtrics is proprietary software, the qualtRics R package is open source and community developed. The package currently contains three core functions: all_surveys() fetches a list of all surveys that you own or have access to from Qualtrics. fetch_survey() downloads a survey from Qualtrics and loads it into R. read_survey() allows you to read CSV files you download manually from Qualtrics. Example of the functions in use: surveys &lt;- all_surveys() This will create a list of all survey names contained within your Qualtrics account, within the data frame surveys. The survey data have not yet been downloaded. You can download the data from one or more individual surveys of your choice using fetch_survey(). In the example below the researcher is bringing in a subset of data from survey number 4, based on the date of responses. mysurvey &lt;- fetch_survey(surveys$id[4], start_date = \"2018-10-01\", end_date = \"2018-10-31\", label = FALSE) If you have already downloaded a *.csv filed from Qualtrics, these can be read in from your computer with the read_survey() function. e.g.  mysurvey &lt;- read_survey(\"/users/aebudden/Desktop/workshop_feedback.csv\") Also contained within the package are the following helper functions: qualtrics_api_credentials() stores your API key and base url in environment variables. survey_questions() retrieves a data frame containing questions and question IDs for a survey; extract_colmap() retrieves a similar data frame with more detailed mapping from columns to labels. metadata() retrieves metadata about your survey, such as questions, survey flow, number of responses etc. qualtRics will only enable you to download and integrate survey data that you have administrative access to. Rest assured that someone else cannot use this package to read in your data. However, one limitation is that the package requires Qualtrics API access, which must be enabled in your account to use it. Depending on the type of account you have, this may require administrative approval from your brand ambassador for institutional accounts. 15.1.5 Google forms Google forms can be a great way to set up surveys, and it is very easy to interact with the results using R. The benefits of using google forms are a simple interface and easy sharing between collaborators, especially when writing the survey instrument. The downside is that google forms has far fewer features than Qualtrics in terms of survey flow and appearance. To show how we can link R into our survey workflows, I’ve set up a simple example survey here. I’ve set up the results so that they are in a new spreadsheet here:. To access them, we will use the googlesheets4 package. First, open up a new R script and load the googlesheets4 library: library(googlesheets4) Next, we can read the sheet in using the same URL that you would use to share the sheet with someone else. Right now, this sheet is public responses &lt;- read_sheet(&quot;https://docs.google.com/spreadsheets/d/1CSG__ejXQNZdwXc1QK8dKouxphP520bjUOnZ5SzOVP8/edit?usp=sharing&quot;) ## ✔ Reading from &quot;Example Survey Form (Responses)&quot;. ## ✔ Range &#39;Form Responses 1&#39;. The first time you run this, you should get a popup window in your web browser asking you to confirm that you want to provide access to your google sheets via the tidyverse (googlesheets) package. My dialog box looked like this: Make sure you click the third check box enabling the Tidyverse API to see, edit, create, and delete your sheets. Note that you will have to tell it to do any of these actions via the R code you write. When you come back to your R environment, you should have a data frame containing the data in your sheet! Bypassing authentication for public sheets If you don’t want to go through a little interactive dialog every time you read in a sheet, and your sheet is public, you can run the function gs4_deauth() to access the sheet as a public user. This is helpful for cases when you want to run your code non-interactively. This is actually how I set it up for this book to build! "],["text-analysis.html", "16 Text Analysis 16.1 Introduction to text analysis 16.2 Sentiment Analysis", " 16 Text Analysis 16.1 Introduction to text analysis 16.1.1 Learning Objectives What a token is and how they are used How to use stop words How to customize stop words Introduction to sentiment analysis 16.1.2 Introduction Much of the information covered in this chapter is based on Text Mining with R: A Tidy Approach by Julia Silge and David Robinson. This is a great book if you want to go deeper into text analysis. Text mining is the process by which unstructured text is transformed into a structured format to prepare it for analysis. This can range from the simple example we show in this lesson, to much more complicated processes such as using OCR (optical character recognition) to scan and extract text from pdfs, or web scraping. Once text is in a structured format, analysis can be performed on it. The inherent benefit of quantitative text analysis is that it is highly scalable. With the right computational techniques, massive quantities of text can be mined and analyzed many, many orders of magnitude faster than it would take a human to do the same task. The downside, is that human language is inherently nuanced, and computers (as you may have noticed) think very differently than we do. In order for an analysis to capture this nuance, the tools and techniques for text analysis need to be set up with care, especially when the analysis becomes more complex. There are a number of different types of text analysis. In this lesson we will show some simple examples of two: word frequency, and sentiment analysis. Setup First we’ll load the libraries we need for this lesson: library(dplyr) library(readr) library(tidytext) library(wordcloud) library(reshape2) Load the survey data back in using the code chunk below: survey_raw &lt;- read_csv(&quot;https://dev.nceas.ucsb.edu/knb/d1/mn/v2/object/urn%3Auuid%3A71cb8d0d-70d5-4752-abcd-e3bcf7f14783&quot;, show_col_types = FALSE) survey_clean &lt;- survey_raw %&gt;% select(-notes) %&gt;% mutate(Q1 = if_else(Q1 == &quot;1&quot;, &quot;below expectations&quot;, Q1)) %&gt;% mutate(Q2 = tolower(Q2)) events &lt;- read_csv(&quot;https://dev.nceas.ucsb.edu/knb/d1/mn/v2/object/urn%3Auuid%3A0a1dd2d8-e8db-4089-a176-1b557d6e2786&quot;, show_col_types = FALSE) survey_joined &lt;- left_join(survey_clean, events, by = &quot;StartDate&quot;) We are going to be working in the “tidy text format.” This format stipulates that the text column of our data frame contains rows with only one token per row. A token, in this case, is a meaningful unit of text. Depending on the analysis, that could be a word, two words, or phrase. First, let’s create a data frame with responses to question 3, with the one token per row. We use the unnest_tokens function from tidytext, after selecting columns of interest. q3 &lt;- survey_joined %&gt;% select(StartDate, location, Q3) %&gt;% unnest_tokens(output = word, input = Q3) You’ll see that we now have a very long data frame with only one word in each row of the text column. Some of the words aren’t so interesting though. The words that are likely not useful for analysis are called “stop words”. There is a list of stop words contained within the tidytext package and we can access it using the data function. We can then use the anti_join function to return only the words that are not in the stop word list. data(stop_words) q3 &lt;- anti_join(q3, stop_words) ## Joining, by = &quot;word&quot; Now, we can do normal dplyr analysis to examine the most commonly used words in question 3. The count function is helpful here. We could also do a group_by and summarize and get the same result. We can also arrange the results, and get the top 10 using slice_head. q3_top &lt;- q3 %&gt;% count(word) %&gt;% arrange(-n) %&gt;% slice_head(n = 10) Term frequency Right now, our counts of the most commonly used non-stop words are only moderately informative because they don’t take into context how many other words, responses, and courses there are. A widely used metric to analyze and draw conclusions from word frequency, including frequency within documents (or courses, in our case) is called tf-idf. This is the term frequency (number of appearances of a term divided by total number of terms), multiplied by the inverse document frequency (the natural log of the number of documents divided by the number of documents containing the term). The tidytext book has great examples on how to calculate this metric easily using some built in functions to the package. Let’s do the same workflow for question 4: q4 &lt;- survey_joined %&gt;% select(StartDate, location, Q4) %&gt;% unnest_tokens(output = word, input = Q4) %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; q4_top &lt;- q4 %&gt;% count(word) %&gt;% arrange(-n) %&gt;% slice_head(n = 10) Perhaps not surprisingly, the word data is mentioned a lot! In this case, it might be useful to add it to our stop words list. You can create a data.frame in place with your word, and an indication of the lexicon (in this case, your own, which we can call custom). Then we use rbind to bind that data frame with our previous stop words data frame. custom_words &lt;- data.frame(word = &quot;data&quot;, lexicon = &quot;custom&quot;) stop_words_full &lt;- rbind(stop_words, custom_words) Now we can run our question 4 analysis again, with the anti_join on our custom list. q4 &lt;- survey_joined %&gt;% select(StartDate, location, Q4) %&gt;% unnest_tokens(output = word, input = Q4) %&gt;% anti_join(stop_words_full) ## Joining, by = &quot;word&quot; q4_top &lt;- q4 %&gt;% count(word) %&gt;% arrange(-n) %&gt;% slice_head(n = 10) 16.2 Sentiment Analysis In sentiment analysis, tokens (in this case our single words) are evaluated against a dictionary of words where a sentiment is assigned to the word. There are many different sentiment lexicons, some with single words, some with more than one word, and some that are aimed at particular disciplines. When embarking on a sentiment analysis project, choosing your lexicon is one that should be done with care. Sentiment analysis can also be done using machine learning algorithms. With that in mind, we will next do a very simple sentiment analysis on our Q3 and Q4 answers using the bing lexicon from Bing Liu and collaborators, which ships with the tidytext package. First we will use the get_sentiments function to load the lexicon. bing &lt;- get_sentiments(&quot;bing&quot;) Next we do an inner join to return the words from question 3 that are contained within the lexicon. q3_sent &lt;- inner_join(q3, bing, by = &quot;word&quot;) DT::datatable(q3_sent) There are a variety of directions you could go from here, analysis wise, such as calculating an overall sentiment index for that question, plotting sentiment against some other variable, or, making a fun word cloud like below! Here we bring in reshape2::acast to create a sentiment matrix for each word, and pass that into wordcloud::comparison.cloud to look at a wordcloud that indicates the frequency and sentiment of the words in our responses. q3_sent %&gt;% count(word, sentiment, sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = 100, title.size = 2) Let’s look at the question 4 word cloud: q4 %&gt;% inner_join(bing, by = &quot;word&quot;) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = 100, title.size = 2) "],["data-visualization.html", "17 Data Visualization 17.1 Publication Graphics", " 17 Data Visualization 17.1 Publication Graphics 17.1.1 Learning Objectives In this lesson, you will learn: The basics of the ggplot2 package to create static plots How to use ggplot2’s theming abilities to create publication-grade graphics 17.1.2 Overview ggplot2 is a popular package for visualizing data in R. From the home page: ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. It’s been around for years and has pretty good documentation and tons of example code around the web (like on StackOverflow). This lesson will introduce you to the basic components of working with ggplot2. ggplot vs base vs lattice vs XYZ… R provides many ways to get your data into a plot. Three common ones are, “base graphics” (plot(), hist(), etc`) lattice ggplot2 All of them work! I use base graphics for simple, quick and dirty plots. I use ggplot2 for most everything else. ggplot2 excels at making complicated plots easy and easy plots simple enough. Setup Open a new RMarkdown document and remove the filler text. First, let’s load the packages we’ll need: library(dplyr) library(tidyr) library(ggplot2) library(viridis) library(scales) Let’s get set up to where we were at the end of the data tidying lesson. In that lesson, we walked through this chunk of code to read in our survey data, tidy it up, and join it to the events data. survey_raw &lt;- read_csv(&quot;https://dev.nceas.ucsb.edu/knb/d1/mn/v2/object/urn%3Auuid%3A71cb8d0d-70d5-4752-abcd-e3bcf7f14783&quot;, show_col_types = FALSE) survey_clean &lt;- survey_raw %&gt;% select(-notes) %&gt;% mutate(Q1 = if_else(Q1 == &quot;1&quot;, &quot;below expectations&quot;, Q1)) %&gt;% mutate(Q2 = tolower(Q2)) events &lt;- read_csv(&quot;https://dev.nceas.ucsb.edu/knb/d1/mn/v2/object/urn%3Auuid%3A0a1dd2d8-e8db-4089-a176-1b557d6e2786&quot;, show_col_types = FALSE) survey_joined &lt;- left_join(survey_clean, events, by = &quot;StartDate&quot;) 17.1.3 Static figures using ggplot2 Every graphic you make in ggplot2 will have at least one aesthetic and at least one geom (layer). The aesthetic maps your data to your geometry (layer). Your geometry specifies the type of plot we’re making (point, bar, etc.). Now, let’s plot our results using ggplot. ggplot uses a mapping aesthetic (set using aes()) and a geometry to create your plot. Additional geometries/aesthetics and theme elements can be added to a ggplot object using +. Let’s start by just plotting the answers to question 1. geom_bar will automatically count them for us. ggplot(survey_joined, aes(x = location)) + geom_bar() What if we want our bars to be blue instad of gray? You might think we could run this: ggplot(survey_joined, aes(x = location, fill = &quot;blue&quot;)) + geom_bar() Why did that happen? Notice that we tried to set the fill color of the plot inside the mapping aesthetic call. What we have done, behind the scenes, is create a column filled with the word “blue” in our dataframe, and then mapped it to the fill aesthetic, which then chose the default fill color of red. What we really wanted to do was just change the color of the bars. If we want do do that, we can call the color option in the geom_bar function, outside of the mapping aesthetics function call. ggplot(survey_joined, aes(x = location)) + geom_bar(fill = &quot;blue&quot;) What if we did want to map the color of the bars to a variable, such as location of the training. ggplot is really powerful because we can easily get this plot to visualize more aspects of our data. ggplot(survey_joined, aes(x = location, fill = Q1)) + geom_bar() If we want to make our bar chart show what percentage of repondents gave each answer, we can switch our bar chart by setting the position argument within geom_bar to fill. ggplot(survey_joined, aes(x = location, fill = Q1)) + geom_bar(position = &quot;fill&quot;) Setting ggplot themes Now let’s work on making this plot look a bit nicer. Add labels using the labs function, and include a built in theme using theme_bw(). There are a wide variety of built in themes in ggplot that help quickly set the look of the plot. Use the RStudio autocomplete theme_ &lt;TAB&gt; to view a list of theme functions. ggplot(survey_joined, aes(x = location, fill = Q1)) + geom_bar(position = &quot;fill&quot;) + labs(title = &quot;To what degree did the course meet expectations?&quot;, x = &quot;&quot;, y = &quot;Percent of Responses&quot;, fill = &quot;Response&quot;) + theme_bw() You can see that the theme_bw() function changed a lot of the aspects of our plot! The background is white, the grid is a different color, etc. There are lots of other built in themes like this that come with the ggplot2 package. Challenge Use the RStudio autocomplete, the ggplot2 documentation, a cheatsheet, or good old google to find other built in themes. Pick out your favorite one and add it to your plot. The built in theme functions change the default settings for many elements that can also be changed invididually using thetheme() function. The theme() function is a way to further fine-tune the look of your plot. This function takes MANY arguments (just have a look at ?theme). Luckily there are many great ggplot resources online so we don’t have to remember all of these, just google “ggplot cheatsheet” and find one you like. Let’s look at an example of a theme call, where we change the rotation of the x axis tick labels. ggplot(survey_joined, aes(x = location, fill = Q1)) + geom_bar(position = &quot;fill&quot;) + labs(title = &quot;To what degree did the course meet expectations?&quot;, x = &quot;&quot;, y = &quot;Percent of Responses&quot;, fill = &quot;Response&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) Note that the theme() call needs to come after any built in themes like theme_bw() are used. Otherwise, theme_bw() will likely override any theme elements that you changed using theme(). You can also save the result of a series of theme() function calls to an object to use on multiple plots. This prevents needing to copy paste the same lines over and over again! my_theme &lt;- theme_bw() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) ggplot(survey_joined, aes(x = location, fill = Q1)) + geom_bar(position = &quot;fill&quot;) + labs(title = &quot;To what degree did the course meet expectations?&quot;, x = &quot;&quot;, y = &quot;Percent of Responses&quot;, fill = &quot;Response&quot;) + my_theme Ordering our responses using factors You might notice that our tick labels don’t appear in a very intuitive order. We would probably prefer to order these so that we have them in order of below, just below, met, just above, and exceeded. We can do this using factors. A factor in R is a class of data (like numeric or character) that represents data that contain a fixed and known set of values. Factors are often used in survey data, like the example we are using, as two of our four questions have a set of known values. We can use the factor function to transform our character vector of Q1 responses into a factor, and the levels argument to order the levels. To do this, we will combine it with the mutate function call. survey_joined &lt;- survey_joined %&gt;% mutate(Q1 = factor(Q1, levels = c(&quot;exceeded expectations&quot;, &quot;just above expectations&quot;, &quot;met expectations&quot;, &quot;just below expectations&quot;, &quot;below expectations&quot;))) ggplot(survey_joined, aes(x = location, fill = Q1)) + geom_bar(position = &quot;fill&quot;) + labs(title = &quot;To what degree did the course meet expectations?&quot;, x = &quot;&quot;, y = &quot;Percent of Responses&quot;, fill = &quot;Response&quot;) + my_theme One last thing we might want to do is to set a nicer color palette. Here we use scale_fill_viridis from the viridis package. We can also add scale_y_continuous with the labels = percent argument to set the y tick labels in percent format. ggplot(survey_joined, aes(x = location, fill = Q1)) + geom_bar(position = &quot;fill&quot;) + scale_fill_viridis(discrete = TRUE) + scale_y_continuous(labels = percent) + labs(title = &quot;To what degree did the course meet expectations?&quot;, x = &quot;&quot;, y = &quot;Percent of Responses&quot;, fill = &quot;Response&quot;) + my_theme 17.1.3.1 Saving plots Saving plots using ggplot is easy! The ggsave function will save either the last plot you created, or any plot that you have saved to a variable. You can specify what output format you want, size, resolution, etc. ggsave(&quot;question1.png&quot;, width = 3, height = 3, units = &quot;in&quot;) Creating multiple plots survey_long &lt;- survey_joined %&gt;% pivot_longer(cols = starts_with(&quot;Q&quot;), names_to = &quot;id&quot;, values_to = &quot;answer&quot;) %&gt;% filter(id %in% c(&quot;Q1&quot;, &quot;Q2&quot;)) What if we wanted to generate a plot for every question with a controlled vocabulary? A fast way to do this uses the function facet_wrap(). This function takes a mapping to a variable using the syntax ~variable_name. The ~ (tilde) is a model operator which tells facet_wrap to model each unique value within variable_name to a facet in the plot. The default behavior of facet wrap is to put all facets on the same x and y scale. You can use the scales argument to specify whether to allow different scales between facet plots. Here, we free the both scales. Note that we have had to modify our axes from the previous plots. We now have answer on the y axis and fill as the location, since our answers have different answers. ggplot(survey_long, aes(x = answer, fill = location)) + geom_bar(position = &quot;dodge&quot;) + scale_fill_viridis(discrete = TRUE) + labs(x = &quot;&quot;, y = &quot;Number of Responses&quot;, fill = &quot;Location&quot;) + facet_wrap(~id, scales = &quot;free&quot;) + my_theme 17.1.4 Resources Lisa Charlotte Rost. (2018) Why not to use two axes, and what to use instead: The case against dual axis charts "],["data-portals.html", "18 Data Portals 18.1 Creating a data portal", " 18 Data Portals 18.1 Creating a data portal A new feature on the Arctic Data Center. Researchers can now easily view project information and datasets all in one place. 18.1.1 What is a Portal? A portal is a collection of Arctic Data Center data packages on a unique webpage. Typically, a research project’s website won’t be maintained beyond the life of the project and all the information on the website that provides context for the data collection is lost. Arctic Data Center portals can provide a means to preserve information regarding the projects’ objectives, scopes, and organization and couple this with the data files so it’s clear how to use and interpret the data for years to come. Portals also leverage Arctic Data Center’s metric features, which create statistics describing the project’s data packages. Information such as total size of data, proportion of data file types, and data collection periods are immediately available from the portal webpage. 18.1.2 Portal Uses Portals allow users to bundle supplementary information about their group, data, or project along with the data packages. Data contributors can organize their project specific data packages into a unique portal and customize the portal’s theme and structure according to the needs of that project. Researchers can also use portals to compare their public data packages and highlight and share them with other teams, as well as the broader Arctic research audience. To see an example of a portal, please view the Toolik Field Station’s portal. 18.1.3 Portal Features Published portals vary in their design according to the needs and preferences of the individual or group. However, when constructing a portal there are three core elements: a data page, a metrics page, and customizable free-form pages. 18.1.3.1 Flexible Content Creation Portals can be constructed as a website providing information about the research and products. Using our flexible ‘free-form’ pages (written in markdown), you can add and re-order pages to meet your needs. These pages might be used as a home, or ‘landing’ page with general project information. They are also used to showcase research products, and communicate news and upcoming events. 18.1.3.2 Curated Collections of Data The data page is arguably the most important component of the Arctic Data Center portal system. This is where users will display the data packages of their choice. Whether these data reflect research products from your research group (collated based on contributor IDs), or thematic areas of research interest (collated based on keyword and search terms), will depend upon the intended use of the portal but in all cases, you are refining the suite of data viewed by your audience. The data page looks and performs just like the main Arctic Data Center catalog - with some added bonuses, see below. 18.1.3.3 Customized Search Capabilities You can also build more precise search capabilities into your portal, leveraging the rich metadata associated with data products preserved at the Arctic Data Center. For example, in the example below the research group have identified seven primary search categories and within these, enable users to search within specific metadata fields or else select from a drop down list of options. In doing so, your audience can construct refined search queries drilling down to the data of interest. (Note that although the SASAP portal is hosted by DataONE, the same functionality exists at the Arctic Data Center. More on this later). 18.1.3.4 Metrics, Metrics, Metrics As with the full Arctic Data Center catalog, we aggregate metrics for the collection of data packages within a portal. This page is not customizable - it comes as a default with the portal - but you can choose to delete it. The metrics provided include a summary of the holdings: number, volume, time period, format of datasets, metadata assessment scores, citations across all packages, and counts of downloads and views. These latter metrics can be particularly useful if wanting to track the usage or reach of your project or group’s activities. 18.1.3.5 Relationship between Arctic Data Center portals and DataONE Both DataONE and the Arctic Data Center use Metacat and MetacatUI software and both have the capability for individuals and groups to develop portals. This is true of other repositories running this software. The difference between a portal at teh Arctic Data Center and one through DataONE is the corpus of data that can be pulled into your portal. Arctic Data Center portals expose only data held in the Arctic Data Center repository. A portal in DataONE can expose data from across the full network of data repositories - including DataONE. This is particularly useful for interdisciplinary research projects, labs that have published data to multiple repositories etc. However, unlike at the Arctic Data Center, there is a cost associated with a DataONE portal as they are part of the organizations sustainability model. 18.1.4 Creating Portals A step-by-step guide on how to navigate the Arctic Data Center and create a new portal. For video tutorials on how to create your first portal, please visit the Arctic Data Center’s website. 18.1.4.1 Getting Started with Portals If you are on the Arctic Data Center’s primary website, select the button on the top right titled ‘Create Portal’, this will take you to sign in with your ORCID id if you are not already signed in. Sign in with your ORCID, which will then take you directly to the page where you can start customizing your portal. You can also get to the page to create a new portal by clicking on your name in the upper right hand corner when you are signed in to the Arctic Data Center with your ORCID. A dropdown will appear, and you would select ‘My Portals’. On your profile settings page, select ‘My Portals’. After the page loads select the green button ‘+ New Portal’ to add a new portal, you’ll automatically be directed to a fresh edit session. 18.1.4.2 Portal Settings Page In a new edit session, the first page you’ll be taken to is the settings page where you’ll be able to add details about your portal. Portal URL Identify a short name for your portal that will become part of the URL. If the name is available, a label will indicate it’s available and if the name is taken already, it will note that the name is already taken. This feature ensures the portals are unique. Portal description Sharing options For the purposes of this training, please leave your portal in ‘Private’ status. You are welcome to return and make the portal public when the portal is complete and is useful to you and others. Permissions Adding collaborators to help you create your portal is as straightforward as copying and pasting in their ORCID into the box below the permissions section. You can choose whether the collaborator can view, edit, or is an owner of the portal. You can have multiples of each role. Partner Logos 18.1.5 Adding Data to Portals When selecting the data tab you will see a page with two sections. These are titled with instructive statements to help explain their function: Add filters to help people find data within your collection build search features for others to use within your portal Add data to your collection construct a search to populate your portal with data We’re going to start with the second section. When adding data to your collection, you can include any of the datasets that are available at the Arctic Data Center. You build rules based on metadata to define which datasets should be included. Of course, where metadata is incomplete or abbreviated, this will impact your results. Data added to the network in the future that match these rules will also be added to your collection. The first thing we notice is the ability to include/exclude data based on all/any metadata criteria. This setting applies across all rules. In the default view you have the starrtig point for a single rule, and a view showing 0 datasets. As we build rules, the page will refresh to show how many datasets are included based on your rule structure. You can continue to add rules (and rule groups) to create complex queries that are specific to your needs. The metadata fields available for rule construction are easily visible in the dropdown option, and grouped for ease of use. You also have the option to define ‘any metadata field’ though your results may be less precise. 18.1.6 Building Search Options for Your Audience This section covers the first part of the ‘data’ page. “Add filters to help people find data within your collection”. Although it appears first, I recommend constructing these filters after you have defined your data as you will have a better undertsanding of the metadata field that are relevant to your portal collection. It appears at the top of this editor page as when published, it will be at the top for users. hence the editor page reflects the layout of the published page. When selecting “add a search filter” you will be presented with a pop-up that comprises three primary elements. The metadata filed you will be querying The way in which you want the user to interact with that metadata Language settings for the filter you are building As you select options for number 1 - the metadata field, the pop-up will refresh to show only those relevant options. Save this filter to close the pop-up, return to the main editor and add another search filter. 18.1.6.1 Data Package Metrics As stated above, the metrics page is a default function provided by the Arctic Data Center. This page cannot be edited and cannot be viewed while editing. Users do have the option to delete the page if they’d like. To delete the page, select the arrow next to the word ‘Metrics’ in the tab and choose ‘Delete’ from the dropdown list. You can alwys change your mind and add a metrics page with the ‘+’ tab. To see metric summaries, navigate to your portal in view mode. See Saving and Editing Portals for more information on how to view portals. 18.1.7 Creating Unique Freeform Pages To watch a tutorial on creating a new freeform page see this video:Creating a Freeform Text Page To add a freeform page to a portal, select the ‘+’ tab next to the data and metric tabs and then choose the freeform option that appears on screen. A freeform page will then populate. Easily customize your banner with a unique image, title, and page description. To change the name of the tab, click on the arrow in the ‘Untitled’ tab and select ‘Rename’ from the dropdown list. Below the banner, there is a markdown text box with some examples on how to use the markdown formatting directives to customize the text display. There is also a formatting header at the top to assist if you’re unfamiliar with markdown. As you write, toggle through the Edit and Preview modes in the markdown text box to make sure your information is displaying as intended. Portals are flexible and can accommodate as many additional freeform pages as needed. The markdown header structure helps to generate the table of contents for the page. Please see these additional resources for help with markdown: Markdown reference Ten minute tutorial For a longer example where you can also preview the results, checkout the Showdown Live Editor 18.1.8 Saving and Editing Portals Be sure to save your portal when you complete a page to ensure your progress is retained. Whenever a portal is saved, a dialogue box will pop up at the top of the page prompting users to view their private portal in view mode. You can choose to ignore this and continue editing. To delete a page from your portal, select the arrow in the tab and choose ‘Delete’ from the dropdown. Users can view and edit their portal from their ‘My Portals’ tab. First, click the arrow your name in the top-right corner to drop down your menu options. Then, select ‘My Portals’ from the dropdown underneath your name. See the section on Getting Started with Portals for more details. Click on the portal title to view it or select the edit button to make changes. 18.1.9 How to Publish Portals New portals are automatically set to private and only visible to the portal creator. The portal will remain private until the owner decides to make it public. To make your portal public, go into the settings of your portal. Under the description, you’ll see a new section called ‘Sharing Options’. You can toggle between your portal being private and your portal being public there. 18.1.10 Sharing Portals In order to share a published portal, users can simply direct recipients to their unique identifier. Portal identifiers are embedded into the Arctic Data Center’s portal URL: https://arcticdata.io/catalog/portals/portal-identifier To view or edit your portal identifier, go into edit mode in your portal. In the settings page, your portal URL will be the first item on the page. 18.1.11 Tutorial Videos For video tutorials on how to create your first portal, please visit the Arctic Data Center video tutorial page. 18.1.12 Acknowledgements Much of this documentation was composed by ESS-DIVE, which can be found here. "],["geospatial-analysis.html", "19 Geospatial Analysis 19.1 Spatial vector analysis using sf", " 19 Geospatial Analysis 19.1 Spatial vector analysis using sf 19.1.1 Learning Objectives In this lesson, you will learn: How to use the sf package to analyze geospatial data Static mapping with ggplot interactive mapping with leaflet 19.1.2 Introduction From the sf vignette: Simple features or simple feature access refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects. It also describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them. The sf package is an R implementation of Simple Features. This package incorporates: a new spatial data class system in R functions for reading and writing data tools for spatial operations on vectors Most of the functions in this package starts with prefix st_ which stands for spatial and temporal. In this tutorial, our goal is to use a shapefile of Alaska regions and data on population in Alaska by community to create a map that looks like this: The data we will be using to create the map are: Alaska regional boundaries Community locations and population Alaksa rivers 19.1.3 Working with geospatial data All of the data used in this tutorial are simplified versions of real datasets available on the KNB. I’ve simplified the original high-resolution geospatial datasets to ease the processing burden on your computers while learning how to do the analysis. These simplified versions of the datasets may contain topological errors. The original version of the datasets are indicated throughout the chapter. Setup For convience, I’ve hosted a zipped copy of all of the files on our test site. Follow these steps to get ready for the next exercise: Navigate to this dataset and download the zip folder. Create a new folder in your training_username project called “shapefiles” Click the “upload” button on RStudio server, and upload the file to the shapefiles directory Open your .gititnore file and add that directory to the list of things to ignore. Save and commit the changes to your .gitignore. The first file we will use is a shapefile of regional boundaries in alaska derived from: Jared Kibele and Jeanette Clark. 2018. State of Alaska’s Salmon and People Regional Boundaries. Knowledge Network for Biocomplexity. doi:10.5063/F1125QWP. Now we can load the libraries we need: library(sf) library(ggplot2) library(leaflet) library(scales) library(ggmap) library(dplyr) Read in the data and look at a plot of it. ## Read in shapefile using sf ak_regions &lt;- read_sf(&quot;shapefiles/ak_regions_simp.shp&quot;) plot(ak_regions) We can also examine it’s class. class(ak_regions) ## [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; sf objects usually have two types - sf and data.frame. Two main differences comparing to a regular data.frame object are spatial metadata (geometry type, dimension, bbox, epsg (SRID), proj4string) and additional column - typically named geometry. Since our shapefile object has the data.frame class, viewing the contents of the object using the head function shows similar results to data we read in using read.csv. head(ak_regions) ## Simple feature collection with 6 features and 3 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -179.2296 ymin: 51.15702 xmax: 179.8567 ymax: 71.43957 ## Geodetic CRS: WGS 84 ## # A tibble: 6 × 4 ## region_id region mgmt_area geometry ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 1 Aleutian Islands 3 (((-171.1345 52.44974, -171.1686 52.4174… ## 2 2 Arctic 4 (((-139.9552 68.70597, -139.9893 68.7051… ## 3 3 Bristol Bay 3 (((-159.8745 58.62778, -159.8654 58.6137… ## 4 4 Chignik 3 (((-155.8282 55.84638, -155.8049 55.8655… ## 5 5 Copper River 2 (((-143.8874 59.93931, -143.9165 59.9403… ## 6 6 Kodiak 3 (((-151.9997 58.83077, -152.0358 58.8271… Coordinate Reference System Every sf object needs a coordinate reference system (or crs) defined in order to work with it correctly. A coordinate reference system contains both a datum and a projection. The datum is how you georeference your points (in 3 dimensions!) onto a spheroid. The projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. All coordinate reference systems require a datum. However, some coordinate reference systems are “unprojected” (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984. ESRI has a blog post that explains these concepts in more detail with very helpful diagrams and examples. You can view what crs is set by using the function st_crs st_crs(ak_regions) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;latitude&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;longitude&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] This is pretty confusing looking. Without getting into the details, that long string says that this data has a greographic coordinate system (WGS84) with no projection. A convenient way to reference crs quickly is by using the EPSG code, a number that represents a standard projection and datum. You can check out a list of (lots!) of EPSG codes here. We will use several EPSG codes in this lesson. Here they are, along with their more readable names: 3338: Alaska Albers 4326: WGS84 (World Geodetic System 1984), used in GPS 3857: Pseudo-Mercator, used in Google Maps, OpenStreetMap, Bing, ArcGIS, ESRI You will often need to transform your geospatial data from one coordinate system to another. The st_transform function does this quickly for us. You may have noticed the maps above looked wonky because of the dateline. We might want to set a different projection for this data so it plots nicer. A good one for Alaska is called the Alaska Albers projection, with an EPSG code of 3338. ak_regions_3338 &lt;- ak_regions %&gt;% st_transform(crs = 3338) st_crs(ak_regions_3338) ## Coordinate Reference System: ## User input: EPSG:3338 ## wkt: ## PROJCRS[&quot;NAD83 / Alaska Albers&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;Alaska Albers (meters)&quot;, ## METHOD[&quot;Albers Equal Area&quot;, ## ID[&quot;EPSG&quot;,9822]], ## PARAMETER[&quot;Latitude of false origin&quot;,50, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8821]], ## PARAMETER[&quot;Longitude of false origin&quot;,-154, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8822]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,55, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,65, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8824]], ## PARAMETER[&quot;Easting at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8826]], ## PARAMETER[&quot;Northing at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8827]]], ## CS[Cartesian,2], ## AXIS[&quot;easting (X)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;northing (Y)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Topographic mapping (small scale).&quot;], ## AREA[&quot;United States (USA) - Alaska.&quot;], ## BBOX[51.3,172.42,71.4,-129.99]], ## ID[&quot;EPSG&quot;,3338]] plot(ak_regions_3338) Much better! 19.1.4 sf &amp; the Tidyverse sf objects can be used as a regular data.frame object in many operations. We already saw the results of plot and head. Challenge Try running some other functions you might use to explore a regular data.frame on your sf flavored data.frame. Since sf objects are data.frames, they play nicely with packages in the tidyverse. Here are a couple of simple examples: select() ak_regions_3338 %&gt;% select(region) ## Simple feature collection with 13 features and 1 field ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -2175328 ymin: 405653 xmax: 1579226 ymax: 2383770 ## Projected CRS: NAD83 / Alaska Albers ## # A tibble: 13 × 2 ## region geometry ## &lt;chr&gt; &lt;MULTIPOLYGON [m]&gt; ## 1 Aleutian Islands (((-1156666 420855.1, -1159837 417990.3, -1161898 41694… ## 2 Arctic (((571289.9 2143072, 569941.5 2142691, 569158.2 2142146… ## 3 Bristol Bay (((-339688.6 973904.9, -339302 972297.3, -339229.2 9710… ## 4 Chignik (((-114381.9 649966.8, -112866.8 652065.8, -108836.8 65… ## 5 Copper River (((561012 1148301, 559393.7 1148169, 557797.7 1148492, … ## 6 Kodiak (((115112.5 983293, 113051.3 982825.9, 110801.3 983211.… ## 7 Kotzebue (((-678815.3 1819519, -677555.2 1820698, -675557.8 1821… ## 8 Kuskokwim (((-1030125 1281198, -1029858 1282333, -1028980 1284032… ## 9 Cook Inlet (((35214.98 1002457, 36660.3 1002038, 36953.11 1001186,… ## 10 Norton Sound (((-848357 1636692, -846510 1635203, -840513.7 1632225,… ## 11 Prince William Sound (((426007.1 1087250, 426562.5 1088591, 427711.6 1089991… ## 12 Southeast (((1287777 744574.1, 1290183 745970.8, 1292940 746262.7… ## 13 Yukon (((-375318 1473998, -373723.9 1473487, -373064.8 147393… Note the sticky geometry column! The geometry column will stay with your sf object even if it is not called explicitly. filter() ak_regions_3338 %&gt;% filter(region == &quot;Southeast&quot;) ## Simple feature collection with 1 feature and 3 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 559475.7 ymin: 722450 xmax: 1579226 ymax: 1410576 ## Projected CRS: NAD83 / Alaska Albers ## # A tibble: 1 × 4 ## region_id region mgmt_area geometry ## * &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [m]&gt; ## 1 12 Southeast 1 (((1287777 744574.1, 1290183 745970.8, 1292940 … Joins You can also use the sf package to create spatial joins, useful for when you want to utilize two datasets together. As an example, let’s ask a question: how many people live in each of these Alaska regions? We have some population data, but it gives the number of people by city, not by region. To determine the number of people per region we will need to: read in the city data from a csv and turn it into an sf object use a spatial join (st_join) to assign each city to a region use group_by and summarize to calculate the total population by region First, read in the population data as a regular data.frame. This data is derived from: Jeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018. Languages used in Alaskan households, 1990-2015. Knowledge Network for Biocomplexity. doi:10.5063/F11G0JHX. Unnecessary columns were removed and the most recent year of data was selected. pop &lt;- read.csv(&quot;shapefiles/alaska_population.csv&quot;) The st_join function is a spatial left join. The arguments for both the left and right tables are objects of class sf which means we will first need to turn our population data.frame with latitude and longitude coordinates into an sf object. We can do this easily using the st_as_sf function, which takes as arguments the coordinates and the crs. The remove = F specification here ensures that when we create our geometry column, we retain our original lat lng columns, which we will need later for plotting. Although it isn’t said anywhere explicitly in the file, let’s assume that the coordinate system used to reference the latitude longitude coordinates is WGS84, which has a crs number of 4236. pop_4326 &lt;- st_as_sf(pop, coords = c(&#39;lng&#39;, &#39;lat&#39;), crs = 4326, remove = F) head(pop_4326) ## Simple feature collection with 6 features and 5 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -176.6581 ymin: 51.88 xmax: -154.1703 ymax: 62.68889 ## Geodetic CRS: WGS 84 ## year city lat lng population geometry ## 1 2015 Adak 51.88000 -176.6581 122 POINT (-176.6581 51.88) ## 2 2015 Akhiok 56.94556 -154.1703 84 POINT (-154.1703 56.94556) ## 3 2015 Akiachak 60.90944 -161.4314 562 POINT (-161.4314 60.90944) ## 4 2015 Akiak 60.91222 -161.2139 399 POINT (-161.2139 60.91222) ## 5 2015 Akutan 54.13556 -165.7731 899 POINT (-165.7731 54.13556) ## 6 2015 Alakanuk 62.68889 -164.6153 777 POINT (-164.6153 62.68889) Now we can do our spatial join! You can specify what geometry function the join uses (st_intersects, st_within, st_crosses, st_is_within_distance, …) in the join argument. The geometry function you use will depend on what kind of operation you want to do, and the geometries of your shapefiles. In this case, we want to find what region each city falls within, so we will use st_within. pop_joined &lt;- st_join(pop_4326, ak_regions_3338, join = st_within) This gives an error! Error: st_crs(x) == st_crs(y) is not TRUE Turns out, this won’t work right now because our coordinate reference systems are not the same. Luckily, this is easily resolved using st_transform, and projecting our population object into Alaska Albers. pop_3338 &lt;- st_transform(pop_4326, crs = 3338) pop_joined &lt;- st_join(pop_3338, ak_regions_3338, join = st_within) head(pop_joined) ## Simple feature collection with 6 features and 8 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -1537925 ymin: 472626.9 xmax: -10340.71 ymax: 1456223 ## Projected CRS: NAD83 / Alaska Albers ## year city lat lng population region_id region ## 1 2015 Adak 51.88000 -176.6581 122 1 Aleutian Islands ## 2 2015 Akhiok 56.94556 -154.1703 84 6 Kodiak ## 3 2015 Akiachak 60.90944 -161.4314 562 8 Kuskokwim ## 4 2015 Akiak 60.91222 -161.2139 399 8 Kuskokwim ## 5 2015 Akutan 54.13556 -165.7731 899 1 Aleutian Islands ## 6 2015 Alakanuk 62.68889 -164.6153 777 13 Yukon ## mgmt_area geometry ## 1 3 POINT (-1537925 472626.9) ## 2 3 POINT (-10340.71 770998.4) ## 3 4 POINT (-400885.5 1236460) ## 4 4 POINT (-389165.7 1235475) ## 5 3 POINT (-766425.7 526057.8) ## 6 4 POINT (-539724.9 1456223) Challenge Like we mentioned above, there are many different types of joins you can do with geospatial data. Examine the help page for these joins (?st_within will get you there). What other joins types might be appropriate for examining the relationship between points and polygyons? What about two sets of polygons? Group and summarize Next we compute the total population for each region. In this case, we want to do a group_by and summarise as this were a regular data.frame - otherwise all of our point geometries would be included in the aggreation, which is not what we want. Our goal is just to get the total population by region. We remove the sticky geometry using as.data.frame, on the advice of the sf::tidyverse help page. pop_region &lt;- pop_joined %&gt;% as.data.frame() %&gt;% group_by(region) %&gt;% summarise(total_pop = sum(population)) head(pop_region) ## # A tibble: 6 × 2 ## region total_pop ## &lt;chr&gt; &lt;int&gt; ## 1 Aleutian Islands 8840 ## 2 Arctic 8419 ## 3 Bristol Bay 6947 ## 4 Chignik 311 ## 5 Cook Inlet 408254 ## 6 Copper River 2294 And use a regular left_join to get the information back to the Alaska region shapefile. Note that we need this step in order to regain our region geometries so that we can make some maps. pop_region_3338 &lt;- left_join(ak_regions_3338, pop_region) ## Joining, by = &quot;region&quot; #plot to check plot(pop_region_3338[&quot;total_pop&quot;]) So far, we have learned how to use sf and dplyr to use a spatial join on two datasets and calculate a summary metric from the result of that join. The group_by and summarize functions can also be used on sf objects to summarize within a dataset and combine geometries. Many of the tidyverse functions have methods specific for sf objects, some of which have additional arguments that wouldn’t be relevant to the data.frame methods. You can run ?sf::tidyverse to get documentation on the tidyverse sf methods. Let’s try some out. Say we want to calculate the population by Alaska management area, as opposed to region. pop_mgmt_338 &lt;- pop_region_3338 %&gt;% group_by(mgmt_area) %&gt;% summarize(total_pop = sum(total_pop)) plot(pop_mgmt_338[&quot;total_pop&quot;]) Notice that the region geometries were combined into a single polygon for each management area. If we don’t want to combine geometries, we can specify do_union = F as an argument. pop_mgmt_3338 &lt;- pop_region_3338 %&gt;% group_by(mgmt_area) %&gt;% summarize(total_pop = sum(total_pop), do_union = F) plot(pop_mgmt_3338[&quot;total_pop&quot;]) Writing the file Save the spatial object to disk using write_sf() and specifying the filename. Writing your file with the extension .shp will assume an ESRI driver driver, but there are many other format options available. write_sf(pop_region_3338, &quot;shapefiles/ak_regions_population.shp&quot;, delete_layer = TRUE) 19.1.5 Visualize with ggplot ggplot2 now has integrated functionality to plot sf objects using geom_sf(). We can plot sf objects just like regular data.frames using geom_sf. ggplot(pop_region_3338) + geom_sf(aes(fill = total_pop)) + theme_bw() + labs(fill = &quot;Total Population&quot;) + scale_fill_continuous(low = &quot;khaki&quot;, high = &quot;firebrick&quot;, labels = comma) We can also plot multiple shapefiles in the same plot. Say if we want to visualize rivers in Alaska, in addition to the location of communities, since many communities in Alaska are on rivers. We can read in a rivers shapefile, doublecheck the crs to make sure it is what we need, and then plot all three shapefiles - the regional population (polygons), the locations of cities (points), and the rivers (linestrings). The rivers shapefile is a simplified version of Jared Kibele and Jeanette Clark. Rivers of Alaska grouped by SASAP region, 2018. Knowledge Network for Biocomplexity. doi:10.5063/F1SJ1HVW. rivers_3338 &lt;- read_sf(&quot;shapefiles/ak_rivers_simp.shp&quot;) st_crs(rivers_3338) ## Coordinate Reference System: ## User input: Albers ## wkt: ## PROJCRS[&quot;Albers&quot;, ## BASEGEOGCRS[&quot;GCS_GRS 1980(IUGG, 1980)&quot;, ## DATUM[&quot;D_unknown&quot;, ## ELLIPSOID[&quot;GRS80&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433]]], ## CONVERSION[&quot;unnamed&quot;, ## METHOD[&quot;Albers Equal Area&quot;, ## ID[&quot;EPSG&quot;,9822]], ## PARAMETER[&quot;Latitude of false origin&quot;,50, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8821]], ## PARAMETER[&quot;Longitude of false origin&quot;,-154, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8822]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,55, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,65, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8824]], ## PARAMETER[&quot;Easting at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8826]], ## PARAMETER[&quot;Northing at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8827]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]] Note that although no EPSG code is set explicitly, with some sluething we can determine that this is EPSG:3338. This site is helpful for looking up EPSG codes. ggplot() + geom_sf(data = pop_region_3338, aes(fill = total_pop)) + geom_sf(data = rivers_3338, aes(size = StrOrder), color = &quot;black&quot;) + geom_sf(data = pop_3338, aes(), size = .5) + scale_size(range = c(0.01, 0.2), guide = F) + theme_bw() + labs(fill = &quot;Total Population&quot;) + scale_fill_continuous(low = &quot;khaki&quot;, high = &quot;firebrick&quot;, labels = comma) ## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please ## use `guide = &quot;none&quot;` instead. Incorporate base maps into static maps using ggmap The ggmap package has some functions that can render base maps (as raster objects) from open tile servers like Google Maps, Stamen, OpenStreetMap, and others. We’ll need to transform our shapefile with population data by community to EPSG:3857 which is the CRS used for rendering maps in Google Maps, Stamen, and OpenStreetMap, among others. pop_3857 &lt;- pop_3338 %&gt;% st_transform(crs = 3857) Next, let’s grab a base map from the Stamen map tile server covering the region of interest. First we include a function that transforms the bounding box (which starts in EPSG:4326) to also be in the EPSG:3857 CRS, which is the projection that the map raster is returned in from Stamen. This is an issue with ggmap described in more detail here # Define a function to fix the bbox to be in EPSG:3857 # See https://github.com/dkahle/ggmap/issues/160#issuecomment-397055208 ggmap_bbox_to_3857 &lt;- function(map) { if (!inherits(map, &quot;ggmap&quot;)) stop(&quot;map must be a ggmap object&quot;) # Extract the bounding box (in lat/lon) from the ggmap to a numeric vector, # and set the names to what sf::st_bbox expects: map_bbox &lt;- setNames(unlist(attr(map, &quot;bb&quot;)), c(&quot;ymin&quot;, &quot;xmin&quot;, &quot;ymax&quot;, &quot;xmax&quot;)) # Coonvert the bbox to an sf polygon, transform it to 3857, # and convert back to a bbox (convoluted, but it works) bbox_3857 &lt;- st_bbox(st_transform(st_as_sfc(st_bbox(map_bbox, crs = 4326)), 3857)) # Overwrite the bbox of the ggmap object with the transformed coordinates attr(map, &quot;bb&quot;)$ll.lat &lt;- bbox_3857[&quot;ymin&quot;] attr(map, &quot;bb&quot;)$ll.lon &lt;- bbox_3857[&quot;xmin&quot;] attr(map, &quot;bb&quot;)$ur.lat &lt;- bbox_3857[&quot;ymax&quot;] attr(map, &quot;bb&quot;)$ur.lon &lt;- bbox_3857[&quot;xmax&quot;] map } Next, we define the bounding box of interest, and use get_stamenmap() to get the basemap. Then we run our function defined above on the result of the get_stamenmap() call. bbox &lt;- c(-170, 52, -130, 64) # This is roughly southern Alaska ak_map &lt;- get_stamenmap(bbox, zoom = 4) ak_map_3857 &lt;- ggmap_bbox_to_3857(ak_map) Finally, plot both the base raster map with the population data overlayed, which is easy now that everything is in the same projection (3857): ggmap(ak_map_3857) + geom_sf(data = pop_3857, aes(color = population), inherit.aes = F) + scale_color_continuous(low = &quot;khaki&quot;, high = &quot;firebrick&quot;, labels = comma) 19.1.6 Visualize sf objects with leaflet We can also make an interactive map from our data above using leaflet. Leaflet (unlike ggplot) will project data for you. The catch is that you have to give it both a projection (like Alaska Albers), and that your shapefile must use a geographic coordinate system. This means that we need to use our shapefile with the 4326 EPSG code. Remember you can always check what crs you have set using st_crs. Here we define a leaflet projection for Alaska Albers, and save it as a variable to use later. epsg3338 &lt;- leaflet::leafletCRS( crsClass = &quot;L.Proj.CRS&quot;, code = &quot;EPSG:3338&quot;, proj4def = &quot;+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot;, resolutions = 2^(16:7)) You might notice that this looks familiar! The syntax is a bit different, but most of this information is also contained within the crs of our shapefile: st_crs(pop_region_3338) ## Coordinate Reference System: ## User input: EPSG:3338 ## wkt: ## PROJCRS[&quot;NAD83 / Alaska Albers&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;Alaska Albers (meters)&quot;, ## METHOD[&quot;Albers Equal Area&quot;, ## ID[&quot;EPSG&quot;,9822]], ## PARAMETER[&quot;Latitude of false origin&quot;,50, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8821]], ## PARAMETER[&quot;Longitude of false origin&quot;,-154, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8822]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,55, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,65, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8824]], ## PARAMETER[&quot;Easting at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8826]], ## PARAMETER[&quot;Northing at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8827]]], ## CS[Cartesian,2], ## AXIS[&quot;easting (X)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;northing (Y)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Topographic mapping (small scale).&quot;], ## AREA[&quot;United States (USA) - Alaska.&quot;], ## BBOX[51.3,172.42,71.4,-129.99]], ## ID[&quot;EPSG&quot;,3338]] Since leaflet requires that we use an unprojected coordinate system, let’s use st_transform yet again to get back to WGS84. pop_region_4326 &lt;- pop_region_3338 %&gt;% st_transform(crs = 4326) m &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;% addPolygons(data = pop_region_4326, fillColor = &quot;gray&quot;, weight = 1) m We can add labels, legends, and a color scale. pal &lt;- colorNumeric(palette = &quot;Reds&quot;, domain = pop_region_4326$total_pop) m &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;% addPolygons(data = pop_region_4326, fillColor = ~pal(total_pop), weight = 1, color = &quot;black&quot;, fillOpacity = 1, label = ~region) %&gt;% addLegend(position = &quot;bottomleft&quot;, pal = pal, values = range(pop_region_4326$total_pop), title = &quot;Total Population&quot;) m We can also add the individual communities, with popup labels showing their population, on top of that! pal &lt;- colorNumeric(palette = &quot;Reds&quot;, domain = pop_region_4326$total_pop) m &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;% addPolygons(data = pop_region_4326, fillColor = ~pal(total_pop), weight = 1, color = &quot;black&quot;, fillOpacity = 1) %&gt;% addCircleMarkers(data = pop_4326, lat = ~lat, lng = ~lng, radius = ~log(population/500), # arbitrary scaling fillColor = &quot;gray&quot;, fillOpacity = 1, weight = 0.25, color = &quot;black&quot;, label = ~paste0(pop_4326$city, &quot;, population &quot;, comma(pop_4326$population))) %&gt;% addLegend(position = &quot;bottomleft&quot;, pal = pal, values = range(pop_region_4326$total_pop), title = &quot;Total Population&quot;) m There is a lot more functionality to sf including the ability to intersect polygons, calculate distance, create a buffer, and more. Here are some more great resources and tutorials for a deeper dive into this great package: Raster analysis in R Spatial analysis in R with the sf package Intro to Spatial Analysis sf github repo Tidy spatial data in R: using dplyr, tidyr, and ggplot2 with sf mapping-fall-foliage-with-sf "],["open-data-and-ethics-summary.html", "20 Open Data and Ethics Summary 20.1 Data Publishing and Documentation, Part Two 20.2 Activity and Discussion", " 20 Open Data and Ethics Summary 20.1 Data Publishing and Documentation, Part Two 20.1.1 Learning Objectives In this lesson, we will: - Summarize data ethics in the context of open science - Write a statement on ethical considerations of your research - Peer review these statements 20.1.2 Open Data and Ethics The foundation of open-science is based on making all aspects of scientific research accessible across broad communities, whether professional, academic or public. This includes publications, data, software, samples, and code, and at its core, open science is built on principles of transparency and capacity for collective knowledge. The practice of open science is being increasingly adopted by researchers across disciplines, and organizations and working groups such as FORCE11 promote and support this through development of principles and guidelines that inform activities. While open-science affords researchers the opportunity to extend the reach of their work (Puebla &amp; Lowenberg, 2021) and increases transparency and public trust, such transparency creates challenges for those researchers working with sensitive data and Indigenous knowledge. Open science principles are informed largely by western knowledge concepts and do not consider Indigenous data sovereignty and Indigenous Knowledge (Research Data Alliance International Indigenous Data Sovereignty Interest Group, 2019). Transparency in data ethics is a vital part of open science. Regardless of discipline, various ethical concerns are always present, including professional ethics such as plagiarism, false authorship, or falsification of data, to ethics regarding the handling of animals, to concerns relevant to human subjects research. Sharing ethical practices openly, similar in the way that data is shared, enables deeper discussion about data management practices, data reuse, sensitivity, sovereignty and other considerations. Further, such transparency promotes awareness and adoption of ethical practices. As the primary data repository for the National Science Foundation Office of Polar Programs Arctic Sciences section, the Arctic Data Center accepts data from all disciplines including social science and other research that may be subject to IRB restrictions, involve Indigenous knowledge, or have been carried out on Indigenous lands. Researchers submitting data now have the opportunity to articulate these and other considerations as part of the metadata record. 20.2 Activity and Discussion Fill out the ethical research considerations form (linked here) In breakout groups, consider the following questions as you review you peer’s answers: Were there any differences between the first version of the ethical considerations, and the second version? What ethical considerations are the most relevant in your research, and why? How can principles such as the CARE principles, be adopted into your research? "],["provenance-and-reproducibility.html", "21 Provenance and Reproducibility 21.1 Reproducible and Transparent Papers", " 21 Provenance and Reproducibility 21.1 Reproducible and Transparent Papers 21.1.1 Learning Objectives In this lesson, we will: Discuss the concept of reproducible workflows Review the importance of computational reproducibility Review the utility of provenance metadata Overview how R packages are great ways to package work reproducibly Learn how to build a reproducible paper in RMarkdown Review tools and techniques for reproducibility supported by the NCEAS and DataONE 21.1.1.1 Reproducibility and Provenance Slides: Accelerating synthesis science through reproducible science practices Reproducible Research: Recap Working in a reproducible manner: increases research efficiency, accelerating the pace of your research and collaborations provides transparency by capturing and communicating scientific workflows enables research to stand on the shoulders of giants (build on work that came before) allows credit for secondary usage and supports easy attribution increases trust in science To enable others to fully interpret, reproduce or build upon our research, we need to provide more comprehensive information than is typically included in a figure or publication. The methods sections of papers are typically inadequate to fully reproduce the work described in the paper. What data were used in this study? What methods applied? What were the parameter settings? What documentation or code are available to us to evaluate the results? Can we trust these data and methods? Are the results reproducible? Computational reproducibility is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions. Practically speaking, reproducibility includes: Preserving the data Preserving the software workflow Documenting what you did Describing how to interpret it all Computational Provenance and Workflows Computational provenance refers to the origin and processing history of data including: Input data Workflow/scripts Output data Figures Methods, dataflow, and dependencies When we put these all together with formal documentation, we create a computational workflow that captures all of the steps from inital data cleaning and integration, through analysis, modeling, and visualization. Here’s an example of a computational workflow from Mark Carls: Mark Carls. Analysis of hydrocarbons following the Exxon Valdez oil spill, Gulf of Alaska, 1989 - 2014. Gulf of Alaska Data Portal. urn:uuid:3249ada0-afe3-4dd6-875e-0f7928a4c171., that represents a three step workflow comprising four source data files and two output visualizations. This screenshot of the dataset page shows how DataONE renders the workflow model information as part of our interactive user interface. You can clearly see which data files were inputs to the process, the scripts that are used to process and visualize the data, and the final output objects that are produced, in this case two graphical maps of Prince William Sound in Alaska. From Provenance to Reproducibility At DataONE we facilitate reproducible science through provenance by: Tracking data derivation history Tracking data inputs and outputs of analyses Preserving and documenting software workflows Tracking analysis and model executions Linking all of these to publications Introducing ProvONE, an extension of W3C PROV ProvONE provides the fundamental information required to understand and analyze scientific workflow-based computational experiments. It covers the main aspects that have been identified as relevant in the provenance literature including data structure. This addresses the most relevant aspects of how the data, both used and produced by a computational process, is organized and represented. For scientific workflows this implies the inputs and outputs of the various tasks that form part of the workflow. Data Citation Data citation best practices are focused on providing credit where credit is due and indexing and exposing data citations across international repository networks. In 2014, Force 11 established a Joint Declaration of Data Citation Principles that includes: Importance of data citation Credit and Attribution Evidence Unique Identification Access Persistence Specificity and Verifiability Interoperability and Flexibility Transitive Credit We want to move towards a model such that when a user cites a research publication we will also know: Which data produced it What software produced it What was derived from it Who to credit down the attribution stack This is transitive credit. And it changes the way in which we think about science communication and traditional publications. 21.1.2 Reproducible Papers with rrtools A great overview of this approach to reproducible papers comes from: Ben Marwick, Carl Boettiger &amp; Lincoln Mullen (2018) Packaging Data Analytical Work Reproducibly Using R (and Friends), The American Statistician, 72:1, 80-88, doi:10.1080/00031305.2017.1375986 This lesson will draw from existing materials: rrtools Reproducible papers with RMarkdown The key idea in Marwick et al. (2018) is that of the “research compendium”: A single container for not just the journal article associated with your research but also the underlying analysis, data, and even the required software environment required to reproduce your work. Research compendia make it easy for researchers to do their work but also for others to inspect or even reproduce the work because all necessary materials are readily at hand due to being kept in one place. Rather than a constrained set of rules, the research compendium is a scaffold upon which to conduct reproducible research using open science tools such as: R RMarkdown git and GitHub Fortunately for us, Ben Marwick (and others) have written an R package called rrtools that helps us create a research compendium from scratch. To start a reproducible paper with rrtools, run: # install `tinytex` (See below) # Upgrade contentid to 0.12.0 #remotes::install_github(&quot;cboettig/contentid&quot;) # Upgrade readr from 1.3.1 to 2.0.1 #install.packages(c(&quot;readr&quot;)) #remotes::install_github(&quot;benmarwick/rrtools&quot;) setwd(&quot;..&quot;) rrtools::use_compendium(&quot;mypaper&quot;) rrtools has created the beginnings of a research compendium for us. At this point, it looks mostly the same as an R package. That’s because it uses the same underlying folder structure and metadata and therefore it technically is an R package. And this means our research compendium will be easy to install, just like an R package. rrtools also helps you set up some key information like: Add a license (always a good idea) Set up a README file in the RMarkdown format Create an analysis folder to hold our reproducible paper usethis::use_apache_license() rrtools::use_readme_rmd() rrtools::use_analysis() This creates a standard, predictable layout for our code and data and outputs that multiple people can understand. At this point, we’re ready to start writing the paper. To follow the structure rrtools has put in place for us, here are some next steps you can take: Edit ./analysis/paper/paper.Rmd to begin writing your paper and your analysis in the same document Add any citations to ./analysis/paper/references.bib Add any longer R scripts that don’t fit in your paper in an R folder at the top level Add raw data to ./data/raw_data Write out any derived data (generated in paper.Rmd) to ./data/derived_data Write out any figures in ./analysis/figures You can then write all of your R code in your RMarkdown, and generate your manuscript all in the format needed for your journal (using it’s .csl file). Here is a look at the beginning of the Rmd: And the rendered PDF: 21.1.3 Reproducible Papers with rticles Note that the rticles package provides a lot of other great templates for formatting your paper specifically to the requirements of many journals. In addition to a custom CSL file for rreference customization, rticles supports custom LATeX templates that fit the formatting requirements of each journals. After installing rticles with a command like remotes::install_github(\"rstudio/rticles\") and restarting your RStudio session, you will be able to create articles from these custom templates using the File | New File | R Markdown... menu, which shows the following dialog: Select the “PNAS” template, give the file a name and choose a location for the files, and click “OK”. You can now Knit the Rmd file to see a highly-targeted article format, like this one for PNAS: While rticles doesn’t produce the detailed folder layout of rrtools, it is relatively simple to use the two packages together. Use rrtools to generate the core directory layout and approach to data handling, and then use articles to create the structure of the paper itself. The combination is incredibly flexible. The 5th Generation of Reproducible Papers Whole Tale simplifies computational reproducibility. It enables researchers to easily package and share ‘tales’. Tales are executable research objects captured in a standards-based tale format complete with metadata. They can contain: data (references) code (computational methods) narrative (traditional science story) compute environment (e.g. RStudio, Jupyter) By combining data, code and the compute environment, tales allow researchers to: re-create the computational results from a scientific study achieve computational reproducibility “set the default to reproducible.” They also empower users to verify and extend results with different data, methods, and environments. You can browse existing tales, run and interact with published tales and create new tales via the Whole Tale Dashboard. By integrating with DataONE and Dataverse, Whole Tale includes over 90 major research repositories from which a user can select datasets to make those datasets the starting point of an interactive data exploration and analysis inside of one of the Whole Tale environments. Within DataONE, we are adding functionality to work with data in the Whole Tale environment directly from the dataset landing page. Full circle reproducibility can be achieved by publishing data, code AND the environment. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
