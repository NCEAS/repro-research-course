[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "",
    "text": "Overview"
  },
  {
    "objectID": "index.html#about-this-training",
    "href": "index.html#about-this-training",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "About this training",
    "text": "About this training\nNCEAS Open Science Synthesis training consists of three 1-week long workshops, geared towards early career researchers. Participants engage in a mix of lectures, exercises, and synthesis research groups to undertake synthesis while learning and implementing best practices for open data science."
  },
  {
    "objectID": "index.html#why-nceas",
    "href": "index.html#why-nceas",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Why NCEAS",
    "text": "Why NCEAS\nThe National Center for Ecological Analysis and Synthesis (NCEAS), a research affiliate of UCSB, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science.\nNCEAS has over 25 years of success with this model among working groups and environmental professionals. Together with the Delta Science Program and the Delta Stewardship Council we are excited to pass along skills, workflows, mindsets learn throughout the years.\n\nWeek 3: Scaling up and presenting synthesis\nOctober 23 – 27, 2023\n\nBig data workflows and parallel computing\nPresenting results using Shiny or flexdashboards\nRevisit reproducible and git workflows\nSynthesis presentations and next steps"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS with the support of DataONE, to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors for this version are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Halina Do-Linh, Matthew B. Jones, Camila Vargas Poulsen. 2023. Open Science Synthesis training Week 3. NCEAS Learning Hub & Delta Stewardship Council.\nAdditional contributors: Ben Bolker, Julien Brun, Amber E. Budden, Jeanette Clark, Samantha Csik, Stephanie Hampton, Natasha Haycock-Chavez, Samanta Katz, Julie Lowndes, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "session_01.html#learning-objectives",
    "href": "session_01.html#learning-objectives",
    "title": "1  Mapping Census Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nReview how the tidycensus package works\nGet acquaint on how to work with spatial census data\nIntroduce tools to create static and interactive maps to visualize census data\n\n\n\n\n\n\n\nAcknowledgement\n\n\n\nThis lesson is heavily based on Kyle Walker’s talk “Mapping And Spatial Analysis with ACS data in R” As part of the Census Data Workshops given at the University of Michigan in February 2023."
  },
  {
    "objectID": "session_01.html#census-data-with-tidycensus",
    "href": "session_01.html#census-data-with-tidycensus",
    "title": "1  Mapping Census Data",
    "section": "1.1 Census data with tidycensus",
    "text": "1.1 Census data with tidycensus\nThe tidycensus package (Walker and Matt (2021)) was developed to systematize the process of working with U.S Census data using R. It integrates the Census Application Programming Interface (API) released by the the U.S Census Bureau, into an R package to facilitate access to census data using R.\ntidycensus main functions:\n\nget_decennial()\nget_acs()\nget_estimates()\nget_pums()\nget_flows()\n\nMore details about these functions in the Intro to tidycensus lesson.\nDuring this lesson we will used get_acs() to access and map data from the American Community Survey (ACS).\n\n1.1.1 American Community Survey (ACS) recap\nProvides detailed demographic information about US population. Covers topics not available in decennial US Census data (e.g. income, education, language, housing characteristics). It is an annual survey of 3.5 million US households. Data is updated annually through the 1-year estimates (for geographies of population 65,000 and greater). And it is also provided as a 5-year estimate. The 5-year ACS is a moving average of data over a 5-year period that covers geographies down to the Census block group. ACS data represent estimates rather than precise counts, therefore data includes margin of error.\n\n\n\n\n\n\nNote\n\n\n\n\n2020 1-year data only available as experimental estimates. Data delivered as estimates characterized by margins of error.\nTo access data of regions with population less than 65K, you have to youse the 5-year estimates ACS.\n\n\n\n\n\n1.1.2 Review on get_acs()\nget_acs() function from tidycensus streamlines the process of working with ACS data.\n\nIt wrangles Census data internally and returns queried data in “tidy” format.\nEach request includes its associated margins of error.\nYou can filter by states and counties using their name (no more looking up FIPS codes!)”\n\nAND..\n\nAutomatically downloads and merges Census geometries to data for mapping.\n\nThis packages get the data for you, it shapes it in a format ready to go for analysis following the “tidy” principles, it pre joins the census geometries this means you get your data and spatial data automatically. And is streamlines the process of doing target requests.\n\nThe functions has three main arguments\n\ngeography: The geographic area of your data\nvariable(s): Character string or vector of character strings of variable IDs. tidycensus automatically returns the estimate and the margin of error associated with the variable.\nyear: The year, or end-year, of the ACS sample. 5-year ACS data is available from 2009 through 2021; 1-year ACS data is available from 2005 through 2021, with the exception of 2020. Defaults to the 5-year estimates ACS for the most recent year of data available. As for now: 2017-2021 5-year ACS. Note: the default might update when the 2022 ACS data is released (expected to be released in December 2023).\n\n\nFor example to get the median income for all counties in the U.S:\n\n## Median income by county. Defaults to 5-year estimates 2017-2021\"\nmedian_income_5yr &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\")\n\nIn addition yu can filter by a specific State or County by adding the argument state= and/or county= and specify a region by their name.\nFollowing the example above, if we want to get the median income for all counties in California, we have to add the agument state = \"CA\".\n\n## Median income in California by county. Defaults to 5-year estimates 2017-2021\"\nmedian_income_5yr &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  state = \"CA\")\n\nFor more information about geographies and variables in tidycensus check out Walker, 2023.\n\n\n\n\n\n\nDecennial Census\n\n\n\nComplete enumeration of the US population to assist with apportionment. It asks a limited set of questions on race, ethnicity, age, sex, and housing tenure. Data from 2000, 2010, and available data from 2020.\nAccess this data using get_decennial()"
  },
  {
    "objectID": "session_01.html#spatial-census-data-in-tidycensus",
    "href": "session_01.html#spatial-census-data-in-tidycensus",
    "title": "1  Mapping Census Data",
    "section": "1.2 Spatial Census Data in tidycensus",
    "text": "1.2 Spatial Census Data in tidycensus\nTo be able to work with “spatial” Census data you would generally have to go and find shapefiles on the Census website, download a CSV with the data, clean and format the data, load the geometries and data to your spatial data software of choice, then align the key fields and join your data with the geometries.\nAgain, tidycensus to the rescue! This packages combines all these steps and makes it very easy to get census data nd its geometries ready for analysis. Let’s see how this work.\n\n1.2.1 Spatial Census data with get_acs()\nAs usual we start by loading the libraries we are going to use today.\n\nlibrary(tidycensus)\nlibrary(mapview)\nlibrary(tigris)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)\n\n\n\n\n\n\n\nAPI Key\n\n\n\nRemember that the first time you work with tidycensus you have to connect your session with the Census data using am API key.\n\nGo to https://api.census.gov/data/key_signup.html\nFill out the form\nCheck your email for your key.\nUse the census_api_key() function to set your key. Note: install = TRUE forces r to write this key to a file in our R environment that will be read every time you use R. This means, by setting this argument to TRUE, you only have to do it once in any computer you are working. If you see this argument as FALSE, R will not remember this key next time you come back.\n\n\ncensus_api_key(\"YOUR KEY GOES HERE\", install = TRUE)\n\n\nLastly, restart R.\n\nNOTE: WE DID THIS LAST TIME SO OUR SERVER SESSION SHOULD BE GOOD TO GO.\n\n\nSo now, if we want to retrieve data for income estimates by county for California with it’s associated geometries we need to know the variable for income estimates (“B19013_001”), call get_acs() with all the necessary information and add the argument geometry = TRUE to get the spatial data for each geography.\n\n## defaults to most recent 5year estimates (2017-2021 5-year ACS)\nca_income &lt;- get_acs(\n    geography = \"county\",\n    variables = \"B19013_001\",\n    state = \"CA\",\n    year = 2021,\n    geometry = TRUE) ## This argument does all of the steps mentioned above.\n\nAnd that’s it!! Now we have the corresponding spatial data bind to our variable of interest. We can plot this data using the base r plot() function.\n\nplot(ca_income[\"estimate\"])\n\n\n\n\nNow we have our data ready to start exploring!\n\n\n1.2.2 What’s under the hood\nThe sf package. As we learned during the last training, the sf package implements a simple features data model for vector spatial data in R. This means that vector geometries: points, lines, and polygons stored in a list-column of a data frame. Making it very easy to work withe spatial data in R, just like you work with any other type of data, in a tabular format.\nLet’s take a look at our data\n\nhead(ca_income)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.4096 ymin: 33.21473 xmax: -117.4133 ymax: 42.00076\nGeodetic CRS:  NAD83\n  GEOID                         NAME   variable estimate  moe\n1 06059    Orange County, California B19013_001   100485  718\n2 06111   Ventura County, California B19013_001    94150 1310\n3 06063    Plumas County, California B19013_001    57885 4555\n4 06015 Del Norte County, California B19013_001    53280 5046\n5 06023  Humboldt County, California B19013_001    53350 2424\n6 06043  Mariposa County, California B19013_001    53304 4026\n                        geometry\n1 MULTIPOLYGON (((-118.1144 3...\n2 MULTIPOLYGON (((-119.4412 3...\n3 MULTIPOLYGON (((-121.497 40...\n4 MULTIPOLYGON (((-124.2175 4...\n5 MULTIPOLYGON (((-124.4086 4...\n6 MULTIPOLYGON (((-120.3944 3...\n\n\nWe can see that this is a Simple feature collection with 6 features and 5 fields. For those of you familiar with GIS, probably this is known terminology. But for those of you that this is all new, you can think of a feature as a simple shape on your data layer. Generally in a GIS perspective feature means a row in the data. In this case for example, Ventura County is a county and the shape of that county it self is a feature. And then, a field in GIS terminology means an attribute of the data or a column.\nSimilar how we saw last time in the Spatial Data lesson, we have a geometry column. This contains all the spatial information we need to map out data.\nA polygon is a two dimensional shape that has a perimeter and an area. A multipolygon are multiple shapes that belong to the same feature. For example if we have census data for the state of Hawaii, we will have multiple polygon, one for each island, representing that row or feature. We also can see the CRS associated to this data and the bounding box that indicates the extension of our data set.\n\n\n\n\n\n\nCoordinate Reference System (CRS)\n\n\n\nHow are the coordinates in our polygons referenced to the earth surface. It handles how the mapping of our data to the actual earth. For more information on CRS and tidycensus checkout Walker 2023\n\n\nIf we look at the other columns in our data, we have the data it self. GEOID, NAME, estimate and moe(margin of error, interpreted at 90% confidence level).\n\n\n\n\n\n\nNote on missing data\n\n\n\nRemember that 5 year ACS data are projections from a sample. Counties with no data means that the population in those counties is not large enough to make these projections.\n\n\n\n\n1.2.3 Adding interactivity\nFor a lot of GIS users it is hard to transition from GIS to working with spatial data in R because GIS provides nice interactive tools. One easy way to make your R maps interactive is the mapview() package. This package wraps up different interactive mapping tools and allows you to explore your data just by running a single line of code. Let’s try this.\n\nmapview(ca_income, zcol = \"estimate\")\n\n\n\n\n\n\nThe zcol = argument, allows us to easily plot data to this interactive map. We can explore the data using the RStudio Viewer.\nLet’s look at another example at a smaller census geography. tidycensus is really helpful to look at spatial data in smaller geography.\n\nsolano_income &lt;- get_acs(\n    geography = \"tract\",\n    variables = \"B19013_001\",\n    state = \"CA\",\n    county = \"Solano\",\n    geometry = \"TRUE\")\n\nhead(solano_income)\n\nWe can again use mapview() to check out our data.\n\nmapview(solano_income, zcol = \"estimate\")\n\n\n\n\n\n\nWith these two packages we can almost instantly explore different data from the ACS surveys. Note that for census tracts, the MOE will be much higher than for county data as estimates are extrapolated to a finer scale.\n\n\n1.2.4 Spatial data structure in tidycensus (long versus wide)\nThe default of tidycensus is to return a data frame in a “long” format. This is generally the preferred way to work and analyze data in R. But, if you rather have a “wide” data frame as the output (GIS users are generally used to wide format) you can do that by adding the argument output = wide. This will return a data frame where each variable is in a different column. For example:\n\nrace_var &lt;- c(\n    Hispanic = \"DP05_0071P\",\n    White = \"DP05_0077P\",\n    Black = \"DP05_0078P\",\n    Asian = \"DP05_0080P\")\n\n## Default long\nalameda_race &lt;- get_acs(\n  geography = \"tract\",\n  variables = race_var,\n  state = \"CA\",\n  county = \"Alameda\",\n  geometry = TRUE)\n\nhead(alameda_race)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.2588 ymin: 37.65598 xmax: -121.7804 ymax: 37.7547\nGeodetic CRS:  NAD83\n        GEOID                                             NAME variable\n1 06001451704 Census Tract 4517.04, Alameda County, California Hispanic\n2 06001451704 Census Tract 4517.04, Alameda County, California    White\n3 06001451704 Census Tract 4517.04, Alameda County, California    Black\n4 06001451704 Census Tract 4517.04, Alameda County, California    Asian\n5 06001428301 Census Tract 4283.01, Alameda County, California Hispanic\n6 06001428301 Census Tract 4283.01, Alameda County, California    White\n  estimate moe                       geometry\n1     11.5 4.3 MULTIPOLYGON (((-121.7985 3...\n2     68.8 8.1 MULTIPOLYGON (((-121.7985 3...\n3      0.0 0.9 MULTIPOLYGON (((-121.7985 3...\n4     14.6 6.6 MULTIPOLYGON (((-121.7985 3...\n5      9.9 6.1 MULTIPOLYGON (((-122.2588 3...\n6     34.2 5.3 MULTIPOLYGON (((-122.2588 3...\n\n\nAnd now in wide format. Every variable (Hispanic, White, Black and Asian) is in a different column as opposed to being stacked into one column named variable.\n\nalameda_race_wide &lt;- get_acs(\n  geography = \"tract\",\n  variables = race_var,\n  state = \"CA\",\n  county = \"Alameda\",\n  geometry = TRUE,\n  output = \"wide\")\n\nhead(alameda_race_wide)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.2978 ymin: 37.5333 xmax: -121.7804 ymax: 37.87881\nGeodetic CRS:  NAD83\n        GEOID                                             NAME HispanicE\n1 06001451704 Census Tract 4517.04, Alameda County, California      11.5\n2 06001428301 Census Tract 4283.01, Alameda County, California       9.9\n3 06001407000    Census Tract 4070, Alameda County, California      37.2\n4 06001422400    Census Tract 4224, Alameda County, California      15.1\n5 06001423200    Census Tract 4232, Alameda County, California      29.0\n6 06001444200    Census Tract 4442, Alameda County, California      32.9\n  HispanicM WhiteE WhiteM BlackE BlackM AsianE AsianM\n1       4.3   68.8    8.1    0.0    0.9   14.6    6.6\n2       6.1   34.2    5.3    4.8    4.1   43.0    6.8\n3      10.4   18.3    6.7   15.2    5.3   18.2    7.5\n4       4.4   36.5    6.6    2.7    2.0   39.5    9.7\n5       7.9   33.3    8.2   20.5    8.6   11.4    5.6\n6       6.5   22.3    4.5    0.4    0.5   37.6    5.8\n                        geometry\n1 MULTIPOLYGON (((-121.7985 3...\n2 MULTIPOLYGON (((-122.2588 3...\n3 MULTIPOLYGON (((-122.2098 3...\n4 MULTIPOLYGON (((-122.2737 3...\n5 MULTIPOLYGON (((-122.2978 3...\n6 MULTIPOLYGON (((-122.0552 3...\n\n\nBoth data frames alameda_race and alameda_race_wide have the same exact information. They are just in a different shape. Depending on what are you want to do with the data which one you should retrieve."
  },
  {
    "objectID": "session_01.html#working-with-census-geometry",
    "href": "session_01.html#working-with-census-geometry",
    "title": "1  Mapping Census Data",
    "section": "1.3 Working with Census Geometry",
    "text": "1.3 Working with Census Geometry\n\n\n\n\n\n\n\n\n“Census and ACS data are associated with geographies, which are units at which the data are aggregated. These defined geographies are represented in the US Census Bureau’s TIGER/Line database, where the acronym TIGER stands for Topologically Integrated Geographic Encoding and Referencing. This database includes a high-quality series of geographic datasets suitable for both spatial analysis and cartographic visualization . Spatial datasets are made available as shapefiles, a common format for encoding geographic data.” (Walker 2023, Chapter 5)\n\n\ntidycensus uses the tigris R package internally to acquire Census shapefiles\nBy default, the Cartographic Boundary shapefiles are used, which are pre-clipped to the US shoreline\ntigris offers a number of features to help with acquisition and display of spatial ACS data. Making your work with ACS data better.\n\nLet’s go back to our map of Solano County.\n\nsolano_income &lt;- get_acs(\n    geography = \"tract\",\n    variables = \"B19013_001\",\n    state = \"CA\",\n    county = \"Solano\",\n    geometry = \"TRUE\")\n\nmapview(solano_income, zcol = \"estimate\")\n\n\n\n\n\n\nWe can see that there are some issues with the interior water areas. They are often not removed from the Cartographic Boundary shapefiles. What can we do about it? We can again leverage on how powerful these tools are on making complex process simple.\nThere is a function in the tigris package that “erase water”! It finds water areas in a shapefile and removes those water areas, giving you a result that allows you to better display your data. Let’s take a look on how this works.\n\nsf_use_s2(FALSE) ## Need to run this so that mapview works.\n\nsolano_erase &lt;- erase_water(solano_income,\n                            year = 2021) ## year to use for water layer\n\nmapview(solano_erase, zcol = \"estimate\")\n\n\n\n\n\n\nAnd just like that! We get a much more accurate map of Solano County."
  },
  {
    "objectID": "session_01.html#mapping-acs-data",
    "href": "session_01.html#mapping-acs-data",
    "title": "1  Mapping Census Data",
    "section": "1.4 Mapping ACS data",
    "text": "1.4 Mapping ACS data\nThere are a several extraordinary packages in R to visualize cartographic data. Today we are going to be using our good ol’ friend ggplot2. In the last section of this lesson you can find resources to other cartography mapping packages like tmap.\nThere is a reason why we use ggplot2 over and over throughout the lessons in this course. It is a very powerful data visualization tool! In fact, is one of the most downloaded packages in R. And, as we learned in the “working with spatial data” lesson, there is a function called geom_sf() that allows us to easily plot spatial data.\nHow do we plot ACS data using ggplot?\nLets make a map with the Hispanic population in Alameda County by Census tract.\nSe we are going to use the alameda_race object we created earlier. And we are going to start by filtering the data for Hispanic population.\n\nalameda_hispanic &lt;- filter(alameda_race,\n                           variable == \"Hispanic\")\n\nggplot(alameda_hispanic,\n       aes(fill = estimate))+\n    geom_sf() ## plots polygons!\n\n\n\n\nHere we have our choropleth map with the Hispanic population in Alameda County! A choropleth plot provides a shade or color to a polygon (or shape) according to a giving attribute (e.g. The percentage of Hispanic population)\nHere we are mapping the estimate column to fill the shape we are plotting, in this case the tract polygon. The geom_sf() plots polygons!\nA choropleth is a map that uses shading to show variation in some sort of data attribute. In this case, the lighter colors represent higher values, this means that tract with lighter shades of blue have higher Hispanic population. And the darker ares represent the lower values, fewer presence of Hispanic/Latino population.\nAs we know, with ggplot2 we can heavily style our plot. Here an example of customization.\n\nggplot(alameda_hispanic, aes(fill = estimate)) + \n  geom_sf() + \n  theme_void() + \n  scale_fill_viridis_c(option = \"rocket\") + \n  labs(title = \"Percent Hispanic by Census tract\",\n       subtitle = \"Alameda County, California\",\n       fill = \"ACS estimate\",\n       caption = \"2017-2021 ACS | tidycensus R package\")\n\n\n\n\nYou can also plot you data in bins instead of a continuous scale.\n\nggplot(alameda_hispanic, aes(fill = estimate)) + \n  geom_sf() + \n  theme_void() + \n  scale_fill_viridis_b(option = \"rocket\", n.breaks = 6) + \n  labs(title = \"Percent Hispanic by Census tract\",\n       subtitle = \"Alameda County, California\",\n       fill = \"ACS estimate\",\n       caption = \"2017-2021 ACS | tidycensus R package\")\n\n\n\n\nWhich style to use will depends on what you want to achieve. We can see that in the plot with bins we loose some resolution. On the other hand the continuous scale can provide a little of a color over load.\nWe can keep leveraging on ggplot2 power and plot more variables of our data. For example create a map for each of the difference races on our data.\n\nggplot(alameda_race, aes(fill = estimate)) + \n  geom_sf(color = NA) +  ## removes delimitation of each tract\n  theme_void() + \n  scale_fill_viridis_c(option = \"rocket\") + \n  facet_wrap(~variable) +\n  labs(title = \"Race / ethnicity by Census tract\",\n       subtitle = \"Alameda County, California\",\n       fill = \"ACS estimate (%)\",\n       caption = \"2017-2021 ACS | tidycensus R package\")\n\n\n\n\n\n\n1.4.1 Mapping Count Data\nSo far we have been mapping percentage. But what if your data is not percentage but count? Choropleth are great for mapping ratios and percentage, but not so great for mapping counts. When you are working with count data, you wanna have a way to represent the extent of the count through symbols. We are going to show this with an example.\nWe start by getting the count data for race/ethnicity. Note that the process is practically the same that we did above, but there is a slight difference in the variable codes we are going to use. Generally, variables that end in “P” means the estimate is in percentage. Variable with out the “P” at the end are count data.\n\nalameda_race_counts &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    Hispanic = \"DP05_0071\",\n    White = \"DP05_0077\",\n    Black = \"DP05_0078\",\n    Asian = \"DP05_0080\"),\n  state = \"CA\",\n  county = \"Alameda\",\n  geometry = TRUE)\n\n## Checking our data. Estimates are in counts not in %\nhead(alameda_race_counts)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.2588 ymin: 37.65598 xmax: -121.7804 ymax: 37.7547\nGeodetic CRS:  NAD83\n        GEOID                                             NAME variable\n1 06001451704 Census Tract 4517.04, Alameda County, California Hispanic\n2 06001451704 Census Tract 4517.04, Alameda County, California    White\n3 06001451704 Census Tract 4517.04, Alameda County, California    Black\n4 06001451704 Census Tract 4517.04, Alameda County, California    Asian\n5 06001428301 Census Tract 4283.01, Alameda County, California Hispanic\n6 06001428301 Census Tract 4283.01, Alameda County, California    White\n  estimate moe                       geometry\n1      494 181 MULTIPOLYGON (((-121.7985 3...\n2     2945 551 MULTIPOLYGON (((-121.7985 3...\n3        0  13 MULTIPOLYGON (((-121.7985 3...\n4      627 277 MULTIPOLYGON (((-121.7985 3...\n5      729 501 MULTIPOLYGON (((-122.2588 3...\n6     2523 485 MULTIPOLYGON (((-122.2588 3...\n\n\nThe first map we are going to plot is a graduate symbol map. This kind of maps are good for count data because the comparison we are making are between symbols of the same shape. The size of the symbol is proportional to the underlying data value. The most common shape to use for this kind of plots are circles.\nThe tricky thing here, and this also speaks to really understanding our data and what we are trying to plot, is that our data is represented as polygons and we want to map points or circle. So we need to convert our data from polygons to circle.\nAs a reminder, polygons are closed shapes with a perimeter and an area. We have to convert this shape to a single point and draw a circle proportional to the corresponding data value.\nThere is a function from the sf package that allows us to do this. This function is st_centroid(). This function converts a shape, for example the shape of a census tract to a point, right in the center of that tract. So lets convert part of our Alameda race data to centroids. We are going to filter for the Asian population.\n\nalameda_asian &lt;- alameda_race_counts %&gt;% \n    filter(variable == \"Asian\")\n\n\ncentroids &lt;- st_centroid(alameda_asian)\n\n\n\n\n\n\n\nWarning message:\n\n\n\nst_centroid assumes attributes are constant over geometries\nThis message is letting us know that you are converting a polygon to a single point, and this point might not truly represent where people in this tract live. Just a heads up of what is happening.\n\n\nNow we plot. Note that we are plotting two layers. One with the polygons to provide context to our data and the other one with the actual data transformed into centroids.\n\nggplot() + \n  geom_sf(data = alameda_asian, color = \"black\", fill = \"lightgrey\") + \n  geom_sf(data = centroids, aes(size = estimate),\n          alpha = 0.7, color = \"navy\") + \n  theme_void() + \n  labs(title = \"Asian population by Census tract\",\n       subtitle = \"2017-2021 ACS, Alameda County, California\",\n       size = \"ACS estimate\") + \n  scale_size_area(max_size = 6)\n\n\n\n\nscale_size_area() argument makes the area of the circles proportional. In this case the area representing 2500 is about half of the area of the 5000 circle. Overall, areas with smaller circles are areas with less Asian population and areas with larger circles have a larger Asian population. This kind of maps makes it easier to visualize change across the different area. For example, larger counts with a very low Asian population are represented with a small circle instead of painting the whole are with a color that represents a low population.\nWe can compere location of a point and size according to the estimated value of the population.\nAnother way of plotting count data is with a dot-density map. This kind of maps excel at plotting multiple variables in one map. On the graduate symbol map, we were able to plot the Asian population and clearly see how it changes among census tract. However, the graduate symbol map doesn’t really allow as to to plot heterogeneity, or the mixing of different racial groups in this case. For example to see how different groups live together or apart. Dot-density maps scatter dots proportionally to data size; dots can be colored to show mixing of categories.\nThe as_dot_density() function allows you to calculate these density dots based on your data. It is design for categorical mapping of ACS and Census Decennial data, taking data in a long format as an input. The other arguments relevant to this function are:\n\nvalue = assigning the column in our data frame that has the values we want to transform to dots. In this case our estimate column.\nvalues_per_dot = dot to data ratio, how many data points does each dot represent. In this case values_per_dot = 200 means that each dot represents 200 people.\ngroup = is an argument that allow as to group out data. In this case group = \"variable\" groups the data by each of the categories in the variable column and creates a dot for each of those categories.\n\n\nalameda_race_dots &lt;- as_dot_density(\n  alameda_race_counts,\n  value = \"estimate\",\n  values_per_dot = 200,\n  group = \"variable\"\n)\n\nalthough coordinates are longitude/latitude, st_intersects assumes that they\nare planar\n\n\nThere are a lot of calculations happening under the hood here. What this function is doing is scattering dots with each census tract proportional to the number of people that are in each group (in this case groups are defined by the categories in the variable column).\nLet’s look at the outcome data. We can already see that this data frame has many more rows than our input data. This is because, each row in this case just represents up to 200 people, as we defined in the values_per_dot argument. We can also see that we have a geometry type POINT.\n\nhead(alameda_race_dots)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -122.296 ymin: 37.58357 xmax: -121.9283 ymax: 37.86856\nGeodetic CRS:  NAD83\n        GEOID                                             NAME variable\n1 06001441100    Census Tract 4411, Alameda County, California    White\n2 06001408200    Census Tract 4082, Alameda County, California    White\n3 06001422100    Census Tract 4221, Alameda County, California Hispanic\n4 06001437701 Census Tract 4377.01, Alameda County, California Hispanic\n5 06001407200    Census Tract 4072, Alameda County, California    Asian\n6 06001435500    Census Tract 4355, Alameda County, California Hispanic\n  estimate moe                   geometry\n1     2039 418 POINT (-121.9283 37.58357)\n2      840 233 POINT (-122.1736 37.77785)\n3      612 269  POINT (-122.296 37.86856)\n4     1926 475 POINT (-122.0731 37.63225)\n5      804 318 POINT (-122.2186 37.77788)\n6     2225 548 POINT (-122.0919 37.68479)\n\n\nNow we can plot this data using the same workflow than our previews map.\n\nggplot() + \n  geom_sf(data = alameda_race_counts, color = \"lightgrey\", fill = \"white\") + \n  geom_sf(data = alameda_race_dots, aes(color = variable), size = 0.5, alpha = 0.8) +\n  scale_color_brewer(palette = \"Set2\") + \n  guides(color = guide_legend(override.aes = list(size = 3))) + ## overrides the size of the dots in the legend to make it more visible\n  theme_void() + \n  labs(color = \"Race / Ethnicity\",\n       caption = \"2017-2021 ACS | 1 dot = approximately 200 people\")\n\n\n\n\nHere we have our map. Each dot represent 200 people and each color a race or ethnicity. This allows us to wee the distribution through out the county and the areas with more racial mix and areas where one race predominates.\nThese are some of the examples of plotting census data or ACS in this case using static maps. There is a lot more to cover when talking about maps! So we encourage you to check out the resources linked below."
  },
  {
    "objectID": "session_01.html#resources",
    "href": "session_01.html#resources",
    "title": "1  Mapping Census Data",
    "section": "1.5 Resources",
    "text": "1.5 Resources\n\nThe tmap package (Tennekes 2018) is an alternative to ggplot2 for creating custom maps. T stands for “Thematic”, refering to the phenomena that is shown or plotted, for example demographical, social, cultural, or economic phenomena. This package includes a wide range of functionality for custom cartography. Example of tmap and tidycensus in Walker 2023, Chapter 6\nReactive mapping with Shiny\nSpatial Analysis with Census Data, Walker 2023, Chapter 7\nModeling Census Data, Walker 2023 Chapter 8. Indices for segregation and diversity are addresed in this chapter."
  },
  {
    "objectID": "session_01.html#your-turn",
    "href": "session_01.html#your-turn",
    "title": "1  Mapping Census Data",
    "section": "1.6 Your Turn",
    "text": "1.6 Your Turn\nNow is your turn to make some maps!\n\n\n\n\n\n\nExercise\n\n\n\n\nUse the load_variables() function to find one or more variables of your interest.\n\n\n\nAnswer\nvars_acs5 &lt;- load_variables(2021, \"acs5\")\n\n\n\nUse get_acs() to get spatial ACS data for the variable you selected in a location and geography of your choice.\n\n\n\nAnswer\n## Data for median gross rent by county in CA\nca_rent &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B25031_001\",\n  state = \"CA\",\n  year = 2021,\n  geometry = TRUE)\n\n## Data for median household income by county in CS\nca_income_county &lt;- get_acs(\n    geography = \"county\",\n    variables = \"B19013E_001\",\n    state = \"CA\",\n    year = 2021,\n    geometry = TRUE)\n\n\n\nUse any of the resources presented above to map the data.\nShare your maps on Slack!"
  },
  {
    "objectID": "session_02.html#learning-objective",
    "href": "session_02.html#learning-objective",
    "title": "2  Communicating Your Science",
    "section": "Learning Objective",
    "text": "Learning Objective\n\nDiscuss about the importance of science communication.\nDistinguish between how scientist communicate sciences vs how the rest of the world communicates.\nIntroduce and practice using the Message Box as a tool to communicate science to a specific audience."
  },
  {
    "objectID": "session_02.html#communicating-science",
    "href": "session_02.html#communicating-science",
    "title": "2  Communicating Your Science",
    "section": "2.1 Communicating Science",
    "text": "2.1 Communicating Science\nHow we communicate our science with the rest of the world matters! There are many different reasons and ways to engage and practice science communication. From writing a blog post as a creative outlet to posting in social media to make science accessible to a broader audience. Science communication is more and more becoming a formal responsibility within researchers as funding organizations encourage or require scientist to do outreach of their work (Borowiec (2023)).\nCommunication can come easy, but doing it well is hard. As scientist, it is important to spread the word of our discoveries, engage with non-scientist audiences, and build empathy and trust on what we do (PLOS SciCom). This way make sure the world understand how important, necessary and meaningful science is.\n\n\n\nJarreau, Paige B (2015): #MySciBlog Interviewee Motivations to Blog about Science\n\n\n\n\n2.1.1 Scholarly publications\n\n“The ingredients of good science are obvious—novelty of research topic, comprehensive coverage of the relevant literature, good data, good analysis including strong statistical support, and a thought-provoking discussion. The ingredients of good science reporting are obvious—good organization, the appropriate use of tables and figures, the right length, writing to the intended audience— do not ignore the obvious” - Bourne 2005\n\nPeer-reviewed publication is the primary mechanism for direct and efficient communication of research findings. Other scholarly communications include abstracts, technical reports, books and book chapters. These communications are largely directed towards students and peers; individuals learning about or engaged in the process of scientific research whether in a university, non-profit, agency, commercial or other setting.\nThe following tabling is a good summary of ‘10 Simple Rules’ for writing research papers (adapted from Zhang 2014, published in Budden and Michener, 2017)\n\n\n\n\n\n\n\n\nMake it a Driving Force\n\n“Design a project with an ultimate paper firmly in mind”\n\n\n\nLess Is More\n\n“Fewer but more significant papers serve both the research community and one’s career better than more papers of less significance”\n\n\n\nPick the Right Audience\n\n“This is critical for determining the organization of the paper and the level of detail of the story, so as to write the paper with the audience in mind.”\n\n\n\nBe Logical\n\n“The foundation of ‘’lively’’ writing for smooth reading is a sound and clear logic underlying the story of the paper.” “An effective tactic to help develop a sound logical flow is to imaginatively create a set of figures and tables, which will ultimately be developed from experimental results, and order them in a logical way based on the information flow through the experiments.”\n\n\n\nBe Thorough and Make It Complete\n\nPresent the central underlying hypotheses; interpret the insights gleaned from figures and tables and discuss their implications; provide sufficient context so the paper is self-contained; provide explicit results so readers do not need to perform their own calculations; and include self-contained figures and tables that are described in clear legends\n\n\n\nBe Concise\n\n“the delivery of a message is more rigorous if the writing is precise and concise”\n\n\n\nBe Artistic\n\n“concentrate on spelling, grammar, usage, and a ‘’lively’’ writing style that avoids successions of simple, boring, declarative sentences”\n\n\n\nBe Your Own Judge\n\nReview, revise and reiterate. “…put yourself completely in the shoes of a referee and scrutinize all the pieces—the significance of the work, the logic of the story, the correctness of the results and conclusions, the organization of the paper, and the presentation of the materials.”\n\n\n\nTest the Water in Your Own Backyard\n\n“…collect feedback and critiques from others, e.g., colleagues and collaborators.”\n\n\n\nBuild a Virtual Team of Collaborators\n\nTreat reviewers as collaborators and respond objectively to their criticisms and recommendations. This may entail redoing research and thoroughly re-writing a paper.\n\n\n\nEven though reporting our results in a clear and efficient way through scholarly publications is important, the ways scientist communicate is not how the rest of the rest of the world typically communicates. In a scientific paper, we establish credibility in the introduction and methods, provide detailed data and results, and then share the significance of our work in the discussion and conclusions. But the rest of the world leads with the impact, the take home message. An example of this are newspaper headlines.\n“But the rest of the world leads with the conclusions, because that’s what people want to know. What are your findings and why is this relevant to them? In other words, what’s your bottom line?”\n\n\n\n2.1.2 Other communications\nCommunicating your research outside of peer-reviewed journal articles is increasingly common, and important. These non academic communications can reach a more broad and diverse audience than traditional publications (and should be tailored for specific audience groups accordingly), are not subject to the same pay-walls as journal articles, and can augment and amplify scholarly publications. For example, this twitter announcement from Dr Heather Kropp, providing a synopsis of their synthesis publication in Environmental Research (note the reference to a repository where the data are located, though a DOI would be better).\n\nWhether this communication occurs through blogs, social media, or via interviews with others, developing practices to refine your messaging is critical for successful communication. One tool to support your communication practice is ‘The Message Box’ developed by COMPASS, an organization that helps scientists develop communications skills in order to share their knowledge and research across broad audiences without compromising the accuracy of their research."
  },
  {
    "objectID": "session_02.html#the-message-box",
    "href": "session_02.html#the-message-box",
    "title": "2  Communicating Your Science",
    "section": "2.2 The Message Box",
    "text": "2.2 The Message Box\n\n\nThe Message Box is a tool that helps researchers take the information they hold about their research and communicate it in a way that resonates with the chosen audience. It can be used to help prepare for interviews with journalists or employers, plan for a presentation, outline a paper or lecture, prepare a grant proposal, or clearly, and with relevancy, communicate your work to others. While the message box can be used in all these ways, you must first identify the audience for your communication.\n“Whether a journalist, a policymaker, a room of colleagues at a professional meeting, or a class of second-graders—doesn’t have deep knowledge of your subject matter. But that doesn’t mean you should explain everything you know in a fire hose of information. In fact, cognitive research tells us that the human brain can only absorb three to five pieces of information at a time. So prioritize what you share based on what your audience needs to know. Your goal as you fill out your Message Box is to identify the information that is critical to your audience—what really matters to them—and share that. Effective science communication is less about expounding on all the exciting details that you might want to convey, and more about knowing your audience and providing them with what they need or want to know from you.”\nThe Message Box comprises five sections to help you sort and distill your knowledge in a way that will resonate with your (chosen) audience.\nThe five sections of the Message Box are provided below. For a detailed explanation of the sections and guidance on how to use the Message Box, work through the Message Box Workbook\n\n2.2.1 Message Box Sections\nThe Issue\nThe Issue section in the center of the box identifies and describes the overarching issue or topic that you’re addressing in broad terms. It’s the big-picture context of your work. This should be very concise and clear; no more than a short phrase. You might find you revisit the Issue after you’ve filled out your Message Box, to see if your thinking on the overarching topic has changed since you started.\n\nDescribes the overarching issue or topic: Big Picture\nBroad enough to cover key points\nSpecific enough to set up what’s to come\nConcise and clear\n‘Frames’ the rest of the message box\n\nThe Problem\nThe Problem is the chunk of the broader issue that you’re addressing in your area of expertise. It’s your piece of the pie, reflecting your work and expert knowledge. Think about your research questions and what aspect of the specific problem you’re addressing would matter to your audience. The Problem is also where you set up the So What and describe the situation you see and want to address.\n\nThe part of the broader issue that your work is addressing\nBuilds upon your work and expert knowledge\nTry to focus on one problem per audience\nOften the Problem is your research question\nThis section sets you up for So What\n\nThe So What\nThe crux of the Message Box, and the critical question the COMPASS team seeks to help scientists answer, is “So what?” Why should your audience care? What about your research or work is important for them to know? Why are you talking to them about it? The answer to this question may change from audience to audience, and you’ll want to be able to adjust based on their interests and needs. We like to use the analogy of putting a message through a prism that clarifies the importance to different audiences. Each audience will be interested in different facets of your work, and you want your message to reflect their interests and accommodate their needs. The prism below includes a spectrum of audiences you might want to reach, and some of the questions they might have about your work.\n\nThis is the crux of the message box\nWhy should you audience care?\nWhat about your research is important for them to know?\nWhy are you talking to them about it?\n\n\nThe Solution\nThe Solution section outlines the options for solving the problem you identified. When presenting possible solutions, consider whether they are something your audience can influence or act upon. And remind yourself of your communication goals: Why are you communicating with this audience? What do you want to accomplish?\n\nOutlines the options for solving the Problem\nCan your audience influence or act upon this?\nThere may be multiple solutions\nMake sure your Solution relates back to the Problem. Edit one or both as needed\n\nThe Benefit\nIn the Benefit section, you list the benefits of addressing the Problem — all the good things that could happen if your Solution section is implemented. This ties into the So What of why your audience cares, but focuses on the positive results of taking action (the So What may be a negative thing — for example, inaction could lead to consequences that your audience cares about). If possible, it can be helpful to be specific here — concrete examples are more compelling than abstract. Who is likely to benefit, and where, and when?\n\nWhat are the benefits of addressing the Problem?\nWhat good things come from implementing your Solution?\nMake sure it connects with your So What\nBenefits and So What may be similar\n\nFinally, to make you message more memorable you should:\n\nSupport your message with data\nLimit the use of numbers and statistics\nUse specific examples\nCompare numbers to concepts, help people relate\nAvoid jargon\nLead with what you know\n\n\nIn addition to the Message Box Workbook, COMPASS have resources on how to increase the impact of your message (include important statistics, draw comparisons, reduce jargon, use examples), exercises for practicing and refining your message and published examples."
  },
  {
    "objectID": "session_02.html#resources",
    "href": "session_02.html#resources",
    "title": "2  Communicating Your Science",
    "section": "2.3 Resources",
    "text": "2.3 Resources\n\n\n\nDataONE Webinar: Communication Strategies to Increase Your Impact from DataONE on Vimeo.\n\n\nBudden, AE and Michener, W (2017) Communicating and Disseminating Research Findings. In: Ecological Informatics, 3rd Edition. Recknagel, F, Michener, W (eds.) Springer-Verlag\nCOMPASS Core Principles of Science Communication\nExample Message Boxes"
  },
  {
    "objectID": "session_02.html#your-turn",
    "href": "session_02.html#your-turn",
    "title": "2  Communicating Your Science",
    "section": "2.4 Your Turn",
    "text": "2.4 Your Turn\nLet’s take a look on how the Message Box looks in practice.\n\n\n\n\n\n\nExercise\n\n\n\n\nLook into real examples of scienctist using the message box here.\nThink about your synthesis project and a potential audience you would like to communicate the results. Define your audience and start filling in the different components of the Message Box.\n\nNote: This is just the first iteration to help your think about your work in a different way.\n\n\n\n\n\n\nBorowiec, Brittney G. 2023. “Ten Simple Rules for Scientists Engaging in Science Communication.” PLOS Computational Biology 19 (7): 1–8. https://doi.org/10.1371/journal.pcbi.1011251."
  },
  {
    "objectID": "session_03.html",
    "href": "session_03.html",
    "title": "3  Hands On Synthesis",
    "section": "",
    "text": "Note\n\n\n\nIn this session, groups convene to collaborate on their respective synthesis projects. The objective for the week is to establish a well-defined roadmap for advancing each project and determine the methods for effectively communicating the results."
  },
  {
    "objectID": "session_04.html#learning-objectives",
    "href": "session_04.html#learning-objectives",
    "title": "4  Flexdashboard",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCreate and design customized dashboards using the R package flexdashboard\nBecome familiar with different flexdashboard components and flexdashboard syntax\nApply Markdown syntax, Shiny elements, and additional R packages like plotly to create visually appealing and interactive flexdashboards"
  },
  {
    "objectID": "session_04.html#what-is-a-flexdashboard",
    "href": "session_04.html#what-is-a-flexdashboard",
    "title": "4  Flexdashboard",
    "section": "4.1 What is a flexdashboard?",
    "text": "4.1 What is a flexdashboard?\nflexdashboard is an R package from RStudio that is built on top of R Markdown and Shiny. It allows us to create flexible, interactive dashboards using simple Markdown documents and syntax. Flexdashboards are designed to be easy to create, and support a wide variety of visualizations and interactive components. We can incorporate Shiny widgets and functionality into flexdashboards, making it a powerful tool for creating interactive reports and dashboards that can be shared with others.\n\n\n\n\n\n\nFlexdashboard is for R Markdown Only\n\n\n\nflexdashboard is only compatible with R Markdown documents, meaning we can’t use Quarto markdown documents (any files ending with .qmd). And that’s okay! Remember just because Quarto exists now, doesn’t mean R Markdown is going away or won’t be maintained - see Yihui Xie’s blog post With Quarto Coming, is R Markdown Going Away? No..\nIf you’re invested in only using Quarto tools - keep an eye on this discussion. Quarto developers are actively working on a dashboarding feature for Quarto as well."
  },
  {
    "objectID": "session_04.html#flexdashboard-vs-shiny",
    "href": "session_04.html#flexdashboard-vs-shiny",
    "title": "4  Flexdashboard",
    "section": "4.2 Flexdashboard vs Shiny",
    "text": "4.2 Flexdashboard vs Shiny\n\n\n\n\n\n\nWhen should I make a flexdashboard?\n\n\n\nFlexdashboards are great for creating lightweight interactive dashboards that require minimal coding expertise (must be familiar with Markdown!). Ultimately, it depends on what your final product is and what skillset your team has. Check out the diagram below and see what scenarios resonate best with you and your project goals.\n\n\n\n\n\n\nflowchart TD\n    A[Goal: Create a web-based application for data interaction]\n    A --&gt; B{Shiny App}\n    A --&gt; C{Flexdashboard}\n    C --&gt; D(Build a Flexdashboard if:)\n    B --&gt; E(Build a Shiny App if:)\n    D --&gt; F[Interested in quickly creating a dashboard prototype]\n    F --&gt; G[Have a preference for R Markdown]\n    G --&gt; H[There are non-programmers who need to create or maintain dashboards]\n    H --&gt; I[Want to blend narrative text with interactivity]\n    I --&gt; J[Prefer a simpler, code-light approach]\n    J --&gt; K[Dashboard requirements are relatively static]\n    E --&gt; L[Dashboard requires a highly customized user interface]\n    L --&gt; M[Dashboard needs to perform complex data analysis with user inputs]\n    M --&gt; N[Dashboard requires real-time data updates]\n    N --&gt; O[There are programmers familiar with reactive programming in R to create or maintain dashboards]\n    O --&gt; P[Dashboard requires a multi-page app with navigation]"
  },
  {
    "objectID": "session_04.html#flexdashboard-layout-features",
    "href": "session_04.html#flexdashboard-layout-features",
    "title": "4  Flexdashboard",
    "section": "4.3 Flexdashboard Layout + Features",
    "text": "4.3 Flexdashboard Layout + Features\nNow let’s familiarize ourselves with how an .Rmd is structured to create a flexdashboard and what the dashboard output looks like using the default template.\nThere are a two default templates for flexdashboard in RStudio - one with a theme and one without. We’ll first look at the template without a theme. To create a flexdashboard .Rmd from a template click:\nFile -&gt; New File -&gt; R Markdown -&gt; From Template -&gt; Flex Dashboard\n\n\n\n\n\n\n\nFlexdashboard Syntax\nIn the flexdashboard template to create the different sections in the dashboard, dashes (---) and equal signs (===) are being used (we’ll see the equal signs in action during the demo). The series of dashes and equal signs were a design choice by the flexdashboard creators to make the different sections stand out in the .Rmd, but are not mandatory to use.\n\n\n\n\n\n\nThe rule of thumb is that level-one headers create pages, level-two headers create columns or rows, and level-three headers create boxes.\n\n\n\n\n\n\n\n\n\n\nFlexdashboard Syntax\nEquivalent Markdown Header Syntax\n\n\n\n\nPage\n==========================\n# Page\n\n\nColumn\n--------------------------\n## Column\n\n\n### Box\n### Box\n\n\n\n\n\nFlexdashboard Attributes\nIn flexdashboard, we can add certain attributes to columns, rows, and boxes. This is similar to adding attributes to headings in typical .Rmd documents. For example, if we didn’t want a subheading to be numbered in a rendered HTML of a .Rmd, we would use ## My Subheading {.unnumbered}.\nIn both typical .Rmd and flexdashboard, the syntax for attributes is {key=value} or {.attribute}.\nSome attributes to add to columns, rows, or boxes include:\n\n\n\n\n\n\n\n{data-width=} and {data-height=}\nboth of these attributes set the relative size of columns, rows, and boxes. See complete size documentation on the flexdashboard website\n\n\n{data-orientation=}\nsets the dashboard layout to either rows or columns. This is a global option set in the YAML. However, if your dashboard has multiple pages and you want to specify the orientation for each page, remove orientation: from the YAML and use this attribute instead\n\n\n{.tabset}\ndivide columns, rows, or charts into tabs\n\n\n{.sidebar}\ncreates a sidebar on the left side. This sidebar is typically used to place Shiny inputs and is an optional step to add Shiny elements to a flexdashboard. See full documentation and steps in section 5.3.1 Getting Started in the R Markdown: Definitive Guide\n\n\n{data-navmenu=\"name of page\"}\nthis attribute creates a new navigation bar heading with the specified page as an item in a drop-down menu. When clicked, the menu item takes you to the associated page. For example, if the syntax is # Foo {data-navmenu=\"Bar\"}, “Bar” becomes a new heading in the navigation bar, and “Foo” is a page with dashboard components listed as a drop-down menu item under “Bar”\n\n\n{.hidden}\nexcludes a specific page from the navigation bar\n\n\n\n\n\n\n\n\n\nTo add multiple attributes within a set of curly braces {} by separating the attributes by either a space or a comma.\n\n\n\n\n\nFlexdashboard Components\nThe different components that can be added to a flexdashboard are:\n\n\n\n\n\n\n\nHTML Widgets\nincorporates JavaScript data visualization tools and libraries into a flexdashboard. This includes features like interactive plots, maps and more. At this time there are 130 htmlwidgets available to use in R, check out the gallery of widgets\n\n\nR Graphics\nany chart, plot or graph that is created using any R package\n\n\nTabular Data\nadd tables using knitr::kable() for simple tables or use the DT package for interactive tables\n\n\nValue Boxes\nuse valueBox() to display single values along with a title and optional icon\n\n\nGauges\ngauges are a type of data visualization that displays values on a meter within a specified range\n\n\nNavigation Bar\nthe navigation bar automatically includes the title, author, and date (if specified in the YAML). New pages are added to the navigation bar starting on the left side. There is also an option to add links to social media and the source code (specify this in the YAML)\n\n\nText\ntext can be added either at the top of the .Rmd before the setup chunk or in a box\n\n\n\n\n\n\n\n\n\nLearn more about flexdashboard components in the flexdashboard documentation for components on the flexdashboard website."
  },
  {
    "objectID": "session_04.html#demo-creating-a-flexdashboard",
    "href": "session_04.html#demo-creating-a-flexdashboard",
    "title": "4  Flexdashboard",
    "section": "4.4 Demo: Creating a flexdashboard",
    "text": "4.4 Demo: Creating a flexdashboard\n\n\n\n\n\n\nSetup\n\n\n\nFork the NCEAS/flexdashboard-demo-lh repository from the NCEAS GitHub organization and use the materials in the Git repo to follow along with the demonstration of flexdashboard examples.\n\n\nThe demonstration will include examples that showcase different flexdashboard features:\n\nBasic Flexdashboard from Template\nInteractive and Multiple Pages Flexdashboard\nReactive Flexdashboard using shiny elements\nThemed Flexdashboard using bslib\n\n\n\n\n\n\n\nExercise: Your turn!\n\n\n\nIn the Themed Flexdashboard, use the palmerpenguins data to complete the following tasks:\n\nFill in the boxes:\n\nIn the Chart A box, add a scatterplot of your choosing.\nIn the Chart B box, add a table using either kable() or DT.\nIn the Chart C box, add a valueBox.\nOptional Explore the htmlwidgets for R gallery, choose one you like and replace Chart D with that widget.\n\nChange the theme using bslib::bs_themer(). To activate the Theme Customizer, complete these steps:\n\nAdd to the YAML runtime: shiny\nAdd bslib::bs_themer() to the setup chunk\nSave the .Rmd\nClick “Run Document” and open the dashboard in a browser window for optimal experience\n\nAdd a new page then create a second Page using {data-navmenu}.\n\nNote: You’re welcome to use the code from the demo so you can quickly start playing with the different flexdashboard features."
  },
  {
    "objectID": "session_04.html#publishing-a-flexdashboard",
    "href": "session_04.html#publishing-a-flexdashboard",
    "title": "4  Flexdashboard",
    "section": "4.5 Publishing a Flexdashboard",
    "text": "4.5 Publishing a Flexdashboard\n\n\n\n\n\nPublish Button in RStudio IDE\n\n\n\nIf your flexdashboard does not have any Shiny components you can publish your flexdashboard using:\n\nRStudio IDE using the Publish Button and select a destination to publish to. See Posit’s documentation.\nGitHub Pages. Recall the lesson from Week Two’s coursebook, Publishing your analysis to the web with GitHub Pages.\n\n\n\nIf your flexdashboard does have Shiny components you will need to publish to shinyapps.io. This can be done using:\n\nRStudio IDE using the Publish Button.\nThe rsconnect package using rsconnect::deployApp().\n\n\n\n\n\n\n\nNote: You will need to create shinyapps.io Account first to publish to shinyapps.io."
  },
  {
    "objectID": "session_04.html#additional-resources",
    "href": "session_04.html#additional-resources",
    "title": "4  Flexdashboard",
    "section": "4.6 Additional Resources",
    "text": "4.6 Additional Resources\n\nRStudio flexdashboard vingettes (The articles under the “Articles” dropdown menu are particularly helpful!)\nRStudio flexdashboard Examples\nR Markdown: The Definitive Guide Chapter 5: Dashboards by Yihui Xie, J. J. Allaire, and Garrett Grolemund\nhtmlwidgets for R: Check out widgets featured either in the gallery or the showcase"
  },
  {
    "objectID": "session_05.html#learning-objectives",
    "href": "session_05.html#learning-objectives",
    "title": "5  Shiny",
    "section": "5.1 Learning Objectives",
    "text": "5.1 Learning Objectives\nIn this lesson we will:\n\nreview the capabilities in Shiny applications\nlearn about the basic layout for Shiny interfaces\nlearn about the server component for Shiny applications\nbuild a simple shiny application for interactive plotting"
  },
  {
    "objectID": "session_05.html#overview",
    "href": "session_05.html#overview",
    "title": "5  Shiny",
    "section": "5.2 Overview",
    "text": "5.2 Overview\nShiny is an R package for creating interactive data visualizations embedded in a web application that you and your colleagues can view with just a web browser. Shiny apps are relatively easy to construct, and provide interactive features for letting others share and explore data and analyses.\nThere are some really great examples of what Shiny can do on the RStudio webite like this one exploring movie metadata. A more scientific example is a tool from the SASAP project exploring proposal data from the Alaska Board of Fisheries. There is also an app for Delta monitoring efforts.\n\nMost any kind of analysis and visualization that you can do in R can be turned into a useful interactive visualization for the web that lets people explore your data more intuitively But, a Shiny application is not the best way to preserve or archive your data. Instead, for preservation use a repository that is archival in its mission like the KNB Data Repository, Zenodo, or Dryad. This will assign a citable identifier to the specific version of your data, which you can then read in an interactive visualiztion with Shiny.\nFor example, the data for the Alaska Board of Fisheries application is published on the KNB and is citable as:\nMeagan Krupa, Molly Cunfer, and Jeanette Clark. 2017. Alaska Board of Fisheries Proposals 1959-2016. Knowledge Network for Biocomplexity. doi:10.5063/F1QN652R.\nWhile that is the best citation and archival location of the dataset, using Shiny, one can also provide an easy-to-use exploratory web application that you use to make your point that directly loads the data from the archival site. For example, the Board of Fisheries application above lets people who are not inherently familiar with the data to generate graphs showing the relationships between the variables in the dataset.\nWe’re going to create a simple shiny app with two sliders so we can interactively control inputs to an R function. These sliders will allow us to interactively control a plot."
  },
  {
    "objectID": "session_05.html#create-a-sample-shiny-application",
    "href": "session_05.html#create-a-sample-shiny-application",
    "title": "5  Shiny",
    "section": "5.3 Create a sample shiny application",
    "text": "5.3 Create a sample shiny application\n\nFile &gt; New &gt; Shiny Web App…\nSet some fields: \n\nName it “myapp” or something else\nSelect “Single File”\nChoose to create it in a new folder called ‘shiny-demo’\nClick Create\n\n\nRStudio will create a new file called app.R that contains the Shiny application.\nRun it by choosing Run App from the RStudio editor header bar. This will bring up the default demo Shiny application, which plots a histogram and lets you control the number of bins in the plot.\n\nNote that you can drag the slider to change the number of bins in the histogram."
  },
  {
    "objectID": "session_05.html#shiny-architecture",
    "href": "session_05.html#shiny-architecture",
    "title": "5  Shiny",
    "section": "5.4 Shiny architecture",
    "text": "5.4 Shiny architecture\nA Shiny application consists of two functions, the ui and the server. The ui function is responsible for drawing the web page, while the server is responsible for any calculations and for creating any dynamic components to be rendered.\nEach time that a user makes a change to one of the interactive widgets, the ui grabs the new value (say, the new slider min and max) and sends a request to the server to re-render the output, passing it the new input values that the user had set. These interactions can sometimes happen on one computer (e.g., if the application is running in your local RStudio instance). Other times, the ui runs on the web browser on one computer, while the server runs on a remote computer somewhere else on the Internet (e.g., if the application is deployed to a web server)."
  },
  {
    "objectID": "session_05.html#interactive-scatterplots",
    "href": "session_05.html#interactive-scatterplots",
    "title": "5  Shiny",
    "section": "5.5 Interactive scatterplots",
    "text": "5.5 Interactive scatterplots\nLet’s modify this application to plot Yolo bypass secchi disk data in a time-series, and allow aspects of the plot to be interactively changed.\n\n5.5.1 Load data for the example\nUse this code to load the data at the top of your app.R script. Note we are using contentId again, and we have filtered for some species of interest.\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\nnames(delta_data)\n\n\n\n5.5.2 Add a simple timeseries using ggplot\nWe know there has been a lot of variation through time in the delta, so let’s plot a time-series of Secchi depth. We do so by switching out the histogram code for a simple ggplot, like so:\n\nserver &lt;- function(input, output) {\n    \n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            theme_light()\n    })\n}\n\nIf you now reload the app, it will display the simple time-series instead of the histogram. At this point, we haven’t added any interactivity.\nIn a Shiny application, the server function provides the part of the application that creates our interactive components, and returns them to the user interface (ui) to be displayed on the page.\n\n\n5.5.3 Add sliders to set the start and end date for the X axis\nTo make the plot interactive, first we need to modify our user interface to include widgits that we’ll use to control the plot. Specifically, we will add a new slider for setting the minDate parameter, and modify the existing slider to be used for the maxDate parameter. To do so, modify the sidebarPanel() call to include two sliderInput() function calls:\n\nsidebarPanel(\n    sliderInput(\"minDate\",\n                \"Min Date:\",\n                min = as.Date(\"1998-01-01\"),\n                max = as.Date(\"2020-01-01\"),\n                value = as.Date(\"1998-01-01\")),\n    sliderInput(\"maxDate\",\n                \"Max Date:\",\n                min = as.Date(\"1998-01-01\"),\n                max = as.Date(\"2020-01-01\"),\n                value = as.Date(\"2005-01-01\"))\n)\n\nIf you reload the app, you’ll see two new sliders, but if you change them, they don’t make any changes to the plot. Let’s fix that.\n\n\n5.5.4 Connect the slider values to the plot\nFinally, to make the plot interactive, we can use the input and output variables that are passed into the server function to access the current values of the sliders. In Shiny, each UI component is given an input identifier when it is created, which is used as the name of the value in the input list. So, we can access the minimum depth as input$minDate and the max as input$maxDate. Let’s use these values now by adding limits to our X axis in the ggplot:\n\n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$minDate,input$maxDate)) +\n            theme_light()\n\nAt this point, we have a fully interactive plot, and the sliders can be used to change the min and max of the Depth axis.\n\nLooks so shiny!\n\n\n5.5.5 Reversed Axes?\nWhat happens if a clever user sets the minimum for the X axis at a greater value than the maximum? You’ll see that the direction of the X axis becomes reversed, and the plotted points display right to left. This is really an error condition. Rather than use two independent sliders, we can modify the first slider to output a range of values, which will prevent the min from being greater than the max. You do so by setting the value of the slider to a vector of length 2, representing the default min and max date for the slider, such as c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")). So, delete the second slider, rename the first, and provide a vector for the value, like this:\n\nsliderInput(\"date\",\n            \"Date:\",\n            min = as.Date(\"1998-01-01\"),\n            max = as.Date(\"2020-01-01\"),\n            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n)\n\nNow, modify the ggplot to use this new date slider value, which now will be returned as a vector of length 2. The first element of the depth vector is the min, and the second is the max value on the slider.\n\n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            theme_light()"
  },
  {
    "objectID": "session_05.html#extending-the-user-interface-with-dynamic-plots",
    "href": "session_05.html#extending-the-user-interface-with-dynamic-plots",
    "title": "5  Shiny",
    "section": "5.6 Extending the user interface with dynamic plots",
    "text": "5.6 Extending the user interface with dynamic plots\nIf you want to display more than one plot in your application, and provide a different set of controls for each plot, the current layout would be too simple. Next we will extend the application to break the page up into vertical sections, and add a new plot in which the user can choose which variables are plotted. The current layout is set up such that the FluidPage contains the title element, and then a sidebarLayout, which is divided horizontally into a sidebarPanel and a mainPanel.\n\n\n5.6.1 Vertical layout\nTo extend the layout, we will first nest the existing sidebarLayout in a new verticalLayout, which simply flows components down the page vertically. Then we will add a new sidebarLayout to contain the bottom controls and graph.\n\nThis mechanism of alternately nesting vertical and horizontal panels can be used to segment the screen into boxes with rules about how each of the panels is resized, and how the content flows when the browser window is resized. The sidebarLayout works to keep the sidebar about 1/3 of the box, and the main panel about 2/3, which is a good proportion for our controls and plots. Add the verticalLayout, and the second sidebarLayout for the second plot as follows:\n\n    verticalLayout(\n        # Sidebar with a slider input for depth axis\n        sidebarLayout(\n            sidebarPanel(\n                sliderInput(\"date\",\n                            \"Date:\",\n                            min = as.Date(\"1998-01-01\"),\n                            max = as.Date(\"2020-01-01\"),\n                            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n            ),\n            # Show a plot of the generated distribution\n            mainPanel(\n               plotOutput(\"distPlot\")\n            )\n        ),\n\n        tags$hr(),\n\n        sidebarLayout(\n            sidebarPanel(\n                selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n            ),\n\n            # Show a plot with configurable axes\n            mainPanel(\n                plotOutput(\"varPlot\")\n            )\n        ),\n        tags$hr()\n\nNote that the second sidebarPanel uses three selectInput elements to provide dropdown menus with the variable columns (cols) from our data frame. To manage that, we need to first set up the cols variable, which we do by saving the variables names from the delta_data data frame to a variable:\n\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\ncols &lt;- names(delta_data)\n\n\n\n5.6.2 Add the dynamic plot\nBecause we named the second plot varPlot in our UI section, we now need to modify the server to produce this plot. Its very similar to the first plot, but this time we want to use the selected variables from the user controls to choose which variables are plotted. These variable names from the $input are character strings, and so would not be recognized as symbols in the aes mapping in ggplot. As recommended by the tidyverse authors, we use the non-standard evaluation syntax of .data[[\"colname\"]] to access the variables.\n\n    output$varPlot &lt;- renderPlot({\n      ggplot(delta_data, aes(x = .data[[input$x_variable]],\n                             y = .data[[input$y_variable]],\n                             color = .data[[input$color_variable]])) +\n        geom_point(size = 4)+\n        theme_light()\n    })"
  },
  {
    "objectID": "session_05.html#finishing-touches-data-citation",
    "href": "session_05.html#finishing-touches-data-citation",
    "title": "5  Shiny",
    "section": "5.7 Finishing touches: data citation",
    "text": "5.7 Finishing touches: data citation\nCiting the data that we used for this application is the right thing to do, and easy. You can add arbitrary HTML to the layout using utility functions in the tags list.\n\n    # Application title\n    titlePanel(\"Yolo Bypass Fish and Water Quality Data\"),\n    p(\"Data for this application are from: \"),\n    tags$ul(\n        tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n        )\n    ),\n    tags$br(),\n    tags$hr(),\n\nThe final application shows the data citation, the depth plot, and the configurable scatterplot in three distinct panels."
  },
  {
    "objectID": "session_05.html#publishing-shiny-applications",
    "href": "session_05.html#publishing-shiny-applications",
    "title": "5  Shiny",
    "section": "5.8 Publishing Shiny applications",
    "text": "5.8 Publishing Shiny applications\nOnce you’ve finished your app, you’ll want to share it with others. To do so, you need to publish it to a server that is set up to handle Shiny apps.\nYour main choices are:\n\nshinyapps.io (Hosted by RStudio)\n\nThis is a service offered by RStudio, which is initially free for 5 or fewer apps and for limited run time, but has paid tiers to support more demanding apps. You can deploy your app using a single button push from within RStudio.\n\nShiny server (On premises)\n\nThis is an open source server which you can deploy for free on your own hardware. It requires more setup and configuration, but it can be used without a fee.\n\nRStudio connect (On premises)\n\nThis is a paid product you install on your local hardware, and that contains the most advanced suite of services for hosting apps and RMarkdown reports. You can publish using a single button click from RStudio.\n\n\nA comparison of publishing features is available from RStudio.\n\n5.8.1 Publishing to shinyapps.io\nThe easiest path is to create an account on shinyapps.io, and then configure RStudio to use that account for publishing. Instructions for enabling your local RStudio to publish to your account are displayed when you first log into shinyapps.io:\n\nOnce your account is configured locally, you can simply use the Publish button from the application window in RStudio, and your app will be live before you know it!"
  },
  {
    "objectID": "session_05.html#summary",
    "href": "session_05.html#summary",
    "title": "5  Shiny",
    "section": "5.9 Summary",
    "text": "5.9 Summary\nShiny is a fantastic way to quickly and efficiently provide data exploration for your data and code. We highly recommend it for its interactivity, but an archival-quality repository is the best long-term home for your data and products. In this example, we used data drawn directly from the EDI repository in our Shiny app, which offers both the preservation guarantees of an archive, plus the interactive data exploration from Shiny. You can utilize the full power of R and the tidyverse for writing your interactive applications."
  },
  {
    "objectID": "session_05.html#full-source-code-for-the-final-application",
    "href": "session_05.html#full-source-code-for-the-final-application",
    "title": "5  Shiny",
    "section": "5.10 Full source code for the final application",
    "text": "5.10 Full source code for the final application\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n# read in the data from EDI\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\ncols &lt;- names(delta_data)\n    \n\n\n# Define UI for application that draws a two plots\nui &lt;- fluidPage(\n    \n    # Application title and data  source\n    titlePanel(\"Sacramento River floodplain fish and water quality dataa\"),\n    p(\"Data for this application are from: \"),\n    tags$ul(\n        tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n        )\n    ),\n    tags$br(),\n    tags$hr(),\n    \n    verticalLayout(\n        # Sidebar with a slider input for time axis\n        sidebarLayout(\n            sidebarPanel(\n                sliderInput(\"date\",\n                            \"Date:\",\n                            min = as.Date(\"1998-01-01\"),\n                            max = as.Date(\"2020-01-01\"),\n                            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n            ),\n            # Show a plot of the generated timeseries\n            mainPanel(\n                plotOutput(\"distPlot\")\n            )\n        ),\n        \n        tags$hr(),\n        \n        sidebarLayout(\n            sidebarPanel(\n                selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n            ),\n            \n            # Show a plot with configurable axes\n            mainPanel(\n                plotOutput(\"varPlot\")\n            )\n        ),\n        tags$hr()\n    )\n)\n\n# Define server logic required to draw the two plots\nserver &lt;- function(input, output) {\n    \n    #  turbidity plot\n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            theme_light()\n    })\n    \n    # mix and  match plot\n    output$varPlot &lt;- renderPlot({\n      ggplot(delta_data, aes(x = .data[[input$x_variable]],\n                             y = .data[[input$y_variable]],\n                             color = .data[[input$color_variable]])) +\n        geom_point(size = 4) +\n        theme_light()\n    })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "session_05.html#a-shinier-app-with-tabs-and-a-map",
    "href": "session_05.html#a-shinier-app-with-tabs-and-a-map",
    "title": "5  Shiny",
    "section": "5.11 A shinier app with tabs and a map!",
    "text": "5.11 A shinier app with tabs and a map!\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(shinythemes)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(snakecase)\n\n# read in the data from EDI\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName)) %&gt;% \n    rename(DissolvedOxygen = DO,\n           Ph = pH,\n           SpecificConductivity = SpCnd)\n\ncols &lt;- names(delta_data)\n\nsites &lt;- delta_data %&gt;% \n    distinct(StationCode, Latitude, Longitude) %&gt;% \n    drop_na() %&gt;% \n    st_as_sf(coords = c('Longitude','Latitude'), crs = 4269,  remove = FALSE)\n\n\n\n# Define UI for application\nui &lt;- fluidPage(\n    navbarPage(theme = shinytheme(\"flatly\"), collapsible = TRUE,\n               HTML('&lt;a style=\"text-decoration:none;cursor:default;color:#FFFFFF;\" class=\"active\" href=\"#\"&gt;Sacramento River Floodplain Data&lt;/a&gt;'), id=\"nav\",\n               windowTitle = \"Sacramento River floodplain fish and water quality data\",\n               \n               tabPanel(\"Data Sources\",\n                        verticalLayout(\n                            # Application title and data  source\n                            titlePanel(\"Sacramento River floodplain fish and water quality data\"),\n                            p(\"Data for this application are from: \"),\n                            tags$ul(\n                                tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                                        tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n                                )\n                            ),\n                            tags$br(),\n                            tags$hr(),\n                            p(\"Map of sampling locations\"),\n                            mainPanel(leafletOutput(\"map\"))\n                        )\n               ),\n               \n               tabPanel(\n                   \"Explore\",\n                   verticalLayout(\n                       mainPanel(\n                           plotOutput(\"distPlot\"),\n                           width =  12,\n                           absolutePanel(id = \"controls\",\n                                         class = \"panel panel-default\",\n                                         top = 175, left = 75, width = 300, fixed=TRUE,\n                                         draggable = TRUE, height = \"auto\",\n                                         sliderInput(\"date\",\n                                                     \"Date:\",\n                                                     min = as.Date(\"1998-01-01\"),\n                                                     max = as.Date(\"2020-01-01\"),\n                                                     value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n                                         \n                           )\n                       ),\n                       \n                       tags$hr(),\n                       \n                       sidebarLayout(\n                           sidebarPanel(\n                               selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                               selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                               selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n                           ),\n                           \n                           # Show a plot with configurable axes\n                           mainPanel(\n                               plotOutput(\"varPlot\")\n                           )\n                       ),\n                       tags$hr()\n                   )\n               )\n    )\n)\n\n# Define server logic required to draw the two plots\nserver &lt;- function(input, output) {\n    \n    \n    output$map &lt;- renderLeaflet({leaflet(sites) %&gt;% \n            addTiles() %&gt;% \n            addCircleMarkers(data = sites,\n                             lat = ~Latitude,\n                             lng = ~Longitude,\n                             radius = 10, # arbitrary scaling\n                             fillColor = \"gray\",\n                             fillOpacity = 1,\n                             weight = 0.25,\n                             color = \"black\",\n                             label = ~StationCode)\n    })\n    \n    #  turbidity plot\n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            labs(x = \"Sample Date\", y = \"Secchi Depth (m)\") +\n            theme_light()\n    })\n    \n    # mix and  match plot\n    output$varPlot &lt;- renderPlot({\n        ggplot(delta_data, mapping = aes(x = .data[[input$x_variable]],\n                                         y = .data[[input$y_variable]],\n                                         color = .data[[input$color_variable]])) +\n            labs(x = to_any_case(input$x_variable, case = \"title\"),\n                 y = to_any_case(input$y_variable, case = \"title\"),\n                 color = to_any_case(input$color_variable, case = \"title\")) +\n            geom_point(size=4) +\n            theme_light()\n    })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "session_05.html#resources",
    "href": "session_05.html#resources",
    "title": "5  Shiny",
    "section": "5.12 Resources",
    "text": "5.12 Resources\n\nMain Shiny site\nOfficial Shiny Tutorial"
  },
  {
    "objectID": "session_06.html#learning-objectives",
    "href": "session_06.html#learning-objectives",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nApply design thinking approaches and tools to dashboard development\nEvaluate whether to build a web application using flexdashboard or shiny\nPractice developing a web application using flexdashboard or shiny\nDemonstrate design knowledge including structure of layout, incorporation of data visualization, and choice of interactive elements\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThis lesson has been adapted by the Women in Data Science (WiDS) Workshop: Dashboard Design Thinking by Jenn Schilling.\nWe highly encourage you to watch the full workshop for more details on applying design thinking to dashboards."
  },
  {
    "objectID": "session_06.html#what-is-design-thinking",
    "href": "session_06.html#what-is-design-thinking",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "6.1 What is Design Thinking?",
    "text": "6.1 What is Design Thinking?\nDesign thinking in data science centers the user’s perspective throughout the entire development of a dashboard. It starts with the user by gaining an understanding on what the user needs and ends with the user by involving them in focus groups or other testing methods to get their feedback. Centering the user means we create better dashboards and improve data-informed decision making.\nDesign thinking is inherently an innovative and collaborative process because it involves exploring multiple options for the same problem.\nThe design thinking process can be encapsulated within these stages:\n\nEmpathize and Define: In these stages, we use tools to understand the problem and our audiences.\nIdeate and Prototype: We use these stages to explore solutions.\nTest and Implement: In these final stages, we use tools to materialize the final product.\n\nThese stages are not meant to be in any particular order. While there is an inherent order to them, you should feel empowered to move between the stages as needed — depending on the development of your dashboard and the needs of your users."
  },
  {
    "objectID": "session_06.html#empathize-and-define",
    "href": "session_06.html#empathize-and-define",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "6.2 Empathize and Define",
    "text": "6.2 Empathize and Define\n\n\n\n\n\n\nEmpathize: Gather information and survey your users to understand what they need to know from the data, what are the use cases for the dashboard and its overall purpose.\n\n\n\n\nEmpathize Tool: Empathy Mapping\nDuring the Empathize and Define stages, you can gather information through internal conversations with your team or conduct interviews with potential users.\nThe goal of an Empathy Map is to capture what a user says, thinks, does, and feels — additionally consider what their goals are (and do their goals align the goals you have in mind for the dashboard you’re developing?). Check out the empathy map the Nielsen Norman Group created for a customer, Jamie, who is buying a television.\n\n\n\nSource: Nielsen Norman Group\n\n\n\n\n\n\n\n\nCreate Your Own Empathy Map\n\n\n\nUse the NCEAS Learning Hub Empathy Map template to get started — adapt it as needed for your project/user(s).\n\n\n\n\n\n\n\n\n\nDefine: Identify the core problems your users have in accessing and interpreting the data in its current state.\n\n\n\n\n\nDefine Tool: Project Brief Wiki\nThe Empathize Stage helped us understand our audience. Now, we need to understand our problem. In the Define Stage, write down problem statements and document the specifications of the dashboard including the scope, timeline, and roles. It’s best to contain all this information within a Project Brief — a condensed version of a project plan.\nYou should feel comfortable sharing this document with not only your team, but your users as well. This ensures that everyone is on the same page. Keeping this in mind, consider having your Project Brief as a Wiki on GitHub in the Git repository for your dashboard.\nWhen defining the dashboard you want to create, consider these types:\n\nExecutive Dashboard: shows the big picture; used for high-level overview\nOperational Dashboard: shows what’s happening right now; used for monitoring\nStrategic Dashboard: shows progress toward a goal; used to inform decisions\nAnalytical Dashboard: shows detailed analysis; used for identifying opportunities and deep analysis\n\n\n\n\n\n\n\nWriting a Project Brief\n\n\n\nCheck out Asana’s article 5 steps to writing a clear project brief as guidance — use only what you need for your project."
  },
  {
    "objectID": "session_06.html#ideate-and-protoype",
    "href": "session_06.html#ideate-and-protoype",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "6.3 Ideate and Protoype",
    "text": "6.3 Ideate and Protoype\n\n\n\n\n\n\nIdeate: Challenge your existing assumptions and create ideas. Look for alternative ways to view the problem and identify solutions.\n\n\n\n\nIdeate Tool: Host a Brainstorm Session\nNow that we’ve learned about our audiences and problems from the Empathize & Define stages, we can start to think about what ideas for dashboard solutions. The primary goal of this stage is to generate as many ideas as possible without judgement and evaluation — it’s all about quantity over quality. Once you’ve generated many ideas, then you can start to narrow it down to a single dashboard solution.\nThere are many ways to ideate individually or as a group, but here are some tips:\n\nUse mind mapping tools like Miro for virtual collaboration.\nIf you’re working together in-person, use a white board, sticky notes, or a large piece of construction paper to capture as many ideas from as many team members as possible.\nTake a break!\nGroup related ideas together.\nSet a time limit. This can vary from a few hours to multiple sessions throughout a week. Either way setting a time limit and making a plan to regroup and narrow down on ideas is helpful so that the ideation period doesn’t feel so nebulous.\n\n\n\n\n\n\n\nShiny vs Flexdashboard\n\n\n\nThis is a good time to consider which R dashboard package may be more useful for your final dashboard product. Recall the diagram from the Flexdashboard Lesson to help determine which tool makes the most sense for your project.\n\n\n\n\n\n\n\n\n\nPrototype: Start to create the solutions. It’s important to experiment here. Produce some inexpensive and low-intensive versions of the product.\n\n\n\n\n\nPrototype Tool: Create a Minimal Viable Dashboard\nAt the Prototype Stage, we can experiment with a few of the ideas from the Ideate Stage to identify what is the best solution. Here we can create a quick, scaled down version of the dashboard. The primary goal is to create something more refined from the Ideate Stage, but not a final or completely usable dashboard. We want the prototype to be a minimal viable product where you can test out some functionality without committing to multiple iterations.\n\n\n\n\n\n\nTools to use for Prototyping\n\n\n\n\nGoogle Slides\nMicrosoft PowerPoint\nR Flexdashboard (this is a great option for prototyping if you’re set on creating a Shiny App since you can insert some Shiny elements in a Flexdashboard)\nGood ole’ pen and paper!"
  },
  {
    "objectID": "session_06.html#test-and-implement",
    "href": "session_06.html#test-and-implement",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "6.4 Test and Implement",
    "text": "6.4 Test and Implement\n\n\n\n\n\n\nTest: Try your solutions and evaluate the results. Return to your users for feedback to incorporate.\n\n\n\n\nTest Tool: Run Tests and Review Work from Previous Stages\nYou’ve completed your prototype — it’s time to test it out and receive feedback on it! Review your project brief and confirm your prototype reflects your users’ needs and the use cases you’re trying to achieve.\nIf you don’t have access to your users’, return to your Empathy Maps to make sure your prototype is aligned with the information you have gathered there.\nOther tests to consider:\n\nDoes your dashboard work across different devices?\nDoes it meet accessibility requirements?\nDoes the users’ dashboard experience align with how you think their experience should go?\n\n\n\n\n\n\n\nInteraction Design Foundation Testing Prototype Guidelines\n\n\n\nRead the Interaction Design Foundation’s article Test Your Prototypes: How to Gather Feedback and Maximize Learning for more tips, tricks, and templates for testing your dashboard.\n\n\n\n\n\n\n\n\n\nImplement: Put your vision into effect! Remember that the process doesn’t have to end here at implementation. Return to your users to gain more feedback and to guide the refinement of your solutions.\n\n\n\nIf the test you run reveal that your dashboard is not meeting the goals of your team or your users, then it’s time to go back to the Prototype Stage (or even a different stage) and iterate before you complete the Implementation stage. You will also want to return to your Project Brief, Empathy Map, and additional successful metrics to ensure that your dashboard meets most of these goals and needs.\nWhen implementing the dashboard:\n\nAdd context and definitions to the dashboard.\nTest again!\nInternally: validate your data.\nExternally: users test and provide final feedback.\nHow are you going to communicate your dashboard and make it accessible to your audience?\n\n\n\n\n\n\n\nThat’s Not All Folks!\n\n\n\nDesign Thinking is an approach that centers the user in the development of a product. When developing dashboards or other data products there are definitely more resources and approaches out there.\nIt’s worth doing additional research on aspects like testing, design, and data visualization to create the most robust data product as possible."
  },
  {
    "objectID": "session_07.html",
    "href": "session_07.html",
    "title": "7  Hands On Synthesis",
    "section": "",
    "text": "Note\n\n\n\nTime for groups to get together and advance in their synthesis projects."
  },
  {
    "objectID": "session_08.html#learning-objectives",
    "href": "session_08.html#learning-objectives",
    "title": "Reproducible Workflows Using targets",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIllustrate importance of utilizing reproducible workflows to increase project efficiency and sharing of analyses, as well as reducing computational resources.\nApply the R package targets workflow framework to scientific projects.\nOrganize projects in modular steps to improve understanding and computational flexibility.\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThis lesson is adapted from the journal article Improving ecological data science with workflow management software by Brousil et al, and the journal’s accompanying example, A worked targets example for ecologists.\n\n\nAll research projects have a workflow of some kind and typically includes steps like: data preparation and harmonization, running analyses or models, creating visualizations, and more.\nHowever, many environmental research projects are becoming increasingly more complex as researchers are utilizing larger datasets that require complicated analytical methods. More complexity means more steps, and more room for error or poor organizational methods that make projects difficult to reproduce.\nThis is where reproducible workflow tools and packages, like the R package targets, can play a huge role in streamlining complex workflows and ease the organization and sharing of projects.\nOther interchangeable terms we hear for workflows are:\n\nWorkflow Management Systems (WMS)\nData pipelines\nData workflow"
  },
  {
    "objectID": "session_08.html#benefits-of-reproducible-workflows",
    "href": "session_08.html#benefits-of-reproducible-workflows",
    "title": "Reproducible Workflows Using targets",
    "section": "8.1 Benefits of Reproducible Workflows",
    "text": "8.1 Benefits of Reproducible Workflows\nA major benefit of WMS is the capacity to track the status of all required files and functions to prevent steps in a larger pipeline from being skipped and by ensuring that data are kept up to date as models or harmonization routines change. Despite WMS’s benefits, adopting a WMS requires moving away from performing serial analytical operations within single or multiple scripts to instead breaking an analysis into smaller functions that are modular (Figure 1), thereby providing more computational flexibility. (Brousil et al. 2023)"
  },
  {
    "objectID": "session_08.html#challenges-of-reproducible-workflows",
    "href": "session_08.html#challenges-of-reproducible-workflows",
    "title": "Reproducible Workflows Using targets",
    "section": "8.2 Challenges of Reproducible Workflows",
    "text": "8.2 Challenges of Reproducible Workflows\nWMS requires moving away from performing serial analytical operations within single or multiple scripts to instead breaking an analysis into smaller functions that are modular, thereby providing more computational flexibility (Brousil et al. 2023)\nIndeed, researchers will face a familiar trade-off—to invest the personnel time in learning and deploying a tool like WMS to save personnel and compute time later or to accept that less efficient, but more familiar analytical frameworks will come with costs in personnel time, compute time and potentially additional associated payments for computing resources. The balance will depend on the complexity of the analysis, the expectation of reusing the code over time and the resources available to the researcher (e.g. funds for computing or personnel) (Brousil et al. 2023)."
  },
  {
    "objectID": "session_08.html#leveraging-reproducible-workflows-tools",
    "href": "session_08.html#leveraging-reproducible-workflows-tools",
    "title": "Reproducible Workflows Using targets",
    "section": "8.3 Leveraging Reproducible Workflows & Tools",
    "text": "8.3 Leveraging Reproducible Workflows & Tools\nWMS may not be needed by most beginners, but learning about these tools early may inspire researchers with the understanding that their analyses can scale with the ambition and complexity of their most pressing research questions. Implementing WMS is an investment that does take time but can save a great deal of time and frustration later (Brousil et al. 2023).\n\nR Package: targets for Reproducible Worklows\n\n\n\n\n\n\nWhat is the targets package?\n\n\n\ntargets is a package for R that analyzes and manages your workflow or analysis pipeline. It helps to save its users time by re-running outdated analysis steps only when needed. targets is a part of rOpenSci.\ntargets can also help users build, visualize, and manage workflows from raw files to outputs."
  },
  {
    "objectID": "session_08.html#exercise-creating-a-pipeline-using-targets",
    "href": "session_08.html#exercise-creating-a-pipeline-using-targets",
    "title": "Reproducible Workflows Using targets",
    "section": "8.4 Exercise: Creating a Pipeline using targets",
    "text": "8.4 Exercise: Creating a Pipeline using targets\n\n\n\n\nBrousil, Matthew R., Alessandro Filazzola, Michael F. Meyer, Sapna Sharma, and Stephanie E. Hampton. 2023. “Improving Ecological Data Science with Workflow Management Software.” Methods in Ecology and Evolution 14 (6): 1381–88. https://doi.org/10.1111/2041-210x.14113."
  },
  {
    "objectID": "session_09.html#learning-objectives",
    "href": "session_09.html#learning-objectives",
    "title": "9  Parellel Processing",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand what parallel computing is and when it may be useful\nUnderstand how parallelism can work\nReview sequential loops and *apply functions\nUnderstand and use the parallel package multicore functions\nUnderstand and use the foreach package functions"
  },
  {
    "objectID": "session_09.html#introduction",
    "href": "session_09.html#introduction",
    "title": "9  Parellel Processing",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nProcessing large amounts of data with complex models can be time consuming. New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America:\n\n\n\nLevy et al. 2016. doi:10.5063/F1Z899CZ\n\n\nThere are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization.\nAlternatively, think of remote sensing data. Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years.\n\n\n\nNEON Data Cube"
  },
  {
    "objectID": "session_09.html#why-parallelism",
    "href": "session_09.html#why-parallelism",
    "title": "9  Parellel Processing",
    "section": "9.2 Why parallelism?",
    "text": "9.2 Why parallelism?\nMuch R code runs fast and fine on a single processor. But at times, computations can be:\n\ncpu-bound: Take too much cpu time\nmemory-bound: Take too much memory\nI/O-bound: Take too much time to read/write from disk\nnetwork-bound: Take too much time to transfer\n\nTo help with cpu-bound computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire cluster of those computers. Plus, these machines also have large amounts of memory to avoid memory-bound computing jobs."
  },
  {
    "objectID": "session_09.html#processors-cpus-and-cores",
    "href": "session_09.html#processors-cpus-and-cores",
    "title": "9  Parellel Processing",
    "section": "9.3 Processors (CPUs) and Cores",
    "text": "9.3 Processors (CPUs) and Cores\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with mutliple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations.\nA computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time.\n\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here we show four quad-core processors for a total of 16 cores in this machine.\n\nYou can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later).\nHistorically, R has only utilized one processor, which makes it single-threaded. Which is a shame, because the 2017 MacBook Pro that I am writing this on is much more powerful than that:\n\njones@powder:~$ sysctl hw.ncpu hw.physicalcpu\nhw.ncpu: 8\nhw.physicalcpu: 4\n\nTo interpret that output, this machine powder has 4 physical CPUs, each of which has two processing cores, for a total of 8 cores for computation. I’d sure like my R computations to use all of that processing power. Because its all on one machine, we can easily use multicore processing tools to make use of those cores. Now let’s look at the computational server aurora at NCEAS:\n\njones@aurora:~$ lscpu | egrep 'CPU\\(s\\)|per core|per socket'\nCPU(s):                88\nOn-line CPU(s) list:   0-87\nThread(s) per core:    2\nCore(s) per socket:    22\nNUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86\nNUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87\n\nNow that’s some compute power! Aurora has 384 GB of RAM, and ample storage. All still under the control of a single operating system.\nHowever, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now:\n\nJetStream\n\n640 nodes, 15,360 cores, 80TB RAM\n\nStampede2 at TACC is coming online in 2017\n\n4200 nodes, 285,600 cores\n\n\nNote that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster. One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine. But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster."
  },
  {
    "objectID": "session_09.html#when-to-parallelize",
    "href": "session_09.html#when-to-parallelize",
    "title": "9  Parellel Processing",
    "section": "9.4 When to parallelize",
    "text": "9.4 When to parallelize\nIt’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!\nIn addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced. Some propose that this may follow Amdahl’s Law, where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines):\n\n\n\n\n\nSo, its important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done. With that, let’s do some parallel computing…"
  },
  {
    "objectID": "session_09.html#loops-and-repetitive-tasks-using-lapply",
    "href": "session_09.html#loops-and-repetitive-tasks-using-lapply",
    "title": "9  Parellel Processing",
    "section": "9.5 Loops and repetitive tasks using lapply",
    "text": "9.5 Loops and repetitive tasks using lapply\nWhen you have a list of repetitive tasks, you may be able to speed it up by adding more computing power. If each task is completely independent of the others, then it is a prime candidate for executing those tasks in parallel, each on its own core. For example, let’s build a simple loop that uses sample with replacement to do a bootstrap analysis. In this case, we select Sepal.Length and Species from the iris dataset, subset it to 100 observations, and then iterate across 10,000 trials, each time resampling the observations with replacement. We then run a logistic regression fitting species as a function of length, and record the coefficients for each trial to be returned.\n\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- 10000\nres &lt;- data.frame()\nsystem.time({\n  trial &lt;- 1\n  while(trial &lt;= trials) {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    r &lt;- coefficients(result1)\n    res &lt;- rbind(res, r)\n    trial &lt;- trial + 1\n  }\n})\n\n   user  system elapsed \n 21.293   0.044  21.342 \n\n\nThe issue with this loop is that we execute each trial sequentially, which means that only one of our 8 processors on this machine are in use. In order to exploit parallelism, we need to be able to dispatch our tasks as functions, with one task going to each processor. To do that, we need to convert our task to a function, and then use the *apply() family of R functions to apply that function to all of the members of a set. In R, using apply used to be faster than the equivalent code in a loop, but now they are similar due to optimizations in R loop handling. However, using the function allows us to later take advantage of other approaches to parallelization. Here’s the same code rewritten to use lapply(), which applies a function to each of the members of a list (in this case the trials we want to run):\n\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- seq(1, 10000)\nboot_fx &lt;- function(trial) {\n  ind &lt;- sample(100, 100, replace=TRUE)\n  result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n  r &lt;- coefficients(result1)\n  res &lt;- rbind(data.frame(), r)\n}\nsystem.time({\n  results &lt;- lapply(trials, boot_fx)\n})\n\n   user  system elapsed \n 22.936   0.008  22.954"
  },
  {
    "objectID": "session_09.html#approaches-to-parallelization",
    "href": "session_09.html#approaches-to-parallelization",
    "title": "9  Parellel Processing",
    "section": "9.6 Approaches to parallelization",
    "text": "9.6 Approaches to parallelization\nWhen parallelizing jobs, one can:\n\nUse the multiple cores on a local computer through mclapply\nUse multiple processors on local (and remote) machines using makeCluster and clusterApply\n\nIn this approach, one has to manually copy data and code to each cluster member using clusterExport\nThis is extra work, but sometimes gaining access to a large cluster is worth it\n\n\n\n9.6.1 Parallelize using: mclapply\nThe parallel library can be used to send tasks (encoded as function calls) to each of the processing cores on your machine in parallel. This is done by using the parallel::mclapply function, which is analogous to lapply, but distributes the tasks to multiple processors. mclapply gathers up the responses from each of these function calls, and returns a list of responses that is the same length as the list or vector of input data (one return per input item).\n\nlibrary(parallel)\nlibrary(MASS)\n\nstarts &lt;- rep(100, 40)\nfx &lt;- function(nstart) kmeans(Boston, 4, nstart=nstart)\nnumCores &lt;- detectCores()\nnumCores\n\n[1] 2\n\nsystem.time(\n  results &lt;- lapply(starts, fx)\n)\n\n   user  system elapsed \n  1.246   0.004   1.251 \n\nsystem.time(\n  results &lt;- mclapply(starts, fx, mc.cores = numCores)\n)\n\n   user  system elapsed \n  0.676   0.095   0.807 \n\n\nNow let’s demonstrate with our bootstrap example: ::: {.cell}\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- seq(1, 10000)\nboot_fx &lt;- function(trial) {\n  ind &lt;- sample(100, 100, replace=TRUE)\n  result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n  r &lt;- coefficients(result1)\n  res &lt;- rbind(data.frame(), r)\n}\nsystem.time({\n  results &lt;- mclapply(trials, boot_fx, mc.cores = numCores)\n})\n\n   user  system elapsed \n 11.842   0.151  12.102 \n\n:::\n\n\n9.6.2 Parallelize using: foreach and doParallel\nThe normal for loop in R looks like:\n\nfor (i in 1:3) {\n  print(sqrt(i))\n}\n\n[1] 1\n[1] 1.414214\n[1] 1.732051\n\n\nThe foreach method is similar, but uses the sequential %do% operator to indicate an expression to run. Note the difference in the returned data structure. ::: {.cell}\nlibrary(foreach)\nforeach (i=1:3) %do% {\n  sqrt(i)\n}\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n:::\nIn addition, foreach supports a parallelizable operator %dopar% from the doParallel package. This allows each iteration through the loop to use different cores or different machines in a cluster. Here, we demonstrate with using all the cores on the current machine: ::: {.cell}\nlibrary(foreach)\nlibrary(doParallel)\n\nLoading required package: iterators\n\nregisterDoParallel(numCores)  # use multicore, set to the number of our cores\nforeach (i=1:3) %dopar% {\n  sqrt(i)\n}\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n# To simplify output, foreach has the .combine parameter that can simplify return values\n\n# Return a vector\nforeach (i=1:3, .combine=c) %dopar% {\n  sqrt(i)\n}\n\n[1] 1.000000 1.414214 1.732051\n\n# Return a data frame\nforeach (i=1:3, .combine=rbind) %dopar% {\n  sqrt(i)\n}\n\n             [,1]\nresult.1 1.000000\nresult.2 1.414214\nresult.3 1.732051\n\n:::\nThe doParallel vignette on CRAN shows a much more realistic example, where one can use `%dopar% to parallelize a bootstrap analysis where a data set is resampled 10,000 times and the analysis is rerun on each sample, and then the results combined:\n\n# Let's use the iris data set to do a parallel bootstrap\n# From the doParallel vignette, but slightly modified\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- 10000\nsystem.time({\n  r &lt;- foreach(icount(trials), .combine=rbind) %dopar% {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n\n   user  system elapsed \n 20.222   0.284  10.904 \n\n# And compare that to what it takes to do the same analysis in serial\nsystem.time({\n  r &lt;- foreach(icount(trials), .combine=rbind) %do% {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n\n   user  system elapsed \n 21.082   0.000  21.126 \n\n# When you're done, clean up the cluster\nstopImplicitCluster()"
  },
  {
    "objectID": "session_09.html#summary",
    "href": "session_09.html#summary",
    "title": "9  Parellel Processing",
    "section": "9.7 Summary",
    "text": "9.7 Summary\nIn this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores. Next, we reviewed the way in which traditional for loops in R can be rewritten as functions that are applied to a list serially using lapply, and then how the parallel package mclapply function can be substituted in order to utilize multiple cores on the local computer to speed up computations. Finally, we installed and reviewed the use of the foreach package with the %dopar operator to accomplish a similar parallelization using multiple cores."
  },
  {
    "objectID": "session_09.html#readings-and-tutorials",
    "href": "session_09.html#readings-and-tutorials",
    "title": "9  Parellel Processing",
    "section": "9.8 Readings and tutorials",
    "text": "9.8 Readings and tutorials\n\nMulticore Data Science with R and Python\nBeyond Single-Core R by Jonoathan Dursi (also see GitHub repo for slide source)\nThe venerable Parallel R by McCallum and Weston (a bit dated on the tooling, but conceptually solid)\nThe doParallel Vignette"
  },
  {
    "objectID": "session_10.html",
    "href": "session_10.html",
    "title": "10  Hands On Synthesis",
    "section": "",
    "text": "Note\n\n\n\nTime for groups to get together and advance in their synthesis projects."
  },
  {
    "objectID": "session_11.html#learning-objectives",
    "href": "session_11.html#learning-objectives",
    "title": "11  Publishing Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nOverview best practices for organizing data for publication\n\nReview what science metadata is and how it can be used\nDemonstrate how data and code can be documented and published in open data archives"
  },
  {
    "objectID": "session_11.html#the-data-life-cycle",
    "href": "session_11.html#the-data-life-cycle",
    "title": "11  Publishing Data",
    "section": "11.1 The Data Life Cycle",
    "text": "11.1 The Data Life Cycle\nThe Data Life Cycle gives you an overview of meaningful steps data goes through in a research project, from planning to archival. This step-by-step breakdown facilitates overseeing individual actions, operations and processes required at each stage.\n\n\n\n\nStep\nDescription\n\n\n\n\nPlan\nMap out the processes and resources for the entire data life cycle. Start with the project goals (desired outputs, outcomes, and impacts) and work backwards to build a data management plan, supporting data policies, and sustainability plans.\n\n\nCollect\nObservations are made either by hand or with sensors or other instruments and the data are placed a into digital form. You can structure the process of collecting data up front to better implement data management.\n\n\nAssure\nEmploy quality assurance and quality control procedures that enhance the quality of data (e.g., training participants, routine instrument calibration) and identify potential errors and techniques to address them.\n\n\nDescribe\nDocument data by describing the why, who, what, when, where, and how of the data. Metadata, or data about data, are key to data sharing and reuse, and many tools such as standards and software are available to help describe data.\n\n\nPreserve\nPlan to preserve data in the short term to minimize potential losses (e.g., via accidents), and in the long term so that project stakeholders and others can access, interpret, and use the data in the future. Decide what data to preserve, where to preserve it, and what documentation needs to accompany the data.\n\n\nDiscover\nIdentify complementary data sets that can add value to project data. Strategies to help endure the data have maximum impact include registering the project on a project directory site, depositing data in an open repository, and adding data descriptions to metadata clearing houses.\n\n\nIntegrate\nData from multiple sources are combined into a form that can be readily analyzed. For example, you could combine citizen science project data with other sources of data to enable new analyses and investigations. Successful data integration depends on documentation of the integration process, clearly citing and making accessable the data you are using, and employing good data management practices throughout the Data Life Cycle.\n\n\nAnalyze\nCreate analyses and visualizations to identify patterns, test hypotheses, and illustrate finding. During this process record your methods, document data processing steps, and ensure your data are reproduceable. Learn about these best practices and more.\n\n\n\nIn this lesson we focus on the Describe and Preserve stages of this cycle. However, best practices on how to organize and document your data, apply to all stages."
  },
  {
    "objectID": "session_11.html#organizing-data",
    "href": "session_11.html#organizing-data",
    "title": "11  Publishing Data",
    "section": "11.2 Organizing Data",
    "text": "11.2 Organizing Data\nThe goal is to operate through the data life cycle with the FAIR and CARE principles in mind and making sure our data is in a tidy format.\n\n\n\nArtwork by Allison Horst\n\n\nBenefits of having clean and tidy data and complete metadata:\n\nDecreases errors from redundant updates\nEnforces data integrity\nHelps you and future researchers to handle large, complex datasets\nEnables powerful search filtering\n\nSome of the best practices to follow are (Borer et al. (2009), White et al. (2013)):\n\nHave scripts for all data wrangling that start with the uncorrected raw data file and clean the data programmatically before analysis.\nDesign your tables to add rows, not columns. A column should be only one variable and a row should be only one observation.\nInclude header lines in your tables\nUse non-proprietary file formats (ie, open source) with descriptive file names without spaces.\n\nNon-proprietary file formats are essential to ensure that your data can still be machine readable long into the future. Open formats include text files and binary formats such as NetCDF.\n\nCommon switches:\n\n\n\n\n\n\n\nProprietary format\nExport to…\n\n\n\n\nMicrosoft Excel (.xlsx) files\ntext (.txt) or comma separated values (.csv)\n\n\nGIS files\nESRI shapefiles (.shp)\n\n\nMATLAB/IDL\nNetCDF\n\n\n\n\n\n\n\n\n\nLarge Data Packages\n\n\n\nWhen you have or are going to generate large data packages (in the terabytes or larger), it’s important to establish a relationship with the data center early on.\nThe data center can help come up with a strategy to tile data structures by subset, such as by spatial region, by temporal window, or by measured variable. They can also help with choosing an efficient tool to store the data (ie NetCDF or HDF), which is a compact data format that helps parallel read and write libraries of data."
  },
  {
    "objectID": "session_11.html#metadata",
    "href": "session_11.html#metadata",
    "title": "11  Publishing Data",
    "section": "11.3 Metadata",
    "text": "11.3 Metadata\nWithin the data life cycle you can be collecting data (creating new data) or integrating data that has all ready been collected. Either way, metadata plays a major role to successfully spin around the cycle because it enables data reuse long after the original collection.\nImagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files?\nThe goal is to have enough information for the researcher to understand the data, interpret the data, and then reuse the data in another study.\nOne way to think about metadata is to answer the following questions with the documentation:\n\nWhat was measured?\nWho measured it?\nWhen was it measured?\nWhere was it measured?\nHow was it measured?\nHow is the data structured?\nWhy was the data collected?\nWho should get credit for this data (researcher AND funding agency)?\nHow can this data be reused (licensing)?\n\nWe also know that it is important to keep in mind how will computers organize and integrate this information. There are a number of metadata standards that make your metadata machine readable and therefore easier for data curators to publish your data (Ecological Metadata Language, Geospatial Metadata Standards, Biological Data Profile, Darwin Core, Metadata Encoding Transmission Standard, etc.)\nToday we are going to be focusing on the Ecological Metadata Language also known as EML. Which is widespread use in the earth and environmental sciences.\n\n“The Ecological Metadata Language (EML) defines a comprehensive vocabulary and a readable XML markup syntax for documenting research data” (https://eml.ecoinformatics.org/)\n\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;eml:eml packageId=\"df35d.442.6\" system=\"knb\" \n    xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.1\"&gt;\n    &lt;dataset&gt;\n        &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through \n            Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt;\n        &lt;creator id=\"1385594069457\"&gt;\n            &lt;individualName&gt;\n                &lt;givenName&gt;Mark&lt;/givenName&gt;\n                &lt;surName&gt;Willette&lt;/surName&gt;\n            &lt;/individualName&gt;\n            &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt;\n            &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt;\n            &lt;address&gt;\n                &lt;city&gt;Soldotna&lt;/city&gt;\n                &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt;\n                &lt;country&gt;USA&lt;/country&gt;\n            &lt;/address&gt;\n            &lt;phone phonetype=\"voice\"&gt;(907)260-2911&lt;/phone&gt;\n            &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt;\n        &lt;/creator&gt;\n        ...\n    &lt;/dataset&gt;\n&lt;/eml:eml&gt;"
  },
  {
    "objectID": "session_11.html#data-identifiers-citation",
    "href": "session_11.html#data-identifiers-citation",
    "title": "11  Publishing Data",
    "section": "11.4 Data Identifiers & Citation",
    "text": "11.4 Data Identifiers & Citation\nMany journals require a DOI - a digital object identifier - be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to.\nKeep in mind that generally, if the data package needs to be updated (which happens in many cases), each version of the package will get its own identifier. This way, researchers can and should cite the exact version of the data set that they used in their analysis. Having the data identified in this manner allows us to accurately track the dataset usage metrics.\n\nFinally, stressed that researchers should get in the habit of citing the data that they use (even if it’s their own data!) in each publication that uses that data. This is important for correct attribution, provenance of your work and ultimately transparency in the scientific process."
  },
  {
    "objectID": "session_11.html#provenance-and-computational-workflow",
    "href": "session_11.html#provenance-and-computational-workflow",
    "title": "11  Publishing Data",
    "section": "11.5 Provenance and Computational Workflow",
    "text": "11.5 Provenance and Computational Workflow\n\nWhile the Knowledge Network for Biocomplexity, and similar repositories do focus on preserving data, we really set our sights much more broadly on preserving entire computational workflows that are instrumental to advances in science. A computational workflow represents the sequence of computational tasks that are performed from raw data acquisition through data quality control, integration, analysis, modeling, and visualization.\nFor example, a data acquisition and cleaning workflow often creates a derived and integrated data product that is then picked up and used by multiple downstream analytical workflows that produce specific scientific findings. These workflows can each be archived as distinct data packages, with the output of the first workflow becoming the input of the second and subsequent workflows.\n\nAdding provenance within your work makes it more reproducible and compliant with the FAIR principles. It is also useful for building on the work of others; you can produce similar visualizations for another location, for example, using the same code.\nTools like Quarto can be used as a provenance tool, as well - by starting with the raw data and cleaning it programmatically, rather than manually, you preserve the steps that you went through and your workflow is reproducible."
  },
  {
    "objectID": "session_11.html#preserving-your-data",
    "href": "session_11.html#preserving-your-data",
    "title": "11  Publishing Data",
    "section": "11.6 Preserving your data",
    "text": "11.6 Preserving your data\n\n\n\n\n\n11.6.1 Data repositories: built for data (and code)\n\nGitHub is not an archival location\nDedicated data repositories: KNB, Arctic Data Center, Zenodo, FigShare\n\nRich metadata\nArchival in their mission\n\nData papers, e.g., Scientific Data\nList of data repositories: http://re3data.org\n\n\nDataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses.\nDataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data form the dozens of members of DataONE, rather than visiting each of the currently 43 repositories one at a time.\n\n\n\n11.6.2 Structure of a data package\nNote that the data set above lists a collection of files that are contained within the data set. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\n These data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets an internal identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4."
  },
  {
    "objectID": "session_11.html#publishing-data-from-the-web",
    "href": "session_11.html#publishing-data-from-the-web",
    "title": "11  Publishing Data",
    "section": "11.7 Publishing data from the web",
    "text": "11.7 Publishing data from the web\nEach data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the EDI, they provide some easy to use web forms for editing and submitting a data package.\n\n11.7.1 Publishing Data to EDI\nShort explanations and DEMO\n\n\n11.7.2 Publishing Data to KNB\nShort Explanation and Practice\n\n\n\n\n\n\nSetup\n\n\n\nDownload the data to be used for the tutorial\nI’ve already uploaded the test data package, and so you can access the data here:\n\nhttps://dev.nceas.ucsb.edu/view/urn:uuid:0702cc63-4483-4af4-a218-531ccc59069f\n\nGrab both CSV files, and the R script, and store them in a convenient folder.\n\n\n\n\n11.7.2.1 Login via ORCID\nWe will walk through web submission on https://demo.nceas.ucsb.edu, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button.\n\nWhen you sign in, you will be redirected to orcid.org, where you can either provide your existing ORCID credentials, or create a new account. ORCID provides multiple ways to login, including using your email address, institutional logins from many universities, and logins from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login.\n\n\n\nCreate and submit the data set\nAfter signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. Required sections are listed with a red asterisk.\n\nClick Add Files to choose the data files for your package\nYou can select multiple files at a time to efficiently upload many files.\n\nThe files will upload showing a progress indicator. You can continue editing metadata while they upload.\n\n\n\nEnter Overview information\nThis includes a descriptive title, abstract, and keywords.\nThe title is the first way a potential user will get information about your dataset. It should be descriptive but succinct, lack acronyms, and include some indication of the temporal and geospatial coverage of the data.\nThe abstract should be sufficiently descriptive for a general scientific audience to understand your dataset at a high level. It should provide an overview of the scientific context/project/hypotheses, how this data package fits into the larger context, a synopsis of the experimental or sampling design, and a summary of the data contents.\n\nKeywords, while not required, can help increase the searchability of your dataset, particularly if they come from a semantically defined keyword thesaurus.\n\nOptionally, you can also enter funding information, including a funding number, which can help link multiple datasets collected under the same grant.\nSelecting a distribution license - either CC-0 or CC-BY is required.\n\n\n\nPeople Information\nInformation about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people:\n\nCreators - all the people who should be in the citation for the dataset\nContacts - one is required, but defaults to the first Creator if omitted\nPrincipal Investigators\nand any other that are relevant\n\nFor each, please strive to provide their ORCID identifier, which helps link this dataset to their other scholarly works.\n\n\n\n11.7.2.1.1 Temporal Information\nAdd the temporal coverage of the data, which represents the time period to which data apply.\n\n\n\nLocation Information\nThe geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that a spatial search will be more precise.\n\nNote that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global. For example:\n\nMission Canyon, Santa Barbara, California, USA\n\n\n\n\nMethods\nMethods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, include details of sampling methods, experimental design, quality assurance procedures, and computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data.\n\n\n\nSave a first version with Submit\nWhen finished, click the Submit Dataset button at the bottom.\nIf there are errors or missing fields, they will be highlighted.\nCorrect those, and then try submitting again. When you are successful, you should see a green banner with a link to the current dataset view. Click the X to close that banner, if you want to continue editing metadata.\n\nSuccess!\n\n\n\nFile and variable level metadata\nThe final major section of metadata concerns the structure and contents of your data files. In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure.\nFor example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information.\n\nThe Attributes tab is where you enter variable (aka attribute) information. In the case of tabular data (such as csv files) each column is an attribute, thus there should be one attribute defined for every column in your dataset. Attribute metadata includes:\n\nvariable name (for programs)\nvariable label (for display)\n\n\n\nvariable definition (be specific)\ntype of measurement\n\n\n\nunits & code definitions\n\n\nYou’ll need to add these definitions for every variable (column) in the file. When done, click Done.\n\nNow the list of data files will show a green checkbox indicating that you have full described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes.\nNote that attribute information is not relevant for data files that do not contain variables, such as the R script in this example. Other examples of data files that might not need attributes are images, pdfs, and non-tabular text documents (such as readme files). The yellow circle in the editor indicates that attributes have not been filled out for a data file, and serves as a warning that they might be needed, depending on the file.\n\nAfter you get the green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes.\n\n\nAdd workflow provenance\nUnderstanding the relationships between files in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, that are often then used in analysis and visualization code to produce final outputs. In DataONE, we support structured descriptions of these relationships, so one can see the flow of data from raw data to derived to outputs.\nYou add provenance by navigating to the data table descriptions, and selecting the Add buttons to link the data and scripts that were used in your computational workflow. On the left side, select the Add circle to add an input data source to the filteredSpecies.csv file. This starts building the provenance graph to explain the origin and history of each data object.\n\nThe linkage to the source dataset should appear.\n\nThen you can add the link to the source code that handled the conversion between the data files by clicking on Add arrow and selecting the R script:\n\nSelect the R script and click “Done.”\n\n\nThe diagram now shows the relationships among the data files and the R script, so click Submit to save another version of the package.\n\nEt voilà! A beatifully preserved data package!\n\n\n\n\nBorer, Elizabeth, Eric Seabloom, Matthew B. Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” Bulletin of the Ecological Society of America 90: 205–14. https://doi.org/10.1890/0012-9623-90.2.205.\n\n\nWhite, Ethan, Elita Baldridge, Zachary Brym, Kenneth Locey, Daniel McGlinn, and Sarah Supp. 2013. “Nine Simple Ways to Make It Easier to (Re)use Your Data.” Ideas in Ecology and Evolution 6 (2). https://doi.org/10.4033/iee.2013.6b.6.f."
  },
  {
    "objectID": "session_12.html",
    "href": "session_12.html",
    "title": "12  Git Workflows",
    "section": "",
    "text": "12.0.1 Learning Objectives\nIn this lesson, you will learn:\n\nNew mechanisms to collaborate using Git\nWhat is a Pull Request in GitHub?\nHow to contribute code to colleague’s repository using Pull Requests\nWhat is a branch in Git?\nHow to use a branch to organize code\nWhat is a tag in Git and how is it useful for collaboration?\n\n\n\n12.0.2 Pull requests\nWe’ve shown in other chapters how to directly collaborate on a repository with colleagues by granting them write privileges as a collaborator to your repository. This is useful with close collaborators, but also grants them tremendous latitude to change files and analyses, to remove files from the working copy, and to modify all files in the repository.\nPull requests represent a mechanism to more judiciously collaborate, one in which a collaborator can suggest changes to a repository, the owner and collaborator can discuss those changes in a structured way, and the owner can then review and accept all or only some of those changes to the repository. This is useful with open source code where a community is contributing to shared analytical software, to students in a lab working on related but not identical projects, and to others who want the capability to review changes as they are submitted.\nTo use pull requests, the general procedure is as follows. The collaborator first creates a fork of the owner’s repository, which is a cloned copy of the original that is linked to the original. This cloned copy is in the collaborator’s GitHub account, which means they have the ability to make changes to it. But they don’t have the right to change the original owner’s copy. So instead, they clone their GitHub copy onto their local machine, which makes the collaborator’s GitHub copy the origin as far as they are concerned. In this scenario, we generally refer to the Collaborator’s repository as the remote origin, and the Owner’s repository as upstream.\n\nPull requests are a mechanism for someone that has a forked copy of a repository to request that the original owner review and pull in their changes. This allows them to collaborate, but keeps the owner in control of exactly what changed.\n\n\n12.0.3 Exercise: Create and merge pull requests\nIn this exercise, work in pairs. Each pair should create a fork of their partner’s training repository, and then clone that onto their local machine. Then they can make changes to that forked repository, and, from the GitHub interface, create a pull request that the owner can incorporate. We’ll walk through the process from both the owner and the collaborator’s perspectives. In the following example, mbjones will be the repository owner, and metamattj will be the collaborator.\n\nChange settings (Owner): Edit the GitHub settings file for your training-test repository, and ensure that the collaborator does not have editing permission. Also, be sure that all changes in your repository are committed and pushed to the origin server.\nFork (Collaborator): Visit the GitHub page for the owner’s GitHub repository on which you’d like to make changes, and click the Fork button. This will create a clone of that repository in your own GitHub account. You will be able to make changes to this forked copy of the repository, but you will not be able to make direct changes to the owner’s copy. After you have forked the repository, visit your GitHub page for your forked repository, copy the url, and create a new RStudio project using that repository url.\n\n\n\nEdit README.md (Collaborator): The collaborator should make one or more changes to the README.md file from their cloned copy of the repository, commit the changes, and push them to their forked copy. At this point, their local repo and GitHub copy both have the changes that they made, but the owner’s repository has not yet been changed. When you now visit your forked copy of the repository on GitHub, you will now see your change has been made, and it will say that This branch is 1 commit ahead of mbjones:main.\n\n\n\nCreate Pull Request (Collaborator): At this point, click the aptly named Pull Request button to create a pull request which will be used to ask that the owner pull in your changes to their copy.\n\n\nWhen you click Create pull request, provide a brief summary of the request, and a more detailed message to start a conversation about what you are requesting. It’s helpful to be polite and concise while providing adequate context for your request. This will start a conversation with the owner in which you can discuss your changes, they can easily review the changes, and they can ask for further changes before the accept and pull them in. The owner of the repository is in control and determines if and when the changes are merged.\n\n\nReview pull request (Owner): The owner will get an email notification that the Pull Request was created, and can see the PR listed in their Pull requests tab of their repsoitory.\n\n\nThe owner can now initiate a conversation about the change, requesting further changes. The interface indicates whether there are any conflicts with the changes, and if not, gives the owner the option to Merge pull request.\n\n\nMerge pull request (Owner): Once the owner thinks the changes look good, they can click the Merge pull request button to accept the changes and pull them into their repository copy. Edit the message, and then click Confirm merge.\n\n\nCongratulations, the PR request has now been merged into the owner’s copy, and has been closed with a note indicating that the changes have been made.\n\n\nSync with owner (Collaborator): Now that the pull request has been merged, there is a new merge commit in the Owner’s repository that is not present in either of the collaborator’s repositories. To fix that, one needs to pull changes from the upstream repository into the collaborator’s local repository, and then push those changes from that local repository to the collaborator’s origin repository.\n\nTo add a reference to the upstream remote (the repository you made your fork from), in the terminal, run:\ngit remote add upstream https://GitHub.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git\nThen to pull from the main branch of the upstream repository, in the terminal, run:\ngit pull upstream main\nAt this point, the collaborator is fully up to date.\n\n\n\n12.0.4 Branches\nBranches are a mechanism to isolate a set of changes in their own thread, allowing multiple types of work to happen in parallel on a repository at the same time. These are most often used for trying out experimental work, or for managing bug fixes for historical releases of software. Here’s an example graph showing a branch2.1 that has changes in parallel to the main branch of development:\n\nThe default branch in almost all repositories is called main, and it is the branch that is typically shown in the GitHub interface and elsewhere. There are many mechanisms to create branches. The one we will try is through RStudio, in which we use the branch dialog to create and switch between branches.\n\n12.0.4.1 Exercise:\nCreate a new branch in your training repository called exp-1, and then make changes to the RMarkdown files in the directory. Commit and push those changes to the branch. Now you can switch between branches using the GitHub interface."
  },
  {
    "objectID": "session_13.html",
    "href": "session_13.html",
    "title": "13  Hands On Synthesis",
    "section": "",
    "text": "Note\n\n\n\nTime for groups to get together and advance in their synthesis projects."
  },
  {
    "objectID": "session_14.html#material-to-include",
    "href": "session_14.html#material-to-include",
    "title": "14  Wrap up and Presentations",
    "section": "14.1 Material to include:",
    "text": "14.1 Material to include:\n\nProject Messaging\nMethods / Analytical Process\nNext Steps\n\n\n14.1.1 Project Messaging\nPresent your message box. High Level. You can structure it within the envelope style visual format or as a section based document.\nMake sure to include:\n\nAudience\nIssue\nProblem\nSo What?\nSolution\nBenefits\n\n\n\n\n14.1.2 Methods / Analytical Process\nProvide an update on your approaches to solving your ‘Problem’. How are you tackling this? If multiple elements, describe each. Present the workflow for your synthesis.\n\n\n\n14.1.3 Next Steps\nArticulate your plan for the next steps of the project. Some things to consider as you plan:"
  }
]