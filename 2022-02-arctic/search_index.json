[["index.html", "Reproducible Practices for Arctic Research Using R Reproducible Practices for Arctic Research Using R 0.1 Schedule", " Reproducible Practices for Arctic Research Using R February 14-18, 2022 Reproducible Practices for Arctic Research Using R The Arctic Data Center conducts training in data science and management, both of which are critical skills for stewardship of data, software, and other products of research that are preserved at the Arctic Data Center. 0.1 Schedule Coming soon. 0.1.1 Code of Conduct Please note that by participating in an NCEAS activity you agree to abide by our Code of Conduct 0.1.2 Logistics 0.1.2.1 Overview Welcome to the NCEAS Reproducible Research Techniques for Synthesis short course. In previous years, we have been able to run this course on site as a 5-day intensive training between 8:00 am and 5:00 pm. Participants traveled to NCEAS in Santa Barbara and we asked that participants were as engaged as possible during the instructional period. Although we have had to switch to a remote training model due to COVID-19, we ask participants for the same level of commitment in this remote setting, while understanding that some conflicts are unavoidable. We will be using the following tools to facilitate this training: Zoom (version 5.3.1) Slack (desktop app preferred) RStudio, accessed through a server on an up-to-date web browser: Firefox (version 80+) Chrome (version 80+) Safari (version 13+) Edge (version 81+) 0.1.2.2 Server You should receive a separate email prompting you to change your password using the NCEAS account service. Please change your password, and then ensure that you can log in at https://included-crab.nceas.ucsb.edu/. 0.1.2.3 Monitors If you have a second monitor or second device, it would be useful for this training. You’ll need enough screen real estate to handle the primary Zoom window, the participant pane in Zoom, Slack, and a browser with tabs for RStudio and our training curriculum. We recommend either using two monitors, or joining the Zoom room from a second device. If you must be on one machine for everything, here’s an example of what it could look like when you are following along with the class and how your screen will shift when you have a more detailed question that requires breakout assistance. When we’re in session, please turn your camera on, and mute your microphone unless you would like to ask a question or contribute to a discussion. 0.1.2.4 Working from Home We recognize that working from home during the pandemic comes with challenges. The appearance or sound of other adults, children, and pets in remote meetings such as this is completely normal and understandable. Having your video on and enabling the instructors and your fellow participants to see you brings some humanity to this physically distant workshop, and we believe that this is a crucial element of its success. If you would like to use the Zoom virtual background feature to hide your surroundings, please do provided your background of choice fits within the code of conduct (and here are some Arctic themed backgrounds if you need inspiration). 0.1.2.5 Non-Verbal Feedback We’ll be using the Zoom “Non Verbal Feedback” buttons throughout this course. We will ask you to put a green check by your name when you’re all set or you understand, and a red x by your name if you’re stuck or need assistance. These buttons can be found in the participants panel of the Zoom room. When you’re asked to answer using these buttons, please ensure that you select one so that the instructor has the feedback that they need to either continue the lesson or pause until everyone gets back on the same page. 0.1.2.6 Questions and Getting Help When you need to ask a question, please do so in one of the following ways: Turn your mic on and ask. If you are uncomfortable interrupting the instructor, you may also raise your virtual hand (in the participant panel) and the session facilitator will ask the instructor to pause and call upon you. Ask your question in the slack channel. If you have an issue/error and get stuck, you can ask for help in the following ways: Turn your mic on and ask for help. See also above regarding the use of a virtual raised hand. Let one of the instructors know in the slack channel. If prompted to do so, put a red X next to your name as your status in the participant window. When you have detailed questions or need one on one coding assistance, we will have zoom breakout rooms available with helpers. The helper will try to help you in Slack first. If the issue requires more in-depth troubleshooting, the helper will invite you to join their named Breakout Room. 0.1.2.7 The Power of Open To facilitate a lively and interactive learning environment, we’ll be calling on folks to share their code and to answer various questions posed by the instructor. It’s completely okay to say “Pass” or “I Don’t Know” - this is a supportive learning environment and we will all learn from each other. The instructors will be able to see your code as you go to help you if you get stuck, and the lead instructor may share participants’ code to show a successful example or illustrate a teaching moment. 0.1.3 About this book These written materials are the result of a continuous effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials. This work is licensed under a Creative Commons Attribution 4.0 International License. Citation: Jones, Matthew B., Amber E. Budden, Bryce Mecum, S. Jeanette Clark, Julien Brun, Julie Lowndes, Erin McLean, Jessica S. Guo, David S. LeBauer. 2021. Reproducible Research Techniques for Synthesis. NCEAS Learning Hub. Additional contributors: Ben Bolker, Stephanie Hampton, Samanta Katz, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Leah Wasser. "],["session-1.html", "1 Session 1 1.1 Introduction to the Arctic Data Center and NSF Standards and Policies", " 1 Session 1 1.1 Introduction to the Arctic Data Center and NSF Standards and Policies 1.1.1 Learning Objectives In this lesson, you will learn: About the mission and structure of the Arctic Data Center How the Arctic Data Center supports the research community About data policies from the NSF Arctic program 1.1.2 Arctic Data Center - History and Introduction The Arctic Data Center is the primary data and software repository for the Arctic section of National Science Foundation’s Office of Polar Programs (NSF OPP). We’re best known in the research community as a data archive – researchers upload their data to preserve it for the future and make it available for re-use. This isn’t the end of that data’s life, though. These data can then be downloaded for different analyses or synthesis projects. In addition to being a data discovery portal, we also offer top-notch tools, support services, and training opportunities. We also provide data rescue services. NSF has long had a commitment to data reuse and sharing. Since our start in 2016, we’ve grown substantially – from that original 4 TB of data from ACADIS to now over 39 TB. In the last year alone, we’ve seen a 16% growth in dataset count, and about a 30% growth in data volume. This increase has come from advances in tools – both ours and of the scientific community, plus active community outreach and a strong culture of data preservation from NSF and from researchers. We plan to add more storage capacity in the coming months, as researchers are coming to us with datasets in the terabytes, and we’re excited to preserve these research products in our archive. We’re projecting our growth to be around several hundred TB this year, which has a big impact on processing time. Give us a heads up if you’re planning on having larger submissions so that we can work with you and be prepared for a large influx of data. The data that we have in the Arctic Data Center comes from a wide variety of disciplines. These different programs within NSF all have different focuses – the Arctic Observing Network supports scientific and community-based observations of biodiversity, ecosystems, human societies, land, ice, marine and freshwater systems, and the atmosphere as well as their social, natural, and/or physical environments, so that encompasses a lot right there in just that one program. We’re also working on a way right now to classify the datasets by discipline, so keep an eye out for that coming soon. Along with that diversity of disciplines comes a diversity of file types. The most common file type we have are image files in four different file types. Probably less than 200-300 of the datasets have the majority of those images – we have some large datasets that have image and/or audio files from drones. Most of those 5800+ datasets are tabular datasets. There’s a large diversity of data files, though, whether you want to look at remote sensing images, listen to passive acoustic audio files, or run applications – or something else entirely. We also cover a long period of time, at least by human standards. The data represented in our repository spans across centuries. We also have data that spans the entire Arctic, as well as the sub-Arctic, regions. 1.1.3 Data Discovery Portal To browse the data catalog, navigate to arcticdata.io. Go to the top of the page and under data, go to search. Right now, you’re looking at the whole catalog. You can narrow your search down by the map area, a general search, or searching by an attribute. Clicking on a dataset brings you to this page. You have the option to download all the files by clicking the green “Download All” button, which will zip together all the files in the dataset to your Downloads folder. You can also pick and choose to download just specific files. All the raw data is in open formats to make it easily accessible and compliant with FAIR principles – for example, tabular documents are in .csv (comma separated values) rather than Excel documents. The metrics at the top give info about the number of citations with this data, the number of downloads, and the number of views. This is what it looks like when you click on the Downloads tab for more information. Scroll down for more info about the dataset – abstract, keywords. Then you’ll see more info about the data itself. This shows the data with a description, as well as info about the attributes (or variables or parameters) that were measured. The green check mark indicates that those attributes have been annotated, which means the measurements have a precise definition to them. Scrolling further, we also see who collected the data, where they collected it, and when they collected it, as well as any funding information like a grant number. For biological data, there is the option to add taxa. 1.1.4 Tools and Infrastructure We have a number of tools available to submitters and researchers who are there to download data. We also partner with other organizations, like Make Data Count and DataONE, and leverage those partnerships to create a better data experience. One of those tools is provenance tracking. With provenance tracking, users of the Arctic Data Center can see exactly what datasets led to what product, using the particular script that the researcher ran. Another tool are our Metadata Quality Checks. We know that data quality is important for researchers to find datasets and to have trust in them to use them for another analysis. For every submitted dataset, the metadata is run through a quality check to increase the completeness of submitted metadata records. These checks are seen by the submitter as well as are available to those that view the data, which helps to increase knowledge of how complete their metadata is before submission. That way, the metadata that is uploaded to the Arctic Data Center is as complete as possible, and close to following the guideline of being understandable to any reasonable scientist. 1.1.5 Support Services Metadata quality checks are the automatic way that we ensure quality of data in the repository, but the real quality and curation support is done by our curation team. The process by which data gets into the Arctic Data Center is iterative, meaning that our team works with the submitter to ensure good quality and completeness of data. When a submitter submits data, our team gets a notification and beings to evaluate the data for upload. They then go in and format it for input into the catalog, communicating back and forth with the researcher if anything is incomplete or not communicated well. This process can take anywhere from a few days or a few weeks, depending on the size of the dataset and how quickly the researcher gets back to us. Once that process has been completed, the dataset is published with a DOI (digital object identifier). 1.1.6 Training and Outreach In addition to the tools and support services, we also interact with the community via trainings like this one and outreach events. We run workshops at conferences like the Ecological Society of America and the American Geophysical Union. We also run an intern and fellows program, and webinars with different organizations. We’re invested in helping to train the Arctic science community in reproducible techniques, since it facilitates a more open culture of data sharing and reuse. We also strive to keep our fingers on the pulse of what researchers like yourselves are looking for in terms of support. We’re active on Twitter to share Arctic updates, data science updates, and specifically Arctic Data Center updates, but we’re also happy to feature new papers or successes that you all have had with working with the data. We can also take data science questions if you’re running into those in the course of your research, or how to make a quality data management plan. Follow us on Twitter and interact with us – we love to be involved in your research as it’s happening as well as after it’s completed. Furthermore, we have a robust Data Fellows program to train early career researchers or recently graduated students in data science. 1.1.7 Data Rescue We also run data rescue operations. We digitiazed Autin Post’s collection of glacier photos that were taken from 1964 to 1997. There were 100,000+ files and almost 5 TB of data to ingest, and we reconstructed flight paths, digitized the images of his notes, and documented image metadata, including the camera specifications. 1.1.8 Who Must Submit Projects that have to submit their data include all Arctic Research Opportunities through the NSF Office of Polar Programs. That data has to be uploaded within two years of collection. The Arctic Observing Network has a shorter timeline – their data products must be uploaded within 6 months of collection. Additionally, we have social science data, though that data often has special exceptions due to sensitive human subjects data. At the very least, the metadata has to be deposited with us. 1.1.9 Summary All the above informtion can be found on our website or if you need help, ask our support team at support@arcticdata.io or tweet us @arcticdatactr! Download slides: Arctic Data Center Overview and NSF Policies "],["session-2.html", "2 Session 2 2.1 Data Documentation and Publishing", " 2 Session 2 2.1 Data Documentation and Publishing 2.1.1 Learning Objectives In this lesson, you will learn: About open data archives, especially the Arctic Data Center What science metadata are and how they can be used How data and code can be documented and published in open data archives Web-based submission 2.1.2 Data sharing and preservation 2.1.3 Data repositories: built for data (and code) GitHub is not an archival location Examples of dedicated data repositories: KNB, Arctic Data Center, tDAR, EDI, Zenodo Rich metadata Archival in their mission Certification for repositories: https://www.coretrustseal.org/ Data papers, e.g., Scientific Data List of data repositories: http://re3data.org Repository finder tool: https://repositoryfinder.datacite.org/ 2.1.4 Metadata Metadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where they were collected, and why they were collected. For consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the metadata for a sockeye salmon dataset: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; That same metadata document can be converted to HTML format and displayed in a much more readable form on the web: https://knb.ecoinformatics.org/#view/doi:10.5063/F1F18WN4 And, as you can see, the whole dataset or its components can be downloaded and reused. Also note that the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data. 2.1.5 Structure of a data package Note that the dataset above lists a collection of files that are contained within the dataset. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document. These data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4,and each of the individual files have their own identifiers as well. 2.1.6 DataONE Federation DataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses. DataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the (currently 44!) repositories one at a time. 2.1.7 Publishing data from the web Each data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the Arctic Data Center, we provide some easy to use web forms for editing and submitting a data package. Let’s walk through a web submission to see what you might expect. 2.1.7.1 Download the data to be used for the tutorial I’ve already uploaded the test data package, and so you can access the data here: https://demo.arcticdata.io/#view/urn:uuid:0702cc63-4483-4af4-a218-531ccc59069f Grab both CSV files, and the R script, and store them in a convenient folder. 2.1.7.2 Login via ORCID We will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button. When you sign in, you will be redirected to orcid.org, where you can either provide your existing ORCID credentials or create a new account. ORCID provides multiple ways to login, including using your email address, an institutional login from many universities, and/or a login from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login. 2.1.7.3 Create and submit the dataset After signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. 2.1.7.3.1 Click Add Files to choose the data files for your package You can select multiple files at a time to efficiently upload many files. The files will upload showing a progress indicator. You can continue editing metadata while they upload. 2.1.7.3.2 Enter Overview information This includes a descriptive title, abstract, and keywords. You also must enter a funding award number and choose a license. The funding field will search for an NSF award identifier based on words in its title or the number itself. The licensing options are CC-0 and CC-BY, which both allow your data to be downloaded and re-used by other researchers. CC-0 Public Domain Dedication: “…can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.” CC-BY: Attribution 4.0 International License: “…free to…copy,…redistribute,…remix, transform, and build upon the material for any purpose, even commercially,…[but] must give appropriate credit, provide a link to the license, and indicate if changes were made.” 2.1.7.3.3 People Information Information about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people: Creators - all the people who should be in the citation for the dataset Contacts - one is required, but defaults to the first Creator if omitted Principal Investigators Any others that are relevant For each, please provide their ORCID identifier, which helps link this dataset to their other scholarly works. 2.1.7.3.4 Location Information The geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that spatial search will be more precise. Note that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global: Mission Canyon, Santa Barbara, California, USA 2.1.7.3.5 Temporal Information Add the temporal coverage of the data, which represents the time period to which data apply. Again, use multiple date ranges if your sampling was discontinuous. 2.1.7.3.6 Methods Methods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data. 2.1.7.3.7 Save a first version with Submit When finished, click the Submit Dataset button at the bottom. If there are errors or missing fields, they will be highlighted. Correct those, and then try submitting again. When you are successful, you should see a large green banner with a link to the current dataset view. Click the X to close that banner if you want to continue editing metadata. Success! 2.1.7.4 File and variable level metadata The final major section of metadata concerns the structure and content of your data files. In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure. For example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information. The Attributes tab is where you enter variable (aka attribute) information, including: variable name (for programs) variable label (for display) - variable definition (be specific) - type of measurement - units &amp; code definitions You’ll need to add these definitions for every variable (column) in the file. When done, click Done. Now, the list of data files will show a green checkbox indicating that you have fully described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes. After you get the big green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes. 2.1.7.5 Add workflow provenance Understanding the relationships between files (aka provenance) in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, which are often then used in analysis and visualization code to produce final outputs. In the DataONE network, we support structured descriptions of these relationships, so researchers can see the flow of data from raw data to derived to outputs. You add provenance by navigating to the data table descriptions and selecting the Add buttons to link the data and scripts that were used in your computational workflow. On the left side, select the Add circle to add an input data source to the filteredSpecies.csv file. This starts building the provenance graph to explain the origin and history of each data object. The linkage to the source dataset should appear. Then you can add the link to the source code that handled the conversion between the data files by clicking on Add arrow and selecting the R script: The diagram now shows the relationships among the data files and the R script, so click Submit to save another version of the package. Et voilà! A beatifully preserved data package! "],["session-3.html", "3 Session 3 3.1 RStudio and Git/GitHub Setup 3.2 Literate Analysis with RMarkdown", " 3 Session 3 3.1 RStudio and Git/GitHub Setup 3.1.1 Learning Objectives In this lesson, you will learn: How to check to make sure your RStudio environment is set up properly for analysis How to set up git 3.1.2 Logging into the RStudio server To help prevent us from spending most of this lesson remotely troubleshooting the myriad of issues that can arise when setting up the R, RStudio, and git environments, we have chosen to have everyone work on a remote server with all of the software you need installed. We will be using a special kind of RStudio just for servers called, aptly, RStudio Server. If you have never worked on a remove server before, you can think of it like working on a different computer via the internet. Note that the server has no knowledge of the files on your local filesystem, but it is easy to transfer files from the server to your local computer, and vice-versa, using the RStudio server interface. Here are the instructions for logging in and getting set up: Setup You should have received an email prompting you to change your password for your server account. If you did not, please let one of the helpers know on Slack. If you were able to successfully change your password, you can log in at: https://included-crab.nceas.ucsb.edu/ In this workshop, we are going to be using R projects to organize our work. An R project is tied to a directory on your local computer, and makes organizing your work and collaborating with others easier. We are going to be doing nearly all of the work in this course in one project. Our version of RStudio server allows you to share projects with others. Sharing your project with the instructors of the course will allow for them to jump into your session and type along with you, should you encounter an error you cannot fix. Setup In your RStudio server session, follow these steps to set up your shared project: In the “File” menu, select “New Project” Click “New Directory” Click “New Project” Under “Directory name” type: training_{USERNAME}, eg: training_jclark Leave “Create Project as subdirectory of:” set to ~ Click “Create Project” Your Rstudio should now open your project. To share your project with the instructor team, locate the “project switcher” dropdown menu in the upper right of your RStudio window. This dropdown has the name if your project (eg: training_clark), and a dropdown arrow. Click the dropdown menu, then “Share Project.” When the dialog box pops up, add the following usernames to your project: jclark aebudden jones brun Once those names show up in the list, click “OK”. Setting up git Before using git, you need to tell it who you are, also known as setting the global options. The only way to do this is through the command line. Newer versions of RStudio have a nice feature where you can open a terminal window in your RStudio session. Do this by selecting Tools -&gt; Terminal -&gt; New Terminal. A terminal tab should now be open where your console usually is. To set the global options, type the following into the command prompt, with your actual name, and press enter: git config --global user.name &quot;Matt Jones&quot; Note that if it ran successfully, it will look like nothing happened. We will check at the end to makre sure it worked. Next, enter the following line, with the email address you used when you created your account on github.com: git config --global user.email &quot;gitcode@magisa.org&quot; Note that these lines need to be run one at a time. Next, we will set our credentials to not time out for a very long time. This is related to the way that our server operating system handles credentials - not doing this will make your PAT (which we will set up soon) expire immediately on the system, even though it is actually valid for a month. git config --global credential.helper &#39;cache --timeout=10000000&#39; Finally, check to make sure everything looks correct by entering this command, which will return the options that you have set. git config --global --list 3.1.2.1 GitHub Authentication GitHub recently deprecated password authentication for accessing repositories, so we need to set up a secure way to authenticate. The book Happy git with R has a wealth of information related to working with git in R, and these instructions are based off of section 10.1. We will be using a PAT (Personal Access Token) in this course, because it is easy to set up. For better security and long term use, we recommend taking the extra steps to set up SSH keys. Steps: Run usethis::create_github_token() in the console In the browser window that pops up, scroll to the bottom and click “generate token.” You may need to log into GitHub first. Copy the token from the green box on the next page Back in RStudio, run credentials::set_github_pat() Paste your token into the dialog box that pops up. 3.1.3 Preparing to work in RStudio The default RStudio setup has a few panes that you will use. Here they are with their default locations: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) You can change the default location of the panes, among many other things: Customizing RStudio. One key question to ask whenever we open up RStudio is “where am I?” Because we like to work in RStudio projects, often this question is synonymous with “what project am I in?” In our setup we have already worked with R projects a little, but haven’t explained much about what they are or why we use them. An R project is really a special kind of working directory, which has its own workspace, history, and settings. Even though it isn’t much more than a special folder, it is a powerful way to organize your work. There are two places that can indicate what project we are in. The first is the project switcher menu in the upper right hand corner of your RStudio window. The second is the working directory path, in the top bar of your console. Note that by default, your working directory is set to the top level of your R project directory unless you change it using the setwd() function. About paths and working directories There are two types of paths in computing: absolute paths and relative paths. An absolute path always starts with the root of your file system and locates files from there. The absolute path to my R project directory is: /home/jclark/training_jclark Relative paths start from some location in your file system that is below the root. Relative paths are combined with the path of that location to locate files on your system. R (and some other languages like MATLAB) refer to the location where the relative path starts as our working directory. RStudio projects automatically set the working directory to the directory of the project. This means that you can reference files from within the project without worrying about where the project directory itself is. If I want to read in a file from the data directory within my project, I can simply type read.csv(\"data/samples.csv\") as opposed to read.csv(\"/home/jclark/training_jclark/data/samples.csv\") This is not only convenient for you, but also when working collaboratively. We will talk more about this later, but if Bryce makes a copy of my R project that I have published on GitHub, and I am using relative paths, he can run my code exactly as I have written it, without going back and changing \"/home/jclark/training_jclark/data/samples.csv\" to \"/home/mecum/training_jclark/data/samples.csv\" Note that once you start working in projects you should basically never need to run the setwd() command. If you are in the habit of doing this, stop and take a look at where and why you do it. Could leveraging the working directory concept of R projects eliminate this need? Almost definitely! Similarly, think about how you work with absolute paths. Could you leverage the working directory of your R project to replace these with relative paths and make your code more portable? Probably! Organizing your project When starting a new research project, one of the first things I do is set up an R project for it (just like we have here!) The next step is to then populate that project with relevant directories. There are many tools out there that can do this automatically. Some examples are rrtools or usethis::create_package(). The goal is to organize your project so that it is a compendium of your research. This means that the project has all of the digital parts needed to replicate your analysis, like code, figures, the manuscript, and data access. There are lots of good examples out there of research compendia. Here is one from a friend of NCEAS, Carl Boettiger, which he put together for a paper he wrote. The complexity of this project reflects years of work. Perhaps more representative of the situation we are in at the start of our course is a project that looks like this one, which we have just started at NCEAS. Currently, the only file in your project is your .Rproj file. This file is what contains the settings specific to your R project. We haven’t changed anything here yet, but will when we learn git. Summary organize your research into projects using R projects use R project working directories instead of setwd use relative paths from those working directories, not absolute paths structure your R project as a compendium 3.1.4 Setting up the R environment on your local computer R Version We will use R version 4.0.5, which you can download and install from CRAN. To check your version, run this in your RStudio console: R.version$version.string If you have R version 4.0.0 that will likely work fine as well. RStudio Version We will be using RStudio version 1.4 or later, which you can download and install here To check your RStudio version, run the following in your RStudio console: RStudio.Version()$version If the output of this does not say 1.4 or higher, you should update your RStudio. Do this by selecting Help -&gt; Check for Updates and follow the prompts. Package installation Run the following lines to check that all of the packages we need for the training are installed on your computer. packages &lt;- c(&quot;dplyr&quot;, &quot;tidyr&quot;, &quot;readr&quot;, &quot;devtools&quot;, &quot;usethis&quot;, &quot;roxygen2&quot;, &quot;leaflet&quot;, &quot;ggplot2&quot;, &quot;DT&quot;, &quot;scales&quot;, &quot;shiny&quot;, &quot;sf&quot;, &quot;ggmap&quot;, &quot;broom&quot;, &quot;captioner&quot;, &quot;MASS&quot;) for (package in packages) { if (!(package %in% installed.packages())) { install.packages(package) } } rm(packages) #remove variable from workspace # Now upgrade any out-of-date packages update.packages(ask=FALSE) If you haven’t installed all of the packages, this will automatically start installing them. If they are installed, it won’t do anything. Next, create a new R Markdown (File -&gt; New File -&gt; R Markdown). If you have never made an R Markdown document before, a dialog box will pop up asking if you wish to install the required packages. Click yes. At this point, RStudio and R should be all set up. Setting up git locally If you haven’t downloaded git already, you can do so here. If you haven’t already, go to github.com and create an account. Then you can follow the instructions that we used above to set your email address and user name. Note for Windows Users If you get “command not found” (or similar) when you try these steps through the RStudio terminal tab, you may need to set the type of terminal that gets launched by RStudio. Under some git install scenarios, the git executable may not be available to the default terminal type. Follow the instructions on the RStudio site for Windows specific terminal options. In particular, you should choose “New Terminals open with Git Bash” in the Terminal options (Tools-&gt;Global Options-&gt;Terminal). In addition, some versions of windows have difficulty with the command line if you are using an account name with spaces in it (such as “Matt Jones”, rather than something like “mbjones”). You may need to use an account name without spaces. Updating a previous R installation This is useful for users who already have R with some packages installed and need to upgrade R, but don’t want to lose packages. If you have never installed R or any R packages before, you can skip this section. If you already have R installed, but need to update, and don’t want to lose your packages, these two R functions can help you. The first will save all of your packages to a file. The second loads the packages from the file and installs packages that are missing. Save this script to a file (eg package_update.R). #&#39; Save R packages to a file. Useful when updating R version #&#39; #&#39; @param path path to rda file to save packages to. eg: installed_old.rda save_packages &lt;- function(path){ tmp &lt;- installed.packages() installedpkgs &lt;- as.vector(tmp[is.na(tmp[,&quot;Priority&quot;]), 1]) save(installedpkgs, file = path) } #&#39; Update packages from a file. Useful when updating R version #&#39; #&#39; @param path path to rda file where packages were saved update_packages &lt;- function(path){ tmp &lt;- new.env() installedpkgs &lt;- load(file = path, envir = tmp) installedpkgs &lt;- tmp[[ls(tmp)[1]]] tmp &lt;- installed.packages() installedpkgs.new &lt;- as.vector(tmp[is.na(tmp[,&quot;Priority&quot;]), 1]) missing &lt;- setdiff(installedpkgs, installedpkgs.new) install.packages(missing) update.packages(ask=FALSE) } Source the file that you saved above (eg: source(package_update.R)). Then, run the save_packages function. save_packages(&quot;installed.rda&quot;) Then quit R, go to CRAN, and install the latest version of R. Source the R script that you saved above again (eg: source(package_update.R)), and then run: update_packages(&quot;installed.rda&quot;) This should install all of your R packages that you had before you upgraded. 3.2 Literate Analysis with RMarkdown 3.2.1 Learning Objectives In this lesson we will: explore an example of RMarkdown as literate analysis learn markdown syntax write and run R code in RMarkdown build and knit an example document 3.2.2 Introduction and motivation The concept of literate analysis dates to a 1984 article by Donald Knuth. In this article, Knuth proposes a reversal of the programming paradigm. Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do. If our aim is to make scientific research more transparent, the appeal of this paradigm reversal is immediately apparent. All too often, computational methods are written in such a way as to be borderline incomprehensible - even to the person who originally wrote the code! The reason for this is obvious, computers interpret information very differently than people do. By switching to a literate analysis model, you help enable human understanding of what the computer is doing. As Knuth describes, in the literate analysis model, the author is an “essayist” who chooses variable names carefully, explains what they mean, and introduces concepts in the analysis in a way that facilitates understanding. RMarkdown is an excellent way to generate literate analysis, and a reproducible workflow. RMarkdown is a combination of two things - R, the programming language, and markdown, a set of text formatting directives. In R, the language assumes that you are writing R code, unless you specify that you are writing prose (using a comment, designated by #). The paradigm shift of literate analysis comes in the switch to RMarkdown, where instead of assuming you are writing code, Rmarkdown assumes that you are writing prose unless you specify that you are writing code. This, along with the formatting provided by markdown, encourages the “essayist” to write understandable prose to accompany the code that explains to the human-beings reading the document what the author told the computer to do. This is in contrast to writing just R code, where the author telling to the computer what to do with maybe a smattering of terse comments explaining the code to a reader. Before we dive in deeper, let’s look at an example of what literate analysis with RMarkdown can look like using a real example. Here is an example of a real analysis workflow written using RMarkdown. There are a few things to notice about this document, which assembles a set of similar data sources on salmon brood tables with different formatting into a single data source. It introduces the data sources using in-line images, links, interactive tables, and interactive maps. An example of data formatting from one source using R is shown. The document executes a set of formatting scripts in a directory to generate a single merged file. Some simple quality checks are performed (and their output shown) on the merged data. Simple analysis and plots are shown. In addition to achieving literate analysis, this document also represents a reproducible analysis. Because the entire merging and quality control of the data is done using the R code in the RMarkdown, if a new data source and formatting script are added, the document can be run all at once with a single click to re-generate the quality control, plots, and analysis of the updated data. RMarkdown is an amazing tool to use for collaborative research, so we will spend some time learning it well now, and use it through the rest of the course. Setup Open a new RMarkdown file using the following prompts: File -&gt; New File -&gt; RMarkdown A popup window will appear. You can just click the OK button here, or give your file a new title if you wish. Leave the output format as HTML. 3.2.3 Basic RMarkdown syntax The first thing to notice is that by opening a file, we are seeing the 4th pane of the RStudio console, which is essentially a text editor. Let’s have a look at this file — it’s not blank; there is some initial text already provided for you. Notice a few things about it: There are white and grey sections. R code is in grey sections, and other text is in white. Let’s go ahead and “Knit” the document by clicking the blue yarn at the top of the RMarkdown file. When you first click this button, RStudio will prompt you to save this file. Save it in the top level of your home directory on the server, and name it something that you will remember (like rmarkdown-intro.Rmd). What do you notice between the two? First, the knit process produced a second file (an HTML file) that popped up in a second window. You’ll also see this file in your directory with the same name as your Rmd, but with the html extension. In it’s simplest format, RMarkdown files come in pairs - the RMarkdown file, and its rendered version. In this case, we are knitting, or rendering, the file into HTML. You can also knit to PDF or Word files. Notice how the grey R code chunks are surrounded by 3 backticks and {r LABEL}. These are evaluated and return the output text in the case of summary(cars) and the output plot in the case of plot(pressure). The label next to the letter r in the code chunk syntax is a chunk label - this can help you navigate your RMarkdown document using the dropdown menu at the bottom of the editor pane. Notice how the code plot(pressure) is not shown in the HTML output because of the R code chunk option echo = FALSE. RMarkdown has lots of chunk options, including ones that allow for code to be run but not shown (echo = FALSE), code to be shown but not run (eval = FALSE), code to be run, but results not shown (results = 'hide'), or any combination of those. Before we get too deeply into the R side, let’s talk about Markdown. Markdown is a formatting language for plain text, and there are only around 15 rules to know. Notice the syntax in the document we just knitted: headers get rendered at multiple levels: #, ## bold: **word** There are some good cheatsheets to get you started, and here is one built into RStudio: Go to Help &gt; Markdown Quick Reference . Important: note that the hash symbol # is used differently in Markdown and in R: in R, a hash indicates a comment that will not be evaluated. You can use as many as you want: # is equivalent to ######. It’s just a matter of style. in Markdown, a hash indicates a level of a header. And the number you use matters: # is a “level one header”, meaning the biggest font and the top of the hierarchy. ### is a level three header, and will show up nested below the # and ## headers. Challenge In Markdown, Write some italic text, make a numbered list, and add a few sub-headers. Use the Markdown Quick Reference (in the menu bar: Help &gt; Markdown Quick Reference). Re-knit your html file and observe your edits. 3.2.3.1 New Rmarkdown editing tools The new version of RStudio (1.4) has a ‘what you see is what you get’ (wysiwyg) editor, which can be a nice way to write markdown without remembering all of the markdown rules. Since there aren’t many rules for markdown, I recommend just learning them - especially since markdown is used in many, many other contexts besides RMarkdown (formatting GitHub comments, for example). To access the editor, click the compass icon in the upper right hand corner of your editor pane. You’ll notice that your document is now formatted as you type, and you can change elements of the formatting using the row of icons in the top of the editor pane. Although I don’t really recommend doing all of your markdown composition in this format, there are two features to this editor that I find immensely helpful - adding citations, and adding tables. From the insert drop down, select “citation.” In the window that appears, there are several options in the left hand panel for the source of your citation. If you had a citation manager, such as Zotero, installed, this would be included in that list. For now, select “From DOI”, and in the search bar enter a DOI of your choice (eg: 10.1038/s41467-020-17726-z), then select “Insert.” After selecting insert, a couple of things happen. First, the citation reference is inserted into your markdown text as [@oke2020]. Second, a file called references.bib containing the BibTex format of the citation is created. Third, that file is added to the YAML header of your RMarkdown document (bibliography: references.bib). Adding another citation will automatically update your references.bib file. So easy! The second task that the markdown editor is convenient for is generating tables. Markdown tables are a bit finicky and annoying to type, and there are a number of formatting options that are difficult to remember if you don’t use them often. In the top icon bar, the “table” drop down gives several options for inserting, editing, and formatting tables. Experiment with this menu to insert a small table. 3.2.4 Code chunks Next, do what I do every time I open a new RMarkdown: delete everything below the “setup chunk” (line 10). The setup chunk is the one that looks like this: knitr::opts_chunk$set(echo = TRUE) This is a very useful chunk that will set the default R chunk options for your entire document. I like keeping it in my document so that I can easily modify default chunk options based on the audience for my RMarkdown. For example, if I know my document is going to be a report for a non-technical audience, I might set echo = FALSE in my setup chunk, that way all of the text, plots, and tables appear in the knitted document. The code, on the other hand, is still run, but doesn’t display in the final document. Now let’s practice with some R chunks. You can Create a new chunk in your RMarkdown in one of these ways: click “Insert &gt; R” at the top of the editor pane type by hand ```{r} ``` use the keyboard shortcut Command + Option + i (for windows, Ctrl + Alt + i) Now, let’s write some R code. x &lt;- 4*3 x Hitting return does not execute this command; remember, it’s just a text file. To execute it, we need to get what we typed in the the R chunk (the grey R code) down into the console. How do we do it? There are several ways (let’s do each of them): copy-paste this line into the console (generally not recommended as a primary method) select the line (or simply put the cursor there), and click ‘Run’. This is available from the bar above the file (green arrow) the menu bar: Code &gt; Run Selected Line(s) keyboard shortcut: command-return click the green arrow at the right of the code chunk Challenge Add a few more commands to your code chunk. Execute them by trying the three ways above. Question: What is the difference between running code using the green arrow in the chunk and the command-return keyboard shortcut? 3.2.5 Literate analysis practice Now that we have gone over the basics, let’s go a little deeper by building a simple, small RMarkdown document that represents a literate analysis using real data. Setup Navigate to the following dataset: https://doi.org/10.18739/A25T3FZ8X Download the file “BGchem2008data.csv” Click the “Upload” button in your RStudio server file browser. In the dialog box, make sure the destination directory is the data directory in your R project, click “choose file,” and locate the BGchem2008data.csv file. Press “ok” to upload the file. 3.2.5.1 Developing code in RMarkdown Experienced R users who have never used RMarkdown often struggle a bit in the transition to developing analysis in RMarkdown - which makes sense! It is switching the code paradigm to a new way of thinking. Rather than starting an R chunk and putting all of your code in that single chunk, here I describe what I think is a better way. Open a document and block out the high-level sections you know you’ll need to include using top level headers. Add bullet points for some high level pseudo-code steps you know you’ll need to take. Start filling in under each bullet point the code that accomplishes each step. As you write your code, transform your bullet points into prose, and add new bullet points or sections as needed. For this mini-analysis, we will just have the following sections and code steps: Introduction read in data Analysis calculate summary statistics calculate mean Redfield ratio plot Redfield ratio Challenge Create the ‘outline’ of your document with the information above. Top level bullet points should be top level sections. The second level points should be a list within each section. Next, write a sentence saying where your dataset came from, including a hyperlink, in the introduction section. Hint: Navigate to Help &gt; Markdown Quick Reference to lookup the hyperlink syntax. 3.2.5.2 Read in the data Now that we have outlined our document, we can start writing code! To read the data into our environment, we will use a function from the readr package. R packages are the building blocks of computational reproducibility in R. Each package contains a set of related functions that enable you to more easily do a task or set of tasks in R. There are thousands of community-maintained packages out there for just about every imaginable use of R - including many that you have probably never thought of! To install a package, we use the syntax install.packages('packge_name'). A package only needs to be installed once, so this code can be run directly in the console if needed. To use a package in our analysis, we need to load it into our environment using library(package_name). Even though we have installed it, we haven’t yet told our R session to access it. Because there are so many packages (many with conflicting namespaces) R cannot automatically load every single package you have installed. Instead, you load only the ones you need for a particular analysis. Loading the package is a key part of the reproducible aspect of our Rmarkdown, so we will include it as an R chunk. It is generally good practice to include all of your library calls in a single, dedicated R chunk near the top of your document. This lets collaborators know what packages they might need to install before they start running your code. You should have already installed readr as part of the setup for this course, so add a new R chunk below your setup chunk that calls the readr library, and run it. It should look like this: library(readr) Now, below the introduction that you wrote, add a chunk that uses the read_csv function to read in your data file. About RMarkdown paths In computing, a path specifies the unique location of a file on the filesystem. A path can come in one of two forms: absolute or relative. Absolute paths start at the very top of your file system, and work their way down the directory tree to the file. Relative paths start at an arbitrary point in the file system. In R, this point is set by your working directory. RMarkdown has a special way of handling relative paths that can be very handy. When working in an RMarkdown document, R will set all paths relative to the location of the RMarkdown file. This way, you don’t have to worry about setting a working directory, or changing your colleagues absolute path structure with the correct user name, etc. If your RMarkdown is stored near where the data it analyses are stored (good practice, generally), setting paths becomes much easier! If you saved your “BGchem2008data.csv” data file in the same location as your Rmd, you can just write the following to read it in. The help page (?read_csv, in the console) for this function tells you that the first argument should be a pointer to the file. Rstudio has some nice helpers to help you navigate paths. If you open quotes and press ‘tab’ with your cursor between the quotes, a popup menu will appear showing you some options. bg_chem &lt;- read_csv(&quot;../data/BGchem2008data.csv&quot;) Parsed with column specification: cols( Date = col_date(format = &quot;&quot;), Time = col_datetime(format = &quot;&quot;), Station = col_character(), Latitude = col_double(), Longitude = col_double(), Target_Depth = col_double(), CTD_Depth = col_double(), CTD_Salinity = col_double(), CTD_Temperature = col_double(), Bottle_Salinity = col_double(), d18O = col_double(), Ba = col_double(), Si = col_double(), NO3 = col_double(), NO2 = col_double(), NH4 = col_double(), P = col_double(), TA = col_double(), O2 = col_double() ) Warning messages: 1: In get_engine(options$engine) : Unknown language engine &#39;markdown&#39; (must be registered via knit_engines$set()). 2: Problem with `mutate()` input `Lower`. ℹ NAs introduced by coercion ℹ Input `Lower` is `as.integer(Lower)`. 3: In mask$eval_all_mutate(dots[[i]]) : NAs introduced by coercion If you run this line in your RMarkdown document, you should see the bg_chem object populate in your environment pane. It also spits out lots of text explaining what types the function parsed each column into. This text is important, and should be examined, but we might not want it in our final document. Challenge Use one of two methods to figure out how to suppress warning and message text in your chunk output: The gear icon in the chunk, next to the play button The RMarkdown reference guide (also under Help &gt; Cheatsheets) Aside Why not use read.csv from base R? We chose to show read_csv from the readr package for a few reasons. One is to introduce the concept of packages and showing how to load them, but read_csv has several advantages over read.csv. more reasonable function defaults (no stringsAsFactors!) smarter column type parsing, especially for dates it is much faster than read.csv, which is helpful for large files 3.2.5.3 Calculate Summary Statistics As our “analysis” we are going to calculate some very simple summary statistics and generate a single plot. In this dataset of oceanographic water samples, we will be examining the ratio of nitrogen to phosphate to see how closely the data match the Redfield ratio, which is the consistent 16:1 ratio of nitrogen to phosphorous atoms found in marine phytoplankton. Under the appropriate bullet point in your analysis section, create a new R chunk, and use it to calculate the mean nitrate (NO3), nitrite (NO2), ammonium (NH4), and phosphorous (P) measured. Save these mean values as new variables with easily understandable names, and write a (brief) description of your operation using markdown above the chunk. nitrate &lt;- mean(bg_chem$NO3) nitrite &lt;- mean(bg_chem$NO2) amm &lt;- mean(bg_chem$NH4) phos &lt;- mean(bg_chem$P) In another chunk, use those variables to calculate the nitrogen:phosphate ratio (Redfield ratio). ratio &lt;- (nitrate + nitrite + amm)/phos You can access this variable in your Markdown text by using R in-line in your text. The syntax to call R in-line (as opposed to as a chunk) is a single backtick `, the letter “r”, whatever your simple R command is - here we will use round(ratio) to print the calculated ratio, and a closing backtick `. So: ` 6 `. This allows us to access the value stored in this variable in our explanatory text without resorting to the evaluate-copy-paste method so commonly used for this type of task. The text as it looks in your RMrakdown will look like this: The Redfield ratio for this dataset is approximately `r round(ratio)`. And the rendered text like this: The Redfield ratio for this dataset is approximately 6. Finally, create a simple plot using base R that plots the ratio of the individual measurements, as opposed to looking at mean ratio. plot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4) Challenge Decide whether or not you want the plotting code above to show up in your knitted document along with the plot, and implement your decision as a chunk option. “Knit” your RMarkdown document (by pressing the Knit button) to observe the results. Aside How do I decide when to make a new chunk? Like many of life’s great questions, there is no clear cut answer. My preference is to have one chunk per functional unit of analysis. This functional unit could be 50 lines of code or it could be 1 line, but typically it only does one “thing.” This could be reading in data, making a plot, or defining a function. It could also mean calculating a series of related summary statistics (as above). Ultimately the choice is one related to personal preference and style, but generally you should ensure that code is divided up such that it is easily explainable in a literate analysis as the code is run. 3.2.6 RMarkdown and environments Let’s walk through an exercise with the document you built together to demonstrate how RMarkdown handles environments. We will be deliberately inducing some errors here for demonstration purposes. First, follow these steps: Restart your R session (Session &gt; Restart R) Run the last chunk in your Rmarkdown by pressing the play button on the chunk Perhaps not surprisingly, we get an error: Error in plot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4) : object &#39;bg_chem&#39; not found This is because we have not run the chunk of code that reads in the bg_chem data. The R part of Rmarkdown works just like a regular R script. You have to execute the code, and the order that you run it in matters. It is relatively easy to get mixed up in a large RMarkdown document - running chunks out of order, or forgetting to run chunks. To resolve this, follow the next step: Select from the “Run” menu (top right of Rmarkdown editor) “Restart R and run all chunks” Observe the bg_chem variable in your environment. This is one of my favorite ways to reset and re-run my code when things seem to have gone sideways. This is great practice to do periodically since it helps ensure you are writing code that actually runs. For the next demonstration: Restart your R session (Session &gt; Restart R) Press Knit to run all of the code in your document Observe the state of your environment pane Assuming your document knitted and produced an html page, your code ran. Yet the environment pane is empty. What happened? The Knit button is rather special - it doesn’t just run all of the code in your document. It actually spins up a fresh R environment separate from the one you have been working in, runs all of the code in your document, generates the output, and then closes the environment. This is one of the best ways RMarkdown helps ensure you have built a reproducible workflow. If, while you were developing your code, you ran a line in the console as opposed to adding it to your RMarkdown document, the code you develop while working actively in your environment will still work. However, when you knit your document, the environment RStudio spins up doesn’t know anything about that working environment you were in. Thus, your code may error because it doesn’t have that extra piece of information. Commonly, library calls are the source of this kind of frustration when the author runs it in the console, but forgets to add it to the script. To further clarify the point on environments, perform the following steps: Select from the “Run” menu (top right of Rmarkdown editor) “Run All” Observe all of the variables in your environment. Aside What about all my R scripts? Some pieces of R code are better suited for R scripts than RMarkdown. A function you wrote yourself that you use in many different analyses is probably better to define in an R script than repeated across many RMarkdown documents. Some analyses have mundane or repetitive tasks that don’t need to be explained very much. For example, in the document shown in the beginning of this lesson, 15 different excel files needed to be reformatted in slightly different, mundane ways, like renaming columns and removing header text. Instead of including these tasks in the primary markdown, I instead chose to write one R script per file and stored them all in a directory. I took the contents of one script and included it in my literate analysis, using it as an example to explain what the scripts did, and then used the source function to run them all from within my RMarkdown. So, just because you know RMarkdown now, doesn’t mean you won’t be using R scripts anymore. Both .R and .Rmd have their roles to play in analysis. With practice, it will become more clear what works well in RMarkdown, and what belongs in a regular R script. 3.2.7 Go Further Create an RMarkdown document with some of your own data. If you don’t have a good dataset handy, use the example dataset here: Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X. Your document might contain the following sections: Introduction to your dataset Include an external link Simple analysis Presentation of a result A table An in-line R command 3.2.8 Resources RMarkdown Reference Guide RMarkdown Home Page RMarkdown Cheat Sheet 3.2.9 Troubleshooting 3.2.9.1 My RMarkdown won’t knit to PDF If you get an error when trying to knit to PDF that says your computer doesn’t have a LaTeX installation, one of two things is likely happening: Your computer doesn’t have LaTeX installed You have an installation of LaTeX but RStudio cannot find it (it is not on the path) If you already use LaTeX (like to write papers), you fall in the second category. Solving this requires directing RStudio to your installation - and isn’t covered here. If you fall in the first category - you are sure you don’t have LaTeX installed - can use the R package tinytex to easily get an installation recognized by RStudio, as long as you have administrative rights to your computer. To install tinytex run: install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() If you get an error that looks like destination /usr/local/bin not writable, you need to give yourself permission to write to this directory (again, only possible if you have administrative rights). To do this, run this command in the terminal: sudo chown -R `whoami`:admin /usr/local/bin and then try the above install instructions again. More information about tinytex can be found here "],["session-4.html", "4 Session 4", " 4 Session 4 "],["session-5.html", "5 Session 5", " 5 Session 5 "],["session-6.html", "6 Session 6", " 6 Session 6 "],["session-7.html", "7 Session 7", " 7 Session 7 "],["session-8.html", "8 Session 8", " 8 Session 8 "],["session-9.html", "9 Session 9", " 9 Session 9 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
