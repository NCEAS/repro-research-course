[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCEAS Learning Hub’s coreR Course",
    "section": "",
    "text": "Preface\nApril 3 - April 7, 2023"
  },
  {
    "objectID": "index.html#welcome-to-the-corer-course",
    "href": "index.html#welcome-to-the-corer-course",
    "title": "NCEAS Learning Hub’s coreR Course",
    "section": "Welcome to the coreR Course",
    "text": "Welcome to the coreR Course\n\n\n\n\n\n\n\nA five-day immersion in R programming for environmental data science. Researchers will gain experience with essential data science tools and best practices to increase their capacity as collaborators, reproducible coders, and open scientists. This course is taught both in-person and virtually.\n\nLearning Goals:\n\nEffectively manage data using tidy data practices and developing quality metadata\nWrite clear and reusable code for scientific workflows\nBuild visually appealing data visualizations and reproducible reports using ggplot, markdown, and GitHub pages."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "NCEAS Learning Hub’s coreR Course",
    "section": "Schedule",
    "text": "Schedule\n[add picture of schedule here]"
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "NCEAS Learning Hub’s coreR Course",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "NCEAS Learning Hub’s coreR Course",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Halina Do-Linh, Camila Vargas Poulsen, Samantha Csik, Daphne Virlar-Knight. 2023. coreR Course. NCEAS Learning Hub.\nAdditional contributors: Ben Bolker, Amber E. Budden, Julien Brun, Natasha Haycock-Chavez, S. Jeanette Clark, Julie Lowndes, Stephanie Hampton, Matthew B. Jones, Samanta Katz, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "session_01.html#learning-objectives",
    "href": "session_01.html#learning-objectives",
    "title": "1  RStudio Server Setup",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPractice creating an R Project\nOrganize an R Project for effective project management\nUnderstand how to move in an R Project using paths and working directories"
  },
  {
    "objectID": "session_01.html#logon-to-the-rstudio-server",
    "href": "session_01.html#logon-to-the-rstudio-server",
    "title": "1  RStudio Server Setup",
    "section": "1.1 Logon to the RStudio Server",
    "text": "1.1 Logon to the RStudio Server\nTo prevent us from spending most of this lesson troubleshooting the myriad of issues that can arise when setting up the R, RStudio, and git environments, we have chosen to have everyone work on a remote server with all of the software you need installed. We will be using a special kind of RStudio just for servers called RStudio Server. If you have never worked on a remote server before, you can think of it like working on a different computer via the internet. Note that the server has no knowledge of the files on your local filesystem, but it is easy to transfer files from the server to your local computer, and vice-versa, using the RStudio server interface.\n\n\n\n\n\n\nServer Setup\n\n\n\nYou should have received an email prompting you to change your password for your server account. If you did not, please put up a post-it and someone will help you.\nAfter you have successfully changed your password log in at: https://included-crab.nceas.ucsb.edu/"
  },
  {
    "objectID": "session_01.html#create-an-r-project",
    "href": "session_01.html#create-an-r-project",
    "title": "1  RStudio Server Setup",
    "section": "1.2 Create an R Project",
    "text": "1.2 Create an R Project\nIn this course, we are going to be using an R project to organize our work. An R project is tied to a directory on your local computer, and makes organizing your work and collaborating with others easier.\nThe Big Idea: using an R project is a reproducible research best practice because it bundles all your work within a working directory. Consider your current data analysis workflow. Where do you import you data? Where do you clean and wrangle it? Where do you create graphs, and ultimately, a final report? Are you going back and forth between multiple software tools like Microsoft Excel, JMP, and Google Docs? An R project and the tools in R that we will talk about today will consolidate this process because it can all be done (and updated) in using one software tool, RStudio, and within one R project.\n\n\n\n\n\n\nR Project Setup\n\n\n\n\nIn the “File” menu, select “New Project”\nClick “New Directory”\nClick “New Project”\nUnder “Directory name” type: training_{USERNAME} (i.e. training_do-linh)\nLeave “Create Project as subdirectory of:” set to ~\nClick “Create Project”\n\nRStudio should open your new project automatically after creating it. One way to check this is by looking at the top right corner and checking for the project name."
  },
  {
    "objectID": "session_01.html#organizing-an-r-project",
    "href": "session_01.html#organizing-an-r-project",
    "title": "1  RStudio Server Setup",
    "section": "1.3 Organizing an R Project",
    "text": "1.3 Organizing an R Project\nWhen starting a new research project, one of the first things I do is create an R Project for it (just like we have here!). The next step is to then populate that project with relevant directories. There are many tools out there that can do this automatically. Some examples are rrtools or usethis::create_package(). The goal is to organize your project so that it is a compendium of your research. This means that the project has all of the digital parts needed to replicate your analysis, like code, figures, the manuscript, and data access.\nSome common directories are:\n\n\n\n\ndata: where we store our data (often contains subdirectories for raw, processed, and metadata data)\nR: contains scripts for cleaning or wrangling, etc. (some find this name misleading if their work has other scripts beyond the R programming language, in which case they call this directory scripts)\nplots or figs: generated plots, graphs, and figures\ndocs: summaries or reports of analysis or other relevant project information\n\nDirectory organization will vary from project to project, but the ultimate goal is to create a well organized project for both reproducibility and collaboration."
  },
  {
    "objectID": "session_01.html#moving-in-an-r-project-using-paths-working-directories",
    "href": "session_01.html#moving-in-an-r-project-using-paths-working-directories",
    "title": "1  RStudio Server Setup",
    "section": "1.4 Moving in an R Project using Paths & Working Directories",
    "text": "1.4 Moving in an R Project using Paths & Working Directories\n\nNow that we have your project created (and notice we know it’s an R Project because we see a .Rproj file in our Files pane), let’s learn how to move in a project. We do this using paths.\nThere are two types of paths in computing: absolute paths and relative paths.\n\nAn absolute path always starts with the root of your file system and locates files from there. The absolute path to my project directory is: /home/do-linh/training_do-linh\nRelative paths start from some location in your file system that is below the root. Relative paths are combined with the path of that location to locate files on your system. R (and some other languages like MATLAB) refer to the location where the relative path starts as our working directory.\n\nRStudio projects automatically set the working directory to the directory of the project. This means that you can reference files from within the project without worrying about where the project directory itself is. If I want to read in a file from the data directory within my project, I can simply type read.csv(\"data/samples.csv\") as opposed to read.csv(\"/home/do-linh/training_do-linh/data/samples.csv\").\nThis is not only convenient for you, but also when working collaboratively. We will talk more about this later, but if Matt makes a copy of my R project that I have published on GitHub, and I am using relative paths, he can run my code exactly as I have written it, without going back and changing /home/do-linh/training_do-linh/data/samples.csv to /home/jones/training_jones/data/samples.csv.\nNote that once you start working in projects you should basically never need to run the setwd() command. If you are in the habit of doing this, stop and take a look at where and why you do it. Could leveraging the working directory concept of R projects eliminate this need? Almost definitely!\nSimilarly, think about how you work with absolute paths. Could you leverage the working directory of your R project to replace these with relative paths and make your code more portable? Probably!"
  },
  {
    "objectID": "session_02.html#learning-objectives",
    "href": "session_02.html#learning-objectives",
    "title": "2  Git and GitHub Setup",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSet global options in your .gitconfig file\nPractice how to set up GitHub Authentication using a Personal Access Token (PAT)"
  },
  {
    "objectID": "session_02.html#set-up-global-options-in-git",
    "href": "session_02.html#set-up-global-options-in-git",
    "title": "2  Git and GitHub Setup",
    "section": "2.1 Set up global options in Git",
    "text": "2.1 Set up global options in Git\nBefore using Git, you need to tell it who you are, also known as setting the global options. The only way to do this is through the command line. Newer versions of RStudio have a nice feature where you can open a terminal window in your RStudio session. Do this by selecting Tools > Terminal > New Terminal.\nA Terminal tab should now be open where your Console usually is.\nTo set the global options, type the following into the command prompt, with your exact GitHub username, and press enter:\ngit config --global user.name \"hdolinh\"\n\n\nNote that if it ran successfully, it will look like nothing happened. We will check at the end to make sure it worked.\nNext, enter the following line, with the email address you used when you created your account on github.com:\ngit config --global user.email \"dolinh@nceas.ucsb.edu\"\n\n\nNote that these lines need to be run one at a time.\n\n\n\n\n\n\nCase and spelling matters!\n\n\n\nWhen you add your username and email to the global options you must use the exact same spelling and case that you use on GitHub otherwise, Git won’t be able to sync to your account to use.\n\n\nNext, we will set our credentials to not time out for a very long time. This is related to the way that our server operating system handles credentials - not doing this will make your Personal Access Token (which we will set up soon) expire immediately on the system, even though it is actually valid for a month.\ngit config --global credential.helper 'cache --timeout=10000000'\nNext, we will set the default branch name to main for any new repositories that are created moving forward. Why are we doing this? Previously, the default branch name was master and this racist terminology for git branches motivates us to update our default branch to main instead.\ngit config --global init.defaultBranch main\nFinally, check to make sure everything looks correct by entering this command, which will return the options that you have set.\ngit config --global --list"
  },
  {
    "objectID": "session_02.html#github-authentication",
    "href": "session_02.html#github-authentication",
    "title": "2  Git and GitHub Setup",
    "section": "2.2 GitHub Authentication",
    "text": "2.2 GitHub Authentication\nGitHub recently deprecated password authentication for accessing repositories, so we need to set up a secure way to authenticate. The book Happy Git and GitHub for the useR has a wealth of information related to working with Git in R, and these instructions are based off of Chapter 9 Personal access token for HTTPS.\nWe will be using a Personal Access Token (PAT) in this course. For better security and long term use, we recommend taking the extra steps to set up SSH keys (check out Chapter 10 Set up Keys for SSH).\n\n\n\n\n\n\nSetting up your PAT\n\n\n\n\nRun usethis::create_github_token() in the Console.\nA new browser window should open up to GitHub, showing all the scopes options. You can review the scopes, but you don’t need to worry about which ones to select this time. Using create_github_token() automatically pre-selects some recommended scopes. Go ahead and scroll to the bottom and click “Generate Token”.\nCopy the generated token.\nBack in RStudio, run gitcreds::gitcreds_set() in the Console.\nFinally, paste your PAT when the prompt asks for it.\n\n\n\nCongrats! Now that you’ve set up your authentication you should be able to work with GitHub in RStudio now."
  },
  {
    "objectID": "session_03.html#learning-objectives",
    "href": "session_03.html#learning-objectives",
    "title": "3  Why R Programming?",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nBe introduced to R programming resources for learning beyond the course\nGet oriented with the RStudio interface"
  },
  {
    "objectID": "session_03.html#welcome-to-programming-in-r",
    "href": "session_03.html#welcome-to-programming-in-r",
    "title": "3  Why R Programming?",
    "section": "3.1 Welcome to Programming in R",
    "text": "3.1 Welcome to Programming in R\n\nThere is a vibrant community out there that is collectively developing increasingly easy to use and powerful open source programming tools. The changing landscape of programming is making learning how to code easier than it ever has been. Incorporating programming into analysis workflows not only makes science more efficient, but also more computationally reproducible. In this course, we will use the programming language R, and the accompanying integrated development environment (IDE) RStudio. R is a great language to learn for data-oriented programming because it is widely adopted, user-friendly, and (most importantly) open source!\nSo what is the difference between R and RStudio? Here is an analogy to start us off. If you were a chef, R is a knife. You have food to prepare, and the knife is one of the tools that you’ll use to accomplish your task.\nAnd if R were a knife, RStudio is the kitchen. RStudio provides a place to do your work! Other tools, communication, community, it makes your life as a chef easier. RStudio makes your life as a researcher easier by bringing together other tools you need to do your work efficiently - like a file browser, data viewer, help pages, terminal, community, support, the list goes on. So it’s not just the infrastructure (the user interface or IDE), although it is a great way to learn and interact with your variables, files, and interact directly with git. It’s also data science philosophy, R packages, community, and more. Although you can prepare food without a kitchen and we could learn R without RStudio, that’s not what we’re going to do. We are going to take advantage of the great RStudio support, and learn R and RStudio together.\nSomething else to start us off is to mention that you are learning a new language here. It’s an ongoing process, it takes time, you’ll make mistakes, it can be frustrating, but it will be overwhelmingly awesome in the long run. We all speak at least one language; it’s a similar process, really. And no matter how fluent you are, you’ll always be learning, you’ll be trying things in new contexts, learning words that mean the same as others, etc, just like everybody else. And just like any form of communication, there will be miscommunication that can be frustrating, but hands down we are all better off because of it.\nWhile language is a familiar concept, programming languages are in a different context from spoken languages and you will understand this context with time. For example: you have a concept that there is a first meal of the day, and there is a name for that: in English it’s “breakfast.” So if you’re learning Spanish, you could expect there is a word for this concept of a first meal. (And you’d be right: “desayuno”). We will get you to expect that programming languages also have words (called functions in R) for concepts as well. You’ll soon expect that there is a way to order values numerically. Or alphabetically. Or search for patterns in text. Or calculate the median. Or reorganize columns to rows. Or subset exactly what you want. We will get you to increase your expectations and learn to ask and find what you’re looking for."
  },
  {
    "objectID": "session_03.html#r-resources",
    "href": "session_03.html#r-resources",
    "title": "3  Why R Programming?",
    "section": "3.2 R Resources",
    "text": "3.2 R Resources\nThis lesson is a combination of excellent lessons by others. Huge thanks to Julie Lowndes for writing most of this content and letting us build on her material, which in turn was built on Jenny Bryan’s materials. We highly recommend reading through the original lessons and using them as reference.\n\n\n\nLearning R Resources\n\nIntroduction to R lesson in Data Carpentry’s R for data analysis course\nJenny Bryan’s Stat 545 course materials\nJulie Lowndes’ Data Science Training for the Ocean Health Index\nLearn R in the console with swirl\nProgramming in R\nR, RStudio, RMarkdown\n\n\n\nCommunity Resources\n\nNCEAS’ EcoDataScience\nR-Ladies\nrOpenSci\nMinorities in R (MiR)\nTwitter - there is a lot here but some hashtags to start with are:\n\n#rstats\n#TidyTuesday\n#dataviz\n\n\n\n\nCheatsheets\n\nBase R Cheatsheet\nLaTeX Equation Formatting\nMATLAB/R Translation Cheatsheet"
  },
  {
    "objectID": "session_03.html#rstudio-ide",
    "href": "session_03.html#rstudio-ide",
    "title": "3  Why R Programming?",
    "section": "3.3 RStudio IDE",
    "text": "3.3 RStudio IDE\nLet’s take a tour of the RStudio interface.\n\nNotice the default panes:\n\nConsole (entire left)\nEnvironment/History (tabbed in upper right)\nFiles/Plots/Packages/Help (tabbed in lower right)\n\n\n\n\n\n\n\nQuick Tip\n\n\n\nYou can change the default location of the panes, among many other things, see Customizing RStudio."
  },
  {
    "objectID": "session_04.html#learning-objectives",
    "href": "session_04.html#learning-objectives",
    "title": "4  Intro to R Programming",
    "section": "Learning Objectives",
    "text": "Learning Objectives"
  },
  {
    "objectID": "session_04.html#but-first-where-am-i",
    "href": "session_04.html#but-first-where-am-i",
    "title": "4  Intro to R Programming",
    "section": "4.1 But first, where am I?",
    "text": "4.1 But first, where am I?\nAn important first question: where are we?\nIf you’ve just opened RStudio for the first time, you’ll be in your Home directory. This is noted by the ~/ at the top of the console. You can see too that the Files pane in the lower right shows what is in the Home directory where you are. You can navigate around within that Files pane and explore, but note that you won’t change where you are: even as you click through you’ll still be Home: ~/."
  },
  {
    "objectID": "session_05.html#learning-objectives",
    "href": "session_05.html#learning-objectives",
    "title": "5  Introduction to RMarkdown",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIntroduce RMarkdown as a tool for literate analysis\nLearn markdown syntax and run R code in RMarkdown\nBuild and knit an example document"
  },
  {
    "objectID": "session_05.html#introduction",
    "href": "session_05.html#introduction",
    "title": "5  Introduction to RMarkdown",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction"
  },
  {
    "objectID": "session_05.html#literate-programming",
    "href": "session_05.html#literate-programming",
    "title": "5  Introduction to RMarkdown",
    "section": "5.2 Literate Programming",
    "text": "5.2 Literate Programming\nAll too often, computational methods are written in such a way as to be borderline incomprehensible even to the person who originally wrote the code! The reason for this is obvious, computers interpret information very differently than people do. In 1984, Donald Knuth proposed a reversal of the programming paradigm by introducing the concept of Literate Programming (Knuth 1984).\n\n“Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.”\n\nIf our aim is to make scientific research more transparent,the appeal of this paradigm reversal is immediately apparent. By switching to a literate analysis model, you help enable human understanding of what the computer is doing. As Knuth describes, in the literate analysis model, the author is an “essayist” who chooses variable names carefully, explains what they mean, and introduces concepts in the analysis in a way that facilitates understanding.\nRMarkdown is an excellent way to generate literate analysis, and a reproducible workflow. RMarkdown is a combination of two things R, the programming language, and markdown, a set of text formatting directives. In an R script, the language assumes that you are writing R code, unless you specify that you are writing prose (using a comment, designated by #). The paradigm shift of literate analysis comes in the switch to RMarkdown, where instead of assuming you are writing code, Rmarkdown assumes that you are writing prose unless you specify that you are writing code. This, along with the formatting provided by markdown, encourages the “essayist” to write understandable prose to accompany the code that explains to the human-beings reading the document what the author told the computer to do. This is in contrast to writing just R code, where the author telling to the computer what to do with maybe a smattering of terse comments explaining the code to a reader.\nBefore we dive in deeper, let’s look at an example of what a rendered literate analysis with RMarkdown can look like using a real example. Here is an example of a real analysis workflow written using RMarkdown.\nThere are a few things to notice about this document, which assembles a set of similar data sources on salmon brood tables with different formatting into a single data source.\n\nIt introduces the data sources using in-line images, links, interactive tables, and interactive maps.\nAn example of data formatting from one source using R is shown.\nThe document executes a set of formatting scripts in a directory to generate a single merged file.\nSome simple quality checks are performed (and their output shown) on the merged data. -Simple analysis and plots are shown.\n\nIn addition to achieving literate analysis, this document also represents a reproducible analysis. Because the entire merging and quality control of the data is done using the R code in the RMarkdown, if a new data source and formatting script are added, the document can be run all at once with a single click to re-generate the quality control, plots, and analysis of the updated data.\nRMarkdown is an amazing tool to use for collaborative research, so we will spend some time learning it well now, and use it through the rest of the course.\n\n5.2.1 RMarkdown Syntax\nAn RMarkdown file has three main components:\n\nYAML metadata to guide the RMarkdown build process\nText to display\nCode chunks to run\n\n\nToday we are going to use Rmarkdown to run some analysis on data. We are specifically going to focus on the code chunk and text components. We will discuss more about the how the YAML works in an RMarkdown later in the course. For now, you just need to know that every RMarkdown file has a YAML and this sets guidelines on how your want the output of your document to look like.\nLet’s open an RMarkdown file following the instructions below.\n\n\n\n\n\n\nSetup\n\n\n\n\nOpen a new RMarkdown file using the following prompts: File > New File > RMarkdown\nA popup window will appear.\nGive your file a new title, e.g “Introduction to RMarkdown”.\nLeave the output format as HTML.\nThen click the OK button.\n\n\n\nThe first thing to notice is that by opening a file, we see the fourth pane of the RStudio pops up. This is our RMarkdown document which is essentially a text editor.\nLet’s have a look at this file — It looks a little different than a R script. It’s not blank; there is some initial text already provided for you. Lets identify the three main components in the image above. We have the YAML a the top, in between the two sets of dashed lines. Then we also see white and grey sections. The gray sections are R code chunks and the white sections are plain text.\nLet’s go ahead and render this file by clicking the “Knit” button, the blue yarn at the top of the RMarkdown file. When you first click this button, RStudio will prompt you to save this file. Save it in the top level of your home directory on the server, and name it something that you will remember (like rmarkdown-intro.Rmd).\n\nWhat do you notice between the two?\nFirst, the knit process produced a second file (an HTML file) that popped up in a second window. You’ll also see this file in your directory with the same name as your Rmd, but with the html extension. In it’s simplest format, RMarkdown files come in pairs the RMarkdown file, and its rendered version. In this case, we are knitting, or rendering, the file into HTML. You can also knit to PDF or Word files.\nNotice how the grey R code chunks are surrounded by 3 back-ticks and {r LABEL}. These are evaluated and return the output text in the case of summary(cars) and the output plot in the case of plot(pressure). The label next to the letter r in the code chunk syntax is a chunk label - this can help you navigate your RMarkdown document using the drop-down menu at the bottom of the editor pane.\nNotice how the code plot(pressure) is not shown in the HTML output because of the R code chunk option echo = FALSE. RMarkdown has lots of chunk options:\n\nAllow for code to be run but not shown (echo = FALSE)\nCode to be shown but not run (eval = FALSE)\nCode to be run, but results not shown (results = 'hide')\nOr any combination of the above and more.\n\nIt is important to emphasize one more time that in an RMarkdown document, the gray areas of the document are code, in this case R code because that is what it is indicated in the ```{r} syntax at the start of this gray area. And the white areas of this Rmd are in Markdown language.\nLet’s start by talking about Markdown. Markdown is a formatting language for plain text, and there are only around 15 rules to know.\nNotice the syntax in the document we just knitted:\n\nHeaders get rendered at multiple levels: #, ##\nBold: **word** There are some good cheatsheets to get you started, and here is one built into RStudio: Go to Help > Markdown Quick Reference.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe hash symbol # is used differently in Markdown and in R\n\nIn an R script or inside an R code chunk, a hash indicates a comment that will not be evaluated. You can use as many as you want: # is equivalent to ######. It’s just a matter of style.\nIn Markdown, a hash indicates a level of a header. And the number you use matters: # is a “level one header”, meaning the biggest font and the top of the hierarchy. ### is a level three header, and will show up nested below the # and ## headers.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nIn Markdown, Write some italic text, make a numbered list, and add a few sub-headers. Use the Markdown Quick Reference (in the menu bar: Help > Markdown Quick Reference).\nRe-knit your html file and observe your edits.\n\n\n\n\n\n5.2.2 RMarkdown Editing Tools\nRecent versions of RStudio, now have a “what you see is what you get” (wysiwyg) editor or Visual editor, which can be a nice way to write markdown without remembering all of the markdown rules. Since there aren’t many rules for markdown, we recommend just learning them especially since markdown is used in many, many other contexts besides RMarkdown (formatting GitHub comments, for example).\nTo access the editor, click the Visual button in the upper left hand corner of your editor pane. You’ll notice that your document is now formatted as you type, and you can change elements of the formatting using the row of icons in the top of the editor pane. Although we don’t really recommend doing all of your markdown composition in the Visual editor, there are two features to this editor that we believe are immensely helpful, adding citations, and adding tables.\nTo add a citation, go to the visual editor and in the insert drop down, select “Citation.” In the window that appears, there are several options in the left hand panel for the source of your citation. If you have a citation manager, such as Zotero, installed, this would be included in that list. For now, select “From DOI”, and in the search bar enter a DOI of your choice (e.g.: 10.1038/s41467-020-17726-z), then select “Insert.”\n\nAfter selecting insert, a couple of things happen. First, the citation reference is inserted into your markdown text as [@oke2020]. Second, a file called references.bib containing the BibTex format of the citation is created. Third, that file is added to the YAML header of your RMarkdown document (bibliography: references.bib). Adding another citation will automatically update your references.bib file. So easy!\nThe second task that the markdown editor is convenient for is generating tables. Markdown tables are a bit finicky and annoying to type, and there are a number of formatting options that are difficult to remember if you don’t use them often. In the top icon bar, the “Table” drop down gives several options for inserting, editing, and formatting tables. Experiment with this menu to insert a small table.\n\n\n5.2.3 Code Chunks\nEvery time when opening a new RMarkdown we should start by deleting everything below the “r setup chunk” (everything below line 11). The setup chunk is the one that looks like this:\n\nknitr::opts_chunk$set(echo = TRUE)\n\nThis is a very useful chunk that will set the default R chunk options for your entire document. It is helpful to keep it in the document to easily modify default chunk options based on the audience. For example, if the document is going to be a report for a non-technical audience, a useful setup in this chunk might set echo = FALSE. That way all of the text, plots, and tables appear in the knitted document. The code, on the other hand, is still run, but doesn’t display in the final document.\nLet’s practice with some R chunks. You can create a new chunk in your RMarkdown in one of these ways:\n\nClick “Insert > R” at the top of the editor pane\nType by hand {r}\nUse the keyboard shortcut command + option + i (for Windows: Ctrl + Alt + i)\n\n\n\n\n\n\n\nAbout code chunks\n\n\n\nEach code chunk needs to have an opening syntax ```{r} and a closing syntax ```. Everything in between these lines will be identified as R code.\n\n\nNow, let’s write some R code.\n\nx <- 4 * 8\n\nhights_ft <- c(5.2, 6.0, 5.7)\n\nimp_coef <- 3.14\n\nHitting return does not execute this command; remember, it’s just a text file. To execute it, we need to get what we typed in the the R chunk (the grey R code) down into the console. How do we do it? There are several ways (let’s do each of them):\n\nCopy-paste this line into the console (generally not recommended as a primary method)\nSelect the line (or simply put the cursor there), and click “Run”. This is available from:\n\nthe bar above the file (green arrow)\nthe menu bar: Code > Run Selected Line(s)\nkeyboard shortcut: command-return\n\nClick the green arrow at the right of the code chunk\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a new code chunk and add few lines of code (more than one line). Execute them by trying the three ways above.\nWhat is the difference between running code using the green arrow in the chunk and the command-return keyboard shortcut?"
  },
  {
    "objectID": "session_05.html#practice-literate-analysis-with-ocean-water-samples",
    "href": "session_05.html#practice-literate-analysis-with-ocean-water-samples",
    "title": "5  Introduction to RMarkdown",
    "section": "5.3 Practice: Literate Analysis with ocean water samples",
    "text": "5.3 Practice: Literate Analysis with ocean water samples\nNow that we have gone over the basics, let’s go a little deeper by building a simple, small RMarkdown document that represents a literate analysis using real data. We are going to work with the seawater chemistry data. We are going to download a file names BGchem2008data.csv from the Arctic Data Center repository. Please follow the steps below to download the data and then upload to your RStudio Server data folder.\n\n\n\n\n\n\nSetup\n\n\n\n\nNavigate to the following dataset: https://doi.org/10.18739/A25T3FZ8X\nDownload the file BGchem2008data.csv\nClick the “Upload” button in your RStudio server file browser.\nIn the dialog box, make sure the destination directory is the data directory in your R project, click “Choose File,” and locate the BGchem2008data.csv file. Press “OK” to upload the file.\nCheck your file was successfully uploaded by navigating into your data folder in the Files pane.\n\n\n\n\n5.3.1 Developing code in RMarkdown\nExperienced R users who have never used RMarkdown often struggle a bit in the transition to developing analysis in RMarkdown — which makes sense! It is switching the code paradigm to a new way of thinking.\nRather than starting an R chunk and putting all of your code in that single chunk, below we describe what we think is a better way.\n\nOpen a document and block out the high-level sections you know you’ll need to include using top level headers.\nAdd bullet points for some high level pseudo-code steps you know you’ll need to take.\nStart filling in under each bullet point the code that accomplishes each step. As you write your code, transform your bullet points into prose, and add new bullet points or sections as needed.\n\nFor this mini-analysis, we will just have the following sections and code steps:\n\nIntroduction\n\nRead in data\n\nAnalysis\n\nCalculate summary statistics\nCalculate mean Redfield ratio\nPlot Redfield ratio\n\nConclusion\n\n\n\n\n\n\n\nExercise\n\n\n\nWrite a sentence saying where the data set came from, including a hyperlink, in the introduction section.\nHint: Navigate to Help > Markdown Quick Reference to look-up the hyperlink syntax.\n\n\n\n\n5.3.2 Read in the data\nNow that we have outlined our document, we can start writing code! To read the data into our environment, we will use a function from the readr package.\nTo use a package in our analysis, we need to first make sure it is installed (you can install a package by running install.package(\"name-of-package\")). Once installed you need to load it into our environment using library(package_name). Even though we have installed it, we haven’t yet told our R session to access it. Because there are so many packages (many with conflicting namespaces) R cannot automatically load every single package you have installed. Instead, you load only the ones you need for a particular analysis. Loading the package is a key part of the reproducible aspect of our Rmarkdown, so we will include it as an R chunk.\n\n\n\n\n\n\nBest Practice\n\n\n\nIt is generally good practice to include all of your library() calls in a single, dedicated R chunk near the top of your document. This lets collaborators know what packages they might need to install before they start running your code.\n\n\nThe server should have already installed readr, so add a new R chunk below your setup chunk that calls the readr library, and run it. It should look like this:\n\nlibrary(readr)\n\nNow, below the introduction that you wrote, add a code chunk that uses the read_csv() function to read in your data file.\n\nbg_chem <- read_csv(\"data/BGchem2008data.csv\")\n\nRows: 70 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): Station\ndbl  (16): Latitude, Longitude, Target_Depth, CTD_Depth, CTD_Salinity, CTD_T...\ndttm  (1): Time\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nWhy read_csv() over read.csv()?\nWe chose to show read_csv() from the readr package to introduce the concept of packages, to show you how to load packages, and read_csv() has several advantages over read.csv() from base R, including:\n\nMore reasonable function defaults (no stringsAsFactors!)\nSmarter column type parsing, especially for dates\nread_csv() is much faster than read.csv(), which is helpful for large files\n\nOnce you run this line in your RMarkdown document, you should see the bg_chem object populate in your environment pane. It also spits out lots of text explaining what types the function parsed each column into. This text is important, and should be examined, but we might not want it in our final document.\n\n\n\n\n\n\nExercise\n\n\n\nUse one of two methods to figure out how to suppress warning and message text in your chunk output:\n\nThe gear icon in the chunk, next to the play button\nThe RMarkdown reference guide (also under Help > Cheatsheets)\n\n\n\n\n\n5.3.3 Calculate Summary Statistics\nAs our “analysis” we are going to calculate some very simple summary statistics and generate a single plot. Using water samples from the Arctic Ocean, we will examine the ratio of nitrogen to phosphate to see how closely the data match the Redfield ratio, which is the consistent 16:1 ratio of nitrogen to phosphorous atoms found in marine phytoplankton.\nLet’s start by exploring the data we just read. Every time we read a new data set, it is important to familiarize yourself with it and make sure that the data looks as expected. Below some useful functions for exploring your data.\nLet’s start by creating a new R chunk and run the following functions. Because this just an exploration and we do not want this chunk to be part of our report, we will indicate that by adding eval=FALSE and echo=FALSE in the setup of the chunk, that way, the code in this chunk will not run and not be displayed when I knit the final document.\n\n## Prints the column names of my data frame\ncolnames(bg_chem)\n\n## General structure of the data frame - shows class of each column\nstr(bg_chem)\n\n## First 6 lines of the data frame\nhead(bg_chem)\n\n## Summary of each column of data\nsummary(bg_chem)\n\n## Opens data frame in its own tab to see each row and column of the data\nView(bg_chem)\n\n## Prints unique values in a column (in this case Date)\nunique(bg_chem$Date)\n\nNow that we know a more about the data set we are working with lets do some analyses. Under the appropriate bullet point in your analysis section, create a new R chunk, and use it to calculate the mean nitrate (NO3), nitrite (NO2), ammonium (NH4), and phosphorous (P) measured.\nSave these mean values as new variables with easily understandable names, and write a (brief) description of your operation using markdown above the chunk. Remember that the $ (aka the subset operator) indicates which column of your data to look into.\n\nnitrate <- mean(bg_chem$NO3)\nnitrite <- mean(bg_chem$NO2)\namm <- mean(bg_chem$NH4)\nphos <- mean(bg_chem$P)\n\nIn another chunk, use those variables to calculate the nitrogen: phosphate ratio (Redfield ratio).\n\nratio <- (nitrate + nitrite + amm)/phos\n\nYou can access this variable in your Markdown text by using R in-line in your text. The syntax to call R in-line (as opposed to as a chunk) is a single backtick `, followed by the letter “r”, then whatever your simple R command is — here we will use round(ratio) to print the calculated ratio, and finally a closing backtick `. This allows us to access the value stored in this variable in our explanatory text without resorting to the evaluate-copy-paste method so commonly used for this type of task.\nSo, the text in you RMarkdown should look like this:\nThe Redfield ratio for this dataset is approximately: `r round(ratio)`\nAnd the rendered text like this:\nThe Redfield ratio for this dataset is approximately 6.\nFinally, create a simple plot using base R that plots the ratio of the individual measurements, as opposed to looking at mean ratio.\n\nplot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4)\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nDecide whether or not you want the plotting code above to show up in your knitted document along with the plot, and implement your decision as a chunk option.\n“Knit” your RMarkdown document (by pressing the Knit button) to render and observe the results.\n\n\n\n\n\n\n\n\nHow do I decide when to make a new code chunk?\n\n\n\nLike many of life’s great questions, there is no clear cut answer. A rule of thumb is to have one chunk per functional unit of analysis. This functional unit could be 50 lines of code or it could be 1 line, but typically it only does one “thing.” This could be reading in data, making a plot, or defining a function. It could also mean calculating a series of related summary statistics (as we’ll see below). Ultimately, the choice is one related to personal preference and style, but generally you should ensure that code is divided up such that it is easily explainable in a literate analysis as the code is run."
  },
  {
    "objectID": "session_05.html#rmarkdown-file-paths-and-environement",
    "href": "session_05.html#rmarkdown-file-paths-and-environement",
    "title": "5  Introduction to RMarkdown",
    "section": "5.4 Rmarkdown file paths and environement",
    "text": "5.4 Rmarkdown file paths and environement\nAs we discussed earlier, in computing, a path specifies the unique location of a file on the filesystem. A path can come in one of two forms: absolute or relative.\n\nAbsolute paths start at the very top of your file system, and work their way down the directory tree to the file.\nRelative paths start at an arbitrary point in the file system. In R, this point is set by your working directory.\n\nRMarkdown has a special way of handling relative paths that can be very handy. When working in an RMarkdown document, R will set all paths relative to the location of the RMarkdown file. This way, you don’t have to worry about setting a working directory, or changing your colleagues absolute path structure with the correct user name, etc. If your RMarkdown is stored near where the data it analyses are stored (good practice, generally), setting paths becomes much easier!\nIf you saved your BGchem2008data.csv data file in the same location as your Rmd, you can just write the following to read it in. Checkout the help page by typing ?read_csv() in the console. This tells you that for this function the first argument should be a pointer to the file. Rstudio has some nice helpers to help you navigate paths. If you open quotes and press tab with your cursor between the quotes, a popup menu will appear showing you some options.\n\n5.4.1 Practice: RMarkdown and Environments\nLet’s walk through an exercise with the document you built together to demonstrate how RMarkdown handles environments. We will be deliberately inducing some errors here for demonstration purposes.\nFirst, follow these steps:\n\n\n\n\n\n\nSetup\n\n\n\n\nRestart your R session (Session > Restart R)\nRun the last chunk in your Rmarkdown by pressing the play button on the chunk\n\nPerhaps not surprisingly, we get an error:\nError in plot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4) : \n  object 'bg_chem' not found\n\n\nThis is because we have not run the chunk of code that reads in the bg_chem data. The R part of Rmarkdown works just like a regular R script. You have to execute the code, and the order that you run it in matters. It is relatively easy to get mixed up in a large RMarkdown document — running chunks out of order, or forgetting to run chunks.\nTo resolve this, follow the next step:\n\n\n\n\n\n\nSetup continued\n\n\n\n\nSelect from the “Run” menu (top right of Rmarkdown editor) “Restart R and run all chunks”\nObserve the bg_chem variable in your environment\n\n\n\nThis is a great way to reset and re-run code when things seem to have gone sideways. It is great practice to do periodically since it helps ensure you are writing code that actually runs and it’s reproducible.\n\n\n\n\n\n\nFor the next exercise:\n\n\n\n\nRestart your R session (Session > Restart R)\nPress “Knit” to run all of the code in your document\nObserve the state of your environment pane\n\nAssuming your document knitted and produced an html page, your code ran. Yet the environment pane is empty. What happened?\n\n\nThe Knit button is rather special — it doesn’t just run all of the code in your document. It actually spins up a fresh R environment separate from the one you have been working in, runs all of the code in your document, generates the output, and then closes the environment. This is one of the best ways RMarkdown helps ensure you have built a reproducible workflow. If, while you were developing your code, you ran a line in the console as opposed to adding it to your RMarkdown document, the code you develop while working actively in your environment will still work. However, when you knit your document, the environment RStudio spins up doesn’t know anything about that working environment you were in. Thus, your code may error because it doesn’t have that extra piece of information. Commonly, library() calls are the source of this kind of frustration when the author runs it in the console, but forgets to add it to the script.\nTo further clarify the point on environments, perform the following steps:\n\n\n\n\n\n\nSetup continued\n\n\n\n\nSelect from the “Run” menu (top right of Rmarkdown editor) “Run All”\nObserve all of the variables in your environment\n\n\n\n\n\n\n\n\n\nWhat about all my R Scripts?\n\n\n\nSome pieces of R code are better suited for R scripts than RMarkdown. A function you wrote yourself that you use in many different analyses is probably better to define in an R script than repeated across many RMarkdown documents. Some analyses have mundane or repetitive tasks that don’t need to be explained very much. For example, in the document shown in the beginning of this lesson, 15 different excel files needed to be reformatted in slightly different, mundane ways, like renaming columns and removing header text. Instead of including these tasks in the primary markdown, I instead chose to write one R script per file and stored them all in a directory. I took the contents of one script and included it in my literate analysis, using it as an example to explain what the scripts did, and then used the source() function to run them all from within my RMarkdown.\nSo, just because you know RMarkdown now, doesn’t mean you won’t be using R scripts anymore. Both .R and .Rmd have their roles to play in analysis. With practice, it will become more clear what works well in RMarkdown, and what belongs in a regular R script.\n\n\nLast but not leas, here an bonus exercise for you to practice all what we have learned in this lesson.\n\n\n\n\n\n\nBonus Exercise\n\n\n\nCreate an RMarkdown document with some of your own data. If you don’t have a good dataset handy, use an example data set here:\nCraig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X.\nYour document might contain the following sections:\n\nIntroduction to your dataset\n\nInclude an external link\n\nSimple analysis\nPresentation of a result\n\nA table\nAn in-line R command"
  },
  {
    "objectID": "session_05.html#additional-rmarkdown-resources",
    "href": "session_05.html#additional-rmarkdown-resources",
    "title": "5  Introduction to RMarkdown",
    "section": "5.5 Additional RMarkdown Resources",
    "text": "5.5 Additional RMarkdown Resources\n\nRMarkdown Reference Guide\nRMarkdown Home Page\nRMarkdown Cheat Sheet\nWhat is RMarkdown Video"
  },
  {
    "objectID": "session_05.html#troubleshooting-my-rmarkdown-wont-knit-to-pdf",
    "href": "session_05.html#troubleshooting-my-rmarkdown-wont-knit-to-pdf",
    "title": "5  Introduction to RMarkdown",
    "section": "5.6 Troubleshooting: My RMarkdown Won’t Knit to PDF",
    "text": "5.6 Troubleshooting: My RMarkdown Won’t Knit to PDF\nIf you get an error when trying to knit to PDF that says your computer doesn’t have a LaTeX installation, one of two things is likely happening:\n\nYour computer doesn’t have LaTeX installed\nYou have an installation of LaTeX but RStudio cannot find it (it is not on the path)\n\nIf you already use LaTeX (like to write papers), you fall in the second category. Solving this requires directing RStudio to your installation - and isn’t covered here.\nIf you fall in the first category - you are sure you don’t have LaTeX installed - can use the R package tinytex to easily get an installation recognized by RStudio, as long as you have administrative rights to your computer.\nTo install tinytex run:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nIf you get an error that looks like destination /usr/local/bin not writable, you need to give yourself permission to write to this directory (again, only possible if you have administrative rights). To do this, run this command in the terminal:\nsudo chown -R `whoami`:admin /usr/local/bin\nand then try the above install instructions again. Learn more about tinytex from Yihui Xie’s online book TinyTeX."
  },
  {
    "objectID": "session_06.html#learning-objectives",
    "href": "session_06.html#learning-objectives",
    "title": "6  FAIR and CARE Principles",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIntroduce FAIR and CARE principles and the value it provides to data\nDevelop a FAIR and CARE lens that can be applied to your data-focused work\nEvaluate the FAIRness and CAREness of your work and the work of others"
  },
  {
    "objectID": "session_06.html#but-first-a-reproducibility-activity-using-lego",
    "href": "session_06.html#but-first-a-reproducibility-activity-using-lego",
    "title": "6  FAIR and CARE Principles",
    "section": "6.1 But first, a reproducibility activity using LEGO®",
    "text": "6.1 But first, a reproducibility activity using LEGO®\nThis activity is largely based on the LEGO® Metadata for Reproducibility game pack, which was developed by Mary Donaldson and Matt Mahon.\n\n\n\n\n\n\nSetup\n\n\n\n\nGet into groups\nGet Lego blocks and template\n\n\n\n\n\nActivity Discussion\n\n\nDid you find this a simple way to document your process?\nWas there anything you found difficult to capture?\nDid those replicating the builds find it straightforward to follow?\nDid you encounter any ambiguity in the instructions?"
  },
  {
    "objectID": "session_06.html#the-fair-and-care-principles",
    "href": "session_06.html#the-fair-and-care-principles",
    "title": "6  FAIR and CARE Principles",
    "section": "6.2 The FAIR and CARE principles",
    "text": "6.2 The FAIR and CARE principles\nThe idea behind these principles is to increase access and usage of complex and large datasets for innovation, discovery, and decision-making. This means making data available to machines, researchers, Indigenous communities, policy makers, etc.\nWith the need to improve the infrastructure supporting the reuse of data, a group of diverse stakeholders from academia, funding agencies, publishers and industry came together to jointly endorse measurable guidelines that enhance the reusability of data (@wilkinson_fair_2016). These guidelines became what we now know as the FAIR Data Principles.\nFollowing the discussion about FAIR and incorporating activities and feedback from the Indigenous Data Sovereignty network, the Global Indigenous Data Alliance developed the CARE principles (@carroll_operationalizing_2021). The CARE principles for Indigenous Data Governance complement the more data-centric approach of the FAIR principles, introducing social responsibility to open data management practices.\n\n6.2.1 What is FAIR?\nIn facilitating use of data resources, the data stewardship community have converged on principles surrounding best practices for open data management One set of these principles is the FAIR principles. FAIR stands for Findable, Accessible, Interoperable, and Reproducible.\nFindable\n\nMetadata and data should be easy to find for both humans and computers.\n\nAccessible\n\nOnce someone finds the required data, they need to know how the data can be accessed.\n\nInteroperable\n\nThe data needs to be easily integrated with other data for analysis, storage, and processing.\n\nReusable\n\nData should be well-described so they can be reused and replicated in different settings.\n\n\n\n\n6.2.2 What is CARE?\n\n“The ‘CARE Principles for Indigenous Data Governance’ were developed by the International Indigenous Data Sovereignty Interest Group (within the Research Data Alliance) in consultation with Indigenous Peoples, scholars, non-profit organizations, and governments (@carroll_care_2020). They address concerns related to the people and purpose of data. It advocates for greater Indigenous control and oversight in order to share data on Indigenous Peoples’ terms.\nThese principles are people and purpose-oriented, reflecting the crucial role of data in advancing Indigenous innovation and self-determination. CARE details that the use of Indigenous data should result in tangible benefits for Indigenous collectives through inclusive development and innovation, improved governance and citizen engagement, and result in equitable outcomes”\nThe CARE and FAIR principles complement each other, encouraging open and other data movements to consider both people and purpose in their advocacy and pursuits. The goal is that stewards and other users of Indigenous data will ’Be FAIR and CARE (@carroll_care_2020).\nCitation [Research Data Alliance International Indigenous Data Sovereignty Interest Group. CARE principles for Indigenous data governance. (The Global Indigenous Data Alliance, GIDA-global.org, September 2019).]\nThe CARE Principles stand for Collective benefits, Authority control, Responsibility and Ethics.\nCollective Benefit\n\nData ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data.\n\nAuthority to Control\n\nIndigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledge and geographical indicators, are represented and identified within data.\n\nResponsibility\n\nThose working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples.\n\nEthics\n\nIndigenous Peoples’ rights and well being should be the primary concern at all stages of the data life cycle and across the data ecosystem."
  },
  {
    "objectID": "session_07.html#learning-objectives",
    "href": "session_07.html#learning-objectives",
    "title": "7  Intro to Git and GitHub",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPractice using Git to track changes of your project\nPractice the Git workflow: pull, stage, commit, push\nPractice setting up a Git repository using different workflows"
  },
  {
    "objectID": "session_07.html#introduction-to-git",
    "href": "session_07.html#introduction-to-git",
    "title": "7  Intro to Git and GitHub",
    "section": "7.1 Introduction to Git",
    "text": "7.1 Introduction to Git\n\n\n\n\n\nEvery file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when problems are discovered. Data files get combined together, then errors are fixed, and then they are split and combined again. In the course of a single analysis, one can expect thousands of changes to files. And yet, all we use to track this are simplistic filenames.\nYou might think there is a better way, and you’d be right: version control."
  },
  {
    "objectID": "session_07.html#a-motivating-example",
    "href": "session_07.html#a-motivating-example",
    "title": "7  Intro to Git and GitHub",
    "section": "7.2 A Motivating Example",
    "text": "7.2 A Motivating Example\nBefore diving into the details of Git and how to use it, let’s start with a motivating example that’s representative of the types of problems Git can help us solve.\nSay, for example, you’re working on an analysis in R and you’ve got it into a state you’re pretty happy with. We’ll call this version 1:\n\nYou come into the office the following day and you have an email from your boss, “Hey, you know what this model needs?”\n\nYou’re not entirely sure what she means but you figure there’s only one thing she could be talking about: more cowbell. So you add it to the model in order to really explore the space.\nBut you’re worried about losing track of the old model so, instead of editing the code in place, you comment out the old code and put as serious a warning as you can muster in a comment above it.\n\nCommenting out code you don’t want to lose is something probably all of us have done at one point or another but it’s really hard to understand why you did this when you come back years later or you when you send your script to a colleague. Luckily, there’s a better way: Version control. Instead of commenting out the old code, we can change the code in place and tell Git to commit our change. So now we have two distinct versions of our analysis and we can always see what the previous version(s) look like.\n\nYou may have noticed something else in the diagram above: Not only can we save a new version of our analysis, we can also write as much text as we like about the change in the commit message. In addition to the commit message, Git also tracks who, when, and where the change was made.\nImagine that some time has gone by and you’ve committed a third version of your analysis, version 3, and a colleague emails with an idea: What if you used machine learning instead?\n\nMaybe you’re not so sure the idea will work out and this is where a tool like Git shines. Without a tool like Git, we might copy analysis.R to another file called analysis-ml.R which might end up having mostly the same code except for a few lines. This isn’t particularly problematic until you want to make a change to a bit of shared code and now you have to make changes in two files, if you even remember to.\nInstead, with Git, we can start a branch. Branches allow us to confidently experiment on our code, all while leaving the old code in tact and recoverable.\n\nSo you’ve been working in a branch and have made a few commits on it and your boss emails again asking you to update the model in some way. If you weren’t using a tool like Git, you might panic at this point because you’ve rewritten much of your analysis to use a different method but your boss wants change to the old method.\n\nBut with Git and branches, we can continue developing our main analysis at the same time as we are working on any experimental branches. Branches are great for experiments but also great for organizing your work generally.\n\nAfter all that hard work on the machine learning experiment, you and your colleague could decide to scrap it. It’s perfectly fine to leave branches around and switch back to the main line of development but we can also delete them to tidy up.\n\nIf, instead, you and your colleague had decided you liked the machine learning experiment, you could also merge the branch with your main development line. Merging branches is analogous to accepting a change in Word’s Track Changes feature but way more powerful and useful.\n\nA key takeaway here is that Git can drastically increase your confidence and willingness to make changes to your code and help you avoid problems down the road. Analysis rarely follows a linear path and we need a tool that respects this.\n\nFinally, imagine that, years later, your colleague asks you to make sure the model you reported in a paper you published together was actually the one you used. Another really powerful feature of Git is tags which allow us to record a particular state of our analysis with a meaningful name. In this case, we are lucky because we tagged the version of our code we used to run the analysis. Even if we continued to develop beyond commit 5 (above) after we submitted our manuscript, we can always go back and run the analysis as it was in the past.\n\n7.2.1 Summary\nWith Git, we can:\n\nAvoid using cryptic filenames and comments to keep track of our work\nDescribe our changes with as much information as we like so it’s easier to understand why our code changed (commits)\nWork on multiple, simultaneous development (branches) of our code at the same time and, optionally, merge them together\nGo back in time to look at (and even run) older versions of our code\nTag specific versions of our code as meaningful (tags)\n\nAnd, as we’ll see below, Git has one extra superpower available to us: It’s distributed. Multiple people can work on the same analysis at the same time on their own computer and everyone’s changes can eventually merged together."
  },
  {
    "objectID": "session_07.html#version-control-and-collaboration-using-git-and-github",
    "href": "session_07.html#version-control-and-collaboration-using-git-and-github",
    "title": "7  Intro to Git and GitHub",
    "section": "7.3 Version control and Collaboration using Git and GitHub",
    "text": "7.3 Version control and Collaboration using Git and GitHub\nFirst, just what are Git and GitHub?\n\nGit: version control software used to track files in a folder (a repository)\n\nGit creates the versioned history of a repository\n\nGitHub: web site that allows users to store their Git repositories and share them with others"
  },
  {
    "objectID": "session_07.html#lets-look-at-a-github-repository",
    "href": "session_07.html#lets-look-at-a-github-repository",
    "title": "7  Intro to Git and GitHub",
    "section": "7.4 Let’s Look at a GitHub Repository",
    "text": "7.4 Let’s Look at a GitHub Repository\nThis screen shows the copy of a repository stored on GitHub, with its list of files, when the files and directories were last modified, and some information on who made the most recent changes.\n\nIf we drill into the “commits” for the repository, we can see the history of changes made to all of the files. Looks like kellijohnson was working on the project and fixing errors in December:\n\nAnd finally, if we drill into one of the changes made on December 20, we can see exactly what was changed in each file:\n\nTracking these changes, how they relate to released versions of software and files is exactly what Git and GitHub are good for. And we will show how they can really be effective for tracking versions of scientific code, figures, and manuscripts to accomplish a reproducible workflow.\n\n7.4.1 The Git Life cycle\nAs a Git user, you’ll need to understand the basic concepts associated with versioned sets of changes, and how they are stored and moved across repositories. Any given Git repository can be cloned so that it exists both locally, and remotely. But each of these cloned repositories is simply a copy of all of the files and change history for those files, stored in Git’s particular format. For our purposes, we can consider a Git repository as a folder with a bunch of additional version-related metadata.\nIn a local Git-enabled folder, the folder contains a workspace containing the current version of all files in the repository. These working files are linked to a hidden folder containing the ‘Local repository’, which contains all of the other changes made to the files, along with the version metadata.\nSo, when working with files using Git, you can use Git commands to indicate specifically which changes to the local working files should be staged for versioning (using the git add command), and when to record those changes as a version in the local repository (using the command git commit).\nThe remaining concepts are involved in synchronizing the changes in your local repository with changes in a remote repository. The git push command is used to send local changes up to a remote repository (possibly on GitHub), and the git pull command is used to fetch changes from a remote repository and merge them into the local repository.\n\nGit commands to use in the terminal:\n\ngit clone: to copy a whole remote repository to local\ngit add (stage): notify Git to track particular changes\ngit commit: store those changes as a version\ngit pull: merge changes from a remote repository to our local repository\ngit push: copy changes from our local repository to a remote repository\ngit status: determine the state of all files in the local repository\ngit log: print the history of changes in a repository\n\nThose seven commands are the majority of what you need to successfully use Git. But this is all super abstract, so let’s explore with some real examples."
  },
  {
    "objectID": "session_07.html#exercise-1-create-a-remote-repository-on-github",
    "href": "session_07.html#exercise-1-create-a-remote-repository-on-github",
    "title": "7  Intro to Git and GitHub",
    "section": "7.5 Exercise 1: Create a remote repository on GitHub",
    "text": "7.5 Exercise 1: Create a remote repository on GitHub\n\n\n\n\n\n\nSetup\n\n\n\n\nLog into GitHub\nClick the New repository button\nName it {FIRSTNAME}_test\nAdd a short description\nCheck the box to add a README.md file\nAdd a .gitignore file using the R template\nSet the LICENSE to Apache 2.0\n\n\n\nIf you were successful, it should look something like this:\n\nYou’ve now created your first repository! It has a couple of files that GitHub created for you, like the README.md file, and the LICENSE file, and the .gitignore file.\n\nFor simple changes to text files, you can make edits right in the GitHub web interface.\n\n\n\n\n\n\nChallenge\n\n\n\nNavigate to the README.md file in the file listing, and edit it by clicking on the pencil icon. This is a regular Markdown file, so you can just add markdown text. Add a new level 2 header called “Purpose” and add some bullet points describing the purpose of the repo. When done, add a commit message, and hit the “Commit changes” button.\n\n\n\nCongratulations, you’ve now authored your first versioned commit! If you navigate back to the GitHub page for the repository, you’ll see your commit listed there, as well as the rendered README.md file.\n\nLet’s point out a few things about this window. It represents a view of the repository that you created, showing all of the files in the repository so far. For each file, it shows when the file was last modified, and the commit message that was used to last change each file. This is why it is important to write good, descriptive commit messages. In addition, the header above the file listing shows the most recent commit, along with its commit message, and its SHA identifier. That SHA identifier is the key to this set of versioned changes. If you click on the SHA identifier (6c18e0a), it will display the set of changes made in that particular commit.\nIn the next section we’ll use the GitHub URL for the GitHub repository you created to clone the repository onto your local machine so that you can edit the files in RStudio. To do so, start by copying the GitHub URL, which represents the repository location:"
  },
  {
    "objectID": "session_07.html#exercise-2-clone-your-repository-and-use-git-locally-in-rstudio",
    "href": "session_07.html#exercise-2-clone-your-repository-and-use-git-locally-in-rstudio",
    "title": "7  Intro to Git and GitHub",
    "section": "7.6 Exercise 2: clone your repository and use git locally in RStudio",
    "text": "7.6 Exercise 2: clone your repository and use git locally in RStudio\nRStudio knows how to work with files under version control with Git, but only if you are working within an RStudio project folder. In this next section,\nwe will clone the repository that you created on GitHub into a local repository as an RStudio project. Here’s what we’re going to do\n\nWe refer to the remote copy of the repository that is on GitHub as the origin repository (the one that we cloned from), and the copy on our local computer as the local copy.\nRStudio knows how to work with files under version control with Git, but only if you are working within an RStudio project folder. In this next section, we will clone the repository that you created on GitHub into a local repository as an RStudio project. Here’s what we’re going to do:\n\n\n\n\n\n\nSetup\n\n\n\n\nIn the File menu, select “New Project”\nIn the dialog that pops up, select the “Version Control” option, and paste the GitHub URL that you copied into the field for the remote repository Repository URL\nWhile you can name the local copy of the repository anything, it’s typical to use the same name as the GitHub repository to maintain the correspondence\n\n\n\n\n\n\n\n\nOnce you hit “Create Project”, a new RStudio window will open with all of the files from the remote repository copied locally. Depending on how your version of RStudio is configured, the location and size of the panes may differ, but they should all be present, including a Git tab and the normal Files tab listing the files that had been created in the remote repository.\n\nYou’ll note that there is one new file halina_test.Rproj, and three files that we created earlier on GitHub (.gitignore, LICENSE, and README.md).\nIn the Git tab, you’ll note that two files are listed. This is the status pane that shows the current modification status of all of the files in the repository. In this case, the .gitignore file is listed as M for Modified, and halina_test.Rproj is listed with a ?? to indicate that the file is untracked. This means that Git has not stored any versions of this file, and knows nothing about the file. As you make version control decisions in RStudio, these icons will change to reflect the current version status of each of the files.\nInspect the history. For now, let’s click on the History button in the Git tab, which will show the log of changes that occurred, and will be identical to what we viewed on GitHub. By clicking on each row of the history, you can see exactly what was added and changed in each of the two commits in this repository.\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nLet’s make a change to the README.md file, this time from RStudio, then commit the README.md change\nAdd a new section to your README.md called “Creator” using a level 2 header, and under it include some information about yourself. Bonus: Add some contact information and link your email using Markdown syntax\n\n\n\nOnce you save, you’ll immediately see the README.md file show up in the Git tab, marked as a modification. You can select the file in the Git tab, and click Diff to see the differences that you saved (but which are not yet committed to your local repository).\n\nAnd here’s what the newly made changes look like compared to the original file. New lines are highlighted in green, while removed lines are in red.\n\nCommit the RStudio changes. To commit the changes you made to the README.md file, check the Staged checkbox next to the file (which tells Git which changes you want included in the commit), then provide a descriptive commit message, and then click “Commit”.\n\nNote that some of the changes in the repository, namely halina_test.Rproj are still listed as having not been committed. This means there are still pending changes to the repository. You can also see the note that says:\nYour branch is ahead of ‘origin/main’ by 1 commit.\nThis means that we have committed 1 change in the local repository, but that commit has not yet been pushed up to the origin repository, where origin is the typical name for our remote repository on GitHub. So, let’s commit the remaining project files by staging them and adding a commit message.\n\nWhen finished, you’ll see that no changes remain in the Git tab, and the repository is clean.\nInspect the history. Note that the message now says:\nYour branch is ahead of ‘origin/main’ by 2 commits.\nThese 2 commits are the two we just made, and have not yet been pushed to GitHub. By clicking on the “History” button, we can see that there are now a total of four commits in the local repository (while there had only been two on GitHub).\n\nPush these changes to GitHub. Now that everything has been changed as desired locally, you can push the changes to GitHub using the Push button. This will prompt you for your GitHub username and password, and upload the changes, leaving your repository in a totally clean and synchronized state. When finished, looking at the history shows all four commits, including the two that were done on GitHub and the two that were done locally on RStudio.\n\nAnd note that the labels indicate that both the local repository (HEAD) and the remote repository (origin/HEAD) are pointing at the same version in the history. So, if we go look at the commit history on GitHub, all the commits will be shown there as well.\n\n\n\n\n\n\n\nLast thing, some Git configuration\n\n\n\nWhen git released version 2.27, a new feature they incorporated allows users to specify how to pull, essentially, otherwise a warning will appear. To suppress this warning we need to configure our git with this line of code:\ngit config pull.rebase false\npull.rebase false is a default strategy for pulling where it will try to auto-merge the files if possible, and if it can’t it will show a merge conflict\n\n\n\n\n\n\n\n\nWhat should I write in my commit message?\n\n\n\nClearly, good documentation of what you’ve done is critical to making the version history of your repository meaningful and helpful. It’s tempting to skip the commit message altogether, or to add some stock blurb like “Updates”. It’s better to use messages that will be helpful to your future self in deducing not just what you did, but why you did it. Also, commit messages are best understood if they follow the active verb convention. For example, you can see that my commit messages all started with a past tense verb, and then explained what was changed.\nWhile some of the changes we illustrated here were simple and so easily explained in a short phrase, for more complex changes, it’s best to provide a more complete message. The convention, however, is to always have a short, terse first sentence, followed by a more verbose explanation of the details and rationale for the change. This keeps the high level details readable in the version log. I can’t count the number of times I’ve looked at the commit log from 2, 3, or 10 years prior and been so grateful for the diligence of my past self and collaborators."
  },
  {
    "objectID": "session_07.html#exercise-3-setting-up-git-on-an-existing-project",
    "href": "session_07.html#exercise-3-setting-up-git-on-an-existing-project",
    "title": "7  Intro to Git and GitHub",
    "section": "7.7 Exercise 3: Setting up Git on an existing project",
    "text": "7.7 Exercise 3: Setting up Git on an existing project\nNow you have two projects set up in your RStudio environment, training_{USERNAME} and {FIRSTNAME}_test. We set you up with the {FIRSTNAME}_test project since we think it is an easy way to introduce you to Git, but more commonly researchers will have an existing directory of code that they then want to make a Git repository out of. For the last exercise of this session, we will do this with your training_{USERNAME} project.\nFirst, switch to your training_{USERNAME} project using the RStudio project dropdown menu. The project dropdown menu is in the upper right corner of your RStudio pane. Click the dropdown next to your project name ({FIRSTNAME}_test), and then select the training_{USERNAME} project from the “recent projects” list.\n\n\n\n\n\nNext, from the Tools menu, select “Project Options.” In the dialog that pops up, select “Git/SVN” from the menu on the left. In the dropdown at the top of this page, select Git and click “Yes” in the confirmation box. Click “Yes” again to restart RStudio.\nWhen RStudio restarts, you should have a Git tab, with two untracked files (.gitignore and training_{USERNAME}.Rproj).\n\n\n\n\n\n\nChallenge\n\n\n\nAdd and commit the .gitignore and training_{USERNAME}.Rproj files to your Git repository.\n\n\nNow we have your local repository all set up. You can make as many commits as you want on this repository, and it will likely still be helpful to you, but the power in Git and GitHub is really in collaboration. As discussed, GitHub facilitates this, so let’s get this repository on GitHub.\n\n\n\n\n\n\nSetup\n\n\n\n\nGo to GitHub, and click on the “New Repository” button.\nIn the repository name field, enter the same name as your R Project. So for me, this would be training_dolinh.\nAdd a description, keep the repository public, and, most importantly: DO NOT INITIALIZE THE REPOSITORY WITH ANY FILES. We already have the repository set up locally so we don’t need to do this. Initializing the repository will only cause merge issues.\n\nHere is what your page should look like:\n\n\n\n\n\n\nClick the “Create repository” button.\n\n\n\nThis will open your empty repository with a page that conveniently gives you exactly the instructions you need. In our case, we are going to “push an existing repository from the command line.”\n\nClick the clipboard icon to copy the code for the middle option of the three on this page. It should have three lines and look like this:\ngit remote add origin https://github.com/hdolinh/training_dolinh.git\ngit branch -M main\ngit push -u origin main\nBack in RStudio, open the terminal by clicking the Terminal tab next to the Console tab. The prompt should look something like this:\ndolinh@included-crab:~/training_dolinh$\nIn the prompt, paste the code that you copied from the GitHub page and press return.\nThe code that you copied and pasted did three things:\n\nAdded the GitHub repository as the remote repository\nRenamed the default branch to main\nPushed the main branch to the remote GitHub repository\n\nIf you go back to your browser and refresh your GitHub repository page, you should now see your files appear.\n\n\n\n\n\n\nChallenge\n\n\n\nOn your repository page, GitHub has a button that will help you add a README.md file. Click the “Add a README” button and use markdown syntax to create a README.md Commit the changes to your repository.\nGo to your local repository (in RStudio) and pull the changes you made."
  },
  {
    "objectID": "session_07.html#go-further-with-git",
    "href": "session_07.html#go-further-with-git",
    "title": "7  Intro to Git and GitHub",
    "section": "7.8 Go further with Git",
    "text": "7.8 Go further with Git\nThere’s a lot we haven’t covered in this brief tutorial. There are some great and much longer tutorials that cover advanced topics, such as:\n\nUsing Git on the command line\nResolving conflicts\nBranching and merging\nPull requests versus direct contributions for collaboration\nUsing .gitignore to protect sensitive data\nGitHub Issues and why they are useful\n\nand much, much more."
  },
  {
    "objectID": "session_07.html#additional-git-resources",
    "href": "session_07.html#additional-git-resources",
    "title": "7  Intro to Git and GitHub",
    "section": "7.9 Additional Git resources",
    "text": "7.9 Additional Git resources\n\nGitHub Documentation\nLearn Git Branching is an interactive tool to learn git on the command line\nSoftware Carpentry Version Control with Git\nBitbucket’s tutorials on Git Workflows"
  },
  {
    "objectID": "session_08.html#learning-objectives",
    "href": "session_08.html#learning-objectives",
    "title": "8  Cleaning & Wrangling Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nWhat the Split-Apply-Combine strategy is and how it applies to data\nThe difference between wide vs. long table formats and how to convert between them\nHow to use dplyr and tidyr to clean and wrangle data for analysis\nHow to join multiple data frames together using dplyr"
  },
  {
    "objectID": "session_08.html#introduction",
    "href": "session_08.html#introduction",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nThe data we get to work with are rarely, if ever, in the format we need to do our analyses. It’s often the case that one package requires data in one format, while another package requires the data to be in another format. To be efficient analysts, we should have good tools for reformatting data for our needs so we can do our actual work like making plots and fitting models. The dplyr and tidyr R packages provide a fairly complete and extremely powerful set of functions for us to do this reformatting quickly and learning these tools well will greatly increase your efficiency as an analyst.\nAnalyses take many shapes, but they often conform to what is known as the Split-Apply-Combine strategy. This strategy follows a usual set of steps:\n\nSplit: Split the data into logical groups (e.g., area, stock, year)\nApply: Calculate some summary statistic on each group (e.g. mean total length by year)\nCombine: Combine the groups back together into a single table\n\n\n\n\n\n\nAs shown above, our original table is split into groups by year, we calculate the mean length for each group, and finally combine the per-year means into a single table.\ndplyr provides a fast and powerful way to express this. Let’s look at a simple example of how this is done:\nAssuming our length data is already loaded in a data.frame called length_data:\n\n\n\nyear\nlength_cm\n\n\n\n\n1991\n5.673318\n\n\n1991\n3.081224\n\n\n1991\n4.592696\n\n\n1992\n4.381523\n\n\n1992\n5.597777\n\n\n1992\n4.900052\n\n\n1992\n4.139282\n\n\n1992\n5.422823\n\n\n1992\n5.905247\n\n\n1992\n5.098922\n\n\n\nWe can do this calculation using dplyr like this:\n\nlength_data %>% \n  group_by(year) %>% \n  summarize(mean_length_cm = mean(length_cm))\n\nAnother exceedingly common thing we need to do is “reshape” our data. Let’s look at an example table that is in what we will call “wide” format:\n\n\n\nsite\n1990\n1991\n…\n1993\n\n\n\n\ngold\n100\n118\n…\n112\n\n\nlake\n100\n118\n…\n112\n\n\n…\n…\n…\n…\n…\n\n\ndredge\n100\n118\n…\n112\n\n\n\nYou are probably quite familiar with data in the above format, where values of the variable being observed are spread out across columns (Here: columns for each year). Another way of describing this is that there is more than one measurement per row. This wide format works well for data entry and sometimes works well for analysis but we quickly outgrow it when using R. For example, how would you fit a model with year as a predictor variable? In an ideal world, we’d be able to just run:\n\nlm(length ~ year)\n\nBut this won’t work on our wide data because lm() needs length and year to be columns in our table.\nOr how would we make a separate plot for each year? We could call plot() one time for each year but this is tedious if we have many years of data and hard to maintain as we add more years of data to our data set.\nThe tidyr package allows us to quickly switch between wide format and long format using the pivot_longer() function:\n\nsite_data %>% \n  pivot_longer(-site, names_to = \"year\", values_to = \"length\")\n\n\n\n\nsite\nyear\nlength\n\n\n\n\ngold\n1990\n101\n\n\nlake\n1990\n104\n\n\ndredge\n1990\n144\n\n\n…\n…\n…\n\n\ndredge\n1993\n145\n\n\n\nIn this lesson we’re going to walk through the functions you’ll most commonly use from the dplyr and tidyr packages:\n\ndplyr\n\nmutate()\ngroup_by()\nsummarize()\nselect()\nfilter()\narrange()\nrename()\n\ntidyr\n\npivot_longer()\npivot_wider()\nunite()\nseparate()"
  },
  {
    "objectID": "session_08.html#data-cleaning-basics",
    "href": "session_08.html#data-cleaning-basics",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.2 Data Cleaning Basics",
    "text": "8.2 Data Cleaning Basics\nTo demonstrate, we’ll be working with a tidied up version of a data set from Alaska Department of Fish & Game containing commercial catch data from 1878-1997. The data set and reference to the original source can be found at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2.\n\n\n\n\n\n\nSetup\n\n\n\nFirst, open a new RMarkdown document. Delete everything below the setup chunk, and add a library chunk that calls dplyr, tidyr, and readr\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\n\nA note on loading packages\nYou may have noticed the following warning messages pop up when you ran your library chunk.\nAttaching package: ‘dplyr’\n\nThe following objects are masked from ‘package:stats’:\n\n    filter, lag\n\nThe following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\nThese are important warnings. They are letting you know that certain functions from the stats and base packages (which are loaded by default when you start R) are masked by different functions with the same name in the dplyr package. It turns out, the order that you load the packages in matters. Since we loaded dplyr after stats, R will assume that if you call filter(), you mean the dplyr version unless you specify otherwise.\nBeing specific about which version of filter(), for example, you call is easy. To explicitly call a function by its unambiguous name, you use the syntax package_name::function_name(...). So, if I wanted to call the stats version of filter() in this Rmarkdown document, I would use the syntax stats::filter(...).\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nWarnings are important, but we might not want them in our final document. After you have read the packages in, adjust the chunk settings in your library chunk to suppress warnings and messages.\n\n\nNow that we have learned a little mini-lesson on functions, let’s get the data that we are going to use for this lesson.\n\n\n\n\n\n\nSetup\n\n\n\n\nNavigate to the salmon catch data set: https://knb.ecoinformatics.org/#view/df35b.304.2\nRight click the “Download” button for the file byerlySalmonByRegion.csv\nSelect “Copy Link Address” from the dropdown menu\nPaste the URL into the read.csv() function\n\nThe code chunk you use to read in the data should look something like this:\n\ncatch_original <- read.csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1\")\n\nNote that Windows users who want to use this method locally also need to use the url() function here with the argument method = \"libcurl\"\n\ncatch_original <- read.csv(url(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1\", method = \"libcurl\"))\n\n\n\nThis data set is relatively clean and easy to interpret as-is. While it may be clean, it’s in a shape that makes it hard to use for some types of analyses so we’ll want to fix that first.\n\n\n\n\n\n\nChallenge\n\n\n\nBefore we get too much further, spend a minute or two outlining your RMarkdown document so that it includes the following sections and steps:\n\nData Sources\n\nRead in the data\n\nClean and Reshape data\n\nRemove unnecessary columns\nCheck column typing\nReshape data"
  },
  {
    "objectID": "session_08.html#about-the-pipe-operator",
    "href": "session_08.html#about-the-pipe-operator",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.3 About the pipe (%>%) operator",
    "text": "8.3 About the pipe (%>%) operator\nBefore we jump into learning tidyr and dplyr, we first need to explain the %>%.\nBoth the tidyr and the dplyr packages use the pipe operator (%>%), which may look unfamiliar. The pipe is a powerful way to efficiently chain together operations. The pipe will take the output of a previous statement, and use it as the input to the next statement.\nSay you want to both filter() out rows of a data set, and select() certain columns.\nInstead of writing:\n\ndf_filtered <- filter(df, ...)\ndf_selected <- select(df_filtered, ...)\n\nYou can write:\n\ndf_cleaned <- df %>% \n    filter(...) %>%\n    select(...)\n\nIf you think of the assignment operator (<-) as reading like “gets”, then the pipe operator would read like “then”.\nSo you might think of the above chunk being translated as:\nThe cleaned data frame gets the original data, and then a filter (of the original data), and then a select (of the filtered data).\nThe benefits to using pipes are that you don’t have to keep track of (or overwrite) intermediate data frames. The drawbacks are that it can be more difficult to explain the reasoning behind each step, especially when many operations are chained together. It is good to strike a balance between writing efficient code (chaining operations), while ensuring that you are still clearly explaining, both to your future self and others, what you are doing and why you are doing it.\n\n\n\n\n\n\nQuick Tip\n\n\n\nRStudio has a keyboard shortcut for %>%\n\nWindows: Ctrl + Shift + M\nMac: cmd + shift + M"
  },
  {
    "objectID": "session_08.html#selecting-or-removing-columns-using-select",
    "href": "session_08.html#selecting-or-removing-columns-using-select",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.4 Selecting or removing columns using select()",
    "text": "8.4 Selecting or removing columns using select()\nThe first issue is the extra columns All and notesRegCode. Let’s select only the columns we want, and assign this to a variable called catch_data.\n\ncatch_data <- catch_original %>% \n  select(Region, Year, Chinook, Sockeye, Coho, Pink, Chum)\n\nhead(catch_data)\n\n  Region Year Chinook Sockeye Coho Pink Chum\n1    SSE 1886       0       5    0    0    0\n2    SSE 1887       0     155    0    0    0\n3    SSE 1888       0     224   16    0    0\n4    SSE 1889       0     182   11   92    0\n5    SSE 1890       0     251   42    0    0\n6    SSE 1891       0     274   24    0    0\n\n\nMuch better!\nselect() also allows you to say which columns you don’t want, by passing unquoted column names preceded by minus (-) signs:\n\ncatch_data <- catch_original %>% \n  select(-All, -notesRegCode)\n\nhead(catch_data)"
  },
  {
    "objectID": "session_08.html#quality-check",
    "href": "session_08.html#quality-check",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.5 Quality Check",
    "text": "8.5 Quality Check\nNow that we have the data we are interested in using, we should do a little quality check to see that it seems as expected. One nice way of doing this is the glimpse() function.\n\ndplyr::glimpse(catch_data)\n\nRows: 1,708\nColumns: 7\n$ Region  <chr> \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\", \"SSE\",…\n$ Year    <int> 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 18…\n$ Chinook <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"3\", \"4\", \"5\", \"9…\n$ Sockeye <int> 5, 155, 224, 182, 251, 274, 207, 189, 253, 408, 989, 791, 708,…\n$ Coho    <int> 0, 0, 16, 11, 42, 24, 11, 1, 5, 8, 192, 161, 132, 139, 84, 107…\n$ Pink    <int> 0, 0, 0, 92, 0, 0, 8, 187, 529, 606, 996, 2218, 673, 1545, 204…\n$ Chum    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 2, 0, 0, 0, 102, 343…\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nNotice the output of the glimpse() function call. Does anything seem amiss with this data set that might warrant fixing?\n\n\nAnswer:\n\nThe Chinook catch data are character class. Let’s fix it using the function mutate() before moving on."
  },
  {
    "objectID": "session_08.html#changing-column-content-using-mutate",
    "href": "session_08.html#changing-column-content-using-mutate",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.6 Changing column content using mutate()",
    "text": "8.6 Changing column content using mutate()\nWe can use the mutate() function to change a column, or to create a new column. First Let’s try to just convert the Chinook catch values to numeric type using the as.numeric() function, and overwrite the old Chinook column.\n\ncatch_clean <- catch_data %>% \n  mutate(Chinook = as.numeric(Chinook))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `Chinook = as.numeric(Chinook)`.\nCaused by warning:\n! NAs introduced by coercion\n\nhead(catch_clean)\n\n  Region Year Chinook Sockeye Coho Pink Chum\n1    SSE 1886       0       5    0    0    0\n2    SSE 1887       0     155    0    0    0\n3    SSE 1888       0     224   16    0    0\n4    SSE 1889       0     182   11   92    0\n5    SSE 1890       0     251   42    0    0\n6    SSE 1891       0     274   24    0    0\n\n\nWe get a warning “NAs introduced by coercion” which is R telling us that it couldn’t convert every value to an integer and, for those values it couldn’t convert, it put an NA in its place. This is behavior we commonly experience when cleaning data sets and it’s important to have the skills to deal with it when it comes up.\nTo investigate, let’s isolate the issue. We can find out which values are NAs with a combination of is.na() and which(), and save that to a variable called i.\n\ni <- which(is.na(catch_clean$Chinook))\ni\n\n[1] 401\n\n\nIt looks like there is only one problem row, lets have a look at it in the original data.\n\ncatch_data[i,]\n\n    Region Year Chinook Sockeye Coho Pink Chum\n401    GSE 1955       I      66    0    0    1\n\n\nWell that’s odd: The value in catch_thousands is I. It turns out that this data set is from a PDF which was automatically converted into a CSV and this value of I is actually a 1.\nLet’s fix it by incorporating the ifelse() function to our mutate() call, which will change the value of the Chinook column to 1 if the value is equal to I, otherwise it will use as.numeric() to turn the character representations of numbers into numeric typed values.\n\ncatch_clean <- catch_data %>% \n  mutate(Chinook = if_else(Chinook == \"I\", \"1\", Chinook)) %>% \n  mutate(Chinook = as.integer(Chinook))\n\nhead(catch_clean)\n\n  Region Year Chinook Sockeye Coho Pink Chum\n1    SSE 1886       0       5    0    0    0\n2    SSE 1887       0     155    0    0    0\n3    SSE 1888       0     224   16    0    0\n4    SSE 1889       0     182   11   92    0\n5    SSE 1890       0     251   42    0    0\n6    SSE 1891       0     274   24    0    0"
  },
  {
    "objectID": "session_08.html#changing-shape-using-pivot_longer-and-pivot_wider",
    "href": "session_08.html#changing-shape-using-pivot_longer-and-pivot_wider",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.7 Changing shape using pivot_longer() and pivot_wider()",
    "text": "8.7 Changing shape using pivot_longer() and pivot_wider()\nThe next issue is that the data are in a wide format and, we want the data in a long format instead. pivot_longer() from the tidyr package helps us do just this conversion:\n\ncatch_long <- catch_clean %>% \n  pivot_longer(cols = -c(Region, Year), names_to = \"species\", values_to = \"catch\")\n\nhead(catch_long)\n\n# A tibble: 6 × 4\n  Region  Year species catch\n  <chr>  <int> <chr>   <int>\n1 SSE     1886 Chinook     0\n2 SSE     1886 Sockeye     5\n3 SSE     1886 Coho        0\n4 SSE     1886 Pink        0\n5 SSE     1886 Chum        0\n6 SSE     1887 Chinook     0\n\n\nThe syntax we used above for pivot_longer() might be a bit confusing so let’s walk though it.\nThe first argument to pivot_longer is the columns over which we are pivoting. You can select these by listing either the names of the columns you do want to pivot, or the names of the columns you are not pivoting over. The names_to argument takes the name of the column that you are creating from the column names you are pivoting over. The values_to argument takes the name of the column that you are creating from the values in the columns you are pivoting over.\nThe opposite of pivot_longer(), pivot_wider(), works in a similar declarative fashion:\n\ncatch_wide <- catch_long %>% \n  pivot_wider(names_from = species, values_from = catch)\n\nhead(catch_wide)\n\n# A tibble: 6 × 7\n  Region  Year Chinook Sockeye  Coho  Pink  Chum\n  <chr>  <int>   <int>   <int> <int> <int> <int>\n1 SSE     1886       0       5     0     0     0\n2 SSE     1887       0     155     0     0     0\n3 SSE     1888       0     224    16     0     0\n4 SSE     1889       0     182    11    92     0\n5 SSE     1890       0     251    42     0     0\n6 SSE     1891       0     274    24     0     0"
  },
  {
    "objectID": "session_08.html#renaming-columns-with-rename",
    "href": "session_08.html#renaming-columns-with-rename",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.8 Renaming columns with rename()",
    "text": "8.8 Renaming columns with rename()\nIf you scan through the data, you may notice the values in the catch column are very small (these are supposed to be annual catches). If we look at the metadata we can see that the catch column is in thousands of fish so let’s convert it before moving on.\nLet’s first rename the catch column to be called catch_thousands:\n\ncatch_long <- catch_long %>% \n  rename(catch_thousands = catch)\n\nhead(catch_long)\n\n# A tibble: 6 × 4\n  Region  Year species catch_thousands\n  <chr>  <int> <chr>             <int>\n1 SSE     1886 Chinook               0\n2 SSE     1886 Sockeye               5\n3 SSE     1886 Coho                  0\n4 SSE     1886 Pink                  0\n5 SSE     1886 Chum                  0\n6 SSE     1887 Chinook               0\n\n\n\n8.8.1 names() versus rename()\nMany people use the base R function names() to rename columns, often in combination with column indexing that relies on columns being in a particular order. Column indexing is often also used to select columns instead of the select() function from dplyr. Although these methods both work just fine, they do have one major drawback: in most implementations they rely on you knowing exactly the column order your data is in.\nTo illustrate why your knowledge of column order isn’t reliable enough for these operations, considering the following scenario:\nYour colleague emails you letting you know that she has an updated version of the conductivity-temperature-depth data from this year’s research cruise, and sends it along. Excited, you re-run your scripts that use this data for your phytoplankton research. You run the script and suddenly all of your numbers seem off. You spend hours trying to figure out what is going on.\nUnbeknownst to you, your colleagues bought a new sensor this year that measures dissolved oxygen. Because of the new variables in the data set, the column order is different. Your script which previously renamed the fourth column, SAL_PSU to salinity now renames the fourth column, O2_MGpL to salinity. No wonder your results looked so weird, good thing you caught it!\nIf you had written your code so that it doesn’t rely on column order, but instead renames columns using the rename() function, the code would have run just fine (assuming the name of the original salinity column didn’t change, in which case the code would have thrown an error in an obvious way). This is an example of a defensive coding strategy, where you try to anticipate issues before they arise, and write your code in such a way as to keep the issues from happening."
  },
  {
    "objectID": "session_08.html#adding-columns-using-mutate",
    "href": "session_08.html#adding-columns-using-mutate",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.9 Adding columns using mutate()",
    "text": "8.9 Adding columns using mutate()\nNow let’s use mutate() again to create a new column called catch with units of fish (instead of thousands of fish).\n\ncatch_long <- catch_long %>% \n  mutate(catch = catch_thousands * 1000)\n\nhead(catch_long)\n\nNow let’s remove the catch_thousands column for now since we don’t need it. Note that here we have added to the expression we wrote above by adding another function call (mutate) to our expression. This takes advantage of the pipe operator by grouping together a similar set of statements, which all aim to clean up the catch_long data.frame.\n\ncatch_long <- catch_long %>% \n  mutate(catch = catch_thousands * 1000) %>% \n  select(-catch_thousands)\n\nhead(catch_long)\n\n# A tibble: 6 × 4\n  Region  Year species catch\n  <chr>  <int> <chr>   <dbl>\n1 SSE     1886 Chinook     0\n2 SSE     1886 Sockeye  5000\n3 SSE     1886 Coho        0\n4 SSE     1886 Pink        0\n5 SSE     1886 Chum        0\n6 SSE     1887 Chinook     0\n\n\nWe’re now ready to start analyzing the data."
  },
  {
    "objectID": "session_08.html#summary-statistics-using-group_by-and-summarize",
    "href": "session_08.html#summary-statistics-using-group_by-and-summarize",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.10 Summary statistics using group_by() and summarize()",
    "text": "8.10 Summary statistics using group_by() and summarize()\nAs I outlined in the Introduction, dplyr lets us employ the Split-Apply-Combine strategy and this is exemplified through the use of the group_by() and summarize() functions:\n\nmean_region <- catch_long %>% \n  group_by(Region) %>%\n  summarize(catch_mean = mean(catch))\n\nhead(mean_region)\n\n# A tibble: 6 × 2\n  Region catch_mean\n  <chr>       <dbl>\n1 ALU        40384.\n2 BER        16373.\n3 BRB      2709796.\n4 CHG       315487.\n5 CKI       683571.\n6 COP       179223.\n\n\nAnother common use of group_by() followed by summarize() is to count the number of rows in each group. We have to use a special function from dplyr, n().\n\nn_region <- catch_long %>% \n  group_by(Region) %>%\n  summarize(n = n())\n\nhead(n_region)\n\n# A tibble: 6 × 2\n  Region     n\n  <chr>  <int>\n1 ALU      435\n2 BER      510\n3 BRB      570\n4 CHG      550\n5 CKI      525\n6 COP      470\n\n\n\n\nQuick Tip If you are finding that you are reaching for this combination of group_by(), summarize() and n() a lot, there is a helpful dplyr function count() that accomplishes this in one function!\n\n\n\n\n\n\nChallenge\n\n\n\n\nFind another grouping and statistic to calculate for each group\nFind out if you can group by multiple variables"
  },
  {
    "objectID": "session_08.html#filtering-rows-using-filter",
    "href": "session_08.html#filtering-rows-using-filter",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.11 Filtering rows using filter()",
    "text": "8.11 Filtering rows using filter()\nfilter() is the verb we use to filter our data.frame to rows matching some condition. It’s similar to subset() from base R.\nLet’s go back to our original data.frame and do some filter()ing:\n\nSSE_catch <- catch_long %>% \n  filter(Region == \"SSE\")\n\nhead(SSE_catch)\n\n# A tibble: 6 × 4\n  Region  Year species catch\n  <chr>  <int> <chr>   <dbl>\n1 SSE     1886 Chinook     0\n2 SSE     1886 Sockeye  5000\n3 SSE     1886 Coho        0\n4 SSE     1886 Pink        0\n5 SSE     1886 Chum        0\n6 SSE     1887 Chinook     0\n\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nFilter to just catches of over one million fish\nFilter to just Chinook from the SSE region"
  },
  {
    "objectID": "session_08.html#sorting-your-data-using-arrange",
    "href": "session_08.html#sorting-your-data-using-arrange",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.12 Sorting your data using arrange()",
    "text": "8.12 Sorting your data using arrange()\narrange() is how we sort the rows of a data.frame. In my experience, I use arrange() in two common cases:\n\nWhen I want to calculate a cumulative sum (with cumsum()) so row order matters\nWhen I want to display a table (like in an .Rmd document) in sorted order\n\nLet’s re-calculate mean catch by region, and then arrange() the output by mean catch:\n\nmean_region <- catch_long %>% \n  group_by(Region) %>% \n  summarize(mean_catch = mean(catch)) %>% \n  arrange(mean_catch)\n\nhead(mean_region)\n\n# A tibble: 6 × 2\n  Region mean_catch\n  <chr>       <dbl>\n1 BER        16373.\n2 KTZ        18836.\n3 ALU        40384.\n4 NRS        51503.\n5 KSK        67642.\n6 YUK        68646.\n\n\nThe default sorting order of arrange() is to sort in ascending order. To reverse the sort order, wrap the column name inside the desc() function:\n\nmean_region <- catch_long %>% \n  group_by(Region) %>% \n  summarize(mean_catch = mean(catch)) %>% \n  arrange(desc(mean_catch))\n\nhead(mean_region)\n\n# A tibble: 6 × 2\n  Region mean_catch\n  <chr>       <dbl>\n1 SSE      3184661.\n2 BRB      2709796.\n3 NSE      1825021.\n4 KOD      1528350 \n5 PWS      1419237.\n6 SOP      1110942."
  },
  {
    "objectID": "session_08.html#splitting-a-column-using-separate-and-unite",
    "href": "session_08.html#splitting-a-column-using-separate-and-unite",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.13 Splitting a column using separate() and unite()",
    "text": "8.13 Splitting a column using separate() and unite()\nseparate() and its complement, unite() allow us to easily split a single column into numerous (or numerous into a single).\nThis can come in really handle when we need to split a column into two pieces by a consistent separator (like a dash).\nLet’s make a new data.frame with fake data to illustrate this. Here we have a set of site identification codes. with information about the island where the site is (the first 3 letters) and a site number (the 3 numbers). If we want to group and summarize by island, we need a column with just the island information.\n\nsites_df <- data.frame(site = c(\"HAW-101\",\n                                \"HAW-103\",\n                                \"OAH-320\",\n                                \"OAH-219\",\n                                \"MAI-039\"))\n\nsites_df %>% \n  separate(site, c(\"island\", \"site_number\"), \"-\")\n\n  island site_number\n1    HAW         101\n2    HAW         103\n3    OAH         320\n4    OAH         219\n5    MAI         039\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nSplit the city column in the data frame cities_df into city and state_code columns\n\ncities_df <- data.frame(city = c(\"Juneau AK\", \n                                 \"Sitka AK\", \n                                 \"Anchorage AK\"))\n\n\n\nunite() does just the reverse of separate(). If we have a data.frame that contains columns for year, month, and day, we might want to unite these into a single date column.\n\ndates_df <- data.frame(year = c(\"1930\",\n                                \"1930\",\n                                \"1930\"),\n                       month = c(\"12\",\n                                \"12\",\n                                \"12\"),\n                       day = c(\"14\",\n                               \"15\",\n                               \"16\"))\n\ndates_df %>% \n  unite(date, year, month, day, sep = \"-\")\n\n        date\n1 1930-12-14\n2 1930-12-15\n3 1930-12-16"
  },
  {
    "objectID": "session_08.html#altogether-now",
    "href": "session_08.html#altogether-now",
    "title": "8  Cleaning & Wrangling Data",
    "section": "8.14 Altogether, now!",
    "text": "8.14 Altogether, now!\nWe just ran through the various things we can do with dplyr and tidyr but if you’re wondering how this might look in a real analysis. Let’s look at that now:\n\ncatch_original <- read.csv(url(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1\", method = \"libcurl\"))\nregion_defs <- read.csv(url(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1\", method = \"libcurl\")) %>% \n    select(code, mgmtArea)\n\nmean_region <- catch_original %>%\n  select(-All, -notesRegCode) %>% \n  mutate(Chinook = ifelse(Chinook == \"I\", 1, Chinook)) %>% \n  mutate(Chinook = as.numeric(Chinook)) %>% \n  pivot_longer(-c(Region, Year), names_to = \"species\", values_to = \"catch\") %>%\n  mutate(catch = catch*1000) %>% \n  group_by(Region) %>% \n  summarize(mean_catch = mean(catch))\n\nhead(mean_region)\n\n# A tibble: 6 × 2\n  Region mean_catch\n  <chr>       <dbl>\n1 ALU        40384.\n2 BER        16373.\n3 BRB      2709796.\n4 CHG       315487.\n5 CKI       683571.\n6 COP       179223."
  },
  {
    "objectID": "session_09.html#learning-objectives",
    "href": "session_09.html#learning-objectives",
    "title": "9  Intro to Tidy Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand basics of relational data models aka tidy data\nLearn how to design and create effective data tables"
  },
  {
    "objectID": "session_09.html#introduction",
    "href": "session_09.html#introduction",
    "title": "9  Intro to Tidy Data",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nIn this lesson we are going to learn what relational data models are, and how they can be used to manage and analyze data efficiently. Relational data models are what relational databases use to organize tables. However, you don’t have to be using a relational database (like mySQL, MariaDB, Oracle, or Microsoft Access) to enjoy the benefits of using a relational data model. Additionally, your data don’t have to be large or complex for you to benefit. Here are a few of the benefits of using a relational data model:\n\nPowerful search and filtering\nHandle large, complex datasets\nEnforce data integrity\nDecrease errors from redundant updates\n\n\n9.1.1 Simple guidelines for data management\nA great paper called Some Simple Guidelines for Effective Data Management [@borer2009] lays out exactly that - guidelines that make your data management, and your reproducible research, more effective. The first six guidelines are straightforward, but worth mentioning here:\n\nUse a scripted program (like R!)\nNon-proprietary file formats are preferred (eg: csv, txt)\nKeep a raw version of data\nUse descriptive file and variable names (without spaces!)\nInclude a header line in your tabular data files\nUse plain ASCII text\n\nThe next three are a little more complex, but all are characteristics of the relational data model:\n\nDesign tables to add rows, not columns\nEach column should contain only one type of information\nRecord a single piece of data only once; separate information collected at different scales into different tables."
  },
  {
    "objectID": "session_09.html#recognizing-untidy-data",
    "href": "session_09.html#recognizing-untidy-data",
    "title": "9  Intro to Tidy Data",
    "section": "9.2 Recognizing Untidy Data",
    "text": "9.2 Recognizing Untidy Data\nBefore we learn how to create a relational data model, let’s look at how to recognize data that does not conform to the model.\n\n9.2.1 Data Organization\nThis is a screenshot of an actual dataset that came across NCEAS. We have all seen spreadsheets that look like this - and it is fairly obvious that whatever this is, it isn’t very tidy. Let’s dive deeper in to exactly why we wouldn’t consider it tidy.\n\n\n\n9.2.2 Multiple Tables\nYour human brain can see from the way this sheet is laid out that it has three tables within it. Although it is easy for us to see and interpret this, it is extremely difficult to get a computer to see it this way, which will create headaches down the road should you try to read in this information to R or another programming language.\n\n\n\n9.2.3 Inconsistent Observations\nRows correspond to observations. If you look across a single row, and you notice that there are clearly multiple observations in one row, the data are likely not tidy.\n\n\n\n9.2.4 Inconsistent Variables\nColumns correspond to variables. If you look down a column, and see that multiple variables exist in the table, the data are not tidy. A good test for this can be to see if you think the column consists of only one unit type.\n\n\n\n9.2.5 Marginal Sums and Statistics\nMarginal sums and statistics also are not considered tidy, and they are not the same type of observation as the other rows. Instead, they are a combination of observations."
  },
  {
    "objectID": "session_09.html#good-enough-data-modeling",
    "href": "session_09.html#good-enough-data-modeling",
    "title": "9  Intro to Tidy Data",
    "section": "9.3 Good Enough Data Modeling",
    "text": "9.3 Good Enough Data Modeling\n\n9.3.1 Denormalized Data\nWhen data are “denormalized” it means that observations about different entities are combined.\n\nIn the above example, each row has measurements about both the site at which observations occurred, as well as observations of two individual plants of possibly different species found at that site. This is not normalized data.\nPeople often refer to this as wide format, because the observations are spread across a wide number of columns. Note that, should one encounter a new species in the survey, we would have to add new columns to the table. This is difficult to analyze, understand, and maintain.\n\n\n9.3.2 Tabluar Data\nObservations. A better way to model data is to organize the observations about each type of entity in its own table. This results in:\n\nSeparate tables for each type of entity measured\nEach row represents a single observed entity\nObservations (rows) are all unique\nThis is normalized data (aka tidy data)\n\nVariables. In addition, for normalized data, we expect the variables to be organized such that:\n\nAll values in a column are of the same type\nAll columns pertain to the same observed entity (e.g., row)\nEach column represents either an identifying variable or a measured variable\n\n\n\n\n\n\n\nChallenge\n\n\n\nTry to answer the following questions:\n\nWhat are the observed entities in the example above?\nWhat are the measured variables associated with those observations?\n\nAnswer \n\n\nIf we use these questions to tidy our data, we should end up with:\n\nOne table for each entity observed\nOne column for each measured variable\nAdditional columns for identifying variables (such as site ID)\n\nHere is what our tidy data look like:\n\n\n\n\n\nNote that this normalized version of the data meets the three guidelines set by [@borer2009]:\n\nDesign tables to add rows, not columns\nEach column should contain only one type of information\nRecord a single piece of data only once; separate information collected at different scales into different tables."
  },
  {
    "objectID": "session_09.html#using-normalized-data",
    "href": "session_09.html#using-normalized-data",
    "title": "9  Intro to Tidy Data",
    "section": "9.4 Using Normalized Data",
    "text": "9.4 Using Normalized Data\nNormalizing data by separating it into multiple tables often makes researchers really uncomfortable. This is understandable! The person who designed this study collected all of these measurements for a reason - so that they could analyze the measurements together. Now that our site and species information are in separate tables, how would we use site elevation as a predictor variable for species composition, for example? The answer is keys - and they are the cornerstone of relational data models.\nWhen one has normalized data, we often use unique identifiers to reference particular observations, which allows us to link across tables. Two types of identifiers are common within relational data:\n\n\nNote Is a primary key necessary to have in a dataset?\n\nPrimary Key: unique identifier for each observed entity, one per row\nForeign Key: reference to a primary key in another table (linkage)\n\n\n\n\n\n\n\nChallenge\n\n\n\nIn our normalized tables below, identify the following:\n\nThe primary key for each table\nAny foreign keys that exist\n\n\n\n\n\n\n\n\nThe primary key of the top table is id. The primary key of the bottom table is site.\nThe site column is the primary key of that table because it uniquely identifies each row of the table as a unique observation of a site. In the first table, however, the site column is a foreign key that references the primary key from the second table. This linkage tells us that the first height measurement for the DAPU observation occurred at the site with the name Taku."
  },
  {
    "objectID": "session_09.html#merging-data",
    "href": "session_09.html#merging-data",
    "title": "9  Intro to Tidy Data",
    "section": "9.5 Merging Data",
    "text": "9.5 Merging Data\nFrequently, analysis of data will require merging these separately managed tables back together. There are multiple ways to join the observations in two tables, based on how the rows of one table are merged with the rows of the other.\nWhen conceptualizing merges, one can think of two tables, one on the left and one on the right. The most common (and often useful) join is when you merge the subset of rows that have matches in both the left table and the right table: this is called an INNER JOIN. Other types of join are possible as well. A LEFT JOIN takes all of the rows from the left table, and merges on the data from matching rows in the right table. Keys that don’t match from the left table are still provided with a missing value (NA) from the right table. A RIGHT JOIN is the same, except that all of the rows from the right table are included with matching data from the left, or a missing value. Finally, a FULL OUTER JOIN includes all data from all rows in both tables, and includes missing values wherever necessary.\n\n\n\n\n\nSometimes people represent these as Venn diagrams showing which parts of the left and right tables are included in the results for each join. These however, miss part of the story related to where the missing value come from in each result.\n\n\n\n\n\nIn the figure above, the blue regions show the set of rows that are included in the result. For the INNER JOIN, the rows returned are all rows in A that have a matching row in B."
  },
  {
    "objectID": "session_09.html#additonal-tidy-data-resources",
    "href": "session_09.html#additonal-tidy-data-resources",
    "title": "9  Intro to Tidy Data",
    "section": "9.6 Additonal Tidy Data Resources",
    "text": "9.6 Additonal Tidy Data Resources\n\nWhite et al. 2013. Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution 6.\nSoftware Carpentry SQL Tutorial\nTidy Data"
  },
  {
    "objectID": "session_10.html#learning-objectives",
    "href": "session_10.html#learning-objectives",
    "title": "\n10  R Practice: Tidy Data & Joins\n",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPractice joining datasets together\nPractice identifying primary and foreign keys\nPractice using common cleaning and wrangling functions\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThese exercises are adapted from Allison Horst’s EDS 221: Scientific Programming Essentials Course for the Bren School’s Master of Environmental Data Science program."
  },
  {
    "objectID": "session_10.html#about-the-data",
    "href": "session_10.html#about-the-data",
    "title": "\n10  R Practice: Tidy Data & Joins\n",
    "section": "About the data",
    "text": "About the data\nThese exercises will be using bird survey data collected from the central Arizona-Phoenix metropolitan area by Arizona State University researchers [@warren2021]."
  },
  {
    "objectID": "session_10.html#exercise-practice-joins",
    "href": "session_10.html#exercise-practice-joins",
    "title": "\n10  R Practice: Tidy Data & Joins\n",
    "section": "\n10.1 Exercise: Practice Joins",
    "text": "10.1 Exercise: Practice Joins\n\n\n\n\n\n\nSetup\n\n\n\n\nCreate a new R Markdown file.\n\nTitle it “R Practice: Tidy Data and Joins”.\nSave the file and name it “r-practice-tidy-data-joins”.\n\n\n\nNote: Double check that you’re in the right project. Where in RStudio can you check where you are?\n\nLoad the following libraries at the top of your R Markdown file.\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate) # for bonus question\n\n# Quick question: Do you get a message after loading the libraries? What is it telling you? Talk to your neighbor about it or write a note in your Rmd.\n\n\nObtain datasets from the EDI Data Portal Ecological and social Interactions in urban parks: bird surveys in local parks in the central Arizona-Phoenix metropolitan area. Download the following datasets:\n\n\n52_pp52_birds_1.csv\n52_pp52_surveys_1.csv\n52_pp52_sites_1.csv\n52_pp52_taxalist_1.csv\n\nNote: It’s up to you on how you want to download and load the data! You can either use the datasets links or manually download the data and then upload the files to RStudio server.\n\nOrganize your R Markdown in a meaningful way. Organization is personal - so this is up to you! Consider the different ways we’ve organized previous files using: headers, bold text, naming code chunks, comments in code chunks. What is most important is organizing and documenting the file so that your future self (or if you share this file with others!) understands it as well as your current self does right now.\n\n\n\n\n10.1.1 Read in the data\n\n\n\n\n\n\nQuestion 1\n\n\n\nRead in the datasets and store the data frames as bird_observations, sites, surveys, and taxalist (it should be clear from the raw file names which is which).\n\n\n\nAnswer# read in data using datasets links\nbird_observations <- read_csv(\"https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-cap.256.10&entityid=53edaa7a0e083013d9bf20322db1780e\")\nsites <- read_csv(\"https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-cap.256.10&entityid=b2466fa5cb5ed7ee1ea91398fc291c59\")\nsurveys <- read_csv(\"https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-cap.256.10&entityid=81bf72420e69077097fb0790dcdc63a6\")\ntaxalist <- read_csv(\"https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-cap.256.10&entityid=58f863b7e3066e68536a9cacdc7bd58e\")\n\n# read in data from the data directory after manually downloading data \nbird_observations <- read_csv(\"data/52_pp52_birds_1.csv\")\nsites <- read_csv(\"data/52_pp52_sites_1.csv\")\nsurveys <- read_csv(\"data/52_pp52_surveys_1.csv\")\ntaxalist <- read_csv(\"data/52_pp52_taxalist_1.csv\")\n\n\n\n10.1.2 Get familiar with the data\n\n\n\n\n\n\nQuestion 2a\n\n\n\nWhat functions can you use to explore the data you just read in?\n\n\n\nAnswer# view data in a spreadsheet-style viewer\nView(bird_observations)\n\n# returns dimensions of the dataframe by number of rows and number of cols\ndim(bird_observations)\n\n# returns the top six rows of the dataframe\nhead(bird_observations)\n\n# returns all the columns and some info about the cols\nglimpse(bird_observations)\n\n# similar to glimpse but returns some summary statistics about the cols\nsummary(bird_observations)\n\n# returns column names \nnames(bird_observations)\n\n# returns unique values in a column. In this case we can see all the different bird species IDs\nunique(bird_observations$species_id)\n\n\n\n\n\n\n\n\nQuestion 2b\n\n\n\nWhat are the primary and foreign keys for each datasets?\nHint: Is a primary key necessary to have in a dataset?\n\n\n\nAnswer:\n\n\nbird_observations: Does not have a primary key and foreign key is survey_id, site_id, species_id\n\n\nsites: Primary key is site_id and foreign key is site_id, survey_id\n\n\nsurveys: Does not have a primary key and foreign key is site_id\n\n\ntaxalist: Does not have a primary key and foreign key is species_id\n\n\n10.1.3 Create a subset of bird_observations\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nWrite code to create a subset of bird_observations called birds_subset that only contains observations for birds with species id BHCO and RWBL, and from sites with site ID LI-W and NU-C.\nHint: What function do you use to subset data by rows?\n\n\n\nAnswerbirds_subset <- bird_observations %>% \n  filter(species_id %in% c(\"BHCO\", \"RWBL\")) %>% \n  filter(site_id %in% c(\"LI-W\", \"NU-C\"))\n\n\n\n10.1.4 Use left_join() to merge birds_subset with the datasets sites and taxalist\n\n\n\n\n\n\n\nQustion 4\n\n\n\nUse left join(s) to update birds_subset so that it also includes sites and taxalist information. For each join, include an explicit argument saying which key you are joining by (even if it will just assume the correct one for you). Store the updated data frame as birds_left. Make sure to look at the output - is what it contains consistent with what you expected it to contain?\n\n\n\nAnswerbirds_left <- birds_subset %>% \n  left_join(y = sites, by = \"site_id\") %>% \n  left_join(y = taxalist, by = \"species_id\")\n# don't see x = birds_subset here because piping in birds_subset means it automatically assumes birds_subset as x.\n\n\n\n10.1.5 Use full_join() to merge birds_subset and sites data\n\n\n\n\n\n\nQuestion 5a\n\n\n\nFirst, answer: what do you expect a full_join() between birds_subset and sites to contain? Write this in your R Markdown or tell a neighbor.\n\n\n\nAnswer:\nI expect to see all columns and all observations from birds_subset and all columns from sites to be merged into one datasets because in a full join everything is kept.\n\n\n\n\n\n\nQuestions 5b\n\n\n\nWrite code to full_join() the birds_subset and sites data into a new object called birds_full. Explicitly include the variable you’re joining by. Look at the output. Is it what you expected?\n\n\n\nAnswerbirds_full <- birds_subset %>% \n  full_join(x = birds_subset, y = sites, by = \"site_id\")\n\n\n\n10.1.6 Use inner_join() to merge birds_subset and taxalist data\n\n\n\n\n\n\nQuestion 6a\n\n\n\nFirst, answer: what do you expect an inner_join() between birds_subset and taxalist to contain? Write this in your R Markdown or tell a neighbor.\n\n\n\nAnswer:\nI expect to only have data merge together based on species_id and since there is only BHCO and RWBL in birds_subset then I will only retain data related to those two species. I will also expect to see the columns from taxalist: common_name and asu_itis to be merged into the datasets.\n\n\n\n\n\n\nQuestion 6b\n\n\n\nWrite code to inner_join() the birds_subset and taxalist, called birds_inner. Include an argument for what variable you’ll be joining by. Make sure you check the output.\n\n\n\nAnswerbirds_inner <- birds_subset %>% \n  inner_join(x = birds_subset, y = taxalist, by = \"species_id\")\n\n\n\n\n\n\n\n\nQuestion 6c\n\n\n\nWhat would you get if instead of inner_join() you’d used left_join() for this example? Write code for the left join and check.\n\n\n\nAnswerbirds_inner_left <- birds_subset %>% \n  left_join(x = birds_subset, y = taxalist, by = \"species_id\")\n\n\n\n\n\n\n\n\nQuestion 6d\n\n\n\nWhy does that make sense for this scenario? In what case would you expect the outcome to differ from an inner_join()? Write this in your R Markdown or tell a neighbor.\n\n\n\nAnswer:\nI have the same datasets after using both inner_join() and left_join() where x = birds_subset and y = taxalist, by = speices_id. We expect the same here because there is no species_id in birds_subset that does not match in taxalist. If there was a species_id in birds_subset that did NOT exist in taxalist, then we would have expected it to see it in the left join because in the left join everything in the left (or x) is kept. If it was an inner join then we would not see the bird species id that did NOT exist in taxalist. It would be removed because inner only keeps what matches in both."
  },
  {
    "objectID": "session_10.html#exercise-practice-wrangling-joining-data",
    "href": "session_10.html#exercise-practice-wrangling-joining-data",
    "title": "\n10  R Practice: Tidy Data & Joins\n",
    "section": "\n10.2 Exercise: Practice Wrangling & Joining Data",
    "text": "10.2 Exercise: Practice Wrangling & Joining Data\n\n10.2.1 Wrangle bird_observations data and merge the data with all the other datasets (sites, surveys, and taxalist)\n\n\n\n\n\n\nQuestion 7a\n\n\n\nStarting with your object bird_observations, rename the notes column to bird_obs_notes (so this doesn’t conflict with notes in the surveys datasets).\n\n\n\nAnswerbird_observations <- bird_observations %>% \n  rename(bird_obs_notes = notes)\n\n\n\n\n\n\n\n\nQuestion 7b\n\n\n\n\nCreate a subset that contains all observations in the birds_observations dataset,\nthen join the taxalist, sites and surveys datasets to it,\nand finally limit to only columns survey_date, common_name, park_name, bird_count, and observer.\n\nYou can decide the order that you want to create this in (e.g. limit the columns first, then join, or the other way around).\nHint: What function do you use to subset data by columns?\n\n\n\nAnswerbird_obs_subset <- bird_observations %>% \n  full_join(y = taxalist, by = \"species_id\") %>% \n  full_join(y = sites, by = c(\"site_id\", \"survey_id\")) %>% \n  full_join(y = surveys, by = \"site_id\") %>%\n  select(c(survey_date, common_name, park_name, bird_count, observer))\n\n\n\n10.2.2 Explore observer data and fix the values within this column so that all values are in the same format\n\n\n\n\n\n\nQuestion 8a\n\n\n\nContinuing with bird_obs_subset, first use unique() to see the different unique values in the column observer. How many observers are there? Which value is unlike the others?\n\n\n\nAnswerunique(bird_obs_subset$observer)\n\n\n\n\n\n\n\n\nQuestion 8b\n\n\n\nReplace “Josh Burns” with a format that matches the other observer names. Then use unique() again to check your work.\nHint: What function do you use when you are making a change to an entire column?\n\n\n\nAnswerbird_obs_subset <- bird_obs_subset %>% \n  mutate(observer = if_else(observer == \"Josh Burns\", \"J. Burns\", observer))\n\nunique(bird_obs_subset$observer)\n\n\n\n10.2.3 Bonus: Use a new package lubridate to wrangle the date data and find the total number of birds by park and month\nHint: How do you learn about a new function or package?\n\n\n\n\n\n\nBonus Question(s)\n\n\n\n\nUse lubridate::month() to add a new column to bird_obs_subset called survey_month, containing only the month number. Then, convert the month number to a factor (again within mutate()).\nUse dplyr::relocate() to move the new survey_month column to immediately after the survey_date column. You can do this in a separate code chunk, or pipe straight into it from your existing code.\nFilter to only include parks Lindo, Orme, Palomino, and Sonrisa.\nFind the total number of birds observed by park and month (Hint: You can use group_by() and summarize()).\n\n\n\n\nAnswerbird_obs_subset <- bird_obs_subset %>% \n  mutate(survey_month = lubridate::month(survey_date)) %>% \n  mutate(survey_month = as.factor(survey_month)) %>% \n  dplyr::relocate(survey_month, .after = survey_date) %>% \n  filter(park_name %in% c(\"Lindo\", \"Orme\", \"Palomino\", \"Sonrisa\")) %>% \n  group_by(park_name, survey_month) %>% \n  summarize(tot_bird_count_month = n())\n\n\nTake a look at your final dataset. Does it give you the outcome you expected? Is it informative? How would you improve this wrangling process?"
  },
  {
    "objectID": "session_11.html#learning-objectives",
    "href": "session_11.html#learning-objectives",
    "title": "11  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nHow to use Git and GitHub to collaborate with colleagues on code\nWhat typically causes conflicts when collaborating\nWorkflows to avoid conflicts\nHow to resolve a conflict"
  },
  {
    "objectID": "session_11.html#introduction",
    "href": "session_11.html#introduction",
    "title": "11  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nGit is a great tool for working on your own, but even better for working with friends and colleagues. Git allows you to work with confidence on your own local copy of files with the confidence that you will be able to successfully synchronize your changes with the changes made by others.\nThe simplest way to collaborate with Git is to use a shared repository on a hosting service such as GitHub, and use this shared repository as the mechanism to move changes from one Collaborator to another. While there are other more advanced ways to sync git repositories, this “hub and spoke” model works really well due to its simplicity.\nIn this model, the Collaborator will clone a copy of the Owner’s repository from GitHub, and the Owner will grant them Collaborator status, enabling the Collaborator to directly pull and push from the Owner’s GitHub repository."
  },
  {
    "objectID": "session_11.html#collaborating-with-a-trusted-colleague-without-conflicts",
    "href": "session_11.html#collaborating-with-a-trusted-colleague-without-conflicts",
    "title": "11  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "11.2 Collaborating with a trusted colleague without conflicts",
    "text": "11.2 Collaborating with a trusted colleague without conflicts\nWe start by enabling collaboration with a trusted colleague. We will designate the Owner as the person who owns the shared repository, and the Collaborator as the person that they wish to grant the ability to make changes to their repository. We start by giving that person access to our GitHub repository.\n\n\n\n\n\n\nSetup\n\n\n\n\nWe will break you into pairs, then choose one person as the Owner and one as the Collaborator\nLog into GitHub as the `Owner\nNavigate to the Owner’s training repository (e.g., training_{USERNAME})\n\nThen, have the Owner visit their training repository created earlier, and visit the Settings page, and select the Manage access screen, and add the username of your Collaborator in the box.\n\n\n\n\n\nOnce the Collaborator has been added, they should check their email for an invitation from GitHub, and click on the acceptance link, which will enable them to collaborate on the repository.\n\n\nWe will start by having the Collaborator make some changes and share those with the Owner without generating any conflicts, In an ideal world, this would be the normal workflow. The instructors are going to demostrate this in the next section.\n\n11.2.1 Step 1: Collaborator clone\nTo be able to contribute to a repository, the Collaborator must clone the repository from the Owner’s GitHub account. To do this, the Collaborator should visit the GitHub page for the Owner’s repository, and then copy the clone URL. In R Studio, the Collaborator will create a new project from version control by pasting this clone URL into the appropriate dialog (see the earlier chapter introducing GitHub).\n\n\n\n11.2.2 Step 2: Collaborator Edits\nWith a clone copied locally, the Collaborator can now make changes to the index.Rmd file in the repository, adding a line or statement somewhere noticeable near the top. Save your changes.\n\n\n11.2.3 Step 3: Collaborator commit and push\nTo sync changes, the Collaborator will need to add, commit, and push their changes to the Owner’s repository. But before doing so, it’s good practice to pull immediately before committing to ensure you have the most recent changes from the Owner. So, in RStudio’s Git tab, first click the “Diff” button to open the Git window, and then press the green “Pull” down arrow button. This will fetch any recent changes from the origin repository and merge them. Next, add the changed index.Rmd file to be committed by clicking the checkbox next to it, type in a commit message, and click “Commit”. Once that finishes, then the Collaborator can immediately click ‘Push’ to send the commits to the Owner’s GitHub repository.\n\n\n\n11.2.4 Step 4: Owner pull\nNow, the Owner can open their local working copy of the code in RStudio, and pull those changes down to their local copy.\nCongrats, the Owner now has your changes!\n\n\n11.2.5 Step 5: Owner edits, commit, and push\nNext, the Owner should do the same. Make changes to a file in the repository, save it, pull to make sure no new changes have been made while editing, and then add, commit, and push the Owner changes to GitHub.\n\n\n11.2.6 Step 6: Collaborator pull\nThe Collaborator can now pull down those Owner changes, and all copies are once again fully synced. And you’re off to collaborating.\n\n\n\n\n\n\nExercise: With a partner, collaborate in a repository using a conflict-free process\n\n\n\nNow that the instructors have demonstrated this conflict-free process, break into pairs and try the same with your partner. Start by designating one person as the Owner and one as the Collaborator, and then repeat the steps described above:\n\nStep 1: Setup permissions for your Collaborator\nStep 2: Collaborator clones the Owner repository\nStep 3: Collaborator edits the README file\nStep 4: Collaborator commits and pushes the file to GitHub\nStep 5: Owner pulls the changes that the Collaborator made\nStep 6: Owner edits, commits, and pushes some new changes\nStep 7: Collaborator pulls the Owners changes from GitHub\nStep 8: Switch roles, and go through Steps 1-7 again"
  },
  {
    "objectID": "session_11.html#merge-conflicts",
    "href": "session_11.html#merge-conflicts",
    "title": "11  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "11.3 Merge conflicts",
    "text": "11.3 Merge conflicts\nSo things can go wrong, which usually starts with a merge conflict, due to both collaborators making incompatible changes to a file. While the error messages from merge conflicts can be daunting, getting things back to a normal state can be straightforward once you’ve got an idea where the problem lies.\nA merge conflict occurs when both the Owner and Collaborator change the same lines in the same file without first pulling the changes that the other has made. This is most easily avoided by good communication about who is working on various sections of each file, and trying to avoid overlaps. But sometimes it happens, and Git is there to warn you about potential problems. And Git will not allow you to overwrite one person’s changes to a file with another’s changes to the same file if they were based on the same version.\n\nThe main problem with merge conflicts is that, when the Owner and Collaborator both make changes to the same line of a file, Git doesn’t know whose changes take precedence. You have to tell Git whose changes to use for that line.\n\n11.3.1 Common ways to resolve a merge conflict\n1. Abort, abort, abort…\nSometimes you just made a mistake. When you get a merge conflict, the repository is placed in a ‘Merging’ state until you resolve it. There’s a terminal command to abort doing the merge altogether:\ngit merge --abort\nOf course, after doing that you still haven’t synced with your Collaborator’s changes, so things are still unresolved. But at least your repository is now usable on your local machine.\n2. Checkout\nThe simplest way to resolve a conflict, given that you know whose version of the file you want to keep, is to use the command line Git program to tell Git to use either your changes (the person doing the merge), or their changes (the other Collaborator).\n\nkeep your Collaborator’s file: git checkout --theirs conflicted_file.Rmd\nkeep your own file: git checkout --ours conflicted_file.Rmd\n\nOnce you have run that command, then run add (staging), commit, and push the changes as normal.\n3. Pull and edit the file\nBut that requires the command line. If you want to resolve from RStudio, or if you want to pick and choose some of your changes and some of your Collaborator’s, then instead you can manually edit and fix the file. When you pulled the file with a conflict, Git notices that there is a conflict and modifies the file to show both your own changes and your Collaborator’s changes in the file. It also shows the file in the Git tab with an orange U icon, which indicates that the file is Unmerged, and therefore awaiting your help to resolve the conflict. It delimits these blocks with a series of less than and greater than signs, so they are easy to find:\n\n\n\n\n\nTo resolve the conflicts, simply find all of these blocks, and edit them so that the file looks how you want (either pick your lines, your Collaborator’s lines, some combination, or something altogether new), and save. Be sure you removed the delimiter lines that started with\n\n<<<<<<<,\n=======,\nand >>>>>>>.\n\nOnce you have made those changes, you simply add (staging), commit, and push the files to resolve the conflict."
  },
  {
    "objectID": "session_11.html#producing-and-resolving-merge-conflicts",
    "href": "session_11.html#producing-and-resolving-merge-conflicts",
    "title": "11  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "11.4 Producing and resolving merge conflicts",
    "text": "11.4 Producing and resolving merge conflicts\nTo illustrate this process, the instructors are going to carefully create a merge conflict step by step, show how to resolve it, and show how to see the results of the successful merge after it is complete. First, the instructors will walk through the exercise to demonstrate the issues. Then, participants will pair up and try the exercise.\n\n11.4.1 Step 1: Owner and Collaborator ensure all changes are updated\nFirst, start the exercise by ensuring that both the Owner and Collaborator have all of the changes synced to their local copies of the Owner’s repository in RStudio. This includes doing a git pull to ensure that you have all changes local, and make sure that the Git tab in RStudio doesn’t show any changes needing to be committed.\n\n\n11.4.2 Step 2: Owner makes a change and commits\nFrom that clean slate, the Owner first modifies and commits a small change including their name on a specific line of the README.md file (we will change line 4). Work to only change that one line, and add your username to the line in some form and commit the changes (but DO NOT push). We are now in a situation where the Owner has unpushed changes that the Collaborator can not yet see.\n\n\n11.4.3 Step 3: Collaborator makes a change and commits on the same line\nNow the Collaborator also makes changes to the same (line 4) of the README.md file in their RStudio copy of the project, adding their name to the line. They then commit. At this point, both the Owner and Collaborator have committed changes based on their shared version of the README.md file, but neither has tried to share their changes via GitHub.\n\n\n11.4.4 Step 4: Collaborator pushes the file to GitHub\nSharing starts when the Collaborator pushes their changes to the GitHub repo, which updates GitHub to their version of the file. The Owner is now one revision behind, but doesn’t know it yet.\n\n\n11.4.5 Step 5: Owner pushes their changes and gets an error\nAt this point, the Owner tries to push their change to the repository, which triggers an error from GitHub. While the error message is long, it basically tells you everything needed (that the Owner’s repository doesn’t reflect the changes on GitHub, and that they need to pull before they can push).\n\n\n\n11.4.6 Step 6: Owner pulls from GitHub to get Collaborator changes\nDoing what the message says, the Owner pulls the changes from GitHub, and gets another, different error message. In this case, it indicates that there is a merge conflict because of the conflicting lines.\n\nIn the Git pane of RStudio, the file is also flagged with an orange U, which stands for an unresolved merge conflict.\n\n\n\n11.4.7 Step 7: Owner edits the file to resolve the conflict\nTo resolve the conflict, the Owner now needs to edit the file. Again, as indicated above, Git has flagged the locations in the file where a conflict occurred with <<<<<<<, =======, and >>>>>>>. The Owner should edit the file, merging whatever changes are appropriate until the conflicting lines read how they should, and eliminate all of the marker lines with <<<<<<<, =======, and >>>>>>>.\n\nOf course, for scripts and programs, resolving the changes means more than just merging the text – whoever is doing the merging should make sure that the code runs properly and none of the logic of the program has been broken.\n\n\n\n11.4.8 Step 8: Owner commits the resolved changes\nFrom this point forward, things proceed as normal. The Owner first ‘Adds’ the file changes to be made, which changes the orange U to a blue M for modified, and then commits the changes locally. The Owner now has a resolved version of the file on their system.\n\n\n\n11.4.9 Step 9: Owner pushes the resolved changes to GitHub\nHave the Owner push the changes, and it should replicate the changes to GitHub without error.\n\n\n\n11.4.10 Step 10: Collaborator pulls the resolved changes from GitHub\nFinally, the Collaborator can pull from GitHub to get the changes the Owner made.\n\n\n11.4.11 Step 11: Both can view commit history\nWhen either the Collaborator or the Owner view the history, the conflict, associated branch, and the merged changes are clearly visible in the history.\n ::: callout-note ### Exercise: With a partner, collaborate in a repository and resolve a merge conflict\nNow it’s your turn. In pairs, intentionally create a merge conflict, and then go through the steps needed to resolve the issues and continue developing with the merged files. See the sections above for help with each of these steps:\n\nStep 1: Owner and Collaborator ensure all changes are updated\nStep 2: Owner makes a change and commits\nStep 3: Collaborator makes a change and commits on the same line\nStep 4: Collaborator pushes the file to GitHub\nStep 5: Owner pushes their changes and gets an error\nStep 6: Owner pulls from GitHub to get Collaborator changes\nStep 7: Owner edits the file to resolve the conflict\nStep 8: Owner commits the resolved changes\nStep 9: Owner pushes the resolved changes to GitHub\nStep 10: Collaborator pulls the resolved changes from GitHub\nStep 11: Both can view commit history\nStep 12: Switch roles, and go through Steps 1-11 again :::"
  },
  {
    "objectID": "session_11.html#workflows-to-avoid-merge-conflicts",
    "href": "session_11.html#workflows-to-avoid-merge-conflicts",
    "title": "11  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "11.5 Workflows to avoid merge conflicts",
    "text": "11.5 Workflows to avoid merge conflicts\nSome basic rules of thumb can avoid the vast majority of merge conflicts, saving a lot of time and frustration. These are words our teams live by:\n\n\n\n\n\nXKCD 1597\n\n\n\nCommunicate often\nTell each other what you are working on\nStart you working session with a pull\nPull immediately before you commit or push\nCommit often in small chunks\n\nA good workflow is encapsulated as follows:\nPull -> Edit -> Save -> Add (stage) -> Pull -> Commit -> Push\nAlways start your working sessions with a pull to get any outstanding changes, then start doing your editing and work. Stage your changes, but before you Commit, Pull again to see if any new changes have arrived. If so, they should merge in easily if you are working in different parts of the program. You can then Commit and immediately Push your changes safely. Good luck, and try to not get frustrated. Once you figure out how to handle merge conflicts, they can be avoided or dispatched when they occur, but it does take a bit of practice."
  },
  {
    "objectID": "session_12.html#publishing-analyses-to-the-web",
    "href": "session_12.html#publishing-analyses-to-the-web",
    "title": "12  Data visualization & Publishing to the Web using GitHub Pages",
    "section": "Publishing Analyses to the Web",
    "text": "Publishing Analyses to the Web"
  },
  {
    "objectID": "session_12.html#learning-objectives",
    "href": "session_12.html#learning-objectives",
    "title": "12  Data visualization & Publishing to the Web using GitHub Pages",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nHow to use git, GitHub (+Pages), and (R)Markdown to publish an analysis to the web"
  },
  {
    "objectID": "session_12.html#introduction",
    "href": "session_12.html#introduction",
    "title": "12  Data visualization & Publishing to the Web using GitHub Pages",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nSharing your work with others in engaging ways is an important part of the scientific process. So far in this course, we’ve introduced a small set of powerful tools for doing open science:\n\nR and its many packages\nRStudio\ngit\nGiHub\nRMarkdown\n\nRMarkdown, in particular, is amazingly powerful for creating scientific reports but, so far, we haven’t tapped its full potential for sharing our work with others.\nIn this lesson, we’re going to take an existing GitHub repository and turn it into a beautiful and easy to read web page using the tools listed above."
  },
  {
    "objectID": "session_12.html#a-minimal-example",
    "href": "session_12.html#a-minimal-example",
    "title": "12  Data visualization & Publishing to the Web using GitHub Pages",
    "section": "12.2 A Minimal Example",
    "text": "12.2 A Minimal Example\n\nUse your existing training_username repository\nAdd a new file at the top level called index.Rmd. The easiest way to do this is through the RStudio menu. Choose File -> New File -> RMarkdown… This will bring up a dialog box. You should create a “Document” in “HTML” format. These are the default options. Be sure to use the exact capitalization (lower case ‘index’) as different operating systems handle capitalization differently and it can interfere with loading your web page later.\nOpen index.Rmd (if it isn’t already open)\nPress Knit\n\nObserve the rendered output\nNotice the new file in the same directory index.html.\nThis is our RMarkdown file rendered as HTML (a web page)\n\nCommit your changes (to both index.Rmd and index.html) and push to GitHub\nOpen your web browser to the GitHub.com page for your repository\nGo to Settings > GitHub Pages and turn on GitHub Pages for the main branch\n\nNow, the rendered website version of your repo will show up at a special URL.\nGitHub Pages follows a convention like this:\n\n\n\ngithub pages url pattern\n\n\nNote that it will no longer be at github.com but github.io\n\nGo to https://{username}.github.io/{repo_name}/ (Note the trailing /) Observe the awesome rendered output\n\nNow that we’ve successfully published a web page from an RMarkdown document, let’s make a change to our RMarkdown document and follow the steps to actually publish the change on the web:\n\nGo back to our index.Rmd\nDelete all the content, except the YAML frontmatter\nType “Hello world”\nCommit, push\nGo back to https://{username}.github.io/{repo_name}/"
  },
  {
    "objectID": "session_12.html#exercise-sharing-your-work",
    "href": "session_12.html#exercise-sharing-your-work",
    "title": "12  Data visualization & Publishing to the Web using GitHub Pages",
    "section": "12.3 Exercise: Sharing your work",
    "text": "12.3 Exercise: Sharing your work\nRMarkdown web pages are a great way to share work in progress with your colleagues. To do so simply requires thinking through your presentation so that it highlights the workflow to be reviewed. You can also include multiple pages and build a simple web site for walking through your work that is accessible to people who aren’t all set up to open your content in R. In this exercise, we’ll publish another RMarkdown page, and create a table of contents on the main page to guide people to the main page.\nFirst, in your training repository, create a new RMarkdown file that describes some piece of your work and note the name. I’ll use an RMarkdown named data-cleaning.Rmd.\nOnce you have an RMarkdown created, Knit the document which will create the HTML version of the file, which in this case will be named data-cleaning.html.\nNow, return to editing your index.Rmd file from the beginning of this lesson. The index file represents the ‘default’ file for a web site, and is returned whenever you visit the web site but don’t specify an explicit file to be returned. Let’s modify the index page, adding a bulleted list, and in that list, include a link to the new markdown page that we created:\n## Analysis plan\n\n- [Data Cleaning](data-cleaning.html)\n- Data Interpolation and Gap filling\n- Linear models\n- Animal movement models based on telemetry\n- Data visualization\nCommit and push the web page to GitHub. Now when you visit your web site, you’ll see the table of contents, and can navigate to the new data cleaning page."
  },
  {
    "objectID": "session_12.html#publication-graphics",
    "href": "session_12.html#publication-graphics",
    "title": "12  Data visualization & Publishing to the Web using GitHub Pages",
    "section": "Publication Graphics",
    "text": "Publication Graphics"
  },
  {
    "objectID": "session_12.html#learning-objectives-1",
    "href": "session_12.html#learning-objectives-1",
    "title": "12  Data visualization & Publishing to the Web using GitHub Pages",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nThe basics of the ggplot2 package to create static plots\nHow to use ggplot2’s theming abilities to create publication-grade graphics\nThe basics of the leaflet package to create interactive maps"
  },
  {
    "objectID": "session_12.html#overview",
    "href": "session_12.html#overview",
    "title": "12  Data visualization & Publishing to the Web using GitHub Pages",
    "section": "12.4 Overview",
    "text": "12.4 Overview\nggplot2 is a popular package for visualizing data in R. From the home page:\n\nggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. It’s been around for years and has pretty good documentation and tons of example code around the web (like on StackOverflow). This lesson will introduce you to the basic components of working with ggplot2.\n\n\n12.4.1 ggplot vs base vs lattice vs XYZ…\nR provides many ways to get your data into a plot. Three common ones are,\n\n“base graphics” (plot(), hist(), etc`)\nlattice\nggplot2\n\nAll of them work! I use base graphics for simple, quick and dirty plots. I use ggplot2 for most everything else. ggplot2 excels at making complicated plots easy and easy plots simple enough.\n\n\nSetup\nOpen a new RMarkdown document and remove the filler text.\nFirst, let’s load the packages we’ll need:\n\n  library(leaflet)\n  library(dplyr)\n  library(tidyr)\n  library(ggplot2)\n  library(DT)\n  library(scales) # install.packages(\"scales\")\n\nLoad the data table directly from the KNB Data Repository, if it isn’t already present on your local computer. This technique only downloads the file if you need it.\n\ndata_url <- \"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Af119a05b-bbe7-4aea-93c6-85434dcb1c5e\"\n\nesc <- tryCatch(\n    read.csv(\"data/escapement.csv\"),\n    error=function(cond) {\n        message(paste(\"Escapement file does not seem to exist, so get it from the KNB.\"))\n        esc <- read.csv(url(data_url, method = \"libcurl\"))\n        return(esc)\n    }\n)\n\nhead(esc)\n\n\nesc <- read.csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Af119a05b-bbe7-4aea-93c6-85434dcb1c5e\")\n\n\nChallenge\nNow that we have the data loaded, use your dplyr and tidyr skills to calculate annual escapement by species and region.\nHint: try to use separate to extract the year, month, and day from the date column.\n\n\n\nHere is the solution:\n\nannual_esc <- esc %>% \n  separate(sampleDate, c(\"Year\", \"Month\", \"Day\"), sep = \"-\") %>% \n  mutate(Year = as.numeric(Year)) %>% \n  group_by(Species, SASAP.Region, Year) %>% \n  summarize(escapement = sum(DailyCount)) %>% \n  filter(Species %in% c(\"Chinook\", \"Sockeye\", \"Chum\", \"Coho\", \"Pink\"))\n\nhead(annual_esc)\n\nThe chunck above used a lot of the dplyr commands that we’ve used, and some that are new. The separate function is used to divide the sampleDate column up into Year, Month, and Day columns, and then we use group_by to indicate that we want to calculate our results for the unique combinations of species, region, and year. We next use summarize to calculate an escapement value for each of these groups. Finally, we use a filter and the %in% operator to select only the salmon species.\n\n\n\n12.4.2 Static figures using ggplot2\nEvery graphic you make in ggplot2 will have at least one aesthetic and at least one geom (layer). The aesthetic maps your data to your geometry (layer). Your geometry specifies the type of plot we’re making (point, bar, etc.).\nNow, let’s plot our results using ggplot. ggplot uses a mapping aesthetic (set using aes()) and a geometry to create your plot. Additional geometries/aesthetics and theme elements can be added to a ggplot object using +.\n\nggplot(annual_esc, \n       aes(x = Species, y = escapement)) +\n  geom_col()\n\nWhat if we want our bars to be blue instead of gray? You might think we could run this:\n\nggplot(annual_esc, \n       aes(x = Species, y = escapement, \n           fill = \"blue\")) +\n  geom_col()\n\nWhy did that happen?\nNotice that we tried to set the fill color of the plot inside the mapping aesthetic call. What we have done, behind the scenes, is create a column filled with the word “blue” in our dataframe, and then mapped it to the fill aesthetic, which then chose the default fill color of red.\nWhat we really wanted to do was just change the color of the bars. If we want do do that, we can call the color option in the geom_bar function, outside of the mapping aesthetics function call.\n\nggplot(annual_esc, \n       aes(x = Species, y = escapement)) +\n  geom_col(fill = \"blue\")\n\nWhat if we did want to map the color of the bars to a variable, such as region.\nggplot is really powerful because we can easily get this plot to visualize more aspects of our data.\n\nggplot(annual_esc, \n       aes(x = Species, y = escapement, \n           fill = SASAP.Region)) +\n  geom_col()\n\n\nAside\nggplot2 and the pipe operator\nJust like in dplyr and tidyr, we can also pipe a data.frame directly into the first argument of the ggplot function using the %>% operator.\nLet’s look at an example using a different geometry. Here, we use the pipe operator to pass in a filtered version of annual_esc, and make a line plot with points at each observation.\n\nannual_esc %>% \n  filter(SASAP.Region == \"Kodiak\") %>% \nggplot(aes(x = Year, y = escapement, color = Species)) + \n    geom_line() +\n    geom_point()\n\nThis can certainly be convenient, especially for cases like the above, but use it carefully! Combining too many data-tidying or subsetting operations with your ggplot call can make your code more difficult to debug and understand.\n\n\nSetting ggplot themes\nNow let’s work on making this plot look a bit nicer. Add a title using ggtitle(), adjust labels using ylab(), and include a built in theme using theme_bw(). There are a wide variety of built in themes in ggplot that help quickly set the look of the plot. Use the RStudio autocomplete theme_ <TAB> to view a list of theme functions.\nFor clarity in the next section, I’ll save the filtered version of the annual escapement data.frame to it’s own object.\n\nkodiak_esc <- annual_esc %>% \n  filter(SASAP.Region == \"Kodiak\")\n\n\nggplot(kodiak_esc, \n       aes(x = Year, y = escapement, \n           color = Species)) + \n    geom_line() +\n    geom_point() +\n    ylab(\"Escapement\") +\n    ggtitle(\"Kodiak Salmon Escapement\") +\n    theme_bw()\n\nYou can see that the theme_bw() function changed a lot of the aspects of our plot! The background is white, the grid is a different color, etc. There are lots of other built in themes like this that come with the ggplot2 package.\n\n\nChallenge\nUse the RStudio autocomplete, the ggplot2 documentation, a cheatsheet, or good old google to find other built in themes. Pick out your favorite one and add it to your plot.\n\n\n\nThe built in theme functions change the default settings for many elements that can also be changed invididually using thetheme() function. The theme() function is a way to further fine-tune the look of your plot. This function takes MANY arguments (just have a look at ?theme). Luckily there are many great ggplot resources online so we don’t have to remember all of these, just google “ggplot cheatsheet” and find one you like.\nLet’s look at an example of a theme call, where we change the position of our plot above from the right side to the bottom, and remove the title from the legend.\n\nggplot(kodiak_esc, \n       aes(x = Year, y = escapement, \n           color = Species)) + \n    geom_line() +\n    geom_point() +\n    ylab(\"Escapement\") +\n    ggtitle(\"Kodiak Salmon Escapement\") +\n    theme_bw() +\n    theme(legend.position = \"bottom\", \n          legend.title = element_blank())\n\nNote that the theme() call needs to come after any built in themes like theme_bw() are used. Otherwise, theme_bw() will likely override any theme elements that you changed using theme().\nYou can also save the result of a series of theme() function calls to an object to use on multiple plots. This prevents needing to copy paste the same lines over and over again!\n\nmy_theme <- theme_bw() + \n  theme(legend.position = \"bottom\", \n        legend.title = element_blank())\n\n\nggplot(kodiak_esc, \n       aes(x = Year, \n           y = escapement, \n           color = Species)) + \n    geom_line() +\n    geom_point() +\n    ylab(\"Escapement\") +\n    ggtitle(\"Kodiak Salmon Escapement\") +\n    my_theme\n\n\n\nChallenge\nUsing whatever method you like, figure out how to rotate the x-axis tick labels to a 45 degree angle.\n\n\nSmarter tick labels using scales\nFixing tick labels in ggplot can be super annoying. The y-axis labels in the plot above don’t look great. We could manually fix them, but it would likely be tedious and error prone.\nThe scales package provides some nice helper functions to easily rescale and relabel your plots. Here, we use scale_y_continuous from ggplot2, with the argument labels, which is assigned to the function name comma, from the scales package. This will format all of the labels on the y-axis of our plot with comma-formatted numbers.\n\nggplot(kodiak_esc, \n       aes(x = Year, \n           y = escapement, \n           color = Species)) + \n    geom_line() +\n    geom_point() +\n    scale_y_continuous(labels = comma) +\n    ylab(\"Escapement\") +\n    ggtitle(\"Kodiak Salmon Escapement\") +\n    my_theme\n\n\n\n12.4.2.1 Saving plots\nSaving plots using ggplot is easy! The ggsave function will save either the last plot you created, or any plot that you have saved to a variable. You can specify what output format you want, size, resolution, etc.\n\nggsave(\"kodiak_esc.png\", width = 3, height = 3, units = \"in\")\n\n\n\nCreating multiple plots\nWhat if we wanted to generate a plot for every region? A fast way to do this uses the function facet_wrap(). This function takes a mapping to a variable using the syntax ~variable_name. The ~ (tilde) is a model operator which tells facet_wrap to model each unique value within variable_name to a facet in the plot.\nThe default behaviour of facet wrap is to put all facets on the same x and y scale. You can use the scales argument to specify whether to allow different scales between facet plots. Here, we free the y scale. You can also specify the number of columns using the n_col argument.\n\nggplot(annual_esc, \n       aes(x = Year, \n           y = escapement, \n           color = Species)) + \n    geom_line() +\n    geom_point() +\n    scale_y_continuous(labels = comma) +\n    facet_wrap(~SASAP.Region, scales = \"free_y\", ncol = 2) +\n    ylab(\"Escapement\") +\n    my_theme\n\n\n\n\n12.4.3 Interactive visualization using leaflet and DT\n\nTables\nNow that we know how to make great static visualizations, lets introduce two other packages that allow us to display our data in interactive ways. These packages really shine when used with GitHub pages, so at the end of this lesson we will publish our figures to the website we created earlier this lesson.\nFirst let’s show an interactive table of unique sampling locations using DT. Write a data.frame containing unique sampling locations with no missing values using two new functions from dplyr and tidyr: distinct() and drop_na().\n\nlocations <- esc %>% \n  distinct(Location, Latitude, Longitude) %>% \n  drop_na()\n\nAnd display it as an interactive table using datatable() from the DT package.\n\ndatatable(locations)\n\n\n\nMaps\nSimilar to ggplot2, you can make a basic leaflet map using just a couple lines of code. Note that unlike ggplot2, the leaflet package uses pipe operators (%>%) and not the additive operator (+).\nThe addTiles() function without arguments will add base tiles to your map from OpenStreetMap. addMarkers() will add a marker at each location specified by the latitude and longitude arguments. Note that the ~ symbol is used here to model the coordinates to the map (similar to facet_wrap in ggplot).\n\nleaflet(locations) %>% \n  addTiles() %>% \n  addMarkers(lng = ~Longitude, \n             lat = ~Latitude, \n             popup = ~ Location)\n\nYou can also use leaflet to import Web Map Service (WMS) tiles. Here is an example that utilizes the General Bathymetric Map of the Oceans (GEBCO) WMS tiles. In this example, we also demonstrate how to create a more simple circle marker, the look of which is explicitly set using a series of style-related arguments..\n\nleaflet(locations) %>% \n  addWMSTiles(\"https://www.gebco.net/data_and_products/gebco_web_services/web_map_service/mapserv?request=getmap&service=wms&BBOX=-90,-180,90,360&crs=EPSG:4326&format=image/jpeg&layers=gebco_latest&width=1200&height=600&version=1.3.0\",\n              layers = 'GEBCO_LATEST',\n              attribution = \"Imagery reproduced from the GEBCO_2022 Grid, WMS 1.3.0 GetMap, www.gebco.net\") %>%\n  addCircleMarkers(lng = ~Longitude,\n                   lat = ~Latitude,\n                   popup = ~ Location,\n                   radius = 5,\n                   # set fill properties\n                   fillColor = \"salmon\",\n                   fillOpacity = 1,\n                   # set stroke properties\n                   stroke = T,\n                   weight = 0.5,\n                   color = \"white\",\n                   opacity = 1)\n\nLeaflet has a ton of functionality that can enable you to create some beautiful, functional maps with relative ease. Here is an example of some we created as part of the SASAP project, created using the same tools we showed you here. This map hopefully gives you an idea of how powerful the combination of RMarkdown and GitHub pages can be.\n\n\n\n12.4.4 Resources\n\nLisa Charlotte Rost. (2018) Why not to use two axes, and what to use instead: The case against dual axis charts"
  },
  {
    "objectID": "session_13.html#learning-objectives",
    "href": "session_13.html#learning-objectives",
    "title": "13  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPractice using common cleaning and wrangling functions\nPractice creating plots using common visualization functions in ggplot\nPractice saving and sharing data visualizations\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThese exercises are adapted from Allison Horst’s EDS 221: Scientific Programming Essentials Course for the Bren School’s Master of Environmental Data Science program."
  },
  {
    "objectID": "session_13.html#about-the-data",
    "href": "session_13.html#about-the-data",
    "title": "13  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "About the data",
    "text": "About the data\nThese exercises will be using data on abundance, size, and trap counts (fishing pressure) of California spiny lobster (Panulirus interruptus) and were collected along the mainland coast of the Santa Barbara Channel by Santa Barbara Coastal LTER researchers [@lter2022]."
  },
  {
    "objectID": "session_13.html#exercise-collaborate-on-an-analysis-and-create-a-report-to-publish-using-github-pages",
    "href": "session_13.html#exercise-collaborate-on-an-analysis-and-create-a-report-to-publish-using-github-pages",
    "title": "13  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "13.1 Exercise: Collaborate on an analysis and create a report to publish using GitHub Pages",
    "text": "13.1 Exercise: Collaborate on an analysis and create a report to publish using GitHub Pages\n\n\n\n\n\n\nSetup\n\n\n\n\nCreate a new repository with a partner\n\nDetermine who is the owner and who is the collaborator\nThe owner creates a repository on GitHub titled with both your names (i.e. If Halina and Camila were partners, and Halina is the owner, she would create a repo called halina-camila)\n\nWhen creating the repository, add a brief description (i.e. coreR R Practice Session: Collaborating on, Wrangling & Visualizing Data), keep the repo Public, and Initialize the repo with a README file and an R .gitignore template.\n\nThe owner adds the collaborator to the repo\nBoth the collaborator and the owner clone the repo into their RStudio\n\n\nStep 2 and Step 3 are meant to be completed at the same time. Step 2 is for the collaborator to complete, and Step 3 is for the owner to complete.\n\nCollaborator creates new files for exercise\n\nThe collaborator creates the following R Markdown files that will be used in the exercise:\n\nTitle it: “Owner Analysis” and save it as: owner-analysis.Rmd\nTitle it: “Collaborator Analysis” and save it as: collaborator-analysis.Rmd\nTitle it: “Lobster Report” and save it as: lobster-report.Rmd\n\nAfter creating the files, the collaborator will stage, commit, write a commit message, and push the files to the remote repository (on github)\nThe owner pulls the changes and R Markdown files into their local repository (their workspace)\n\nOwner downloads data from the EDI Data Portal SBC LTER: Reef: Abundance, size and fishing effort for California Spiny Lobster (Panulirus interruptus), ongoing since 2012.\n\nCreate two new folders one called data and one called figs\nDownload the following datasets and upload them to the data folder:\n\nTime-series of lobster abundance and size\nTime-series of lobster trap buoy counts\n\nAfter creating the data folder and adding the datasets, the owner will stage, commit, write a commit message, and push the files to the remote repository (on github)\nThe collaborator pulls the changes and datasets into their local repository (their workspace)\n\n\n\n\n\n13.1.1 Explore, clean and wrangle data\nFor this portion of the exercise, the owner will be working with the lobster abundance and size dataset, and the collaborator will be working with the lobster trap buoy counts dataset.\nQuestions 1-5 you will be working independently since you’re working with different datasets, but you’re welcome to check in with each other.\n\nOwnerCollaborator\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nOpen the R Markdown file owner-analysis.Rmd\n\nCheck the YAML and add your name to the author field\nCreate a new section with a level 2 header and title it “Exercise: Explore, Clean, and Wrangle Data”\n\nLoad the following libraries at the top of your R Markdown file\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\nRead in the dataset and store the data frame as lobster_abundance\n\n\nlobster_abundance <- read_csv(\"data/Lobster_Abundance_All_Years_20220829.csv\")\n\n\n\n\n13.2 Convert missing values using mutate() and na_if()\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe variable SIZE_MM uses -99999 as the code for missing values (see metadata or use unique()). This has the potential to cause conflicts with our analyses, so let’s convert -99999 to an NA value. Do this using mutate() and na_if(). Look up the help page to see how to use na_if(). Check your output data using unique().\n\n\n\n\nAnswer\nlobster_abundance <- lobster_abundance %>% \n    mutate(SIZE_MM = na_if(SIZE_MM, -99999))\n\n\n\n\n13.3 filter() practice\n\n\n\n\n\n\nQuestion 2\n\n\n\nCreate and store a subset that does NOT include observations from Naples Reef (NAPL). Check your output data frame to ensure that NAPL is NOT in the data frame.\n\n\n\n\nAnswer\nnot_napl <- lobster_abundance %>% \n    filter(SITE != \"NAPL\")\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCreate and store a subset with lobsters at Arroyo Quemado (AQUE) OR with a carapace length greater than 70 mm. Check your output.\n\n\n\n\nAnswer\naque_70mm <- lobster_abundance %>% \n    filter(SITE == \"AQUE\", SIZE_MM >= 70)\n\n\n\n\n13.4 mutate() practice\n\n\n\n\n\n\nQuestion 4\n\n\n\nAdd a new column that contains lobster carapace length converted to centimeters. There are 10 millimeters in 1 centimeter. Check your output.\n\n\n\n\nAnswer\nlobster_cm <- lobster_abundance %>% \n  mutate(SIZE_CM = SIZE_MM / 10)\n\n\n\n\n13.5 group_by() %>% summarize() practice\n\n\n\n\n\n\nQuestion 5\n\n\n\nFind the mean of lobster carapace length using mean(), and group by SITE. Think about how you want to treat the NA values in SIZE_MM (Hint: check the arguments in mean()). Check your output.\n\n\n\n\nAnswer\nlobster_mean <- lobster_abundance %>% \n  group_by(SITE) %>% \n  summarize(MEAN_LENGTH = mean(SIZE_MM, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nOpen the R Markdown file collaborator-analysis.Rmd\n\nCheck the YAML and add your name to the author field\nCreate a new section with a level 2 header and title it “Exercise: Explore, Clean, and Wrangle Data”\n\nLoad the following libraries at the top of your R Markdown file.\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\nRead in the dataset and store the data frame as lobster_traps\n\n\nlobster_traps <- read_csv(\"data/Lobster_Trap_Counts_All_Years_20210519.csv\")\n\n\n\n\n13.6 Convert missing values using mutate() and na_if()\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe variable TRAPS uses -99999 as the code for missing values (see metadata or use unique()). This has the potential to cause conflicts with our analyses, so let’s convert -99999 to an NA value. Do this using mutate() and na_if(). Look up the help page to see how to use na_if(). Check your output data using unique().\n\n\n\n\nAnswer\nlobster_traps <- lobster_traps %>% \n    mutate(TRAPS = na_if(TRAPS, -99999))\n\n\n\n\n13.7 filter() practice\n\n\n\n\n\n\nQuestion 2\n\n\n\nCreate and store a subset that does NOT include observations from Naples Reef (NAPL). Check your output data frame to ensure that NAPL is NOT in the data frame.\n\n\n\n\nAnswer\nnot_napl <- lobster_traps %>% \n    filter(SITE != \"NAPL\")\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCreate and store a subset with lobsters at Carpinteria Reef (CARP) OR number of commercial trap floats is greater than 20. Check your output.\n\n\n\n\nAnswer\ncarp_20_traps <- lobster_traps %>% \n    filter(SITE == \"CARP\", TRAPS > 20)\n\n\n\n\n13.8 mutate() practice\n\n\n\n\n\n\nQuestion 4\n\n\n\nUsing the object you created above (we called it carp_20_traps), add a new column that contains the proportion of lobster traps at site CARP with more than 20 commercial trap floats. The total number you are using in your percent calculation is 801. Check your output.\n\n\n\n\nAnswer\n# how we calculated 801\n# total number of traps at site \"CARP\" and more than 20 traps counted \nlobster_carp_total <- sum(carp_20_traps$TRAPS, na.rm = TRUE)\n    \nlobster_percent <- carp_20_traps %>% \n  mutate(PERCENT_TRAPS = (TRAPS / lobster_carp_total) * 100)\n\n\n\n\n13.9 group_by() %>% summarize() practice\n\n\n\n\n\n\nQuestion 5\n\n\n\nFind the mean of lobster commercial trap floats using mean(), and group by SITE. Think about how you want to treat the NA values in TRAPS (Hint: check the arguments in mean()). Check your output.\n\n\n\n\nAnswer\nmean_lobster_traps <- lobster_traps %>% \n    group_by(SITE) %>%\n    summarize(MEAN_TRAPS = mean(TRAPS, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.9.1 Create visually appealing and informative data visualization\n\nOwnerCollaborator\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nStay in the R Markdown file owner-analysis.Rmd and create a new section with a level 2 header and title it “Exercise: Data Visualization”\n\nStructure of the data visualization exercises:\n\nQuestions 7-9 will have you create the necessary subsets to create the data visualizations.\nQuestion 10, return to the data visualization code you’ve written and add styling code to it.\nLastly, you will save the final visualizations to the figs folder before collaborating on the lobster-report.Rmd.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nCreate a multi-panel plot of lobster carapace length (SIZE_MM) using ggplot(), geom_histogram(), and facet_wrap(). Use the variable SITE in facet_wrap(). Use the object lobster_abundance.\n\n\n\n\nAnswer\nggplot(data = lobster_abundance, aes(x = SIZE_MM)) +\n    geom_histogram() +\n    facet_wrap(~SITE)\n\n\n\n\nPlots\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nCreate a line graph of the number of total lobsters observed (y-axis) by year (x-axis) in the study, grouped by SITE.\n\n\nFirst, you’ll need to create a new dataset subset called lobsters_summarize:\n\nGroup the data by SITE AND YEAR\nCalculate the total number of lobsters observed using count()\n\n\n\nAnswer\nlobsters_summarize <- lobster_abundance %>% \n  group_by(SITE, YEAR) %>% \n  summarize(COUNT = n())\n\n\nNext, create a line graph using ggplot() and geom_line(). Use geom_point() to make the data points more distinct, but ultimately up to you if you want to use it or not. We also want SITE information on this graph, do this by specifying the variable in the color argument. Where should the color argument go? Inside or outside of aes()? Why or why not?\n\n\nAnswer\n# line plot\ngplot(data = lobsters_summarize, aes(x = YEAR, y = COUNT)) +\n  geom_line(aes(color = SITE)) \n\n# line and point plot\nggplot(data = lobsters_summarize, aes(x = YEAR, y = COUNT)) +\n  geom_point(aes(color = SITE)) +\n  geom_line(aes(color = SITE)) \n\n\n\n\nPlots\n\n\n\n\n\n\n\nLine plot\n\n\n\n\n\n\n\nLine and point plot\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nCreate a bar graph that shows the amount of small and large sized carapace lobsters at each SITE from 2019-2021. Note: The small and large metrics are completely made up and are not based on any known facts.\n\n\nFirst, you’ll need to create a new dataset subset called lobster_size_lrg:\n\nfilter() for the years 2019, 2020, and 2021\nAdd a new column called SIZE_BIN that contains the values “small” or “large”. A “small” carapace size is <= 70 mm, and a “large” carapace size is greater than 70 mm. Use mutate() and if_else(). Check your output\nCalculate the number of “small” and “large” sized lobsters using group() and summarize(). Check your output\nRemove the NA values from the subsetted data. Hint: check out drop_na(). Check your output\n\n\n\nAnswer\nlobster_size_lrg <- lobster_abundance %>% \n    filter(YEAR %in% c(2019, 2020, 2021)) %>% \n    mutate(SIZE_BIN = if_else(SIZE_MM <= 70, true = \"small\", false = \"large\")) %>% \n    group_by(SITE, SIZE_BIN) %>%\n    summarize(COUNT = n()) %>%\n    drop_na()\n\n\nNext, create a bar graph using ggplot() and geom_bar(). Note that geom_bar() automatically creates a stacked bar chart. Try using the argument position = \"dodge\" to make the bars side by side. Pick which bar position you like best.\n\n\nAnswer\n# bar plot\nggplot(data = lobster_size_lrg, aes(x = SITE, y = COUNT, fill = SIZE_BIN)) +\n    geom_bar(stat = \"identity\")\n\n# dodged bar plot\nggplot(data = lobster_size_lrg, aes(x = SITE, y = COUNT, fill = SIZE_BIN)) +\n    geom_bar(stat = \"identity\", position = \"dodge\")\n\n\n\n\nPlots\n\n\n\n\n\n\n\nBar plot\n\n\n\n\n\n\n\nDodged bar plot\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\n\n\nGo back to your visualization code and add some styling code (aka make your plots pretty!). Here’s a list of functions to help you get started (this is not an exhaustive list!) or revisit the data visualization lesson:\n\n\nlabs(): modifying axis, legend and plot labels\ntheme_(): add a complete theme to your plot (i.e. theme_light())\ntheme(): use to customize non-data components of a plot. We’ve listed out some parameters here, but run ?theme to see the full list (there’s a lot of customization you can do!)\n\naxis.title.y\npanel.background\nplot.background\npanel.grid.major.*\ntext\n\nscale_*_date(): use with dates and update breaks, limits, and labels\nscale_*_continuous(): use with continuous variables and update breaks, limits, and labels\nscale_*_discrete(): use with discrete variables and update breaks, limits, and labels\nscales package: use this within the above scale functions and you can do things like add percents to axes labels\ngeom_() within a geom function you can modify:\n\nfill: updates fill colors (e.g. column, density, violin, & boxplot interior fill color)\ncolor: updates point & border line colors (generally)\nshape: update point style\nalpha: update transparency (0 = transparent, 1 = opaque)\nsize: point size or line width\nlinetype: update the line type (e.g. “dotted”, “dashed”, “dotdash”, etc.)\n\n\n\nOnce you’re happy with how your plot looks, assign it to an object, and save it to the figs directory using ggsave()\n\n\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nStay in the R Markdown file collaborator-analysis.Rmd and create a new section with a level 2 header and title it “Exercise: Data Visualization”\n\nStructure of the data visualization exercises:\n\nQuestions 7-9 will have you create the necessary subsets to create the data visualizations.\nQuestion 10, return to the data visualization code you’ve written and add styling code to it.\nLastly, you will save the final visualizations to the figs folder before collaborating on the lobster-report.Rmd.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nCreate a multi-panel plot of lobster commercial traps (TRAPS) grouped by year, using ggplot(), geom_histogram(), and facet_wrap(). Use the variable YEAR in facet_wrap(). Use the object lobster_traps.\n\n\n\n\nAnswer\nggplot(data = lobster_traps, aes(x = TRAPS)) +\n    geom_histogram() +\n    facet_wrap( ~ YEAR)\n\n\n\n\nPlots\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nCreate a line graph of the number of total lobster commercial traps observed (y-axis) by year (x-axis) in the study, grouped by SITE.\n\n\nFirst, you’ll need to create a new dataset subset called lobsters_traps_summarize:\n\nGroup the data by SITE AND YEAR\nCalculate the total number of lobster commercial traps observed using sum(). Look up sum() if you need to. Call the new column TOTAL_TRAPS. Don’t forget about NAs here!\n\n\n\nAnswer\nlobsters_traps_summarize <- lobster_traps %>% \n  group_by(SITE, YEAR) %>% \n  summarize(TOTAL_TRAPS = sum(TRAPS, na.rm = TRUE))\n\n\nNext, create a line graph using ggplot() and geom_line(). Use geom_point() to make the data points more distinct, but ultimately up to you if you want to use it or not. We also want SITE information on this graph, do this by specifying the variable in the color argument. Where should the color argument go? Inside or outside of aes()? Why or why not?\n\n\nAnswer\n# line plot\nggplot(data = lobsters_traps_summarize, aes(x = YEAR, y = TOTAL_TRAPS)) +\n    geom_line(aes(color = SITE))\n\n# line and point plot\nggplot(data = lobsters_traps_summarize, aes(x = YEAR, y = TOTAL_TRAPS)) +\n    geom_point(aes(color = SITE)) +\n    geom_line(aes(color = SITE))\n\n\n\n\nPlots\n\n\n\n\n\n\n\nLine plot\n\n\n\n\n\n\n\nLine and point plot\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nCreate a bar graph that shows the amount of high and low fishing pressure of lobster commercial traps at each SITE from 2019-2021. Note: The high and low fishing pressure metrics are completely made up and are not based on any known facts.\n\n\nFirst, you’ll need to create a new dataset subset called lobster_traps_fishing_pressure:\n\nfilter() for the years 2019, 2020, and 2021\nAdd a new column called FISHING_PRESSURE that contains the values “high” or “low”. A “high” fishing pressure has exactly or more than 8 traps, and a “low” fishing pressure has less than 8 traps. Use mutate() and if_else(). Check your output\nCalculate the number of “high” and “low” observations using group() and summarize(). Check your output\nRemove the NA values from the subsetted data. Hint: check out drop_na(). Check your output\n\n\n\nAnswer\nlobster_traps_fishing_pressure <- lobster_traps %>% \n    filter(YEAR %in% c(2019, 2020, 2021)) %>%\n    mutate(FISHING_PRESSURE = if_else(TRAPS >= 8, true = \"high\", false = \"low\")) %>%\n    group_by(SITE, FISHING_PRESSURE) %>%\n    summarize(COUNT = n()) %>%\n    drop_na()\n\n\nNext, create a bar graph using ggplot() and geom_bar(). Note that geom_bar() automatically creates a stacked bar chart. Try using the argument position = \"dodge\" to make the bars side by side. Pick which bar position you like best.\n\n\nAnswer\n# bar plot\nggplot(data = lobster_traps_fishing_pressure, aes(x = SITE, y = COUNT, fill = FISHING_PRESSURE)) +\n    geom_bar(stat = \"identity\")\n\n# dodged bar plot\nggplot(data = lobster_traps_fishing_pressure, aes(x = SITE, y = COUNT, fill = FISHING_PRESSURE)) +\n    geom_bar(stat = \"identity\", position = \"dodge\")\n\n\n\n\nPlots\n\n\n\n\n\n\n\nBar plot\n\n\n\n\n\n\n\nDodged bar plot\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\n\n\nGo back to your visualization code and add some styling code (aka make your plots pretty!). Here’s a list of functions to help you get started (this is not an exhaustive list!) or revisit the data visualization lesson:\n\n\nlabs(): modifying axis, legend and plot labels\ntheme_(): add a complete theme to your plot (i.e. theme_light())\ntheme(): use to customize non-data components of a plot. We’ve listed out some parameters here, but run ?theme to see the full list (there’s a lot of customization you can do!)\n\naxis.title.y\npanel.background\nplot.background\npanel.grid.major.*\ntext\n\nscale_*_date(): use with dates and update breaks, limits, and labels\nscale_*_continuous(): use with continuous variables and update breaks, limits, and labels\nscale_*_discrete(): use with discrete variables and update breaks, limits, and labels\nscales package: use this within the above scale functions and you can do things like add percents to axes labels\ngeom_() within a geom function you can modify:\n\nfill: updates fill colors (e.g. column, density, violin, & boxplot interior fill color)\ncolor: updates point & border line colors (generally)\nshape: update point style\nalpha: update transparency (0 = transparent, 1 = opaque)\nsize: point size or line width\nlinetype: update the line type (e.g. “dotted”, “dashed”, “dotdash”, etc.)\n\n\n\nOnce you’re happy with how your plot looks, assign it to an object, and save it to the figs directory using ggsave()\n\n\n\n\n\n\n\n\n13.9.2 Collaborate on a report and publish using GitHub pages\nThe final step! Time to work together again. Collaborate with your partner in lobster-report.Rmd to create a report to publish to GitHub pages.\n\n\n\n\n\n\nCode Review\n\n\n\nAs you’re adding owner and collaborator code in the report document, conduct a lightweight code review and walk each other through your the code for the data visualizations you plan to include in the report (this includes the code you wrote to create the subset for the plot and the code to create the plot).\nThis is a lightweight code review, so think brief and give feedback on:\n\ncode readability\ncode logic\n\n\n\nMake sure your R Markdown is well organized and includes the following elements:\n\ncitation of the data\nbrief summary of the abstract (i.e. 1-2 sentences)\nowner analysis and visualizations (you choose which plots you want to include)\n\nadd alternative text to your plots\n\ncollaborator analysis and visualizations (you choose which plots you want to include)\n\nadd alternative text to your plots\n\n\nFinally, publish on GitHub pages (from owner’s repository). Refer back to Chapter 9 for steps on how to publish using GitHub pages."
  },
  {
    "objectID": "session_13.html#convert-missing-values-using-mutate-and-na_if",
    "href": "session_13.html#convert-missing-values-using-mutate-and-na_if",
    "title": "13  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "13.2 Convert missing values using mutate() and na_if()",
    "text": "13.2 Convert missing values using mutate() and na_if()\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe variable SIZE_MM uses -99999 as the code for missing values (see metadata or use unique()). This has the potential to cause conflicts with our analyses, so let’s convert -99999 to an NA value. Do this using mutate() and na_if(). Look up the help page to see how to use na_if(). Check your output data using unique().\n\n\n\n\nAnswer\nlobster_abundance <- lobster_abundance %>% \n    mutate(SIZE_MM = na_if(SIZE_MM, -99999))"
  },
  {
    "objectID": "session_13.html#filter-practice",
    "href": "session_13.html#filter-practice",
    "title": "13  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "13.3 filter() practice",
    "text": "13.3 filter() practice\n\n\n\n\n\n\nQuestion 2\n\n\n\nCreate and store a subset that does NOT include observations from Naples Reef (NAPL). Check your output data frame to ensure that NAPL is NOT in the data frame.\n\n\n\n\nAnswer\nnot_napl <- lobster_abundance %>% \n    filter(SITE != \"NAPL\")\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCreate and store a subset with lobsters at Arroyo Quemado (AQUE) OR with a carapace length greater than 70 mm. Check your output.\n\n\n\n\nAnswer\naque_70mm <- lobster_abundance %>% \n    filter(SITE == \"AQUE\", SIZE_MM >= 70)"
  },
  {
    "objectID": "session_13.html#mutate-practice",
    "href": "session_13.html#mutate-practice",
    "title": "13  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "13.4 mutate() practice",
    "text": "13.4 mutate() practice\n\n\n\n\n\n\nQuestion 4\n\n\n\nAdd a new column that contains lobster carapace length converted to centimeters. There are 10 millimeters in 1 centimeter. Check your output.\n\n\n\n\nAnswer\nlobster_cm <- lobster_abundance %>% \n  mutate(SIZE_CM = SIZE_MM / 10)"
  },
  {
    "objectID": "session_13.html#group_by-summarize-practice",
    "href": "session_13.html#group_by-summarize-practice",
    "title": "13  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "13.5 group_by() %>% summarize() practice",
    "text": "13.5 group_by() %>% summarize() practice\n\n\n\n\n\n\nQuestion 5\n\n\n\nFind the mean of lobster carapace length using mean(), and group by SITE. Think about how you want to treat the NA values in SIZE_MM (Hint: check the arguments in mean()). Check your output.\n\n\n\n\nAnswer\nlobster_mean <- lobster_abundance %>% \n  group_by(SITE) %>% \n  summarize(MEAN_LENGTH = mean(SIZE_MM, na.rm = TRUE))"
  },
  {
    "objectID": "session_13.html#convert-missing-values-using-mutate-and-na_if-1",
    "href": "session_13.html#convert-missing-values-using-mutate-and-na_if-1",
    "title": "13  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "13.6 Convert missing values using mutate() and na_if()",
    "text": "13.6 Convert missing values using mutate() and na_if()\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe variable TRAPS uses -99999 as the code for missing values (see metadata or use unique()). This has the potential to cause conflicts with our analyses, so let’s convert -99999 to an NA value. Do this using mutate() and na_if(). Look up the help page to see how to use na_if(). Check your output data using unique().\n\n\n\n\nAnswer\nlobster_traps <- lobster_traps %>% \n    mutate(TRAPS = na_if(TRAPS, -99999))"
  },
  {
    "objectID": "session_13.html#filter-practice-1",
    "href": "session_13.html#filter-practice-1",
    "title": "13  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "13.7 filter() practice",
    "text": "13.7 filter() practice\n\n\n\n\n\n\nQuestion 2\n\n\n\nCreate and store a subset that does NOT include observations from Naples Reef (NAPL). Check your output data frame to ensure that NAPL is NOT in the data frame.\n\n\n\n\nAnswer\nnot_napl <- lobster_traps %>% \n    filter(SITE != \"NAPL\")\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCreate and store a subset with lobsters at Carpinteria Reef (CARP) OR number of commercial trap floats is greater than 20. Check your output.\n\n\n\n\nAnswer\ncarp_20_traps <- lobster_traps %>% \n    filter(SITE == \"CARP\", TRAPS > 20)"
  },
  {
    "objectID": "session_13.html#mutate-practice-1",
    "href": "session_13.html#mutate-practice-1",
    "title": "13  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "13.8 mutate() practice",
    "text": "13.8 mutate() practice\n\n\n\n\n\n\nQuestion 4\n\n\n\nUsing the object you created above (we called it carp_20_traps), add a new column that contains the proportion of lobster traps at site CARP with more than 20 commercial trap floats. The total number you are using in your percent calculation is 801. Check your output.\n\n\n\n\nAnswer\n# how we calculated 801\n# total number of traps at site \"CARP\" and more than 20 traps counted \nlobster_carp_total <- sum(carp_20_traps$TRAPS, na.rm = TRUE)\n    \nlobster_percent <- carp_20_traps %>% \n  mutate(PERCENT_TRAPS = (TRAPS / lobster_carp_total) * 100)"
  },
  {
    "objectID": "session_13.html#group_by-summarize-practice-1",
    "href": "session_13.html#group_by-summarize-practice-1",
    "title": "13  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "13.9 group_by() %>% summarize() practice",
    "text": "13.9 group_by() %>% summarize() practice\n\n\n\n\n\n\nQuestion 5\n\n\n\nFind the mean of lobster commercial trap floats using mean(), and group by SITE. Think about how you want to treat the NA values in TRAPS (Hint: check the arguments in mean()). Check your output.\n\n\n\n\nAnswer\nmean_lobster_traps <- lobster_traps %>% \n    group_by(SITE) %>%\n    summarize(MEAN_TRAPS = mean(TRAPS, na.rm = TRUE))"
  },
  {
    "objectID": "session_13.html#bonus-add-marine-protected-area-mpa-designation-to-the-data",
    "href": "session_13.html#bonus-add-marine-protected-area-mpa-designation-to-the-data",
    "title": "13  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "13.10 Bonus: Add marine protected area (MPA) designation to the data",
    "text": "13.10 Bonus: Add marine protected area (MPA) designation to the data\nThe sites IVEE and NAPL are marine protected areas (MPAs). Add this designation to your data set using a new function called case_when(). Then create some new plots using this new variable. Does it change how you think about the data? What new plots or analysis can you do with this new variable?\n\nLobster Abundance & Size DataLobster Trap Buoy Counts Data\n\n\nUse the object lobster_abundance and add a new column called DESIGNATION that contains “MPA” if the site is IVEE or NAPL, and “not MPA” for all other values.\n\n\nAnswer\nlobster_mpa <- lobster_abundance %>% \n    mutate(DESIGNATION = case_when(\n    SITE %in% c(\"IVEE\", \"NAPL\") ~ \"MPA\",\n    SITE %in% c(\"AQUE\", \"CARP\", \"MOHK\") ~ \"not MPA\"\n  ))\n\n\n\n\nUse the object lobster_traps and add a new column called DESIGNATION that contains “MPA” if the site is IVEE or NAPL, and “not MPA” for all other values.\n\n\nAnswer\nlobster_traps_mpa <- lobster_traps %>%\n    mutate(DESIGNATION = case_when(\n    SITE %in% c(\"IVEE\", \"NAPL\") ~ \"MPA\",\n    SITE %in% c(\"AQUE\", \"CARP\", \"MOHK\") ~ \"not MPA\"\n  ))"
  },
  {
    "objectID": "session_14.html#learning-objectives",
    "href": "session_14.html#learning-objectives",
    "title": "14  Writing Data Management Plans, Metadata Best Practices & Publishing Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the major components of a data management plan\nPractice using metadata guidelines that are best for reproducibility\nBecome familiar with environmental data repositories for accessing and publishing data"
  },
  {
    "objectID": "session_14.html#the-data-life-cycle",
    "href": "session_14.html#the-data-life-cycle",
    "title": "14  Writing Data Management Plans, Metadata Best Practices & Publishing Data",
    "section": "14.1 The Data Life Cycle",
    "text": "14.1 The Data Life Cycle\nThe data life cycle is a tool for facilitating successful management and preservation of data throughout a research project. Multiple versions of the data life cycle exist and vary in practice across domains or communities. For example, a meta-analysis project may only focus on the Discover, Integrate, and Analyze phases of the cycle.\n\n\n\n\n\nSource: DataOne\n\n\nDataOne’s Data Management Skillbuilding Hub offers several best practices on how to effectively work with your data throughout all stages of the data life cycle."
  },
  {
    "objectID": "session_14.html#writing-data-management-plans-dmps",
    "href": "session_14.html#writing-data-management-plans-dmps",
    "title": "14  Writing Data Management Plans, Metadata Best Practices & Publishing Data",
    "section": "14.2 Writing Data Management Plans (DMPs)",
    "text": "14.2 Writing Data Management Plans (DMPs)\nA DMP is a document that describes how you will use your data during a research project, as well as what you will do with your data long after the project ends. Often a DMP encompasses all phases of the data life cycle - from planning, to collecting, to analyzing and ultimately to preservation and storage of the data.\nThese are important project aspects to deeply consider because a well-thought-out plan means you are more likely to:\n\nstay organized,\nwork efficiently,\ntruly share data,\nengage your team,\nmeet funder requirements as DMPs are become more common in the submission process for proposals.\n\nA DMP is both a straightforward blueprint for how you manage your data, and provides guidelines for your and your team on policies, access, roles, and more. While it is important to plan, it is equally important to recognize that no plan is perfect as change is inevitable. To make your DMP as robust as possible, treat it as a “living document” that you periodically review with your team and adjust as the needs of the project change.\n\n14.2.1 How to Plan\n\nPlan early: research shows that over time data loss\nPlan in collaboration: high engagement of your team and stakeholders is not only a benefit to your project, but it also makes your DMP more resilient. When you include diverse expertise and perspectives to the planning stages, you’re more likely to overcome obstacles in the future.\nUtilize existing resources: don’t reinvent the wheel! There are many great DMP resources out there. Consider the article Ten Simple Rules for Creating a Good Data Management Plan [@michener2015], which has succinct guidelines on what to include in a DMP. Or use an online tool like DMPTool, which provides official DMP templates from funders like NSF, example answers, and allows for collaboration.\nMake revising part of the process: Don’t let your DMP collect dust after your initially write it. Make revising the DMP part of your research project and use it as a guide to ensure you’re keeping on track.\n\n\n\n14.2.2 What to include in a DMP\n\n\n\n\n\n\n\nDMP Section\nGuiding Questions / Things to Consider\n\n\n\n\nFunder Requirements\n\nDoes the funder have a template or specific DMP guidelines?\nDo you thoroughly understand all the requirements? Or do you need to reach out for clarification?\nIs there a page-limit to what you can submit in your proposal? Would it beneficial to have an appendix or a longer version of your DMP for internal use elsewhere (and not for submission)?\n\n\n\nStudy Design\n\nWhat analytical methods do you plan to use?\nWhat experiments, if any, are needed to answer your research question?\nWhat are the end products you plan to produce?\nWhat ethical considerations do you have about your project?\n\n\n\nData Collection\n\nWhat type of data do you plan to collect (text, audio files, images, models, spreadsheets)?\nWhere do you plan to source your data? Is it observational, already existing, or does it need to be collected? Do you need to obtain a license to access the data? Do you need an IRB review?\nHow much data do you plan to collect or use?\nWhat format is the data in? Is it open source or is it proprietary?\n\n\n\nData Organization\n\nHow will you manage your data? Will you be using open source or proprietary software programs?\nDo you need a database to manage your data? Are there existing databases you can utilize or do you need to build one?\nWhat software tools do you plan to use to manage and organize your data?\n\n\n\nQuality Assurance and Quality Control\n\nHow will you ensure that your data is of quality?\nHow will you maintain data integrity throughout your analysis?\nWhat tests will you run on your raw data and processed data?\nWill you be utilizing outside partners to implement testing or QA/QC measures?\n\n\n\nData Policies\n\nWhat licenses do you plan to use for your data? Are there open source licenses that meet your funders requirements?\nWhat are the policies for sharing, retaining, and licensing the data? Whose responsibility is that?\nAre there any legal or ethical restrictions on your data? Do you have sensitive data that cannot be shared? Is a metadata documentation appropriate?\n\n\n\nData documentation & Metadata\n\nWhat information is required for you and others to accurately interpret, reuse, and access your data?\nWill you be using a metadata standard?\nWhat information is needed for you to write comprehensive metadata?\nWhere and how will you maintain this documentation? Is it possible for you to have the documentation open source?\n\n\n\nData Sharing\n\nHow will the data be shared after the project ends? Is this an accessible location?\nWhen will the data and project be available? Immediately after the project ends or a time period after?\nWill you be publishing the project and the data to a journal?\nWhat data products do you plan to share?\n\n\n\nRoles and Responsibilities\n\nWho is in charge of collecting the data? Managing it? Storing it? Archiving it? Running quality control? Overall project management? There are lots of roles to consider here.\nWhat kind of expertise is needed for these roles?\nWhat happens if a role needs to change? How do you plan to handle this kind of change?\n\n\n\nLong-term Storage & Data Preservation\n\nWhere do you plan to archive your data?\nHow long will the data be accessible?\nHow will the data be accessed for future use?\nHow will you be storing the data during your project? Is this different than where you will store it after the project ends?\nDoes your institution or funder have long-term storage options for you to use?\n\n\n\nBudget\n\nDo you need to purchase any proprietary software?\nDo you need to purchase any hardware?\nDo you need to pay for any services?\nWill you need to hire employees? Consultants?\nDo you anticipate that you will need to pay for any professional development or training either for yourself or your team?"
  },
  {
    "objectID": "session_14.html#metadata-best-practices",
    "href": "session_14.html#metadata-best-practices",
    "title": "14  Writing Data Management Plans, Metadata Best Practices & Publishing Data",
    "section": "14.3 Metadata Best Practices",
    "text": "14.3 Metadata Best Practices\nMetadata (data about data) is an important part of the data life cycle because it enables data reuse long after the original collection. Imagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files?\nThe goal is to have enough information for the researcher to understand the data, interpret the data, and then reuse the data in another study.\n\n14.3.1 Overall Guidelines\nAnother way to think about metadata is to answer the following questions with the documentation:\n\nWhat was measured?\nWho measured it?\nWhen was it measured?\nWhere was it measured?\nHow was it measured?\nHow is the data structured?\nWhy was the data collected?\nWho should get credit for this data (researcher AND funding agency)?\nHow can this data be reused (licensing)?\n\n\n\n14.3.2 Bibliographic Guidelines\nThe details that will help your data be cited correctly are:\n\nGlobal identifier like a digital object identifier (DOI)\nDescriptive title that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data\nDescriptive abstract that serves as a brief overview off the specific contents and purpose of the data package\nFunding information like the award number and the sponsor\nPeople and organizations like the creator of the dataset (i.e. who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset\n\n\n\n14.3.3 Discovery Guidelines\nThe details that will help your data be discovered correctly are:\n\nGeospatial coverage of the data, including the field and laboratory sampling locations, place names and precise coordinates\nTemporal coverage of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to\nTaxonomic coverage of the data, including what species were measured and what taxonomy standards and procedures were followed\nAny other contextual information as needed\n\n\n\n14.3.4 Interpretation Guidelines\nThe details that will help your data be interpreted correctly are:\n\nCollection methods for both field and laboratory data the full experimental and project design as well as how the data in the dataset fits into the overall project\nProcessing methods for both field and laboratory samples\nAll sample quality control procedures\nProvenance information to support your analysis and modelling methods\nInformation about the hardware and software used to process your data, including the make, model, and version\nComputing quality control procedures like testing or code review\n\n\n\n14.3.5 Data Structure and Contents\n\nEverything needs a description: the data model, the data objects (like tables, images, matricies, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation.\nVariable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (i.e. 0 = not collected), and any missing values (i.e. 999 = NA).\n\nNot only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable).\nFor example, if you were to search for the character string “carbon dioxide flux” in a data repository, not all relevant results will be shown due to varying vocabulary conventions (i.e., “CO2 flux” instead of “carbon dioxide flux”) across disciplines — only datasets containing the exact words “carbon dioxide flux” are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search.\n\n\n14.3.6 Rights and Attribution\nCorrectly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like:\n\nCitation format to be used when giving credit for the data\nAttribution expectations for the dataset\nReuse rights, which describe who may use the data and for what purpose\nRedistribution rights, which describe who may copy and redistribute the metadata and the data\nLegal terms and conditions like how the data are licensed for reuse.\n\n\n\n14.3.7 Metadata Standards\nSo, how do you organize all this information? There are a number of metadata standards (think, templates) that you could use including:\n\nEcological Metadata Language (EML)\nGeospatial Metadata Standards (ISO 19115 and ISO 19139)\n\nSee NOAA’s ISO Workbook\n\nBiological Data Profile (BDP)\nDublin Core\nDarwin Core\nPREservation Metadata: Implementation Strategies (PREMIS)\nMetadata Encoding Transmission Standard (METS)\n\nNote this is not an exhaustive list.\n\n\n14.3.8 Data Identifiers\nMany journals require a DOI - a digital object identifier - be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to.\nSome data repositories assign a DOI for each dataset you publish on their repository. But, if you need to update the datasets, check the policy of the data repository. Some repositories assign a new DOI after you update the dataset. If this is the case, researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available.\n\n\n14.3.9 Data Citation\nResearchers should get in the habit of citing the data that they use (even if it’s their own data!) in each publication that uses that data."
  },
  {
    "objectID": "session_14.html#publishing-data",
    "href": "session_14.html#publishing-data",
    "title": "14  Writing Data Management Plans, Metadata Best Practices & Publishing Data",
    "section": "14.4 Publishing Data",
    "text": "14.4 Publishing Data\n\n14.4.1 Data Sharing & Preservation\n\n\n\n14.4.2 Data Repositories: Built for Data (and code)\n\nGitHub is not an archival location\nExamples of dedicated data repositories:\n\nKNB\nArctic Data Center\ntDAR\nEDI\nZenodo\n\nDedicated data repositories are:\n\nRich in metadata\nArchival in their mission\nCertified\n\nData papers, e.g., Scientific Data\nre3data is a global registry of research data repositories\nRepository Finder is a pilot project and tool to help researchers find an appropriate repository for their work\n\n\n\n14.4.3 Data Packages\nWe define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\nThese data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes a publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. In the graphic to the side, the package can be cited with the DOI doi:10.5063/F1F18WN4,and each of the individual files have their own identifiers as well.\n\n\n\n\n\n14.4.4 DataOne Federation\nDataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses.\nDataONE can be searched on the web, which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the (currently 44!) repositories one at a time."
  },
  {
    "objectID": "session_14.html#exercise-outline-a-dmp-and-publish-a-data-package-on-the-knb-repository",
    "href": "session_14.html#exercise-outline-a-dmp-and-publish-a-data-package-on-the-knb-repository",
    "title": "14  Writing Data Management Plans, Metadata Best Practices & Publishing Data",
    "section": "14.5 Exercise: Outline a DMP and Publish a Data Package on the KNB Repository",
    "text": "14.5 Exercise: Outline a DMP and Publish a Data Package on the KNB Repository\nGoal of the exercise: Practice outlining a DMP plan, identify quality metadata, and publish data on the KNB repository.\n\n14.5.1 Instructions\n\nConsidering your group’s research question and datasets outline a DMP for your project.\n\nUse the google doc template from the Setup box below.\n\nInvestigate the metadata in the provided data.\n\nDoes the metadata meet the standards we talked about? How so?\nIf not, how would you improve the metadata based on the standards we talked about?\n\nPublish your project on NCEAS’ demo website for the KNB repository.\n\nMake sure to logon to KNB using your ORCiD.\n\nElect someone to share back to the group the following:\n\nWhat went well?\nWhat was challenging?\nWhat did you like about submitting a data package? What features are missing?\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nGo to this google sheet and find your group number\nGo to this google doc for DMP template\n\n\n\n\n\n\n\n\n\nExtra Challenge\n\n\n\n\nSearch for additional data on these repositories: DataOne, KNB, EDI"
  },
  {
    "objectID": "session_15.html#learning-objectives",
    "href": "session_15.html#learning-objectives",
    "title": "15  Using sf for Spatial Analysis & Intro to Making Maps",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this lesson, you will learn:\n\nHow to use the sf package to analyze geospatial data\nStatic mapping with ggplot\ninteractive mapping with leaflet\n\n\n15.0.1 Introduction\nFrom the sf vignette:\n\nSimple features or simple feature access refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects. It also describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them.\n\nThe sf package is an R implementation of Simple Features. This package incorporates:\n\na new spatial data class system in R\n\nfunctions for reading and writing data\n\ntools for spatial operations on vectors\n\nMost of the functions in this package starts with prefix st_ which stands for spatial and temporal.\nIn this tutorial, our goal is to use a shapefile of Alaska regions and data on population in Alaska by community to create a map that looks like this:\n\nThe data we will be using to create the map are:\n\nAlaska regional boundaries\nCommunity locations and population\nAlaksa rivers\n\n\n\n15.0.2 Working with geospatial data\nAll of the data used in this tutorial are simplified versions of real datasets available on the KNB. I’ve simplified the original high-resolution geospatial datasets to ease the processing burden on your computers while learning how to do the analysis. These simplified versions of the datasets may contain topological errors. The original version of the datasets are indicated throughout the chapter.\n\n\n\n\n\n\nSetup\n\n\n\nFor convience, I’ve hosted a zipped copy of all of the files on our test site.\nFollow these steps to get ready for the next exercise:\n\nNavigate to this dataset and download the zip folder.\nMove the downloaded zip into the training directory and unzip it.\n\nThe first file we will use is a shapefile of regional boundaries in alaska derived from: Jared Kibele and Jeanette Clark. 2018. State of Alaska’s Salmon and People Regional Boundaries. Knowledge Network for Biocomplexity. doi:10.5063/F1125QWP.\n\n\nNow we can load the libraries we need:\n\nlapply(names(sessionInfo()$otherPkgs), function(pkgs)\n  detach(\n    paste0('package:', pkgs),\n    character.only = T,\n    unload = T,\n    force = T\n  ))\n\n\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(leaflet)\nlibrary(scales)\nlibrary(ggmap)\nlibrary(dplyr)\n\nRead in the data and look at a plot of it.\n\n## Read in shapefile using sf\nak_regions <- read_sf(\"shapefiles/ak_regions_simp.shp\")\n\nplot(ak_regions)  \n\n\n\n\nWe can also examine it’s class.\n\nclass(ak_regions)\n\nsf objects usually have two types - sf and data.frame. Two main differences comparing to a regular data.frame object are spatial metadata (geometry type, dimension, bbox, epsg (SRID), proj4string) and additional column - typically named geometry.\nSince our shapefile object has the data.frame class, viewing the contents of the object using the head function shows similar results to data we read in using read.csv.\n\nhead(ak_regions)\n\n\n15.0.2.1 Coordinate Reference System\nEvery sf object needs a coordinate reference system (or crs) defined in order to work with it correctly. A coordinate reference system contains both a datum and a projection. The datum is how you georeference your points (in 3 dimensions!) onto a spheroid. The projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. All coordinate reference systems require a datum. However, some coordinate reference systems are “unprojected” (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984.\nESRI has a blog post that explains these concepts in more detail with very helpful diagrams and examples.\nYou can view what crs is set by using the function st_crs\n\nst_crs(ak_regions)\n\nThis is pretty confusing looking. Without getting into the details, that long string says that this data has a greographic coordinate system (WGS84) with no projection. A convenient way to reference crs quickly is by using the EPSG code, a number that represents a standard projection and datum. You can check out a list of (lots!) of EPSG codes here.\nWe will use several EPSG codes in this lesson. Here they are, along with their more readable names:\n\n3338: Alaska Albers\n4326: WGS84 (World Geodetic System 1984), used in GPS\n3857: Pseudo-Mercator, used in Google Maps, OpenStreetMap, Bing, ArcGIS, ESRI\n\nYou will often need to transform your geospatial data from one coordinate system to another. The st_transform function does this quickly for us. You may have noticed the maps above looked wonky because of the dateline. We might want to set a different projection for this data so it plots nicer. A good one for Alaska is called the Alaska Albers projection, with an EPSG code of 3338.\n\nak_regions_3338 <- ak_regions %>%\n  st_transform(crs = 3338)\n\nst_crs(ak_regions_3338)\n\n\nplot(ak_regions_3338)\n\nMuch better!\n\n\n\n15.0.3 sf & the Tidyverse\nsf objects can be used as a regular data.frame object in many operations. We already saw the results of plot and head.\n\n\n\n\n\n\nChallenge\n\n\n\nTry running some other functions you might use to explore a regular data.frame on your sf flavored data.frame.\n\n\nSince sf objects are data.frames, they play nicely with packages in the tidyverse. Here are a couple of simple examples:\nselect()\n\nak_regions_3338 %>%\n  select(region)\n\nNote the sticky geometry column! The geometry column will stay with your sf object even if it is not called explicitly.\nfilter()\n\nak_regions_3338 %>%\n  filter(region == \"Southeast\")\n\n\n15.0.3.1 Joins\nYou can also use the sf package to create spatial joins, useful for when you want to utilize two datasets together. As an example, let’s ask a question: how many people live in each of these Alaska regions?\nWe have some population data, but it gives the number of people by city, not by region. To determine the number of people per region we will need to:\n\nread in the city data from a csv and turn it into an sf object\nuse a spatial join (st_join) to assign each city to a region\nuse group_by and summarize to calculate the total population by region\n\nFirst, read in the population data as a regular data.frame. This data is derived from: Jeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018. Languages used in Alaskan households, 1990-2015. Knowledge Network for Biocomplexity. doi:10.5063/F11G0JHX. Unnecessary columns were removed and the most recent year of data was selected.\n\npop <- read.csv(\"shapefiles/alaska_population.csv\")\n\n\n\n\nThe st_join function is a spatial left join. The arguments for both the left and right tables are objects of class sf which means we will first need to turn our population data.frame with latitude and longitude coordinates into an sf object.\nWe can do this easily using the st_as_sf function, which takes as arguments the coordinates and the crs. The remove = F specification here ensures that when we create our geometry column, we retain our original lat lng columns, which we will need later for plotting. Although it isn’t said anywhere explicitly in the file, let’s assume that the coordinate system used to reference the latitude longitude coordinates is WGS84, which has a crs number of 4236.\n\npop_4326 <- st_as_sf(pop, \n                  coords = c('lng', 'lat'),\n                  crs = 4326,\n                  remove = F)\n\nhead(pop_4326)\n\nNow we can do our spatial join! You can specify what geometry function the join uses (st_intersects, st_within, st_crosses, st_is_within_distance, …) in the join argument. The geometry function you use will depend on what kind of operation you want to do, and the geometries of your shapefiles.\nIn this case, we want to find what region each city falls within, so we will use st_within.\n\npop_joined <- st_join(pop_4326, ak_regions_3338, join = st_within)\n\nThis gives an error!\nError: st_crs(x) == st_crs(y) is not TRUE\nTurns out, this won’t work right now because our coordinate reference systems are not the same. Luckily, this is easily resolved using st_transform, and projecting our population object into Alaska Albers.\n\npop_3338 <- st_transform(pop_4326, crs = 3338)\n\n\npop_joined <- st_join(pop_3338, ak_regions_3338, join = st_within)\n\nhead(pop_joined)\n\n\n\n\n\n\n\nChallenge\n\n\n\nLike we mentioned above, there are many different types of joins you can do with geospatial data. Examine the help page for these joins (?st_within() will get you there). What other joins types might be appropriate for examining the relationship between points and polygyons? What about two sets of polygons?\n\n\n\n\n15.0.3.2 Group and summarize\nNext we compute the total population for each region. In this case, we want to do a group_by and summarise as this were a regular data.frame - otherwise all of our point geometries would be included in the aggreation, which is not what we want. Our goal is just to get the total population by region. We remove the sticky geometry using as.data.frame, on the advice of the sf::tidyverse help page.\n\npop_region <- pop_joined %>% \n  as.data.frame() %>% \n  group_by(region) %>% \n  summarise(total_pop = sum(population))\n\nhead(pop_region)\n\nAnd use a regular left_join to get the information back to the Alaska region shapefile. Note that we need this step in order to regain our region geometries so that we can make some maps.\n\npop_region_3338 <- left_join(ak_regions_3338, pop_region)\n\n#plot to check\nplot(pop_region_3338[\"total_pop\"])\n\nSo far, we have learned how to use sf and dplyr to use a spatial join on two datasets and calculate a summary metric from the result of that join.\nThe group_by and summarize functions can also be used on sf objects to summarize within a dataset and combine geometries. Many of the tidyverse functions have methods specific for sf objects, some of which have additional arguments that wouldn’t be relevant to the data.frame methods. You can run ?sf::tidyverse to get documentation on the tidyverse sf methods.\nLet’s try some out. Say we want to calculate the population by Alaska management area, as opposed to region.\n\npop_mgmt_338 <- pop_region_3338 %>% \n  group_by(mgmt_area) %>% \n  summarize(total_pop = sum(total_pop))\n\nplot(pop_mgmt_338[\"total_pop\"])\n\nNotice that the region geometries were combined into a single polygon for each management area.\nIf we don’t want to combine geometries, we can specify do_union = F as an argument.\n\npop_mgmt_3338 <- pop_region_3338 %>% \n  group_by(mgmt_area) %>% \n  summarize(total_pop = sum(total_pop), do_union = F)\n\nplot(pop_mgmt_3338[\"total_pop\"])\n\n\n\n15.0.3.3 Writing the file\nSave the spatial object to disk using write_sf() and specifying the filename. Writing your file with the extension .shp will assume an ESRI driver driver, but there are many other format options available.\n\nwrite_sf(pop_region_3338, \"shapefiles/ak_regions_population.shp\", delete_layer = TRUE)\n\n\n\n\n15.0.4 Visualize with ggplot\nggplot2 now has integrated functionality to plot sf objects using geom_sf().\nWe can plot sf objects just like regular data.frames using geom_sf.\n\nggplot(pop_region_3338) +\n  geom_sf(aes(fill = total_pop)) +\n  theme_bw() +\n  labs(fill = \"Total Population\") +\n  scale_fill_continuous(low = \"khaki\", high =  \"firebrick\", labels = comma)\n\nWe can also plot multiple shapefiles in the same plot. Say if we want to visualize rivers in Alaska, in addition to the location of communities, since many communities in Alaska are on rivers. We can read in a rivers shapefile, doublecheck the crs to make sure it is what we need, and then plot all three shapefiles - the regional population (polygons), the locations of cities (points), and the rivers (linestrings).\nThe rivers shapefile is a simplified version of Jared Kibele and Jeanette Clark. Rivers of Alaska grouped by SASAP region, 2018. Knowledge Network for Biocomplexity. doi:10.5063/F1SJ1HVW.\n\nrivers_3338 <- read_sf(\"shapefiles/ak_rivers_simp.shp\")\nst_crs(rivers_3338)\n\n\n\n\nNote that although no EPSG code is set explicitly, with some sluething we can determine that this is EPSG:3338. This site is helpful for looking up EPSG codes.\n\nggplot() +\n  geom_sf(data = pop_region_3338, aes(fill = total_pop)) +\n  geom_sf(data = rivers_3338, aes(size = StrOrder), color = \"black\") +\n  geom_sf(data = pop_3338, aes(), size = .5) +\n  scale_size(range = c(0.01, 0.2), guide = F) +\n  theme_bw() +\n  labs(fill = \"Total Population\") +\n  scale_fill_continuous(low = \"khaki\", high =  \"firebrick\", labels = comma)\n\n\n15.0.4.1 Incorporate base maps into static maps using ggmap\nThe ggmap package has some functions that can render base maps (as raster objects) from open tile servers like Google Maps, Stamen, OpenStreetMap, and others.\nWe’ll need to transform our shapefile with population data by community to EPSG:3857 which is the CRS used for rendering maps in Google Maps, Stamen, and OpenStreetMap, among others.\n\npop_3857 <- pop_3338 %>%\n  st_transform(crs = 3857)\n\nNext, let’s grab a base map from the Stamen map tile server covering the region of interest. First we include a function that transforms the bounding box (which starts in EPSG:4326) to also be in the EPSG:3857 CRS, which is the projection that the map raster is returned in from Stamen. This is an issue with ggmap described in more detail here\n\n# Define a function to fix the bbox to be in EPSG:3857\n# See https://github.com/dkahle/ggmap/issues/160#issuecomment-397055208\nggmap_bbox_to_3857 <- function(map) {\n  if (!inherits(map, \"ggmap\")) stop(\"map must be a ggmap object\")\n  # Extract the bounding box (in lat/lon) from the ggmap to a numeric vector, \n  # and set the names to what sf::st_bbox expects:\n  map_bbox <- setNames(unlist(attr(map, \"bb\")), \n                       c(\"ymin\", \"xmin\", \"ymax\", \"xmax\"))\n  \n  # Coonvert the bbox to an sf polygon, transform it to 3857, \n  # and convert back to a bbox (convoluted, but it works)\n  bbox_3857 <- st_bbox(st_transform(st_as_sfc(st_bbox(map_bbox, crs = 4326)), 3857))\n  \n  # Overwrite the bbox of the ggmap object with the transformed coordinates \n  attr(map, \"bb\")$ll.lat <- bbox_3857[\"ymin\"]\n  attr(map, \"bb\")$ll.lon <- bbox_3857[\"xmin\"]\n  attr(map, \"bb\")$ur.lat <- bbox_3857[\"ymax\"]\n  attr(map, \"bb\")$ur.lon <- bbox_3857[\"xmax\"]\n  map\n}\n\nNext, we define the bounding box of interest, and use get_stamenmap() to get the basemap. Then we run our function defined above on the result of the get_stamenmap() call.\n\nbbox <- c(-170, 52, -130, 64)   # This is roughly southern Alaska\nak_map <- get_stamenmap(bbox, zoom = 4)\nak_map_3857 <- ggmap_bbox_to_3857(ak_map)\n\nFinally, plot both the base raster map with the population data overlayed, which is easy now that everything is in the same projection (3857):\n\nggmap(ak_map_3857) + \n  geom_sf(data = pop_3857, aes(color = population), inherit.aes = F) +\n  scale_color_continuous(low = \"khaki\", high =  \"firebrick\", labels = comma)\n\n\n\n\n15.0.5 Visualize sf objects with leaflet\nWe can also make an interactive map from our data above using leaflet.\nLeaflet (unlike ggplot) will project data for you. The catch is that you have to give it both a projection (like Alaska Albers), and that your shapefile must use a geographic coordinate system. This means that we need to use our shapefile with the 4326 EPSG code. Remember you can always check what crs you have set using st_crs.\nHere we define a leaflet projection for Alaska Albers, and save it as a variable to use later.\n\nepsg3338 <- leaflet::leafletCRS(\n  crsClass = \"L.Proj.CRS\",\n  code = \"EPSG:3338\",\n  proj4def =  \"+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\",\n  resolutions = 2^(16:7))\n\nYou might notice that this looks familiar! The syntax is a bit different, but most of this information is also contained within the crs of our shapefile:\n\nst_crs(pop_region_3338)\n\nSince leaflet requires that we use an unprojected coordinate system, let’s use st_transform yet again to get back to WGS84.\n\npop_region_4326 <- pop_region_3338 %>% st_transform(crs = 4326)\n\n\nm <- leaflet(options = leafletOptions(crs = epsg3338)) %>%\n        addPolygons(data = pop_region_4326, \n                    fillColor = \"gray\",\n                    weight = 1)\n\nm\n\nWe can add labels, legends, and a color scale.\n\npal <- colorNumeric(palette = \"Reds\", domain = pop_region_4326$total_pop)\n\nm <- leaflet(options = leafletOptions(crs = epsg3338)) %>%\n        addPolygons(data = pop_region_4326, \n                    fillColor = ~pal(total_pop),\n                    weight = 1,\n                    color = \"black\",\n                    fillOpacity = 1,\n                    label = ~region) %>% \n        addLegend(position = \"bottomleft\",\n                  pal = pal,\n                  values = range(pop_region_4326$total_pop),\n                  title = \"Total Population\")\n\nm\n\nWe can also add the individual communities, with popup labels showing their population, on top of that!\n\npal <- colorNumeric(palette = \"Reds\", domain = pop_region_4326$total_pop)\n\nm <- leaflet(options = leafletOptions(crs = epsg3338)) %>%\n        addPolygons(data = pop_region_4326, \n                    fillColor = ~pal(total_pop),\n                    weight = 1,\n                    color = \"black\",\n                    fillOpacity = 1) %>% \n        addCircleMarkers(data = pop_4326,\n                         lat = ~lat,\n                         lng = ~lng,\n                         radius = ~log(population/500), # arbitrary scaling\n                         fillColor = \"gray\",\n                         fillOpacity = 1,\n                         weight = 0.25,\n                         color = \"black\",\n                         label = ~paste0(pop_4326$city, \", population \", comma(pop_4326$population))) %>%\n        addLegend(position = \"bottomleft\",\n                  pal = pal,\n                  values = range(pop_region_4326$total_pop),\n                  title = \"Total Population\")\n\nm\n\nThere is a lot more functionality to sf including the ability to intersect polygons, calculate distance, create a buffer, and more. Here are some more great resources and tutorials for a deeper dive into this great package:\nRaster analysis in R\nSpatial analysis in R with the sf package\nIntro to Spatial Analysis\nsf github repo\nTidy spatial data in R: using dplyr, tidyr, and ggplot2 with sf\nmapping-fall-foliage-with-sf"
  },
  {
    "objectID": "session_16.html#learning-objectives",
    "href": "session_16.html#learning-objectives",
    "title": "16  R Practice: Wrangling Spatial Data & Making Maps",
    "section": "Learning Objectives",
    "text": "Learning Objectives"
  },
  {
    "objectID": "session_17.html#learning-objectives",
    "href": "session_17.html#learning-objectives",
    "title": "17  Git Workflows: Branches & Pull Requests",
    "section": "Learning Objectives",
    "text": "Learning Objectives"
  },
  {
    "objectID": "session_18.html#learning-objectives",
    "href": "session_18.html#learning-objectives",
    "title": "18  Reproducibility & Provenance",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this lesson, we will:\n\nDiscuss the concept of reproducible workflows\nReview the importance of computational reproducibility\nReview the utility of provenance metadata\nOverview how R packages are great ways to package work reproducibly\nLearn how to build a reproducible paper in RMarkdown\nReview tools and techniques for reproducibility supported by the NCEAS and DataONE\n\n\nReproducible Research: Recap\nWorking in a reproducible manner:\n\nincreases research efficiency, accelerating the pace of your research and collaborations\nprovides transparency by capturing and communicating scientific workflows\nenables research to stand on the shoulders of giants (build on work that came before)\nallows credit for secondary usage and supports easy attribution\nincreases trust in science\n\nTo enable others to fully interpret, reproduce or build upon our research, we need to provide more comprehensive information than is typically included in a figure or publication. The methods sections of papers are typically inadequate to fully reproduce the work described in the paper.\n\nWhat data were used in this study? What methods applied? What were the parameter settings? What documentation or code are available to us to evaluate the results? Can we trust these data and methods?\nAre the results reproducible?\nComputational reproducibility is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions.\nPractically speaking, reproducibility includes:\n\nPreserving the data\nPreserving the software workflow\nDocumenting what you did\nDescribing how to interpret it all\n\nA recent study of publicly-available datasets in the Harvard Database repository containing R files found that only 26% of R files ran without error in the initial execution. 44% were able to be run after code cleaning, showing the importance of good programming practice [@trisovic2022]. The figure below from Trisovic et al. shows a sankey diagram of how code cleaning was able to fix common errors.\n\n\n\nComputational Provenance and Workflows\nComputational provenance refers to the origin and processing history of data including:\n\nInput data\nWorkflow/scripts\nOutput data\nFigures\nMethods, dataflow, and dependencies\n\nWhen we put these all together with formal documentation, we create a computational workflow that captures all of the steps from initial data cleaning and integration, through analysis, modeling, and visualization.\nHere’s an example of a computational workflow from Mark Carls: Mark Carls. Analysis of hydrocarbons following the Exxon Valdez oil spill, Gulf of Alaska, 1989 - 2014. Gulf of Alaska Data Portal. urn:uuid:3249ada0-afe3-4dd6-875e-0f7928a4c171., that represents a three step workflow comprising four source data files and two output visualizations.\n\n\nThis screenshot of the dataset page shows how DataONE renders the workflow model information as part of our interactive user interface. You can clearly see which data files were inputs to the process, the scripts that are used to process and visualize the data, and the final output objects that are produced, in this case two graphical maps of Prince William Sound in Alaska.\nFrom Provenance to Reproducibility\n\nAt DataONE we facilitate reproducible science through provenance by:\n\nTracking data derivation history\nTracking data inputs and outputs of analyses\nPreserving and documenting software workflows\nTracking analysis and model executions\nLinking all of these to publications\n\nIntroducing ProvONE, an extension of W3C PROV\n\nProvONE provides the fundamental information required to understand and analyze scientific workflow-based computational experiments. It covers the main aspects that have been identified as relevant in the provenance literature including data structure. This addresses the most relevant aspects of how the data, both used and produced by a computational process, is organized and represented. For scientific workflows this implies the inputs and outputs of the various tasks that form part of the workflow.\n\n\n\n\n18.0.1 Data Citation and Transitive Credit\nWe want to move towards a model such that when a user cites a research publication we will also know:\n\nWhich data produced it\nWhat software produced it\nWhat was derived from it\nWho to credit down the attribution stack\n\n\nThis is transitive credit. And it changes the way in which we think about science communication and traditional publications.\n\n\n18.0.2 Reproducible Papers with rrtools\nA great overview of this approach to reproducible papers comes from:\n\nBen Marwick, Carl Boettiger & Lincoln Mullen (2018) Packaging Data Analytical Work Reproducibly Using R (and Friends), The American Statistician, 72:1, 80-88, doi:10.1080/00031305.2017.1375986\n\nThis lesson will draw from existing materials:\n\nrrtools\nReproducible papers with RMarkdown\n\nThe key idea in Marwick et al. (2018) is that of the “research compendium”: A single container for not just the journal article associated with your research but also the underlying analysis, data, and even the required software environment required to reproduce your work.\n\n\n\nResearch compendia from Marwick et al.\n\n\nResearch compendia make it easy for researchers to do their work but also for others to inspect or even reproduce the work because all necessary materials are readily at hand due to being kept in one place. Rather than a constrained set of rules, the research compendium is a scaffold upon which to conduct reproducible research using open science tools such as:\n\nR\nRMarkdown\ngit and GitHub\n\nFortunately for us, Ben Marwick (and others) have written an R package called rrtools that helps us create a research compendium from scratch.\nTo start a reproducible paper with rrtools, first close your username-training project. In the project switcher dropdown, just click “close project.” This will set your working directory back to your home directory.\n\n\n\n\nrrtools::use_compendium(\"mypaper\")\n\nrrtools has created the beginnings of a research compendium for us. At this point, it looks mostly the same as an R package. That’s because it uses the same underlying folder structure and metadata and therefore it technically is an R package (called mypaper). And this means our research compendium will be easy to install, just like an R package.\nrrtools also helps you set up some key information like:\n\nAdd a license (always a good idea)\nSet up a README file in the RMarkdown format\nCreate an analysis folder to hold our reproducible paper\n\n\nusethis::use_apache_license()\nrrtools::use_readme_rmd()\nrrtools::use_analysis()\n\nThis creates a standard, predictable layout for our code and data and outputs that multiple people can understand. At this point, we’re ready to start writing the paper. To follow the structure rrtools has put in place for us. You’ll notice a paper directory that contains a template Rmd, which references a journal citation style language (CSL) file so that the paper is knitted to a word document formatted for the journal (the Journal of Archaeological Science, in this example). For this lesson, let’s take things a step further with the rticles package. With that in mind, let’s delete the existing paper directory. We’ll create a new one shortly.\n\n18.0.2.1 RMarkdown templates with rticles\nThe rticles package provides a lot of other great templates for formatting your paper specifically to the requirements of many journals. In addition to a custom CSL file for reference customization, rticles supports custom LATeX templates that fit the formatting requirements of each journals. After installing rticles with a command like install.packages('rticles') and restarting your RStudio session, you will be able to create articles from these custom templates using the File | New File | R Markdown... menu, which shows the following dialog:\n\nSelect the “PNAS” template, give the file a name and set the location of the files to be mypaper, and click “OK”. You can now Knit the Rmd file to see a highly-targeted article format, like this one for PNAS:\n\nUse rrtools to generate the core directory layout and approach to data handling, and then use articles to create the structure of the paper itself. The combination is incredibly flexible.\nHere are more things we can do with our research compendium:\n\nEdit ./analysis/paper/paper.Rmd to begin writing your paper and your analysis in the same document\nAdd any citations to ./analysis/paper/pnas-sample.bib\nAdd any longer R scripts that don’t fit in your paper in an R folder at the top level\nAdd raw data to ./data/raw_data\nWrite out any derived data (generated in paper.Rmd) to ./data/derived_data\nWrite out any figures in ./analysis/figures\n\nYou can then write all of your R code in your RMarkdown, and generate your manuscript all in the format needed for your journal (using it’s .csl file, stored in the paper directory).\nHere is a look at the beginning of the Rmd:\n\nAnd the rendered PDF:\n\n\n\n18.0.2.2 Adding renv, Docker, GitHub Actions\nrrtools has a couple more tricks up it’s sleeve to help your compendium be as reproducible and portable as possible.\nTo capture the R packages and versions this project depends on, the renv package is used. If you run renv::init(), it will initiate tracking of the R packages in your project. renv::init() automatically detects dependencies in your code (by looking for library calls, at the DESCRIPTION file, etc.) and installs them to a private project specific library. Importantly, this means that your project mypaper can use a different version of dplyr than another project which may need an older version without any hassle. renv also write the package dependencies to a special file in the repository called renv.lock. In the Dockerfile for Binder, the renv.lock file is read, and the versions of the packages needed are installed using renv::restore(). While you are developing code, if any of your packages get updated, renv::snapshot() will update the renv.lock file and your project-installed packages.\nThe rrtools package then uses this renv.lock file to build what is called a Dockerfile. Docker allows you to build what are called containers, a standard unit of software that packages up code and all its dependencies so an application runs quickly and reliably from one computing environment to another. A container is an “image” of all the software specified, and this image can be run on other computers such that the software stack looks exactly as you specify. This is important when it comes to reproducibility, because if I’m running someone elses code, I may get different results or errors if I’m using different versions of software (like an old version of dplyr, for example) than they are. A Dockerfile contains the instructions for how to build the image.\nrrtools has a function called rrtools::use_dockerfile() which creates a Dockerfile that first loads a standard image for using R with the tidyverse, and then has more instructions for how to create the environment so that it has the very specific R packages and versions you need. Notice in the file below the call to renv::restore(), as described above. The last line of the docker file renders our RMarkdown reproducible paper!\n# get the base image, the rocker/verse has R, RStudio and pandoc\nFROM rocker/verse:4.1.0\n\n# required\nMAINTAINER Jeanette Clark <jclark@nceas.ucsb.edu>\n\nCOPY . /mypaper\n\n# go into the repo directory\nRUN . /etc/environment \\\n  # Install linux depedendencies here\n  # e.g. need this for ggforce::geom_sina\n  && sudo apt-get update \\\n  && sudo apt-get install libudunits2-dev -y \\\n  # build this compendium package\n  && R -e \"install.packages('remotes', repos = c(CRAN = 'https://cloud.r-project.org'))\" \\\n  && R -e \"install.packages(c('renv', 'rmarkdown'))\" \\\n  # install pkgs we need\n  && R -e \"renv::restore()\" \\\n  # render the manuscript into a pdf,\n  && R -e \"rmarkdown::render('mypaper/analysis/paper/paper.Rmd')\"\nAfter running rrtools::use_dockerfile(), the package also sets up GitHub Actions for you. Actions are processes that are triggered in GitHub events (like a push) that run automatically. In this case, the Action that is set up will make is so that your image is created on GitHub, some code that knits your paper is run, and an updated version of your paper is knit. This is called “continuous integration,” and is extremely convenient for developing products like this, since the build step can be taken care of automatically as you push to your repository.\n\n\n\n\nThe 5th Generation of Reproducible Papers\n\nWhole Tale simplifies computational reproducibility. It enables researchers to easily package and share ‘tales’. Tales are executable research objects captured in a standards-based tale format complete with metadata. They can contain:\n\ndata (references)\ncode (computational methods)\nnarrative (traditional science story)\ncompute environment (e.g. RStudio, Jupyter)\n\n\nBy combining data, code and the compute environment, tales allow researchers to:\n\nre-create the computational results from a scientific study\nachieve computational reproducibility\n“set the default to reproducible.”\n\nThey also empower users to verify and extend results with different data, methods, and environments. You can browse existing tales, run and interact with published tales and create new tales via the Whole Tale Dashboard.\n\nBy integrating with DataONE and Dataverse, Whole Tale includes over 90 major research repositories from which a user can select datasets to make those datasets the starting point of an interactive data exploration and analysis inside of one of the Whole Tale environments. Within DataONE, we are adding functionality to work with data in the Whole Tale environment directly from the dataset landing page.\n\n\nFull circle reproducibility can be achieved by publishing data, code AND the environment."
  }
]