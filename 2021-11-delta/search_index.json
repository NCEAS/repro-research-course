[["index.html", "Open Science Synthesis for the Delta Science Program: Week 3 Open Science Synthesis for the Delta Science Program: Week 3 Schedule", " Open Science Synthesis for the Delta Science Program: Week 3 November 1-5, 2021 Open Science Synthesis for the Delta Science Program: Week 3 This book is for week 3 of 3, one-week facilitated synthesis and training events for Delta researchers that will revolve around scientific computing and scientific software for reproducible science. Week 3 will focus on publishing data, data visualization and results communication, and will revisit reproducible workflows. Schedule Code of Conduct Please note that by participating in an NCEAS activity you agree to abide by our Code of Conduct The Power of Open To facilitate a lively and interactive learning environment, we’ll be calling on folks to share their code and to answer various questions posed by the instructor. It’s completely okay to say “Pass” or “I Don’t Know” - this is a supportive learning environment and we will all learn from each other. The instructors will be able to see your code as you go to help you if you get stuck, and the lead instructor may share participants’ code to show a successful example or illustrate a teaching moment. About this book These written materials are the result of a continuous effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials. This work is licensed under a Creative Commons Attribution 4.0 International License. Citation: Jones, Matthew B., Amber E. Budden, Bryce Mecum, S. Jeanette Clark, Julien Brun, Julie Lowndes, Erin McLean, Jessica S. Guo, David S. LeBauer. 2021. Reproducible Research Techniques for Synthesis. NCEAS Learning Hub. Additional contributors: Ben Bolker, Stephanie Hampton, Samanta Katz, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Leah Wasser. "],["session-1-model-analysis.html", "Session 1: Model analysis", " Session 1: Model analysis "],["session-2-collaborative-synthesis.html", "Session 2: Collaborative synthesis", " Session 2: Collaborative synthesis "],["session-3-web-based-data-archival.html", "Session 3: Web based data archival Best Practices: Data and Metadata Data Documentation and Publishing", " Session 3: Web based data archival Best Practices: Data and Metadata Learning Objectives In this lesson, you will learn: How to acheive practical reproducibility Some best practices for data and metadata management Best Practices: Overview The data life cycle has 8 stages: Plan, Collect, Assure, Describe, Preserve, Discover, Integrate, and Analyze. In this section we will cover the following best practices that can help across all stages of the data life cycle: Organizing Data File Formats Large Data Packages Metadata Data Identifiers Provenance Licensing and Distribution Organizing Data We’ll spend an entire lesson later on that’s dedicated to organizing your data in a tidy and effective manner, but first, let’s focus on the benefits on having “clean” data and complete metadata. Decreases errors from redundant updates Enforces data integrity Helps you and future researchers to handle large, complex datasets Enables powerful search filtering Much has been written on effective data management to enable reuse. The following two papers offer words of wisdom: Some simple guidelines for effective data management. Borer et al. 2009. Bulletin of the Ecological Society of America. Nine simple ways to make it easier to (re)use your data. White et al. 2013. Ideas in Ecology and Evolution 6. In brief, some of the best practices to follow are: Have scripts for all data manipulation that start with the uncorrected raw data file and clean the data programmatically before analysis. Design your tables to add rows, not columns. A column should be only one variable and a row should be only one observation. Include header lines in your tables Use non-proprietary file formats (ie, open source) with descriptive file names without spaces. Non-proprietary file formats are essential to ensure that your data can still be machine readable long into the future. Open formats include text files and binary formats such as NetCDF. Common switches: Microsoft Excel (.xlsx) files - export to text (.txt) or comma separated values (.csv) GIS files - export to ESRI shapefiles (.shp) MATLAB/IDL - export to NetCDF When you have or are going to generate large data packages (in the terabytes or larger), it’s important to establish a relationship with the data center early on. The data center can help come up with a strategy to tile data structures by subset, such as by spatial region, by temporal window, or by measured variable. They can also help with choosing an efficient tool to store the data (ie NetCDF or HDF), which is a compact data format that helps parallel read and write libraries of data. Metadata Guidelines Metadata (data about data) is an important part of the data life cycle because it enables data reuse long after the original collection. Imagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files? The goal is to have enough information for the researcher to understand the data, interpret the data, and then re-use the data in another study. Another way to think about it is to answer the following questions with the documentation: What was measured? Who measured it? When was it measured? Where was it measured? How was it measured? How is the data structured? Why was the data collected? Who should get credit for this data (researcher AND funding agency)? How can this data be reused (licensing)? Bibliographic Details The details that will help your data be cited correctly are: a global identifier like a digital object identifier (DOI); a descriptive title that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data a descriptive abstract that serves as a brief overview off the specific contents and purpose of the data package funding information like the award number and the sponsor; the people and organizations like the creator of the dataset (ie who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset Discovery Details The details that will help your data be discovered correctly are: the geospatial coverage of the data, including the field and laboratory sampling locations, place names and precise coordinates; the temporal coverage of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to; the taxonomic coverage of the data, including what species were measured and what taxonomy standards and procedures were followed; as well as any other contextual information as needed. Interpretation Details The details that will help your data be interpreted correctly are: the collection methods for both field and laboratory data; the full experimental and project design as well as how the data in the dataset fits into the overall project; the processing methods for both field and laboratory samples IN FULL; all sample quality control procedures; the provenance information to support your analysis and modelling methods; information about the hardware and software used to process your data, including the make, model, and version; and the computing quality control procedures like any testing or code review. Data Structure and Contents Well constructed metadata also includes information about the data structure and contents. Everything needs a description: the data model, the data objects (like tables, images, matricies, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation. Variable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (such as 0 = not collected), and any missing values (such as 999 = NA). Not only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable). For example, if you were to search for the character string carbon dioxide flux in the general search box at the Arctic Data Center, not all relevant results will be shown due to varying vocabulary conventions (ie, CO2 flux instead of carbon dioxide flux) across disciplines — only datasets containing the exact words carbon dioxide flux are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search. Demonstrates a typical search for “carbon dioxide flux”, yielding 20 datasets. (right) Illustrates an annotated search for “carbon dioxide flux”, yielding 29 datasets. Note that if you were to interact with the site and explore the results of the figure on the right, the dataset in red of Figure 3 will not appear in the typical search for “carbon dioxide flux.” Rights and Attribution Correctly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like: the citation format to be used when giving credit for the data; the attribution expectations for the dataset; the reuse rights, which describe who may use the data and for what purpose; the redistribution rights, which describe who may copy and redistribute the metadata and the data; and the legal terms and conditions like how the data are licensed for reuse. So, how do you organize all this information? There are a number of metadata standards (think, templates) that you could use, including the Ecological Metadata Language (EML), Geospatial Metadata Standards like ISO 19115 and ISO 19139, the Biological Data Profile (BDP), Dublin Core, Darwin Core, PREMIS, the Metadata Encoding and Transmission Standard (METS), and the list goes on and on. The Arctic Data Center runs on EML. Data Identifiers Many journals require a DOI - a digital object identifier - be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to. At the Arctic Data Center, we assign a DOI to each published dataset. But, sometimes datasets need to be updated. Each version of a dataset published with the Arctic Data Center has a unique identifier associated with it. Researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available. When there is a newer version available, that will be clearly marked on the original dataset page with a yellow banner indicating as such. Having the data identified in this manner allows us to accurately track the dataset usage metrics. The Arctic Data Center tracks the number of citations, the number of downloads, and the number of views of each dataset in the catalog. We filter out most views by internet bots and repeat views within a small time window in order to make these metrics COUNTER compliant. COUNTER is a standard that libraries and repositories use to provide users with consistent, credible, and comparable usage data. Data Citation Researchers should get in the habit of citing the data that they use - even if it’s their own data! - in each publication that uses that data. The Arctic Data Center has taken multiple steps towards providing data citation information for all datasets we hold in our catalog, including a feature enabling dataset owners to directly register citations to their datasets. We recently implemented this “Register Citation” feature to allow researchers to register known citations to their datasets. Researchers may register a citation for any occasions where they know a certain publication uses or refers to a certain dataset, and the citation will be viewable on the dataset profile within 24 hours. To register a citation, navigate to the dataset using the DOI and click on the citations tab. Once there, this dialog box will pop up and you’ll be able to register the citation with us. Click that button and you’ll see a very simple form asking for the DOI of the paper and if the paper CITES the dataset (that is, the dataset is explicitly identified or linked to somewhere in the text or references) or USES the dataset (that is, uses the dataset but doesn’t formally cite it). We encourage you to make this part of your workflow, and for you to let your colleagues know about it too! Provanance &amp; Preserving Computational Workflows While the Arctic Data Center, Knowledge Network for Biocomplexity, and similar repositories do focus on preserving data, we really set our sights much more broadly on preserving entire computational workflows that are instrumental to advances in science. A computational workflow represents the sequence of computational tasks that are performed from raw data acquisition through data quality control, integration, analysis, modeling, and visualization. In addition, these workflows are often not executed all at once, but rather are divided into multiple workflows, earch with its own purpose. For example, a data acquistion and cleaning workflow often creates a derived and integrated data product that is then picked up and used by multiple downstream analytical workflows that produce specific scientific findings. These workflows can each be archived as distinct data packages, with the output of the first workflow becoming the input of the second and subsequent workflows. In an effort to make data more reproducible, datasets also support provenance tracking. With provenance tracking, users of the Arctic Data Center can see exactly what datasets led to what product, using the particular script or workflow that the researcher used. This is a useful tool to make data more compliant with the FAIR principles. In addition to making data more reproducible, it is also useful for building on the work of others; you can produce similar visualizations for another location, for example, using the same code. RMarkdown itself can be used as a provenance tool, as well - by starting with the raw data and cleaning it programmatically, rather than manually, you preserve the steps that you went through and your workflow is reproducible. Data Documentation and Publishing Learning Objectives In this lesson, you will learn: About open data archives What science metadata is and how it can be used How data and code can be documented and published in open data archives Data sharing and preservation Data repositories: built for data (and code) GitHub is not an archival location Dedicated data repositories: KNB, Arctic Data Center, Zenodo, FigShare Rich metadata Archival in their mission Data papers, e.g., Scientific Data List of data repositories: http://re3data.org Metadata Metadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where it was collected, and why it was collected. For consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the metadata for a sockeye salmon data set: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; That same metadata document can be converted to HTML format and displayed in a much more readable form on the web: https://knb.ecoinformatics.org/#view/doi:10.5063/F1F18WN4 And as you can see, the whole data set or its components can be downloaded and reused. Also note that the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data. Structure of a data package Note that the data set above lists a collection of files that are contained within the data set. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document. These data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets an internal identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4. DataONE Federation DataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses. DataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data form the dozens of members of DataONE, rather than visiting each of the currently 43 repositories one at a time. Publishing data from the web Each data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the Arctic Data Center, we provide some easy to use web forms for editing and submitting a data package. Let’s walk through a web submission to see what you might expect. Download the data to be used for the tutorial I’ve already uploaded the test data package, and so you can access the data here: https://dev.nceas.ucsb.edu/view/urn:uuid:0702cc63-4483-4af4-a218-531ccc59069f Grab both CSV files, and the R script, and store them in a convenient folder. Login via ORCID We will walk through web submission on https://demo.nceas.ucsb.edu, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button. When you sign in, you will be redirected to orcid.org, where you can either provide your existing ORCID credentials, or create a new account. ORCID provides multiple ways to login, including using your email address, institutional logins from many universities, and logins from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login. Create and submit the data set After signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. Required sections are listed with a red asterisk. Click Add Files to choose the data files for your package You can select multiple files at a time to efficiently upload many files. The files will upload showing a progress indicator. You can continue editing metadata while they upload. Enter Overview information This includes a descriptive title, abstract, and keywords. The title is the first way a potential user will get information about your dataset. It should be descriptive but succinct, lack acronyms, and include some indication of the temporal and geospatial coverage of the data. The abstract should be sufficently descriptive for a general scientific audience to understand your dataset at a high level. It should provide an overview of the scientific context/project/hypotheses, how this data package fits into the larger context, a synopsis of the experimental or sampling design, and a summary of the data contents. Keywords, while not required, can help increase the searchability of your dataset, particularly if they come from a semantically defined keyword thesaurus. Optionally, you can also enter funding information, including a funding number, which can help link multiple datasets collected under the same grant. Selecting a distribution license - either CC-0 or CC-BY is required. People Information Information about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people: Creators - all the people who should be in the citation for the dataset Contacts - one is required, but defaults to the first Creator if omitted Principal Investigators and any other that are relevant For each, please strive to provide their ORCID identifier, which helps link this dataset to their other scholarly works. Temporal Information Add the temporal coverage of the data, which represents the time period to which data apply. Location Information The geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that a spatial search will be more precise. Note that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global. For example: Mission Canyon, Santa Barbara, California, USA Methods Methods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, include details of sampling methods, experimental design, quality assurance procedures, and computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data. Save a first version with Submit When finished, click the Submit Dataset button at the bottom. If there are errors or missing fields, they will be highlighted. Correct those, and then try submitting again. When you are successful, you should see a green banner with a link to the current dataset view. Click the X to close that banner, if you want to continue editing metadata. Success! File and variable level metadata The final major section of metadata concerns the structure and contents of your data files. In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure. For example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information. The Attributes tab is where you enter variable (aka attribute) information. In the case of tabular data (such as csv files) each column is an attribute, thus there should be one attribute defined for every column in your dataset. Attribute metadata includes: variable name (for programs) variable label (for display) variable definition (be specific) type of measurement units &amp; code definitions You’ll need to add these definitions for every variable (column) in the file. When done, click Done. Now the list of data files will show a green checkbox indicating that you have full described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes. Note that attribute information is not relevant for data files that do not contain variables, such as the R script in this example. Other examples of data files that might not need attributes are images, pdfs, and non-tabular text documents (such as readme files). The yellow circle in the editor indicates that attributes have not been filled out for a data file, and serves as a warning that they might be needed, depending on the file. After you get the green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes. Add workflow provenance Understanding the relationships between files in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, that are often then used in analysis and visualization code to produce final outputs. In DataONE, we support structured descriptions of these relationships, so one can see the flow of data from raw data to derived to outputs. You add provenance by navigating to the data table descriptions, and selecting the Add buttons to link the data and scripts that were used in your computational workflow. On the left side, select the Add circle to add an input data source to the filteredSpecies.csv file. This starts building the provenance graph to explain the origin and history of each data object. The linkage to the source dataset should appear. Then you can add the link to the source code that handled the conversion between the data files by clicking on Add arrow and selecting the R script: Select the R script and click “Done.” The diagram now shows the relationships among the data files and the R script, so click Submit to save another version of the package. Et voilà! A beatifully preserved data package! "],["session-5-collaborative-synthesis.html", "Session 5: Collaborative synthesis", " Session 5: Collaborative synthesis "],["session-6-shiny.html", "Session 6: Shiny Introduction to Shiny", " Session 6: Shiny Introduction to Shiny Learning Objectives In this lesson we will: review the capabilities in Shiny applications learn about the basic layout for Shiny interfaces learn about the server component for Shiny applications build a simple shiny application for interactive plotting Overview Shiny is an R package for creating interactive data visualizations embedded in a web application that you and your colleagues can view with just a web browser. Shiny apps are relatively easy to construct, and provide interactive features for letting others share and explore data and analyses. There are some really great examples of what Shiny can do on the RStudio webite like this one exploring movie metadata. A more scientific example is a tool from the SASAP project exploring proposal data from the Alaska Board of Fisheries. There is also an app for Delta monitoring efforts. Most any kind of analysis and visualization that you can do in R can be turned into a useful interactive visualization for the web that lets people explore your data more intuitively But, a Shiny application is not the best way to preserve or archive your data. Instead, for preservation use a repository that is archival in its mission like the KNB Data Repository, Zenodo, or Dryad. This will assign a citable identifier to the specific version of your data, which you can then read in an interactive visualiztion with Shiny. For example, the data for the Alaska Board of Fisheries application is published on the KNB and is citable as: Meagan Krupa, Molly Cunfer, and Jeanette Clark. 2017. Alaska Board of Fisheries Proposals 1959-2016. Knowledge Network for Biocomplexity. doi:10.5063/F1QN652R. While that is the best citation and archival location of the dataset, using Shiny, one can also provide an easy-to-use exploratory web application that you use to make your point that directly loads the data from the archival site. For example, the Board of Fisheries application above lets people who are not inherently familiar with the data to generate graphs showing the relationships between the variables in the dataset. We’re going to create a simple shiny app with two sliders so we can interactively control inputs to an R function. These sliders will allow us to interactively control a plot. Create a sample shiny application File &gt; New &gt; Shiny Web App… Set some fields: Name it “myapp” or something else Select “Single File” Choose to create it in a new folder called ‘shiny-demo’ Click Create RStudio will create a new file called app.R that contains the Shiny application. Run it by choosing Run App from the RStudio editor header bar. This will bring up the default demo Shiny application, which plots a histogram and lets you control the number of bins in the plot. Note that you can drag the slider to change the number of bins in the histogram. Shiny architecture A Shiny application consists of two functions, the ui and the server. The ui function is responsible for drawing the web page, while the server is responsible for any calculations and for creating any dynamic components to be rendered. Each time that a user makes a change to one of the interactive widgets, the ui grabs the new value (say, the new slider min and max) and sends a request to the server to re-render the output, passing it the new input values that the user had set. These interactions can sometimes happen on one computer (e.g., if the application is running in your local RStudio instance). Other times, the ui runs on the web browser on one computer, while the server runs on a remote computer somewhere else on the Internet (e.g., if the application is deployed to a web server). Interactive scatterplots Let’s modify this application to plot YOLO bypass secchi disk data in a time-series, and allow aspects of the plot to be interactively changed. Load data for the example Use this code to load the data at the top of your app.R script. Note we are using contentId again, and we have filtered for some species of interest. library(shiny) library(contentid) library(dplyr) library(ggplot2) library(lubridate) sha1 &lt;- &#39;hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0&#39; delta.file &lt;- contentid::resolve(sha1, registries=c(&quot;dataone&quot;), store = TRUE) # fix the sample date format, and filter for species of interest delta_data &lt;- read.csv(delta.file) %&gt;% mutate(SampleDate = mdy(SampleDate)) %&gt;% filter(grepl(&quot;Salmon|Striped Bass|Smelt|Sturgeon&quot;, CommonName)) names(delta_data) Add a simple timeseries using ggplot We know there has been a lot of variation through time in the delta, so let’s plot a time-series of Secchi depth. We do so by switching out the histogram code for a simple ggplot, like so: server &lt;- function(input, output) { output$distPlot &lt;- renderPlot({ ggplot(delta_data, mapping = aes(SampleDate, Secchi)) + geom_point(colour=&quot;red&quot;, size=4) + theme_light() }) } If you now reload the app, it will display the simple time-series instead of the histogram. At this point, we haven’t added any interactivity. In a Shiny application, the server function provides the part of the application that creates our interactive components, and returns them to the user interface (ui) to be displayed on the page. Add sliders to set the start and end date for the X axis To make the plot interactive, first we need to modify our user interface to include widgits that we’ll use to control the plot. Specifically, we will add a new slider for setting the minDate parameter, and modify the existing slider to be used for the maxDate parameter. To do so, modify the sidebarPanel() call to include two sliderInput() function calls: sidebarPanel( sliderInput(&quot;minDate&quot;, &quot;Min Date:&quot;, min = as.Date(&quot;1998-01-01&quot;), max = as.Date(&quot;2020-01-01&quot;), value = as.Date(&quot;1998-01-01&quot;)), sliderInput(&quot;maxDate&quot;, &quot;Max Date:&quot;, min = as.Date(&quot;1998-01-01&quot;), max = as.Date(&quot;2020-01-01&quot;), value = as.Date(&quot;2005-01-01&quot;)) ) If you reload the app, you’ll see two new sliders, but if you change them, they don’t make any changes to the plot. Let’s fix that. Connect the slider values to the plot Finally, to make the plot interactive, we can use the input and output variables that are passed into the server function to access the current values of the sliders. In Shiny, each UI component is given an input identifier when it is created, which is used as the name of the value in the input list. So, we can access the minimum depth as input$minDate and the max as input$maxDate. Let’s use these values now by adding limits to our X axis in the ggplot: ggplot(delta_data, mapping = aes(SampleDate, Secchi)) + geom_point(colour=&quot;red&quot;, size=4) + xlim(c(input$minDate,input$maxDate)) + theme_light() At this point, we have a fully interactive plot, and the sliders can be used to change the min and max of the Depth axis. Looks so shiny! Negative depths? What happens if a clever user sets the minimum for the X axis at a greater value than the maximum? You’ll see that the direction of the X axis becomes reversed, and the plotted points display right to left. This is really an error condition. Rather than use two independent sliders, we can modify the first slider to output a range of values, which will prevent the min from being greater than the max. You do so by setting the value of the slider to a vector of length 2, representing the default min and max date for the slider, such as c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")). So, delete the second slider, rename the first, and provide a vector for the value, like this: sliderInput(&quot;date&quot;, &quot;Date:&quot;, min = as.Date(&quot;1998-01-01&quot;), max = as.Date(&quot;2020-01-01&quot;), value = c(as.Date(&quot;1998-01-01&quot;), as.Date(&quot;2020-01-01&quot;))) ) Now, modify the ggplot to use this new date slider value, which now will be returned as a vector of length 2. The first element of the depth vector is the min, and the second is the max value on the slider. ggplot(delta_data, mapping = aes(SampleDate, Secchi)) + geom_point(colour=&quot;red&quot;, size=4) + xlim(c(input$date[1],input$date[2])) + theme_light() Extending the user interface with dynamic plots If you want to display more than one plot in your application, and provide a different set of controls for each plot, the current layout would be too simple. Next we will extend the application to break the page up into vertical sections, and add a new plot in which the user can choose which variables are plotted. The current layout is set up such that the FluidPage contains the title element, and then a sidebarLayout, which is divided horizontally into a sidebarPanel and a mainPanel. Vertical layout To extend the layout, we will first nest the existing sidebarLayout in a new verticalLayout, which simply flows components down the page vertically. Then we will add a new sidebarLayout to contain the bottom controls and graph. This mechanism of alternately nesting vertical and horizontal panels can be used to segment the screen into boxes with rules about how each of the panels is resized, and how the content flows when the browser window is resized. The sidebarLayout works to keep the sidebar about 1/3 of the box, and the main panel about 2/3, which is a good proportion for our controls and plots. Add the verticalLayout, and the second sidebarLayout for the second plot as follows: verticalLayout( # Sidebar with a slider input for depth axis sidebarLayout( sidebarPanel( sliderInput(&quot;date&quot;, &quot;Date:&quot;, min = as.Date(&quot;1998-01-01&quot;), max = as.Date(&quot;2020-01-01&quot;), value = c(as.Date(&quot;1998-01-01&quot;), as.Date(&quot;2020-01-01&quot;))) ), # Show a plot of the generated distribution mainPanel( plotOutput(&quot;distPlot&quot;) ) ), tags$hr(), sidebarLayout( sidebarPanel( selectInput(&quot;x_variable&quot;, &quot;X Variable&quot;, cols, selected = &quot;SampleDate&quot;), selectInput(&quot;y_variable&quot;, &quot;Y Variable&quot;, cols, selected = &quot;Count&quot;), selectInput(&quot;color_variable&quot;, &quot;Color&quot;, cols, selected = &quot;CommonName&quot;) ), # Show a plot with configurable axes mainPanel( plotOutput(&quot;varPlot&quot;) ) ), tags$hr() Note that the second sidebarPanel uses three selectInput elements to provide dropdown menus with the variable columns (cols) from our data frame. To manage that, we need to first set up the cols variable, which we do by saving the variables names from the delta_data data frame to a variable: sha1 &lt;- &#39;hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0&#39; delta.file &lt;- contentid::resolve(sha1, registries=c(&quot;dataone&quot;), store = TRUE) # fix the sample date format, and filter for species of interest delta_data &lt;- read.csv(delta.file) %&gt;% mutate(SampleDate = mdy(SampleDate)) %&gt;% filter(grepl(&quot;Salmon|Striped Bass|Smelt|Sturgeon&quot;, CommonName)) cols &lt;- names(delta_data) Add the dynamic plot Because we named the second plot varPlot in our UI section, we now need to modify the server to produce this plot. Its very similar to the first plot, but this time we want to use the selected variables from the user controls to choose which variables are plotted. These variable names from the $input are character strings, and so would not be recognized as symbols in the aes mapping in ggplot. Instead, we can use aes_string() to provide character names for the variables to be used in the mappings. output$varPlot &lt;- renderPlot({ ggplot(delta_data, mapping = aes_string(x = input$x_variable, y = input$y_variable, color = input$color_variable)) + geom_point(size=4) + theme_light() }) Finishing touches: data citation Citing the data that we used for this application is the right thing to do, and easy. You can add arbitrary HTML to the layout using utility functions in the tags list. # Application title titlePanel(&quot;Yolo Bypass Fish and Water Quality Data&quot;), p(&quot;Data for this application are from: &quot;), tags$ul( tags$li(&quot;Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.&quot;, tags$a(&quot;doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f&quot;, href=&quot;http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f&quot;) ) ), tags$br(), tags$hr(), The final application shows the data citation, the depth plot, and the configurable scatterplot in three distinct panels. Publishing Shiny applications Once you’ve finished your app, you’ll want to share it with others. To do so, you need to publish it to a server that is set up to handle Shiny apps. Your main choices are: shinyapps.io (Hosted by RStudio) This is a service offered by RStudio, which is initially free for 5 or fewer apps and for limited run time, but has paid tiers to support more demanding apps. You can deploy your app using a single button push from within RStudio. Shiny server (On premises) This is an open source server which you can deploy for free on your own hardware. It requires more setup and configuration, but it can be used without a fee. RStudio connect (On premises) This is a paid product you install on your local hardware, and that contains the most advanced suite of services for hosting apps and RMarkdown reports. You can publish using a single button click from RStudio. A comparison of publishing features is available from RStudio. Publishing to shinyapps.io The easiest path is to create an account on shinyapps.io, and then configure RStudio to use that account for publishing. Instructions for enabling your local RStudio to publish to your account are displayed when you first log into shinyapps.io: Once your account is configured locally, you can simply use the Publish button from the application window in RStudio, and your app will be live before you know it! Summary Shiny is a fantastic way to quickly and efficiently provide data exploration for your data and code. We highly recommend it for its interactivity, but an archival-quality repository is the best long-term home for your data and products. In this example, we used data drawn directly from the EDI repository in our Shiny app, which offers both the preservation guarantees of an archive, plus the interactive data exploration from Shiny. You can utilize the full power of R and the tidyverse for writing your interactive applications. Full source code for the final application library(shiny) library(contentid) library(dplyr) library(ggplot2) library(lubridate) # read in the data from EDI sha1 &lt;- &#39;hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0&#39; delta.file &lt;- contentid::resolve(sha1, registries=c(&quot;dataone&quot;), store = TRUE) # fix the sample date format, and filter for species of interest delta_data &lt;- read.csv(delta.file) %&gt;% mutate(SampleDate = mdy(SampleDate)) %&gt;% filter(grepl(&quot;Salmon|Striped Bass|Smelt|Sturgeon&quot;, CommonName)) cols &lt;- names(delta_data) # Define UI for application that draws a two plots ui &lt;- fluidPage( # Application title and data source titlePanel(&quot;Sacramento River floodplain fish and water quality dataa&quot;), p(&quot;Data for this application are from: &quot;), tags$ul( tags$li(&quot;Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.&quot;, tags$a(&quot;doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f&quot;, href=&quot;http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f&quot;) ) ), tags$br(), tags$hr(), verticalLayout( # Sidebar with a slider input for time axis sidebarLayout( sidebarPanel( sliderInput(&quot;date&quot;, &quot;Date:&quot;, min = as.Date(&quot;1998-01-01&quot;), max = as.Date(&quot;2020-01-01&quot;), value = c(as.Date(&quot;1998-01-01&quot;), as.Date(&quot;2020-01-01&quot;))) ), # Show a plot of the generated timeseries mainPanel( plotOutput(&quot;distPlot&quot;) ) ), tags$hr(), sidebarLayout( sidebarPanel( selectInput(&quot;x_variable&quot;, &quot;X Variable&quot;, cols, selected = &quot;SampleDate&quot;), selectInput(&quot;y_variable&quot;, &quot;Y Variable&quot;, cols, selected = &quot;Count&quot;), selectInput(&quot;color_variable&quot;, &quot;Color&quot;, cols, selected = &quot;CommonName&quot;) ), # Show a plot with configurable axes mainPanel( plotOutput(&quot;varPlot&quot;) ) ), tags$hr() ) ) # Define server logic required to draw the two plots server &lt;- function(input, output) { # turbidity plot output$distPlot &lt;- renderPlot({ ggplot(delta_data, mapping = aes(SampleDate, Secchi)) + geom_point(colour=&quot;red&quot;, size=4) + xlim(c(input$date[1],input$date[2])) + theme_light() }) # mix and match plot output$varPlot &lt;- renderPlot({ ggplot(delta_data, mapping = aes_string(x = input$x_variable, y = input$y_variable, color = input$color_variable)) + geom_point(size=4) + theme_light() }) } # Run the application shinyApp(ui = ui, server = server) A shinier app with tabs and a map! library(shiny) library(contentid) library(dplyr) library(tidyr) library(ggplot2) library(lubridate) library(shinythemes) library(sf) library(leaflet) library(snakecase) # read in the data from EDI sha1 &lt;- &#39;hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0&#39; delta.file &lt;- contentid::resolve(sha1, registries=c(&quot;dataone&quot;), store = TRUE) # fix the sample date format, and filter for species of interest delta_data &lt;- read.csv(delta.file) %&gt;% mutate(SampleDate = mdy(SampleDate)) %&gt;% filter(grepl(&quot;Salmon|Striped Bass|Smelt|Sturgeon&quot;, CommonName)) %&gt;% rename(DissolvedOxygen = DO, Ph = pH, SpecificConductivity = SpCnd) cols &lt;- names(delta_data) sites &lt;- delta_data %&gt;% distinct(StationCode, Latitude, Longitude) %&gt;% drop_na() %&gt;% st_as_sf(coords = c(&#39;Longitude&#39;,&#39;Latitude&#39;), crs = 4269, remove = FALSE) # Define UI for application ui &lt;- fluidPage( navbarPage(theme = shinytheme(&quot;flatly&quot;), collapsible = TRUE, HTML(&#39;&lt;a style=&quot;text-decoration:none;cursor:default;color:#FFFFFF;&quot; class=&quot;active&quot; href=&quot;#&quot;&gt;Sacramento River Floodplain Data&lt;/a&gt;&#39;), id=&quot;nav&quot;, windowTitle = &quot;Sacramento River floodplain fish and water quality data&quot;, tabPanel(&quot;Data Sources&quot;, verticalLayout( # Application title and data source titlePanel(&quot;Sacramento River floodplain fish and water quality data&quot;), p(&quot;Data for this application are from: &quot;), tags$ul( tags$li(&quot;Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.&quot;, tags$a(&quot;doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f&quot;, href=&quot;http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f&quot;) ) ), tags$br(), tags$hr(), p(&quot;Map of sampling locations&quot;), mainPanel(leafletOutput(&quot;map&quot;)) ) ), tabPanel( &quot;Explore&quot;, verticalLayout( mainPanel( plotOutput(&quot;distPlot&quot;), width = 12, absolutePanel(id = &quot;controls&quot;, class = &quot;panel panel-default&quot;, top = 175, left = 75, width = 300, fixed=TRUE, draggable = TRUE, height = &quot;auto&quot;, sliderInput(&quot;date&quot;, &quot;Date:&quot;, min = as.Date(&quot;1998-01-01&quot;), max = as.Date(&quot;2020-01-01&quot;), value = c(as.Date(&quot;1998-01-01&quot;), as.Date(&quot;2020-01-01&quot;))) ) ), tags$hr(), sidebarLayout( sidebarPanel( selectInput(&quot;x_variable&quot;, &quot;X Variable&quot;, cols, selected = &quot;SampleDate&quot;), selectInput(&quot;y_variable&quot;, &quot;Y Variable&quot;, cols, selected = &quot;Count&quot;), selectInput(&quot;color_variable&quot;, &quot;Color&quot;, cols, selected = &quot;CommonName&quot;) ), # Show a plot with configurable axes mainPanel( plotOutput(&quot;varPlot&quot;) ) ), tags$hr() ) ) ) ) # Define server logic required to draw the two plots server &lt;- function(input, output) { output$map &lt;- renderLeaflet({leaflet(sites) %&gt;% addTiles() %&gt;% addCircleMarkers(data = sites, lat = ~Latitude, lng = ~Longitude, radius = 10, # arbitrary scaling fillColor = &quot;gray&quot;, fillOpacity = 1, weight = 0.25, color = &quot;black&quot;, label = ~StationCode) }) # turbidity plot output$distPlot &lt;- renderPlot({ ggplot(delta_data, mapping = aes(SampleDate, Secchi)) + geom_point(colour=&quot;red&quot;, size=4) + xlim(c(input$date[1],input$date[2])) + labs(x = &quot;Sample Date&quot;, y = &quot;Secchi Depth (m)&quot;) + theme_light() }) # mix and match plot output$varPlot &lt;- renderPlot({ ggplot(delta_data, mapping = aes_string(x = input$x_variable, y = input$y_variable, color = input$color_variable)) + labs(x = to_any_case(input$x_variable, case = &quot;title&quot;), y = to_any_case(input$y_variable, case = &quot;title&quot;), color = to_any_case(input$color_variable, case = &quot;title&quot;)) + geom_point(size=4) + theme_light() }) } # Run the application shinyApp(ui = ui, server = server) Resources Main Shiny site Official Shiny Tutorial "],["session-7-collaborative-synthesis.html", "Session 7: Collaborative synthesis", " Session 7: Collaborative synthesis "],["session-8-reproducible-workflows-revisited.html", "Session 8: Reproducible workflows revisited Parallel Computing in R", " Session 8: Reproducible workflows revisited Parallel Computing in R Learning Outcomes Understand what parallel computing is and when it may be useful Understand how parallelism can work Review sequential loops and *apply functions Understand and use the parallel package multicore functions Understand and use the foreach package functions Introduction Processing large amounts of data with complex models can be time consuming. New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America: Levy et al. 2016. doi:10.5063/F1Z899CZ There are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization. Alternatively, think of remote sensing data. Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years. NEON Data Cube Why parallelism? Much R code runs fast and fine on a single processor. But at times, computations can be: cpu-bound: Take too much cpu time memory-bound: Take too much memory I/O-bound: Take too much time to read/write from disk network-bound: Take too much time to transfer To help with cpu-bound computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire cluster of those computers. Plus, these machines also have large amounts of memory to avoid memory-bound computing jobs. Processors (CPUs) and Cores A modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with mutliple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations. A computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time. A typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here we show four quad-core processors for a total of 16 cores in this machine. You can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later). Historically, R has only utilized one processor, which makes it single-threaded. Which is a shame, because the 2017 MacBook Pro that I am writing this on is much more powerful than that: jones@powder:~$ sysctl hw.ncpu hw.physicalcpu hw.ncpu: 8 hw.physicalcpu: 4 To interpret that output, this machine powder has 4 physical CPUs, each of which has two processing cores, for a total of 8 cores for computation. I’d sure like my R computations to use all of that processing power. Because its all on one machine, we can easily use multicore processing tools to make use of those cores. Now let’s look at the computational server aurora at NCEAS: jones@aurora:~$ lscpu | egrep &#39;CPU\\(s\\)|per core|per socket&#39; CPU(s): 88 On-line CPU(s) list: 0-87 Thread(s) per core: 2 Core(s) per socket: 22 NUMA node0 CPU(s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86 NUMA node1 CPU(s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87 Now that’s some compute power! Aurora has 384 GB of RAM, and ample storage. All still under the control of a single operating system. However, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now: JetStream 640 nodes, 15,360 cores, 80TB RAM Stampede2 at TACC is coming online in 2017 4200 nodes, 285,600 cores Note that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster. One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine. But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster. When to parallelize It’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer! In addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced. Some propose that this may follow Amdahl’s Law, where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines): So, its important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done. With that, let’s do some parallel computing… Loops and repetitive tasks using lapply When you have a list of repetitive tasks, you may be able to speed it up by adding more computing power. If each task is completely independent of the others, then it is a prime candidate for executing those tasks in parallel, each on its own core. For example, let’s build a simple loop that uses sample with replacement to do a bootstrap analysis. In this case, we select Sepal.Length and Species from the iris dataset, subset it to 100 observations, and then iterate across 10,000 trials, each time resampling the observations with replacement. We then run a logistic regression fitting species as a function of length, and record the coefficients for each trial to be returned. x &lt;- iris[which(iris[,5] != &quot;setosa&quot;), c(1,5)] trials &lt;- 10000 res &lt;- data.frame() system.time({ trial &lt;- 1 while(trial &lt;= trials) { ind &lt;- sample(100, 100, replace=TRUE) result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit)) r &lt;- coefficients(result1) res &lt;- rbind(res, r) trial &lt;- trial + 1 } }) ## user system elapsed ## 26.302 1.765 32.508 The issue with this loop is that we execute each trial sequentially, which means that only one of our 8 processors on this machine are in use. In order to exploit parallelism, we need to be able to dispatch our tasks as functions, with one task going to each processor. To do that, we need to convert our task to a function, and then use the *apply() family of R functions to apply that function to all of the members of a set. In R, using apply used to be faster than the equivalent code in a loop, but now they are similar due to optimizations in R loop handling. However, using the function allows us to later take advantage of other approaches to parallelization. Here’s the same code rewritten to use lapply(), which applies a function to each of the members of a list (in this case the trials we want to run): x &lt;- iris[which(iris[,5] != &quot;setosa&quot;), c(1,5)] trials &lt;- seq(1, 10000) boot_fx &lt;- function(trial) { ind &lt;- sample(100, 100, replace=TRUE) result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit)) r &lt;- coefficients(result1) res &lt;- rbind(data.frame(), r) } system.time({ results &lt;- lapply(trials, boot_fx) }) ## user system elapsed ## 28.457 0.879 30.366 Approaches to parallelization When parallelizing jobs, one can: Use the multiple cores on a local computer through mclapply Use multiple processors on local (and remote) machines using makeCluster and clusterApply In this approach, one has to manually copy data and code to each cluster member using clusterExport This is extra work, but sometimes gaining access to a large cluster is worth it Parallelize using: mclapply The parallel library can be used to send tasks (encoded as function calls) to each of the processing cores on your machine in parallel. This is done by using the parallel::mclapply function, which is analogous to lapply, but distributes the tasks to multiple processors. mclapply gathers up the responses from each of these function calls, and returns a list of responses that is the same length as the list or vector of input data (one return per input item). library(parallel) library(MASS) starts &lt;- rep(100, 40) fx &lt;- function(nstart) kmeans(Boston, 4, nstart=nstart) numCores &lt;- detectCores() numCores ## [1] 3 system.time( results &lt;- lapply(starts, fx) ) ## user system elapsed ## 1.323 0.159 1.631 system.time( results &lt;- mclapply(starts, fx, mc.cores = numCores) ) ## user system elapsed ## 0.947 0.271 1.211 Now let’s demonstrate with our bootstrap example: x &lt;- iris[which(iris[,5] != &quot;setosa&quot;), c(1,5)] trials &lt;- seq(1, 10000) boot_fx &lt;- function(trial) { ind &lt;- sample(100, 100, replace=TRUE) result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit)) r &lt;- coefficients(result1) res &lt;- rbind(data.frame(), r) } system.time({ results &lt;- mclapply(trials, boot_fx, mc.cores = numCores) }) ## user system elapsed ## 20.992 1.811 22.507 Parallelize using: foreach and doParallel The normal for loop in R looks like: for (i in 1:3) { print(sqrt(i)) } ## [1] 1 ## [1] 1.414214 ## [1] 1.732051 The foreach method is similar, but uses the sequential %do% operator to indicate an expression to run. Note the difference in the returned data structure. library(foreach) foreach (i=1:3) %do% { sqrt(i) } ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1.414214 ## ## [[3]] ## [1] 1.732051 In addition, foreach supports a parallelizable operator %dopar% from the doParallel package. This allows each iteration through the loop to use different cores or different machines in a cluster. Here, we demonstrate with using all the cores on the current machine: library(foreach) library(doParallel) ## Loading required package: iterators registerDoParallel(numCores) # use multicore, set to the number of our cores foreach (i=1:3) %dopar% { sqrt(i) } ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1.414214 ## ## [[3]] ## [1] 1.732051 # To simplify output, foreach has the .combine parameter that can simplify return values # Return a vector foreach (i=1:3, .combine=c) %dopar% { sqrt(i) } ## [1] 1.000000 1.414214 1.732051 # Return a data frame foreach (i=1:3, .combine=rbind) %dopar% { sqrt(i) } ## [,1] ## result.1 1.000000 ## result.2 1.414214 ## result.3 1.732051 The doParallel vignette on CRAN shows a much more realistic example, where one can use `%dopar% to parallelize a bootstrap analysis where a data set is resampled 10,000 times and the analysis is rerun on each sample, and then the results combined: # Let&#39;s use the iris data set to do a parallel bootstrap # From the doParallel vignette, but slightly modified x &lt;- iris[which(iris[,5] != &quot;setosa&quot;), c(1,5)] trials &lt;- 10000 system.time({ r &lt;- foreach(icount(trials), .combine=rbind) %dopar% { ind &lt;- sample(100, 100, replace=TRUE) result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit)) coefficients(result1) } }) ## user system elapsed ## 27.170 2.521 18.440 # And compare that to what it takes to do the same analysis in serial system.time({ r &lt;- foreach(icount(trials), .combine=rbind) %do% { ind &lt;- sample(100, 100, replace=TRUE) result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit)) coefficients(result1) } }) ## user system elapsed ## 26.935 0.891 28.636 # When you&#39;re done, clean up the cluster stopImplicitCluster() Summary In this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores. Next, we reviewed the way in which traditional for loops in R can be rewritten as functions that are applied to a list serially using lapply, and then how the parallel package mclapply function can be substituted in order to utilize multiple cores on the local computer to speed up computations. Finally, we installed and reviewed the use of the foreach package with the %dopar operator to accomplish a similar parallelization using multiple cores. Readings and tutorials Multicore Data Science with R and Python Beyond Single-Core R by Jonoathan Dursi (also see GitHub repo for slide source) The venerable Parallel R by McCallum and Weston (a bit dated on the tooling, but conceptually solid) The doParallel Vignette "],["session-9-message-box-revisited.html", "Session 9: Message box revisited", " Session 9: Message box revisited "],["session-10-collaborative-synthesis.html", "Session 10: Collaborative synthesis", " Session 10: Collaborative synthesis "],["session-11-collaborative-synthesis.html", "Session 11: Collaborative Synthesis", " Session 11: Collaborative Synthesis "],["session-12-collaborative-synthesis.html", "Session 12: Collaborative Synthesis", " Session 12: Collaborative Synthesis "],["session-13-collaborative-synthesis-and-wrap-up.html", "Session 13: Collaborative Synthesis and Wrap-up", " Session 13: Collaborative Synthesis and Wrap-up "],["session-4-programmatic-metadata-and-data-access.html", "Session 4: Programmatic metadata and data access Reproducible Data Access Publishing local data identifiers Programming Metadata and Data Publishing", " Session 4: Programmatic metadata and data access Reproducible Data Access ## ## Attaching package: &#39;contentid&#39; ## The following object is masked from &#39;package:pins&#39;: ## ## pin Learning Objectives In this lesson, you will learn: Why we strive for reproducible data access How content identifiers differ from DOIs How content identifiers make research more reproducible Ways to register and resolve content identifiers for unpublished data How content identifiers can resolve to published data sources Barriers to data access Traditional ways of working with data – as files on a file system – limit the reproducibility of code to local compute environments. A typical R analysis file will load one or many data files from the local disk with code like this: delta_catch &lt;- readr::read_csv(&#39;/Users/jkresearcher/Projects/2018/Delta_Analysis/delta_catch.csv&#39;) delta_taxa &lt;- readr::read_csv(&#39;../../Delta_2021/delta_taxa.csv&#39;) delta_effort &lt;- readr::read_csv(&#39;delta_effort.csv&#39;) delta_sites &lt;- readr::read_csv(&#39;data/delta_sites.csv&#39;) Which of those file paths are the most portable? And which will run unmodified on both the original computer that they were written on, and on colleagues’ computers? In reality, none of them, in that they require that a specific data file be present in a specific location for the code to work properly, and these assumptions are rarely met and hard to maintain. The Web partly solves this problem, because it allows code to access data that is located somewhere on the Internet with a web URI. For example, loading data from a web site can be much more portable than loading the equivalent data from a local computer. delta_sites_edi &lt;- &#39;https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&amp;entityid=6a82451e84be1fe82c9821f30ffc2d7d&#39; delta_sites &lt;- readr::read_csv(delta_sites_edi, show_col_types = FALSE) head(delta_sites) ## # A tibble: 6 × 4 ## MethodCode StationCode LatitudeLocation LongitudeLocation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BSEIN AL1 38.5 -122. ## 2 BSEIN AL2 38.5 -122. ## 3 BSEIN AL3 38.5 -122. ## 4 BSEIN AL4 38.5 -122. ## 5 BSEIN BL1 38.5 -122. ## 6 BSEIN BL2 38.4 -122. In theory, that code will work from anyone’s computer with an internet connection. But code that downloads data each and every time it is run is not particularly efficient, and will be prohibitive for all but the smallest datasets. A simple solution to this issue is to cache a local copy of the dataset, and only retrieve the original from the web when we don’t have a local copy. In this way, people running code or a script will download the data the first time their code is run, but use a local copy from thence forward. While this can be accomplished with some simple conditional logic in R, the pattern has been simplified using the pins package: delta_sites_edi &lt;- pins::pin(&#39;https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&amp;entityid=6a82451e84be1fe82c9821f30ffc2d7d&#39;) delta_sites &lt;- readr::read_csv(delta_sites_edi, show_col_types = FALSE) head(delta_sites) ## # A tibble: 6 × 4 ## MethodCode StationCode LatitudeLocation LongitudeLocation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BSEIN AL1 38.5 -122. ## 2 BSEIN AL2 38.5 -122. ## 3 BSEIN AL3 38.5 -122. ## 4 BSEIN AL4 38.5 -122. ## 5 BSEIN BL1 38.5 -122. ## 6 BSEIN BL2 38.4 -122. You’ll note that code takes longer the first time it is run, as the data file is downloaded only the first time. While this works well over the short term, abundant evidence shows that web URIs have short lifespan. Most URIs are defunct within a few years (e.g., see McCown et al. 2005). Only the most carefully curated web sites maintain the viability of their links for longer. And maintaining them for decade-long periods requires a focus on archival principles and dedicated staff to ensure that files and the URLs at which they are published remain accessible. This is precisely the role of archival data repositories like the Arctic Data Center, the KNB Data Repository, and the Environmental Data Initiative (EDI). Finally, no discussion of data access and persistence would be complete without discussing the use of Digital Object Identifiers (DOIs). DOIs have become the dominant means to create persistent links to academic articles, publications, and datasets. They work by assigning a DOI name to a published work, and then ensuring that the DOI name always redirects to the current web location of the resource. Journals, societies, and data repositories actively maintain the redirection between a DOI such as doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f and its current location on the EDI Repository. DOIs are commonly assigned to published datasets, and include the bibliographic metadata needed to properly cite and access the dataset. The challenge with DOIs as they are typically implemented is that they are usually assigned to a Dataset, which is a collection of digital objects that are composed to form the whole Dataset and that can be accessed individually or through an API. Typically, the metadata attached to DOIs does not include an enumeration of those digital objects or a clear mechanism to get to the actual data – rather, the DOI redirects to a dataset landing page that provides a human readable summary of the dataset, and often various types of links to find and eventually download the data. Despite advances in metadata interoperability from DCAT and schema.org/Dataset, there is currently no reliable way to universally go from a known DOI for a dataset to the list of current locations of all of the digital objects that compose that dataset. And yet, this is exactly what we need for portable and persistent data access. In conclusion, DOIs are a great approach to uniquely citing a dataset, but they do not provde a way for code to download specific, versioned digital objects from a dataset in a portable way that is persistent over many years. We want data access to be: Portable Persistent Versioned Traceable Transparent Citable Persistent and portable data access for improving reproducibility We’ll be working with the following IEP dataset that is stored on EDI: Interagency Ecological Program (IEP), B. Schreier, B. Davis, and N. Ikemiyagi. 2019. Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018. ver 2. Environmental Data Initiative. https://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f (Accessed 2021-10-30). You can view this IEP dataset on DataONE: It also is visible from the EDI dataset landing page: It contains several data files, each of which is at a specific web URI, including: Fish catch and water quality Fish taxonomy Trap Effort Site locations delta_catch_edi &lt;- &#39;https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&amp;entityid=015e494911cf35c90089ced5a3127334&#39; delta_taxa_edi &lt;- &#39;https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&amp;entityid=0532048e856d4bd07deea11583b893dd&#39; delta_effort_edi &lt;- &#39;https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&amp;entityid=ace1ef25f940866865d24109b7250955&#39; delta_sites_edi &lt;- &#39;https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&amp;entityid=6a82451e84be1fe82c9821f30ffc2d7d&#39; Registering a content identifier from a URI Use the contentid package for portable access to data. delta_catch_id &lt;- store(delta_catch_url) delta_taxa_id &lt;- store(delta_taxa_url) delta_effort_id &lt;- store(delta_effort_url) delta_sites_id &lt;- store(delta_sites_url) print(c(delta_catch_id=delta_catch_id, delta_taxa_id=delta_taxa_id, delta_effort_id=delta_effort_id, delta_sites_id=delta_sites_id)) ## delta_catch_id ## &quot;hash://sha256/e0dc10d7f36cfc5ac147956abb91f24f2b2df9f914a004bbf1e85e7d9cf52f41&quot; ## delta_taxa_id ## &quot;hash://sha256/1473de800f3c5577da077507fb006be816a9194ddd417b1b98836be92eaea49d&quot; ## delta_effort_id ## &quot;hash://sha256/f2433efab802f55fa28c4aab628f3d529f4fdaf530bbc5c3a67ab92b5e8f71b2&quot; ## delta_sites_id ## &quot;hash://sha256/e25498ffc0208c3ae0e31a23204b856a9309f32ced2c87c8abcdd6f5cef55a9b&quot; Loading data from a content identifier delta_taxa_file &lt;- contentid::resolve(delta_taxa_id, store = TRUE) delta_taxa &lt;- readr::read_csv(delta_taxa_file, show_col_types=FALSE) delta_catch_file &lt;- contentid::resolve(delta_catch_id, store = TRUE) delta_catch &lt;- readr::read_csv(delta_catch_file, show_col_types=FALSE) ## Warning: One or more parsing issues, see `problems()` for details head(delta_catch) ## # A tibble: 6 × 32 ## SampleDate SampleTime StationCode MethodCode GearID CommonName ## &lt;chr&gt; &lt;time&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1/16/1998 14:05 YB BSEIN SEIN50 Threadfin Shad ## 2 1/16/1998 15:00 YB BSEIN SEIN50 Inland Silverside ## 3 1/19/1998 12:17 YB FNET FKNT Threadfin Shad ## 4 1/19/1998 12:17 YB FNET FKNT Chinook Salmon ## 5 1/19/1998 11:00 YB PSEIN PSEIN100 Threadfin Shad ## 6 1/19/1998 11:30 YB PSEIN PSEIN100 Inland Silverside ## # … with 26 more variables: GeneticallyConfirmed &lt;chr&gt;, GeneticID &lt;lgl&gt;, ## # Field_ID_CommonName &lt;chr&gt;, ForkLength &lt;dbl&gt;, Count &lt;dbl&gt;, FishSex &lt;chr&gt;, ## # Race &lt;chr&gt;, MarkCode &lt;chr&gt;, CWTSample &lt;lgl&gt;, FishTagID &lt;chr&gt;, ## # StageCode &lt;chr&gt;, Dead &lt;chr&gt;, GearConditionCode &lt;dbl&gt;, WeatherCode &lt;chr&gt;, ## # WaterTemperature &lt;dbl&gt;, Secchi &lt;dbl&gt;, Conductivity &lt;dbl&gt;, SpCnd &lt;dbl&gt;, ## # DO &lt;dbl&gt;, pH &lt;dbl&gt;, Turbidity &lt;dbl&gt;, SubstrateCode &lt;chr&gt;, Tide &lt;chr&gt;, ## # VolumeSeined &lt;dbl&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt; delta_sites_file &lt;- contentid::resolve(delta_sites_id, store = TRUE) delta_sites &lt;- readr::read_csv(delta_sites_file, show_col_types = FALSE) This approach is portable, as anyone can run it without having the data local beforehand. This approach is persistent, because it pulls data from persistent archives, and can take advantage of archive redundancy. For example, here is the list of locations that can be currently used to retrieve this data file: #contentid::query_sources(delta_catch_id, cols=c(&quot;identifier&quot;, &quot;source&quot;, &quot;date&quot;, &quot;status&quot;, &quot;sha1&quot;, &quot;sha256&quot;), registries = content_dir()) contentid::query_sources(delta_catch_id, cols=c(&quot;identifier&quot;, &quot;source&quot;, &quot;date&quot;, &quot;status&quot;, &quot;sha1&quot;, &quot;sha256&quot;)) ## identifier ## 1 hash://sha256/e0dc10d7f36cfc5ac147956abb91f24f2b2df9f914a004bbf1e85e7d9cf52f41 ## 2 hash://sha256/e0dc10d7f36cfc5ac147956abb91f24f2b2df9f914a004bbf1e85e7d9cf52f41 ## source ## 1 /Users/runner/Library/Application Support/org.R-project.R/R/contentid/sha256/e0/dc/e0dc10d7f36cfc5ac147956abb91f24f2b2df9f914a004bbf1e85e7d9cf52f41 ## 2 https://gmn.edirepository.org/mn/v2/object/https%3A%2F%2Fpasta.lternet.edu%2Fpackage%2Fdata%2Feml%2Fedi%2F233%2F2%2F015e494911cf35c90089ced5a3127334 ## date status sha1 ## 1 2021-11-01 03:33:46 200 &lt;NA&gt; ## 2 2020-04-12 23:05:21 200 sha1-MX1/hA5Zj1875zKrDgTwCoBRxtA= ## sha256 ## 1 hash://sha256/e0dc10d7f36cfc5ac147956abb91f24f2b2df9f914a004bbf1e85e7d9cf52f41 ## 2 sha256-4NwQ1/Ns/FrBR5Vqu5HyTyst+fkUoAS78ehefZz1L0E= # BUG TO FILE: `query_sources` should not return an error on inaccessible repos -- it should skip them and produce a warning, so that the local repo will still work when disconnected from the internet This approach is reproducible, as the exact version will be used every time (even if someone changes the data at the original web URI). This approach is traceable because there is a reference in the code to the specific data used, and the only way to change which data are used is to change the checksum thst is being referenced to a new version. Publishing local data identifiers # Store a local file vostok_co2 &lt;- system.file(&quot;extdata&quot;, &quot;vostok.icecore.co2&quot;, package = &quot;contentid&quot;) id &lt;- store(vostok_co2) vostok &lt;- retrieve(id) co2 &lt;- read.table(vostok, col.names = c(&quot;depth&quot;, &quot;age_ice&quot;, &quot;age_air&quot;, &quot;co2&quot;), skip = 21) head(co2) ## depth age_ice age_air co2 ## 1 149.1 5679 2342 284.7 ## 2 173.1 6828 3634 272.8 ## 3 177.4 7043 3833 268.1 ## 4 228.6 9523 6220 262.2 ## 5 250.3 10579 7327 254.6 ## 6 266.0 11334 8113 259.6 Improvements to content identifiers Content identifiers are not well-linked to DOIs DOIs are the current standard for citing data, and carry the citation metadata for data packages (such as author, title, publication year, etc.) Content identifiers are opaque, and not particularly transparent Need mechanisms to transparently know what a contentid refers to Luckily, we have everything we need to look up that info in DataONE and similar systems We can even retrieve the data citation for a content identifier Final example, showing use of contentid with metadata and a citation Programming Metadata and Data Publishing Learning Objectives In this lesson, you will learn: How to write standardized metadata in R How to publish data packages to the Arctic Data Center programmatically For datasets with a relatively small number of files that do not have many columns, the Arctic Data Center web form is the most efficient way to create metadata. However, for datasets that have many hundreds of files with a similar structure, or a large number of attributes, programmatic metadata creation may be more efficient. Creating metadata and publishing programmatically also allows for more a streamlined approach to updating datasets that have been published. By incorporating this documentation and publishing approach into your scientific workflow, you can improve the reproducibility and transparency of your work. Creating Metadata About Ecological Metadata Language (EML) EML, the metadata standard that the Arctic Data Center uses, looks like this: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; Earlier in this course we learned how to create an EML document like this using the Arctic Data Center web form. Now, we will learn how to create it using R. This can be especially useful to decrease the repetitiveness of metadata creation when you have many files with the same format, files with many attributes, or many data packages with a similar format. When you create metadata using the web form, the form creates valid metadata for you. Valid, structured metadata is what enables computers to predictably parse the information in a metadata document, enabling search, display, and even meta-analysis. When we create metadata in R, there aren’t as many user-friendly checks in place to ensure we create valid EML, so we need to understand the structure of the document more completely in order to make sure that it will be compatible with the Arctic Data Center. Let’s look at a simplified version of the example above: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; EML is written in XML (eXtensisble Markup Language). One of the key concepts in XML is the element. An XML element is everything that is encompassed by a start tag (&lt;...&gt;) and an end tag (&lt;/...&gt;). So, a simple element in the example above is &lt;surName&gt;Willette&lt;/surName&gt;. The name of this element is surName and the value is simple text, “Willette”. Each element in EML has a specific (case sensitive!) name and description that is specified in the schema. The description of the surName element is: “The surname field is used for the last name of the individual associated with the resource. This is typically the family name of an individual…” The EML schema specifies not only the names and descriptions of all EML elements, but also certain requirements, like which elements must be included, what type of values are allowed within elements, and how the elements are organized. An EML document is valid when it adheres to all of the requirements speficied in the EML schema. You’ll notice that some elements, rather than having a simple value, instead contain other elements that are nested within them. Let’s look at individualName. &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; This element is a collection of other elements (sometimes referred to as child elements). In this example, the individualName element has a givenName and a surName child element. We can check the requirements of any particular element by looking at the schema documents, which includes some helpful diagrams. The diagram in the individualName section of the schema looks like this: This shows that within individualName there are 3 possible child elements: salutation, givenName, and surName. The yellow circle icon with stacked papers tells you that the elements come in series, so you can include one or more of the child elements (as opposed to a switch symbol, which means that you choose one element from a list of options). The bold line tells you that surName is required, and the 0..inf indicates that you can include 0 or more salultation or givenName elements. So, to summarize, EML is the metadata standard used by the Arctic Data Center. It is written in XML, which consists of a series of nested elements. The element definitions, required elements, and structure are all defined by a schema. When you write EML, in order for it to be valid, your EML document must conform to the requirements given in the schema. Metadata in R: a simple example Now, let’s learn how the EML package can help us create EML in R. First, load the EML package in R. library(EML) The EML package relies on named list structures to generate name-value pairs for elements. “Named list structures” may sound unfamiliar, but they aren’t dissimilar from data.frames. A data.frame is just a named list of vectors of the same length, where the name of the vector is the column name. In this section, we will use the familiar $ operator to dig down into the named list structure that we generate. To show how this works, let’s create the individualName element, and save it as an object called me. Remember the schema requirements - indvidualName has child elements salutation, givenName, and surName. At least surName is required. me &lt;- list(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;) me So we have created the contents of an individualName element, with child elements givenName and surName, and assigned the values of those child elements to my name. This might look confusing, hard to remember, and if you aren’t intimitely familiar with the EML schema, you are probably feeling a little intimidated. Luckily the EML package has a set of helper list constructor functions which tell you what child elements can be used in a parent element. The helper functions have the format eml$elementName(). When combined with the RStudio autocomplete functionality, the whole process gets a lot easier! me &lt;- eml$individualName(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;) We can then use this object me, which represents the individualName element, and insert it into a larger EML document. At the top level of a complete EML document are the packageId and system elements. This is how your document can be uniquely identified within whatever system manages it. The packageId element typically contains the DOI (Digital Object Identifier) or other identifier for the dataset. Nested within the top level is the dataset element. All EML documents must have, at a minimum, a title, creator, and contact, in addition to the packageId and system. Let’s create a minimal valid EML dataset, with an arbitrary packageId and system. doc &lt;- list(packageId = &quot;dataset-1&quot;, system = &quot;local&quot;, dataset = eml$dataset(title = &quot;A minimial valid EML dataset&quot;, creator = eml$creator(individualName = me), contact = eml$contact(individualName = me))) Unlike the web editor, in R there is nothing stopping you from inserting arbitrary elements into your EML document. A critical step to creating EML is validating your EML document to make sure it conforms to the schema requirements. In R this can be done using the eml_validate function. eml_validate(doc) We can write our EML using write_eml. write_eml(doc, &quot;../files/simple_example.xml&quot;) Here is what the written file looks like: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:stmml=&quot;http://www.xml-cml.org/schema/stmml-1.1&quot; packageId=&quot;id&quot; system=&quot;system&quot; xsi:schemaLocation=&quot;eml://ecoinformatics.org/eml-2.1.1/ eml.xsd&quot;&gt; &lt;dataset&gt; &lt;title&gt;A minimal valid EML dataset&lt;/title&gt; &lt;creator&gt; &lt;individualName&gt; &lt;givenName&gt;Jeanette&lt;/givenName&gt; &lt;surName&gt;Clark&lt;/surName&gt; &lt;/individualName&gt; &lt;/creator&gt; &lt;contact&gt; &lt;individualName&gt; &lt;givenName&gt;Jeanette&lt;/givenName&gt; &lt;surName&gt;Clark&lt;/surName&gt; &lt;/individualName&gt; &lt;/contact&gt; &lt;/dataset&gt; &lt;/eml:eml&gt; Validation Errors One of the hardest parts about creating EML in R is diagnosing validation errors. I won’t get into too much detail, but here is a simple example. The eml$... family of helpers can help prevent validation errors, since they have a set list of arguments which are allowed. Here, I bypass the eml$dataset() helper function to show what the error looks like. doc &lt;- list(packageId = &quot;dataset-1&quot;, system = &quot;local&quot;, dataset = list(title = &quot;A minimial valid EML dataset&quot;, creator = eml$creator(individualName = me), contact = eml$contact(individualName = me), arbitrary = &quot;This element isn&#39;t in the schema&quot;)) eml_validate(doc) This error essentially says that the element arbitrary is not expected, and gives you a hint of some elements that were expected. Validation errors can be tricky, especially when there are lots of them. Validate early and often! Metadata in R: A more complete example As you might imagine, creating a complete metadata record like what is shown on this page would be pretty tedious if we did it just using the generic list or eml$... helpers, since the nesting structure can be very deep. The EML package has a set of higher level helper functions beginning with set_ that create some of the more complex elements. To demonstrate the use of these we are going to create an EML document that contains the following information: title creator and contact abstract methods geographic and temporal coverage description of a tabular data file and a script We will edit these elements using a mix of helpers and generic techniques. To get set up, navigate to this dataset and download the CSV file and the R script. Put them in a directory called files that is a sub-directory of the location of this RMarkdown file. Title, creator, contact To start, lets create a basic EML skeleton using our example above, but with more information in the creator and contact information besides just my name. # eml creator and contact have identical schema requirements (both fall under `responsibleParty`) me &lt;- eml$creator(individualName = eml$individualName(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;), organizationName = &quot;National Center for Ecological Analysis and Synthesis&quot;, electronicMailAddress = &quot;jclark@nceas.ucsb.edu&quot;, userId = list(directory = &quot;https://orcid.org&quot;, userId = &quot;https://orcid.org/0000-0003-4703-1974&quot;)) doc &lt;- list(packageId = &quot;dataset-1&quot;, system = &quot;local&quot;, dataset = eml$dataset(title = &quot;A more robust valid EML dataset&quot;, creator = me, contact = me)) Because we have used the eml$dataset helper, all of the possible sub-elements have been populated in our EML document, allowing us to easily access and edit them using R autocomplete. Abstract We can use this to dive down into sub-elements and edit them. Let’s do the abstract. This is a simple element so we can just assign the value of the abstract to a character string. doc$dataset$abstract &lt;- &quot;A brief but comprehensive description of the who, what, where, when, why, and how of my dataset.&quot; Methods We can use the set_methods function to parse a markdown (or word) document and insert it into the methods section. This way of adding text is especially nice because it preserves formatting. doc$dataset$methods &lt;- set_methods(&quot;../files/methods.md&quot;) doc$dataset$methods Coverage The geographic and temporal coverage can be set using set_coverage. doc$dataset$coverage &lt;- set_coverage(beginDate = 2001, endDate = 2010, geographicDescription = &quot;Alaska, United States&quot;, westBoundingCoordinate = -179.9, eastBoundingCoordinate = -120, northBoundingCoordinate = 75, southBoundingCoordinate = 55) Data file: script Information about data files (or entity level information) can be added in child elements of the dataset element. Here we will use the element otherEntity (other options include spatialVector, spatialRaster, and dataTable) to represent the R script. First, some high level information. doc$dataset$otherEntity &lt;- eml$otherEntity(entityName = &quot;../files/datfiles_processing.R&quot;, entityDescription = &quot;Data processing script&quot;, entityType = &quot;application/R&quot;) We can use the set_physical helper to set more specific information about the file, like its size, delimiter, and checksum. This function automatically detects fundamental characteristics about the file if you give it a path to your file on your system. doc$dataset$otherEntity$physical &lt;- set_physical(&quot;../files/datfiles_processing.R&quot;) Data file: tabular Here we will use the element dataTable to describe the tabular data file. As before, we set the entityName, entityDescription, and the physical sections. doc$dataset$dataTable &lt;- eml$dataTable(entityName = &quot;../files/my-data.csv&quot;, entityDescription = &quot;Temperature data from in-situ loggers&quot;) doc$dataset$dataTable$physical &lt;- set_physical(&quot;../files/my-data.csv&quot;) Next, perhaps the most important part of metadata, but frequently the most difficult to document in a metadata standard: attribute level information. An attribute is a variable in your dataset. For tabular data, this is information about columns within data tables, critical to understanding what kind of information is actually in the table! The set_attributes function will take a data.frame that gives required attribute information. This data.frame contains rows corresponding to column names (or attributes) in the dataset, and columns: attributeName (any text) attributeDefinition (any text) unit (required for numeric data, use get_unitList() to see a list of standard units) numberType (required for numeric data, one of: real, natural, whole, integer) formatString (required for dateTime data) definition (required for textDomain data) Two other sub-elements, the domain and measurementScale, can be inferred from the col_classes argument to set_attributes. Let’s create our attributes data.frame. atts &lt;- data.frame(attributeName = c(&quot;time&quot;, &quot;temperature&quot;, &quot;site&quot;), attributeDefinition = c(&quot;time of measurement&quot;, &quot;measured temperature in degrees Celsius&quot;, &quot;site identifier&quot;), unit = c(NA, &quot;celsius&quot;, NA), numberType = c(NA, &quot;real&quot;, NA), formatString = c(&quot;HH:MM:SS&quot;, NA, NA), definition = c(NA, NA, &quot;site identifier&quot;)) We will then use this in our set_attributes function, along with the col_classes argument, to generate a complete attributeList. doc$dataset$dataTable$attributeList &lt;- set_attributes(attributes = atts, col_classes = c(&quot;Date&quot;, &quot;numeric&quot;, &quot;character&quot;)) As you might imagine, this can get VERY tedious with wide data tables. The function shiny_attributes calls an interactive table that can not only automatically detect and attempt to fill in attribute information from a data.frame, but also helps with on the fly validation. Note: this requires that the shinyjs package is installed. atts_shiny &lt;- shiny_attributes(data = read.csv(&quot;../files/my-data.csv&quot;)) This produces a data.frame that you can insert into set_attributes as above. Validating and writing the file Finally, we need to validate and write our file. eml_validate(doc) write_eml(doc, &quot;../files/complex_example.xml&quot;) Publish data to the Arctic Data Center test site Setup and Introduction Now let’s see how to use the dataone and datapack R packages to upload data to DataONE member repositories like the KNB Data Repository and the Arctic Data Center. The dataone package The dataone R package provides methods to enable R scripts to interact with DataONE to search for, download, upload and update data and metadata. The purpose of uploading data from R is to automate the repetitive tasks for large datasets with many files. For small datasets, the web submission form will certainly be simpler. The dataone package interacts with two major parts of the DataONE infrastructure: Coordinating Nodes (or cn) and Member Nodes (or mn). Coordinating nodes maintain a complete catalog of all data and provide the core DataONE services, including search and discovery. The cn that is exposed through search.dataone.org is called the production (or PROD) cn. Member Nodes expose their data and metadata through a common set of interfaces and services. The Arctic Data Center is an mn. To post data to the Arctic Data Center, we need to interact with both the coordinating and member nodes. In addition to the production cn, there are also several test coordinating node environments, and corresponding test member node environments. In this tutorial, we will be posting data to the test Arctic Data Center environment. The datapack package The datapack R package represents the set of files in a dataset as a datapack::DataPackage. This DataPackage is just a special R object class that is specified in the datapack package. Each object in that DataPackage is represented as a DataObject, another R object class specified by datapack. When you are publishing your dataset, ideally you aren’t only publishing a set of observations. There are many other artifacts of your research, such as scripts, derived data files, and derived figures, that should be archived in a data package. Each of the types of files shown in the workflow below should be considered a data object in your package, including the metadata file that describes the individual components of your workflow. Each of the files in the diagram above has a relationship with the other files, and using datapack you can describe these relationships explicitly and unambiguously using controlled vocabularies and conceptual frameworks. For example, we know that the fine “Image 1” was generated by “Mapping Script.” We also know that both “Image 1” and “Mapping Script” are described by the file “Metadata.” Both of these relationsips are represented in datapack using speficic ontologies. Using datapack we will create a DataPackage that represents a (very simple) scientific workflow and the data objects within it, where the relationships between those objects are explicitly described. Then, we will upload this DataPackage to a test version of the Arctic Data Center repository using dataone. Before uploading any data to a DataONE repository, you must login to get an authentication token, which is a character string used to identify yourself. This token can be retrieved by logging into the test repository and copying the token into your R session. Obtain an ORCID ORCID is a researcher identifier that provides a common way to link your researcher identity to your articles and data. An ORCID is to a researcher as a DOI is to a research article. To obtain an ORCID, register at https://orcid.org. Log in to the test repository and copy your token We will be using a test server, so login and retrieve your token at https://test.arcticdata.io. Once you are logged in, navigate to your Profile Settings, and locate the “Authentication Token” section, and then copy the token for R to your clipboard. Finally, paste the token into the R Console to register it as an option for this R session. You are now logged in. But note that you need to keep this token private; don’t paste it into scripts or check it into git, as it is just as sensitive as your password. Uploading A Package Using R with uploadDataPackage Datasets and metadata can be uploaded individually or as a collection. Such a collection, whether contained in local R objects or existing on a DataONE repository, will be informally referred to as a package. Load the libraries: library(dataone) library(datapack) First we need to create an R object that describes what coordinating and member nodes we will be uploading our dataset to. We do this with the dataone function D1Client (DataONE client). The first argument specifies the DataONE coordinating node (in this case a test node called STAGING) and the second specifices the member node. We’ll also create an object that only represents the member node, which is helpful for some functions. d1c &lt;- D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) mn &lt;- d1c@mn Next, we create a DataPackage as a container for our data, metadata, and scripts using the new function. This just creates an empty object with the class DataPackage dp &lt;- new(&quot;DataPackage&quot;) dp We then need to add a metadata file and data file to this package. First we generate some identifiers for the objects. We’ll use the uuid scheme for all of our objects. If we were uploading to production, you would likely want use an identifier with the doi scheme for your metadata document. data_id &lt;- generateIdentifier(mn, scheme = &quot;uuid&quot;) script_id &lt;- generateIdentifier(mn, scheme = &quot;uuid&quot;) metadata_id &lt;- generateIdentifier(mn, scheme = &quot;uuid&quot;) Now we need to modify our EML document to include these identifiers. This increases the accessibility of the files in our dataset. First read in the EML that we created earlier. doc &lt;- read_eml(&quot;files/complex_example.xml&quot;) Let’s replace the arbitrary packageId and system that we set in the example above to reflect the identifier we created for this package, and the system we are uploading the package to. doc$packageId &lt;- metadata_id doc$system &lt;- mn@identifier Now let’s add a distribution URL to the physical section of our entity information. All of the distribution URLs look something like this https://test.arcticdata.io/metacat/d1/mn/v2/object/urn:uuid:A0cc95b46-a318-4dd1-8f8a-0e6c280e1d95, where https://test.arcticdata.io/metacat/d1/mn/v2/ is the member node end point, and urn:uuid:A0cc95b46-a318-4dd1-8f8a-0e6c280e1d95 is the identifier of the object. We can easily construct this URL using the paste0 function, and insert it into the physical section of the dataTable element. # set url for csv doc$dataset$dataTable$physical$distribution$online$url &lt;- paste0(mn@endpoint, &quot;object/&quot;, data_id) # set url for script doc$dataset$otherEntity$physical$distribution$online$url &lt;- paste0(mn@endpoint, &quot;object/&quot;, script_id) write_eml(doc, &quot;files/complex_example.xml&quot;) Now we have a full metadata document ready to be uploaded. We now need to add our files to the DataPackage. First, let’s create a new DataObject, which is another object class specific to datapack. Our metadata file, data file, and script will all need to be added as a DataObject. Remember that all files in a package are considered data objects, not just the ones that we you traditionally think of as being “data”. The format argument specifies the type of file, and should match one of the list of DataONE formatIds (listed in the Id field) here. # Add the metadata document to the package metadataObj &lt;- new(&quot;DataObject&quot;, id = metadata_id, format =&quot;eml://ecoinformatics.org/eml-2.1.1&quot;, filename = &quot;files/complex_example.xml&quot;) After creating the DataObject that represents the metadata file, we add it to the data package using addMember. dp &lt;- addMember(dp, metadataObj) dp We add the data file and the script similarly. The only difference is in the addMember function we have to set the mo (metadata object) argument equal to the DataObject that represents the metadata file. Adding our csv file to the package this way not only adds the file to the data package, but it also specifies that the csv is described by the metadata. # Add our data file to the package sourceObj &lt;- new(&quot;DataObject&quot;, id = data_id, format = &quot;text/csv&quot;, filename = &quot;files/my-data.csv&quot;) dp &lt;- addMember(dp, sourceObj, mo = metadataObj) dp Next we add our script in the same way. # Add our script to the package scriptObj &lt;- new(&quot;DataObject&quot;, id = script_id, format = &quot;application/R&quot;, filename = &quot;files/datfiles_processing.R&quot;) dp &lt;- addMember(dp, scriptObj, mo = metadataObj) dp You can also specify other information about the relationships between files in your data package by adding provenance information. Here, we specify that the R script (program) uses the csv (sources) by including them as the specified arguments in the describeWorkflow function. dp &lt;- describeWorkflow(dp, sources = sourceObj, program = scriptObj) dp Each object in a data package has an access policy. There are three levels of access you can grant either to individual files, or the package as a whole. read: the ability to view when published write: the ablity to edit after publishing changePermission: the ability to grant other people read, write, or changePermission access Here, I give my colleague (via his ORCID) “read” and “write” permission to my entire data package using addAccessRule. dp &lt;- addAccessRule(dp, subject = &quot;http://orcid.org/0000-0003-0077-4738&quot;, permission = c(&quot;read&quot;,&quot;write&quot;), getIdentifiers(dp)) Finally, we upload the package to the test server for the Arctic Data Center using uploadDataPackage. This function takes as arguments d1c (the DataONE client which specifies which coordinating and member nodes to use), dp (the data package itself), whether you want the package to include public read access (public = TRUE). packageId &lt;- uploadDataPackage(d1c, dp, public = TRUE) You can now search for and view the package at https://test.arcticdata.io: "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
