[["index.html", "Open Science Synthesis for the Delta Science Program: Week 3 Open Science Synthesis for the Delta Science Program: Week 3 0.1 Schedule", " Open Science Synthesis for the Delta Science Program: Week 3 November 1-5, 2021 Open Science Synthesis for the Delta Science Program: Week 3 This book is for week 3 of 3, one-week facilitated synthesis and training events for Delta researchers that will revolve around scientific computing and scientific software for reproducible science. Week 3 will focus on publishing data, data visualization and results communication, and will revisit reproducible workflows. 0.1 Schedule 0.1.1 Code of Conduct Please note that by participating in an NCEAS activity you agree to abide by our Code of Conduct 0.1.1.1 The Power of Open To facilitate a lively and interactive learning environment, we’ll be calling on folks to share their code and to answer various questions posed by the instructor. It’s completely okay to say “Pass” or “I Don’t Know” - this is a supportive learning environment and we will all learn from each other. The instructors will be able to see your code as you go to help you if you get stuck, and the lead instructor may share participants’ code to show a successful example or illustrate a teaching moment. 0.1.2 About this book These written materials are the result of a continuous effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials. This work is licensed under a Creative Commons Attribution 4.0 International License. Citation: Jones, Matthew B., Amber E. Budden, Bryce Mecum, S. Jeanette Clark, Julien Brun, Julie Lowndes, Erin McLean, Jessica S. Guo, David S. LeBauer. 2021. Reproducible Research Techniques for Synthesis. NCEAS Learning Hub. Additional contributors: Ben Bolker, Stephanie Hampton, Samanta Katz, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Leah Wasser. "],["session-1-time-series-and-forecasting.html", "1 Session 1: Time Series and Forecasting 1.1 Learning Objectives 1.2 Linear regression, with time as a predictor 1.3 Time series ‘feature extraction’ 1.4 Time Series objects in R 1.5 Ecological Forecasting 1.6 Your Turn - Some Actual Data! 1.7 Growth Curves 1.8 References", " 1 Session 1: Time Series and Forecasting 1.1 Learning Objectives These lessons are the equivalent of a city bus tour. They are a high level tour of some useful functions. There are plenty of more detailed lessons on the internet. I would recommend starting with the “Forecasting Principles and Practice v2” by Rob Hyndman who wrote the forecast package. Today we will take a tour with the predict function in R, as well as some advanced time series statistics that are provided by the forecast package. In this session you will learn how to Simulate data and test if your model is working as expected Use linear regression to fit and predict time series Visualize data Decompose a time series into seasonal, trend, and error a note on the forecast package Hydnman and others are creating the successor to the forecast package called ‘feasts’. The next version of the text uses this package https://otexts.com/fpp3/. 1.2 Linear regression, with time as a predictor Both the simulation of data and the fitting of the model here is a general approach. It is demonstrated with a simple regression model of increasing \\(Y\\) as a funciton of time. But simulating data can be very useful in understanding your data, your model, and the about statistics in general. First, lets define our data model. We want to fit. simulate a time series of one observations on each of ten subsequent dates: Our data model is effectively \\[ Y_{\\textrm{linear}}=1+2t +\\epsilon\\\\ \\epsilon\\sim N(0,1) \\] Where y is the mass of your favorite organism. They are born on day 0 with 1 g and growth on average is 2 grams per day. library(forecast) ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union library(broom) set.seed(103) days &lt;- 0:9 mass &lt;- vector(length = 10) for(t in seq_along(days)){ mass[t] &lt;- 1 + 2 * days[t] + rnorm(1, 0, 1) } linear_data &lt;- data.frame(day = days, mass = mass) plot(mass~day, data = linear_data, ylab = &#39;mass (g)&#39;, ylim = c(1, 20), xlim = c(0, 15)) Now, we can fit a linear regression, and estimate the parameters. Did the model find the ‘right’ values? mod_linear &lt;- lm(mass ~ 1 + days, data = linear_data) tidy(mod_linear) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.316 0.560 0.565 0.588 ## 2 days 2.07 0.105 19.8 0.0000000447 Did we recover the correct parameters from the simulated data? Don’t forget to check assumptions! plot(lm) mod_linear ## ## Call: ## lm(formula = mass ~ 1 + days, data = linear_data) ## ## Coefficients: ## (Intercept) days ## 0.3162 2.0740 ## you can look at model statistics summary(mod_linear) ## ## Call: ## lm(formula = mass ~ 1 + days, data = linear_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.47732 -0.56699 0.04552 0.57197 1.35582 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.3162 0.5601 0.565 0.588 ## days 2.0740 0.1049 19.768 4.47e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.953 on 8 degrees of freedom ## Multiple R-squared: 0.9799, Adjusted R-squared: 0.9774 ## F-statistic: 390.8 on 1 and 8 DF, p-value: 4.466e-08 ## don&#39;t forget to check assumptions, etc ## this is helpful even when you simulate and you know ## that the data generating process is valid; get an idea of what is &#39;okay&#39; # plot(mod_linear) 1.3 Time series ‘feature extraction’ We’ve already extracted features from our time series. What are ‘features’? These are properties underlying the data. In this case, the information in our time series has been condensed into a slope and intercept. There are a lot of other mathematical functions that can be fit to your data, and properties of these functions that can be extracted, and used in subsequent analyses. Bolker (2007) provides a ‘bestiary of functions’ in chapter 3 of his book “Ecological Models and Data in R”. It provides some biological meanings and useful contexts for a variety of linear and non-linear, continuous and discontinuous functions. This is a common and very useful way of converting a time series into features. For an example of Bayesian logistic regression using JAGS, see the work done by Jessica Guo and me: https://github.com/genophenoenvo/JAGS-logistic-growth. Dietze’s Ecological forecasting course shows how to simulate a logistic growth curve using Monte Carlo methods. https://github.com/EcoForecast/EF_Activities/blob/master/Exercise_02_Logistic.Rmd There are a huge number of other methods for ‘dimensionality reduction’. Principal components analysis is an important one. Another is taking a full resolution image and reducing the resolution to a few pixels square, and everything in between. In the ‘machine learning’ world a wide range of methods are used in a process that they call ‘encoding’ . 1.3.1 The ‘predict’ function: forecasting with a fitted regression model The predict function in R has a variety of uses. Last week we learned how to use it to impute missing data values. This week, we will learn how to use it to predict the next time points in our time series. First step, lets predict what is the expected value on the subsequent few days. This is using the fitted slope and intercept to plot a single value on the regression line for each day. We will plot the data in black, the new points in red, and the regression line. newdays &lt;- 11:15 newdat &lt;- data.frame(days = newdays) preds &lt;- predict(mod_linear, newdata = newdat) plot(mass ~ day, data = linear_data, ylab = &#39;mass (g)&#39;, ylim = c(1, 30), xlim = c(0, 15)) + points(newdays, preds, col = &#39;red&#39;) + abline(coef(mod_linear)) + abline(confint(mod_linear)[,1], lty = 2) + abline(confint(mod_linear)[,2], lty = 2) ## integer(0) Now, we have predicted the next five days of growth! 1.4 Time Series objects in R R has a specific type of time series ‘object’. that is analogous to a ‘data.frame’, but special. It is called a time series object - ‘ts’ (or ‘mts’ for a multivariate time series). See ?ts Lets create time series objects so that we can use some of the basic functions for time series analysis. How to make a time series object. Last week when we learned about the imputeTS package, we skipped over what we actually did to make a time series object. We set the start and end by year and index, and then either a time step ‘deltat’ or frequency. Values of frequency (per year): - daily data: 365 - monthly: 12 - hourly: 365*24 - etc Lets make some sample monthly time series for three cases for white noise: \\(Y=\\epsilon\\) with autocorrelation with a 1 month lag \\(Y_t=\\frac{Y_{t-1}+Y_{t}}{2}\\) with autocorrelation and a trend \\(Y_t=\\frac{Y_{t-1}+Y_{t}}{2} + \\frac{t}{48}+\\epsilon\\) with seasonal patterns \\(Y_t=\\sin(\\frac{2\\pi t}{12})+\\epsilon\\) with seasonal patterns and a trend \\(Y_t=\\sin(\\frac{2\\pi t}{12})+\\frac{t}{48} + \\epsilon\\) Where \\(t\\) is the time step (in units of months). The second equation takes the average of the last time step and the current one (a moving window of size 2). The third equation adds a trend - every month the value increases by 1/120. set.seed(210) months &lt;- 1:240 noise &lt;- rnorm(length(months)) lag &lt;- vector() for(t in 1:length(months)){ if(t == 1){ lag[t] &lt;- rnorm(1) } else { lag[t] &lt;- (lag[t-1] + noise[t]) / 2 } } lag_trend &lt;- lag + months / 48 seasonal &lt;- 2*sin(2*pi*months/12) + noise seasonal_trend &lt;- seasonal + months / 48 Now lets create the multivariate time series object: all &lt;- ts(data = data.frame(noise, lag, lag_trend, seasonal, seasonal_trend), frequency = 12) plot(all) tsdisplay(all) Lets look at some basic statistics: which of these have a lag? This plot will show the correlation between \\(Y_t\\) and \\(Y_{t-\\textrm{lag}}\\) lag.plot(all, set.lags = c(1, 3, 6, 9)) 1.4.1 Autocorrelation plots acf(all[,&#39;noise&#39;], xlab = &#39;Lag (years)&#39;) acf(all[,&#39;lag&#39;], xlab = &#39;Lag (years)&#39;) acf(all[,&#39;seasonal&#39;], xlab = &#39;Lag (years)&#39;) ### Time Series Decomposition We want to look at a time series in terms of its components. R has a number of handy functions for basic time series analysis. Lets take a look. Lets look at some of the basic components of a time series seasonal patterns trend residuals dec &lt;- decompose(all[,&#39;seasonal_trend&#39;]) plot(dec) dec_df &lt;- data.frame(trend = dec$trend, month = months) dec_df &lt;- dec_df[!is.na(dec_df),] tidy(lm(trend ~ month, data = dec_df)) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.101 0.0345 2.92 3.85e- 3 ## 2 month 0.0197 0.000251 78.5 6.10e-166 1.4.2 Seasonal Trend with Loess smoothing (STL) decomposition The seasonal component of the decomposed time series is very regular. The acf() function creates a seasonal component using the means. Lets look at a more sophisticated model - the seasonal trend with local (Loess) smoothing If we are interested in a longer term trend, lets use the Seasonal trend w/ local smoothing Loess (STL) to smooth over a few years. trend window &gt; seasonal window has some rule of thumbs for estimating the parameters seasonal_stl &lt;- stl(all[,&#39;seasonal_trend&#39;], s.window = 6) plot(seasonal_stl) ## note how you can access each component of the decomposed time series ## plot(seasonal_stl$time.series[,c(&#39;trend&#39;, &#39;seasonal&#39;, &#39;remainder&#39;)]) 1.4.3 Now we can analyze the trend tmp &lt;- data.frame(month = months, trend = seasonal_stl$time.series[,&#39;trend&#39;]) #plot(tmp$month, tmp$trend) fit &lt;- lm(trend ~ month, data = tmp) coef(fit) ## (Intercept) month ## 0.10136347 0.01981641 tidy(fit) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.101 0.0277 3.66 3.14e- 4 ## 2 month 0.0198 0.000199 99.4 8.74e-196 z &lt;- rnorm(10000, 0.019, 0.0002208) What does that coefficient ‘month’ mean? Month is the rate of change per month. Check the equation above that we used to generate the time series. What is the slope of the trend that we added? 1.4.4 Some Time Series Model Acronyms ARIMA Models You will often see time series models specified as ARIMA(p,d,q) AR: AutoRegressive time series I: Integrated MA: Moving Average smoothing p number of lags d degree of differencing q size of moving average window The forecast::auto.arima function automatically fits these parameters. See https://otexts.com/fpp2/arima-r.html for an explanation of how this function works. STL Seasonal Seasonal Decomposition of Time Series by Loess. Extracts trend, seasonal, and locally smoothed moving average (described above). ETS Exponential smoothing state space model. 1.4.5 State Space Models Latent variable - this is the process of interest. Not directly measured, but measured with an ‘observation model’ a.k.a. whatever is used to actually measure - often boils down to changes in an electric current that capture a property of interest. It could be a model of a pendulum or similar dynamical system. https://upload.wikimedia.org/wikipedia/commons/2/24/Oscillating_pendulum.gif Ruryk, CC BY-SA 3.0, via Wikimedia Commons Imagine you are interested in ‘growth rate’. Then you measure the size (how? mass? length?…) of your tree, fish, or other organism of interest. Yesterday, today, and tomorrow. You can then estimate the growth rate by these differences. In turn, the growth rate could also be controlled by time, temperature, resource availability. These might each have its own latent and observed variables. State-space models State space models are models of how the state of a system changes over time. A state-space They provide a series of equations that represent how a system evolves in time. These models are widely used in the study of complex systems. For example, a model of population dynamics that accounts for growth, reproduction, and death, geophysical at a particular place. Rocket scientists and robotics engineers, car manufacturers and video game designers all use complex models of a system that function in this way. Such a model can take parameters for equations that control the the dynamics of the system (e.g ‘growth rate’). you know the state of a system, and how the system changes in time, you can use a state-space framework. once it represents the current state, it can be represent the state of a system. Conceptually, they are a way of modeling time series when you have a model of the ‘process’ or mechanism that moves the system state from \\(t\\) to \\(t+1\\). The ‘state’ of the system may be partially unobserved. But we can infer these states based on observations. The concept of a ‘data generating process’ is an important concept when modeling, and when synthesizing data from many different locations. A ‘data generating process’ includes both the system being studied and the tools used to observe. It is common to hear people refer to data as if it is the truth. But data can only represent an incomplete view of the system itself. In the end both data and models are representations of a system. Consider satellite imagery, radiative transfer models. 1.5 Ecological Forecasting Like time series, forecasting is a very large area of research. Ecological forecasting is an emerging discipline, and it covers both basic research and the application of ecological understanding to applications. Forecasts can help provide insights into the future state of a system as well as provide guidance on management scenarios. That is exactly what it sounds like you need to do! Some examples of ecological forecasting problems: Where is the hurricane going to end up? How much carbon can the land store? How will flooding affect delta planton and fish populations? Predict the potential yield of different crops under future climates Predict forest green up and senescence Key references include Clark et al 2001 and Dietze et al 2018 below. The Ecological Forecasting Initiative (EFI, ecoforecast.org) is a Research Coordination Network that you can join to learn and contribute to the development of this field. 1.5.1 Forecasting Challenges There are a variety of forecasting challenges. These can be found on sites like Kaggle. Lets look at a few EFI NEON forecasting Challenge phenology, net ecosystem exchange, beetle abundance, water temperature and dissolved oxygen Kaggle: Predict end of season Sorghum biomass from photograps HiveMind / Agrimetrics UK Wheat yield forecast market Yield 21 A note on challenges. Framing a problem as a challenge is a great way to engage the machine learning world. And these challenges provide a way to engage other communities, and lower the barrier to entry and more level playing field than many scientific pursuits. For challenges like those on Kaggle and the ones run by EFI, the best model wins. 1.5.2 The forecasting toolbox As we discussed last week, there are some simple or ‘naiive’ ways to forecast. These approaches are useful, sometimes because they perform well and other times because they provide a reasonable null hypothesis. Some use the same methods that we used to interpolate. The forecast package has a lot of handy functions for time series data. Lets start with the moving average. ts_fit &lt;- tslm(all[,&#39;seasonal_trend&#39;] ~ trend + season, data = all) summary(ts_fit) ## ## Call: ## tslm(formula = all[, &quot;seasonal_trend&quot;] ~ trend + season, data = all) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.38078 -0.75775 0.00508 0.66206 3.07359 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.1748074 0.2541379 4.623 6.36e-06 *** ## trend 0.0198631 0.0009559 20.779 &lt; 2e-16 *** ## season2 0.4392801 0.3240430 1.356 0.176566 ## season3 0.7756155 0.3240472 2.394 0.017501 * ## season4 0.8870021 0.3240543 2.737 0.006687 ** ## season5 -0.2126591 0.3240642 -0.656 0.512344 ## season6 -1.1311337 0.3240769 -3.490 0.000579 *** ## season7 -2.1118427 0.3240924 -6.516 4.60e-10 *** ## season8 -2.9644339 0.3241107 -9.146 &lt; 2e-16 *** ## season9 -2.8423310 0.3241318 -8.769 4.33e-16 *** ## season10 -2.7612880 0.3241558 -8.518 2.25e-15 *** ## season11 -1.8829512 0.3241826 -5.808 2.12e-08 *** ## season12 -1.1995549 0.3242122 -3.700 0.000271 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.025 on 227 degrees of freedom ## Multiple R-squared: 0.7859, Adjusted R-squared: 0.7746 ## F-statistic: 69.44 on 12 and 227 DF, p-value: &lt; 2.2e-16 plot(forecast(ts_fit, h = 20)) Now if you aren’t careful, it can automagically fit some fancy model. In this case, the STL+ETS(A,A,N). Is a combination of a Seasonal Trend with Loess and an Exponential smoothing state space model. ts_fit &lt;- stlf(all[,&#39;seasonal_trend&#39;]) plot(forecast(ts_fit)) summary(ts_fit) ## ## Forecast method: STL + ETS(A,A,N) ## ## Model Information: ## ETS(A,A,N) ## ## Call: ## ets(y = na.interp(x), model = etsmodel, allow.multiplicative.trend = allow.multiplicative.trend) ## ## Smoothing parameters: ## alpha = 0.0012 ## beta = 1e-04 ## ## Initial states: ## l = 0.2543 ## b = 0.0196 ## ## sigma: 0.9432 ## ## AIC AICc BIC ## 1293.233 1293.489 1310.636 ## ## Error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set -0.0212728 0.935272 0.7629923 -67.57334 137.2387 0.6723794 ## ACF1 ## Training set -0.1277774 ## ## Forecasts: ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## Jan 21 5.421475 4.212761 6.630189 3.5729060 7.270044 ## Feb 21 6.082280 4.873564 7.290995 4.2337091 7.930850 ## Mar 21 7.348352 6.139636 8.557069 5.4997796 9.196925 ## Apr 21 7.201374 5.992656 8.410092 5.3527988 9.049948 ## May 21 5.801197 4.592477 7.009916 3.9526193 7.649774 ## Jun 21 4.807066 3.598345 6.015788 2.9584861 6.655646 ## Jul 21 3.655230 2.446506 4.863953 1.8066464 5.503813 ## Aug 21 2.825036 1.616310 4.033761 0.9764489 4.673622 ## Sep 21 3.036527 1.827798 4.245255 1.1879362 4.885117 ## Oct 21 3.037751 1.829020 4.246482 1.1891561 4.886346 ## Nov 21 3.905910 2.697176 5.114644 2.0573106 5.754509 ## Dec 21 5.085809 3.877071 6.294546 3.2372042 6.934413 ## Jan 22 5.650127 4.441386 6.858868 3.8015168 7.498737 ## Feb 22 6.310932 5.102187 7.519677 4.4623156 8.159547 ## Mar 22 7.577004 6.368255 8.785753 5.7283817 9.425626 ## Apr 22 7.430025 6.221272 8.638779 5.5813961 9.278655 ## May 22 6.029848 4.821090 7.238607 4.1812116 7.878485 ## Jun 22 5.035718 3.826954 6.244482 3.1870732 6.884363 ## Jul 22 3.883881 2.675112 5.092651 2.0352281 5.732535 ## Aug 22 3.053687 1.844912 4.262463 1.2050249 4.902350 ## Sep 22 3.265178 2.056397 4.473960 1.4165064 5.113851 ## Oct 22 3.266403 2.057614 4.475191 1.4177202 5.115085 ## Nov 22 4.134562 2.925766 5.343357 2.2858684 5.983255 ## Dec 22 5.314460 4.105657 6.523264 3.4657555 7.163165 plot(forecast(ts_fit, h = 20)) 1.6 Your Turn - Some Actual Data! 1.6.1 Example 1: A twenty year history of weather in Maricopa, AZ We looked at this lastweek. These are daily statistics from ‘DayMet’. Daymet isn’t actually observed data - it is ‘imputed’ data. So, it is available for everywhere in the continental US from 1980 on a 1km grid. We will also use it in the model analysis lesson to compare this imputed ‘model’ data with ground truth observations. You can learn more about it here: https://daymet.ornl.gov/ and Thornton et al 2021. While we have a sample of the dataset in the lesson repository for a particular site, it is useful to know how to access the data for your site. Just change the lat, lon, start and end times to look at your favorite site! library(daymetr) mac_daymet_list &lt;- download_daymet(site = &quot;Maricopa Agricultural Center&quot;, lat = 33.068941, lon = -111.972244, start = 2000, end = 2020, internal = TRUE) # rename variables, create a date column mac_daymet &lt;- mac_daymet_list$data %&gt;% transmute(date = ymd(paste0(year, &#39;01-01&#39;))+ days(yday) -1, precip = prcp..mm.day., tmax = tmax..deg.c., tmin = tmin..deg.c., tmean = (tmax + tmin) / 2, trange = tmax - tmin, srad = srad..W.m.2., vpd = vp..Pa.) readr::write_csv(mac_daymet, file = &#39;../data/mac_daymet.csv&#39;) Lets read in and look at the data mac_daymet &lt;- readr::read_csv(&#39;../data/mac_daymet.csv&#39;) %&gt;% select(date, precip, tmean, srad, vpd) ## Rows: 7665 Columns: 8 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (7): precip, tmax, tmin, tmean, trange, srad, vpd ## date (1): date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(mac_daymet) ## # A tibble: 6 × 5 ## date precip tmean srad vpd ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2000-01-01 0 12.2 203. 936. ## 2 2000-01-02 0 8.24 225. 680. ## 3 2000-01-03 0 6.72 284. 504. ## 4 2000-01-04 0 8.92 287. 582. ## 5 2000-01-05 0 8.86 292. 566. ## 6 2000-01-06 0 8.45 288. 566. tmean.ts &lt;- ts(mac_daymet$tmean, start = c(2000, 1), end = c(2020, 365), deltat = 1/365) mac_ts &lt;- ts(mac_daymet, start = c(2000, 1), end = c(2020, 365), deltat = 1/365) Lets take a look plot(tmean.ts, ylab = &quot;Daily mean T&quot;, xlab = &quot;Year&quot;) lag.plot(tmean.ts, set.lags = c(1, 10, 100, 180, 360)) 1.6.2 Autocorrelation The lag plot shows the correlation between each point and the points at t+1. We can see that the interpolation between every other point. Lets get some monthly temperature data and start working with that. tmean_mo &lt;- mac_daymet %&gt;% mutate(year = year(date), month = month(date)) %&gt;% group_by(year, month) %&gt;% summarise(tmean = mean(tmean), .groups = &#39;keep&#39;) %&gt;% ungroup() %&gt;% select(tmean) tmean.mo.ts &lt;- ts(tmean_mo, start = c(2000, 1), end = c(2020, 12), frequency = 12) lag.plot(tmean.mo.ts, lags = 12) plot(acf(tmean.mo.ts)) Your turn - decompose, plot, look at a lag plot, acf, fit using auto.arima. which of the other variables are similar to temperature? Which are most different? vpd_mo &lt;- mac_daymet %&gt;% mutate(year = year(date), month = month(date)) %&gt;% group_by(year, month) %&gt;% summarise(vpd = mean(vpd), .groups = &#39;keep&#39;) %&gt;% ungroup() %&gt;% select(vpd) lag.plot(vpd_mo, lags = 12) acf(vpd_mo) ccf(tmean_mo, vpd_mo) ma_tmean &lt;- ma(tmean.mo.ts, order = 13, centre = TRUE) plot(ma_tmean, xlab = &#39;Year&#39;, ylab = &#39;trend&#39;) plot(tmean.mo.ts) + lines(ma_tmean) ## integer(0) plot(tmean.mo.ts - ma_tmean) plot(ma_tmean) acf(tmean.ts, lag.max = 180) # What does this mean? lag.plot(tmean.ts) Your Turn: Plot a few of the other variables. How do the seasonal patterns and trends compare? precip.ts &lt;- ts(mac_daymet$precip, start = c(2000, 1), end = c(2020, 365), deltat = 1/365) lag.plot(precip.ts) vpd.ts &lt;- ts(mac_daymet$vpd, start = c(2000, 1), end = c(2020, 365), deltat = 1/365) lag.plot(vpd.ts, ) all.ts &lt;- ts(mac_daymet, start = c(2000, 1), end = c(2020, 365), deltat = 1/365) lag.plot(all.ts) ccf(vpd.ts, precip.ts) ccf(precip.ts, tmean.ts) https://cals.arizona.edu/azmet/06.htm https://cals.arizona.edu/azmet/raw2003.htm head(mac_daymet) ## # A tibble: 6 × 5 ## date precip tmean srad vpd ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2000-01-01 0 12.2 203. 936. ## 2 2000-01-02 0 8.24 225. 680. ## 3 2000-01-03 0 6.72 284. 504. ## 4 2000-01-04 0 8.92 287. 582. ## 5 2000-01-05 0 8.86 292. 566. ## 6 2000-01-06 0 8.45 288. 566. mac_azmet &lt;- readr::read_csv(&#39;https://cals.arizona.edu/azmet/data/0620rd.txt&#39;, col_select = 1:4, col_names = c(&#39;year&#39;, &#39;day&#39;, &#39;hour&#39;, &#39;air_temp&#39;)) %&gt;% mutate(doy = day + hour/24) ## Rows: 366 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): year, day, hour, air_temp ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. a &lt;- acf(mac_azmet$air_temp) 1.7 Growth Curves This is from the NEON Ecological Forecasting Initiative Challenge library(ggplot2) library(tidyr) gcc &lt;- readr::read_csv(&#39;https://data.ecoforecast.org/targets/phenology/phenology-targets.csv.gz&#39;) ## Rows: 14288 Columns: 6 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): siteID ## dbl (4): gcc_90, rcc_90, gcc_sd, rcc_sd ## date (1): time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. gcc_wide &lt;- gcc %&gt;% dplyr::select(time, siteID, gcc_90) %&gt;% pivot_wider(id_cols = time, names_from = siteID, values_from = gcc_90) head(gcc_wide) ## # A tibble: 6 × 9 ## time HARV BART SCBI STEI UKFS GRSM DELA CLBJ ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2016-12-13 0.329 NA 0.325 NA NA NA NA NA ## 2 2016-12-14 0.328 0.344 0.325 NA NA NA NA NA ## 3 2016-12-15 0.330 0.346 0.326 NA NA NA NA NA ## 4 2016-12-16 0.329 0.342 0.326 NA NA NA 0.345 NA ## 5 2016-12-17 0.332 0.350 0.326 NA NA NA 0.349 NA ## 6 2016-12-18 0.332 0.348 0.327 NA NA NA 0.347 NA ggplot(gcc, aes(time, gcc_90)) + geom_line() + facet_wrap(~siteID) ## Warning: Removed 1 row(s) containing missing values (geom_path). gcc_ts &lt;- ts(gcc_wide$BART, frequency = 365) plot(gcc_ts) gcc_ts_interp &lt;- na.interp((gcc_ts)) plot(decompose(gcc_ts_interp)) plot(forecast(gcc_ts)) f &lt;- auto.arima(gcc_ts) plot(f) 1.8 References Daymet Thornton, M.M., R. Shrestha, Y. Wei, P.E. Thornton, S. Kao, and B.E. Wilson. 2020. Daymet: Daily Surface Weather Data on a 1-km Grid for North America, Version 4. ORNL DAAC, Oak Ridge, Tennessee, USA. https://doi.org/10.3334/ORNLDAAC/1840 Daymet: Daily Surface Weather Data on a 1-km Grid for North America, Version 4 https://doi.org/10.3334/ORNLDAAC/1840 Some of the material is based on the following courses and texts Ben Bolker 2007 Ecological Models and Data in R. Princeton university press. - The author makes a early version pre-print available on his website https://ms.mcmaster.ca/~bolker/emdbook/book.pdf Ethan White and Morgan Earnst Ecological Dynamics and Forecasting https://github.com/weecology/forecasting-course - Dietze, Michael. Ecological forecasting. Princeton University Press, 2017. - Dietze, Michael. Ecological forecasting. Course materials https://github.com/EcoForecast/EF_Activities Hyndman, R.J., &amp; Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. OTexts.com/fpp2. Accessed on 2021-10-31 Dietze, Michael C., et al. “Iterative near-term ecological forecasting: Needs, opportunities, and challenges.” Proceedings of the National Academy of Sciences 115.7 (2018): 1424-1432. https:doi.org/10.1073/pnas.1710231115 "],["session-2-collaborative-synthesis.html", "2 Session 2: Collaborative synthesis", " 2 Session 2: Collaborative synthesis "],["session-3-web-based-data-archival.html", "3 Session 3: Web based data archival 3.1 Best Practices: Data and Metadata 3.2 Data Documentation and Publishing", " 3 Session 3: Web based data archival 3.1 Best Practices: Data and Metadata 3.1.1 Learning Objectives In this lesson, you will learn: How to acheive practical reproducibility Some best practices for data and metadata management 3.1.2 Best Practices: Overview The data life cycle has 8 stages: Plan, Collect, Assure, Describe, Preserve, Discover, Integrate, and Analyze. In this section we will cover the following best practices that can help across all stages of the data life cycle: Organizing Data File Formats Large Data Packages Metadata Data Identifiers Provenance Licensing and Distribution Our goal, generally, is to operate within this lifecycle in ways that are FAIR The FAIR principles are principles that help guide research so that it is more reusable. 3.1.2.1 Organizing Data We’ll spend an entire lesson later on that’s dedicated to organizing your data in a tidy and effective manner, but first, let’s focus on the benefits on having “clean” data and complete metadata. Decreases errors from redundant updates Enforces data integrity Helps you and future researchers to handle large, complex datasets Enables powerful search filtering Much has been written on effective data management to enable reuse. The following two papers offer words of wisdom: Some simple guidelines for effective data management. Borer et al. 2009. Bulletin of the Ecological Society of America. Nine simple ways to make it easier to (re)use your data. White et al. 2013. Ideas in Ecology and Evolution 6. In brief, some of the best practices to follow are: Have scripts for all data manipulation that start with the uncorrected raw data file and clean the data programmatically before analysis. Design your tables to add rows, not columns. A column should be only one variable and a row should be only one observation. Include header lines in your tables Use non-proprietary file formats (ie, open source) with descriptive file names without spaces. Non-proprietary file formats are essential to ensure that your data can still be machine readable long into the future. Open formats include text files and binary formats such as NetCDF. Common switches: Microsoft Excel (.xlsx) files - export to text (.txt) or comma separated values (.csv) GIS files - export to ESRI shapefiles (.shp) MATLAB/IDL - export to NetCDF When you have or are going to generate large data packages (in the terabytes or larger), it’s important to establish a relationship with the data center early on. The data center can help come up with a strategy to tile data structures by subset, such as by spatial region, by temporal window, or by measured variable. They can also help with choosing an efficient tool to store the data (ie NetCDF or HDF), which is a compact data format that helps parallel read and write libraries of data. 3.1.2.2 Metadata Guidelines Metadata (data about data) is an important part of the data life cycle because it enables data reuse long after the original collection. Imagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files? The goal is to have enough information for the researcher to understand the data, interpret the data, and then re-use the data in another study. Another way to think about it is to answer the following questions with the documentation: What was measured? Who measured it? When was it measured? Where was it measured? How was it measured? How is the data structured? Why was the data collected? Who should get credit for this data (researcher AND funding agency)? How can this data be reused (licensing)? Bibliographic Details The details that will help your data be cited correctly are: a global identifier like a digital object identifier (DOI); a descriptive title that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data a descriptive abstract that serves as a brief overview off the specific contents and purpose of the data package funding information like the award number and the sponsor; the people and organizations like the creator of the dataset (ie who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset Discovery Details The details that will help your data be discovered correctly are: the geospatial coverage of the data, including the field and laboratory sampling locations, place names and precise coordinates; the temporal coverage of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to; the taxonomic coverage of the data, including what species were measured and what taxonomy standards and procedures were followed; as well as any other contextual information as needed. Interpretation Details The details that will help your data be interpreted correctly are: the collection methods for both field and laboratory data; the full experimental and project design as well as how the data in the dataset fits into the overall project; the processing methods for both field and laboratory samples IN FULL; all sample quality control procedures; the provenance information to support your analysis and modelling methods; information about the hardware and software used to process your data, including the make, model, and version; and the computing quality control procedures like any testing or code review. Data Structure and Contents Well constructed metadata also includes information about the data structure and contents. Everything needs a description: the data model, the data objects (like tables, images, matricies, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation. Variable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (such as 0 = not collected), and any missing values (such as 999 = NA). Not only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable). For example, if you were to search for the character string carbon dioxide flux in the general search box at the Arctic Data Center, not all relevant results will be shown due to varying vocabulary conventions (ie, CO2 flux instead of carbon dioxide flux) across disciplines — only datasets containing the exact words carbon dioxide flux are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search. Demonstrates a typical search for “carbon dioxide flux”, yielding 20 datasets. (right) Illustrates an annotated search for “carbon dioxide flux”, yielding 29 datasets. Note that if you were to interact with the site and explore the results of the figure on the right, the dataset in red of Figure 3 will not appear in the typical search for “carbon dioxide flux.” Rights and Attribution Correctly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like: the citation format to be used when giving credit for the data; the attribution expectations for the dataset; the reuse rights, which describe who may use the data and for what purpose; the redistribution rights, which describe who may copy and redistribute the metadata and the data; and the legal terms and conditions like how the data are licensed for reuse. So, how do you organize all this information? There are a number of metadata standards (think, templates) that you could use, including the Ecological Metadata Language (EML), Geospatial Metadata Standards like ISO 19115 and ISO 19139, the Biological Data Profile (BDP), Dublin Core, Darwin Core, PREMIS, the Metadata Encoding and Transmission Standard (METS), and the list goes on and on. The Arctic Data Center runs on EML. 3.1.3 Data Identifiers Many journals require a DOI - a digital object identifier - be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to. At the Arctic Data Center, we assign a DOI to each published dataset. But, sometimes datasets need to be updated. Each version of a dataset published with the Arctic Data Center has a unique identifier associated with it. Researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available. When there is a newer version available, that will be clearly marked on the original dataset page with a yellow banner indicating as such. Having the data identified in this manner allows us to accurately track the dataset usage metrics. The Arctic Data Center tracks the number of citations, the number of downloads, and the number of views of each dataset in the catalog. We filter out most views by internet bots and repeat views within a small time window in order to make these metrics COUNTER compliant. COUNTER is a standard that libraries and repositories use to provide users with consistent, credible, and comparable usage data. 3.1.4 Data Citation Researchers should get in the habit of citing the data that they use - even if it’s their own data! - in each publication that uses that data. The Arctic Data Center has taken multiple steps towards providing data citation information for all datasets we hold in our catalog, including a feature enabling dataset owners to directly register citations to their datasets. We recently implemented this “Register Citation” feature to allow researchers to register known citations to their datasets. Researchers may register a citation for any occasions where they know a certain publication uses or refers to a certain dataset, and the citation will be viewable on the dataset profile within 24 hours. To register a citation, navigate to the dataset using the DOI and click on the citations tab. Once there, this dialog box will pop up and you’ll be able to register the citation with us. Click that button and you’ll see a very simple form asking for the DOI of the paper and if the paper CITES the dataset (that is, the dataset is explicitly identified or linked to somewhere in the text or references) or USES the dataset (that is, uses the dataset but doesn’t formally cite it). We encourage you to make this part of your workflow, and for you to let your colleagues know about it too! 3.1.5 Provanance &amp; Preserving Computational Workflows While the Arctic Data Center, Knowledge Network for Biocomplexity, and similar repositories do focus on preserving data, we really set our sights much more broadly on preserving entire computational workflows that are instrumental to advances in science. A computational workflow represents the sequence of computational tasks that are performed from raw data acquisition through data quality control, integration, analysis, modeling, and visualization. In addition, these workflows are often not executed all at once, but rather are divided into multiple workflows, earch with its own purpose. For example, a data acquistion and cleaning workflow often creates a derived and integrated data product that is then picked up and used by multiple downstream analytical workflows that produce specific scientific findings. These workflows can each be archived as distinct data packages, with the output of the first workflow becoming the input of the second and subsequent workflows. In an effort to make data more reproducible, datasets also support provenance tracking. With provenance tracking, users of the Arctic Data Center can see exactly what datasets led to what product, using the particular script or workflow that the researcher used. This is a useful tool to make data more compliant with the FAIR principles. In addition to making data more reproducible, it is also useful for building on the work of others; you can produce similar visualizations for another location, for example, using the same code. RMarkdown itself can be used as a provenance tool, as well - by starting with the raw data and cleaning it programmatically, rather than manually, you preserve the steps that you went through and your workflow is reproducible. 3.2 Data Documentation and Publishing 3.2.1 Learning Objectives In this lesson, you will learn: About open data archives What science metadata is and how it can be used How data and code can be documented and published in open data archives 3.2.2 Data sharing and preservation 3.2.3 Data repositories: built for data (and code) GitHub is not an archival location Dedicated data repositories: KNB, Arctic Data Center, Zenodo, FigShare Rich metadata Archival in their mission Data papers, e.g., Scientific Data List of data repositories: http://re3data.org 3.2.4 Metadata Metadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where it was collected, and why it was collected. For consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the metadata for a sockeye salmon data set: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; That same metadata document can be converted to HTML format and displayed in a much more readable form on the web: https://knb.ecoinformatics.org/#view/doi:10.5063/F1F18WN4 And as you can see, the whole data set or its components can be downloaded and reused. Also note that the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data. 3.2.5 Structure of a data package Note that the data set above lists a collection of files that are contained within the data set. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document. These data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets an internal identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4. 3.2.6 DataONE Federation DataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses. DataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data form the dozens of members of DataONE, rather than visiting each of the currently 43 repositories one at a time. Publishing data from the web Each data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the Arctic Data Center, we provide some easy to use web forms for editing and submitting a data package. Let’s walk through a web submission to see what you might expect. Download the data to be used for the tutorial I’ve already uploaded the test data package, and so you can access the data here: https://dev.nceas.ucsb.edu/view/urn:uuid:0702cc63-4483-4af4-a218-531ccc59069f Grab both CSV files, and the R script, and store them in a convenient folder. Login via ORCID We will walk through web submission on https://demo.nceas.ucsb.edu, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button. When you sign in, you will be redirected to orcid.org, where you can either provide your existing ORCID credentials, or create a new account. ORCID provides multiple ways to login, including using your email address, institutional logins from many universities, and logins from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login. Create and submit the data set After signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. Required sections are listed with a red asterisk. Click Add Files to choose the data files for your package You can select multiple files at a time to efficiently upload many files. The files will upload showing a progress indicator. You can continue editing metadata while they upload. Enter Overview information This includes a descriptive title, abstract, and keywords. The title is the first way a potential user will get information about your dataset. It should be descriptive but succinct, lack acronyms, and include some indication of the temporal and geospatial coverage of the data. The abstract should be sufficently descriptive for a general scientific audience to understand your dataset at a high level. It should provide an overview of the scientific context/project/hypotheses, how this data package fits into the larger context, a synopsis of the experimental or sampling design, and a summary of the data contents. Keywords, while not required, can help increase the searchability of your dataset, particularly if they come from a semantically defined keyword thesaurus. Optionally, you can also enter funding information, including a funding number, which can help link multiple datasets collected under the same grant. Selecting a distribution license - either CC-0 or CC-BY is required. People Information Information about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people: Creators - all the people who should be in the citation for the dataset Contacts - one is required, but defaults to the first Creator if omitted Principal Investigators and any other that are relevant For each, please strive to provide their ORCID identifier, which helps link this dataset to their other scholarly works. 3.2.6.0.1 Temporal Information Add the temporal coverage of the data, which represents the time period to which data apply. Location Information The geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that a spatial search will be more precise. Note that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global. For example: Mission Canyon, Santa Barbara, California, USA Methods Methods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, include details of sampling methods, experimental design, quality assurance procedures, and computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data. Save a first version with Submit When finished, click the Submit Dataset button at the bottom. If there are errors or missing fields, they will be highlighted. Correct those, and then try submitting again. When you are successful, you should see a green banner with a link to the current dataset view. Click the X to close that banner, if you want to continue editing metadata. Success! File and variable level metadata The final major section of metadata concerns the structure and contents of your data files. In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure. For example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information. The Attributes tab is where you enter variable (aka attribute) information. In the case of tabular data (such as csv files) each column is an attribute, thus there should be one attribute defined for every column in your dataset. Attribute metadata includes: variable name (for programs) variable label (for display) variable definition (be specific) type of measurement units &amp; code definitions You’ll need to add these definitions for every variable (column) in the file. When done, click Done. Now the list of data files will show a green checkbox indicating that you have full described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes. Note that attribute information is not relevant for data files that do not contain variables, such as the R script in this example. Other examples of data files that might not need attributes are images, pdfs, and non-tabular text documents (such as readme files). The yellow circle in the editor indicates that attributes have not been filled out for a data file, and serves as a warning that they might be needed, depending on the file. After you get the green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes. Add workflow provenance Understanding the relationships between files in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, that are often then used in analysis and visualization code to produce final outputs. In DataONE, we support structured descriptions of these relationships, so one can see the flow of data from raw data to derived to outputs. You add provenance by navigating to the data table descriptions, and selecting the Add buttons to link the data and scripts that were used in your computational workflow. On the left side, select the Add circle to add an input data source to the filteredSpecies.csv file. This starts building the provenance graph to explain the origin and history of each data object. The linkage to the source dataset should appear. Then you can add the link to the source code that handled the conversion between the data files by clicking on Add arrow and selecting the R script: Select the R script and click “Done.” The diagram now shows the relationships among the data files and the R script, so click Submit to save another version of the package. Et voilà! A beatifully preserved data package! "],["session-4-programmatic-metadata-and-data-access.html", "4 Session 4: Programmatic metadata and data access 4.1 Reproducible Data Access 4.2 Programming Metadata and Data Publishing", " 4 Session 4: Programmatic metadata and data access 4.1 Reproducible Data Access ## ## Attaching package: &#39;contentid&#39; ## The following object is masked from &#39;package:pins&#39;: ## ## pin ## ## Attaching package: &#39;dataone&#39; ## The following object is masked from &#39;package:solrium&#39;: ## ## ping ## The following objects are masked from &#39;package:contentid&#39;: ## ## query, resolve 4.1.1 Learning Objectives In this lesson, you will learn: Why we strive for reproducible data access How content identifiers differ from DOIs How content identifiers make research more reproducible Ways to register and resolve content identifiers for unpublished data How content identifiers can resolve to published data sources 4.1.2 Barriers to data access Traditional ways of working with data – as files on a file system – limit the reproducibility of code to local compute environments. A typical R analysis file will load one or many data files from the local disk with code like this: delta_catch &lt;- readr::read_csv(&#39;/Users/jkresearcher/Projects/2018/Delta_Analysis/delta_catch.csv&#39;) delta_taxa &lt;- readr::read_csv(&#39;../../Delta_2021/delta_taxa.csv&#39;) delta_effort &lt;- readr::read_csv(&#39;delta_effort.csv&#39;) delta_sites &lt;- readr::read_csv(&#39;data/delta_sites.csv&#39;) Which of those file paths are the most portable? And which will run unmodified on both the original computer that they were written on, and on colleagues’ computers? In reality, none of them, in that they require that a specific data file be present in a specific location for the code to work properly, and these assumptions are rarely met and hard to maintain. Hardcoded paths like these are often spread deeply through the scripts that researchers write, and can become a surprise when they are encountered during execution. The Web partly solves this problem, because it allows code to access data that is located somewhere on the Internet with a web URI. For example, loading data from a web site can be much more portable than loading the equivalent data from a local computer. delta_sites_edi &lt;- &#39;https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&amp;entityid=6a82451e84be1fe82c9821f30ffc2d7d&#39; delta_sites &lt;- readr::read_csv(delta_sites_edi, show_col_types = FALSE) head(delta_sites) ## # A tibble: 6 × 4 ## MethodCode StationCode LatitudeLocation LongitudeLocation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BSEIN AL1 38.5 -122. ## 2 BSEIN AL2 38.5 -122. ## 3 BSEIN AL3 38.5 -122. ## 4 BSEIN AL4 38.5 -122. ## 5 BSEIN BL1 38.5 -122. ## 6 BSEIN BL2 38.4 -122. In theory, that code will work from anyone’s computer with an internet connection. But code that downloads data each and every time it is run is not particularly efficient, and will be prohibitive for all but the smallest datasets. A simple solution to this issue is to cache a local copy of the dataset, and only retrieve the original from the web when we don’t have a local copy. In this way, people running code or a script will download the data the first time their code is run, but use a local copy from thence forward. While this can be accomplished with some simple conditional logic in R, the pattern has been simplified using the pins package: delta_sites_edi &lt;- pins::pin(&#39;https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&amp;entityid=6a82451e84be1fe82c9821f30ffc2d7d&#39;) delta_sites &lt;- readr::read_csv(delta_sites_edi, show_col_types = FALSE) head(delta_sites) ## # A tibble: 6 × 4 ## MethodCode StationCode LatitudeLocation LongitudeLocation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BSEIN AL1 38.5 -122. ## 2 BSEIN AL2 38.5 -122. ## 3 BSEIN AL3 38.5 -122. ## 4 BSEIN AL4 38.5 -122. ## 5 BSEIN BL1 38.5 -122. ## 6 BSEIN BL2 38.4 -122. You’ll note that code takes longer the first time it is run, as the data file is downloaded only the first time. While this works well over the short term, abundant evidence shows that web URIs have short lifespan. Most URIs are defunct within a few years (e.g., see McCown et al. 2005). Only the most carefully curated web sites maintain the viability of their links for longer. And maintaining them for decade-long periods requires a focus on archival principles and dedicated staff to ensure that files and the URLs at which they are published remain accessible. This is precisely the role of archival data repositories like the Arctic Data Center, the KNB Data Repository, and the Environmental Data Initiative (EDI). Finally, no discussion of data access and persistence would be complete without discussing the use of Digital Object Identifiers (DOIs). DOIs have become the dominant means to create persistent links to academic articles, publications, and datasets. As authority-based identifiers, they work when an authority assigns a DOI name to a published work, and then ensures that the DOI name always redirects to the current web location of the resource. This is a lot of work, and there is no guarantees that the authorities will keep the links up-to-date. Journals, societies, and data repositories actively maintain the redirection between a DOI such as doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f and its current location on the EDI Repository. DOIs are commonly assigned to published datasets, and include the bibliographic metadata needed to properly cite and access the dataset. The challenge with DOIs as they are typically implemented is that they are usually assigned to a Dataset, which is a collection of digital objects that are composed to form the whole Dataset and that can be accessed individually or through an API. Typically, the metadata attached to DOIs does not include an enumeration of those digital objects or a clear mechanism to get to the actual data – rather, the DOI redirects to a dataset landing page that provides a human readable summary of the dataset, and often various types of links to find and eventually download the data. Despite advances in metadata interoperability from DCAT and schema.org/Dataset, there is currently no reliable way to universally go from a known DOI for a dataset to the list of current locations of all of the digital objects that compose that dataset. And yet, this is exactly what we need for portable and persistent data access. In addition, we frequently work with data that doesn’t have a DOI yet as we are creating derived data products for analysis locally before they are published. In conclusion, DOIs are a great approach to uniquely citing a dataset, but they do not provde a way for code to download specific, versioned digital objects from a dataset in a portable way that is persistent over many years. Thus, we want data access to be: Portable – works for anyone, even if they don’t already have the data Persistent – over long time periods Versioned – the specific version of data used is guaranteed to be returned Traceable – references to the provenance of data processing can be made Transparent – it is clear from the script what data were used Citable – it is clear how to properly cite the associated Dataset for attribution A powerful approach to solving these problems is by using content-based identifiers, rather than authority-based identifiers like DOIs. A content-based identifier, or contentid for short, can be calculated from the content in a data file itself, and is unique (within constraints) to that content. This is accomplished by using a “hash” function, which calculates a relatively short, fixed-length, and unique value for any given input. Hash functions form the basis of secure cryptography for secure messaging, and so there are many tools available for conveniently hashing data inputs. In our use case, we can use commonly available cryptographic hash functions (such as SHA-256 and SHA-1) to calculate a unique identifier for any given file. This gives us a unique identifier for the file which can be calculated by anyone with a copy of the file, and which can be registered as metadata in repositories that hold those files. Once we have a content identifier for an object, we can cache the file locally (just like we did with pins), and we can query repositories to see if they contain a copy of that file. Unlike authority-based identifiers, anyone who possesses a copy of a specific version of a data file can calculate the content-identifier for it, enabling us to build systems to find and access those data files across the repository landscape, and really across any web-accessible location. This has all of the power of cacheing and pinning web resources that we demonstrated before, but has the advantage that all holders of the content will use an identical identifier, avoiding broken links. And because content-identifiers can be determined locally before files are published on the web, we can use them in our scripts for data files that have yet to be published and yet know that they will work for others once the files have been published in a repository. 4.1.3 Persistent and portable data access for improving reproducibility We’ll be working with the following IEP dataset that is stored on EDI: Interagency Ecological Program (IEP), B. Schreier, B. Davis, and N. Ikemiyagi. 2019. Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018. ver 2. Environmental Data Initiative. https://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f (Accessed 2021-10-30). You can view this IEP dataset on DataONE: It also is visible from the EDI dataset landing page: It contains several data files, each of which is at a specific web URI, including: Fish catch and water quality Fish taxonomy Trap Effort Site locations delta_catch_edi &lt;- &#39;https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&amp;entityid=015e494911cf35c90089ced5a3127334&#39; delta_taxa_edi &lt;- &#39;https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&amp;entityid=0532048e856d4bd07deea11583b893dd&#39; delta_effort_edi &lt;- &#39;https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&amp;entityid=ace1ef25f940866865d24109b7250955&#39; delta_sites_edi &lt;- &#39;https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&amp;entityid=6a82451e84be1fe82c9821f30ffc2d7d&#39; 4.1.4 Storing a content identifier from a URI Use the contentid package for portable access to data. First, using a web URI, store the content identifier in your local content registry to cache it on your machine. The contentid::store() function retrieves the data from the URL, calculates a hash value for the content, and stores both in a local registry on your machine. This is very similar to the pins::pin function, but it uses the content identifier to point to the data. delta_catch_id &lt;- store(delta_catch_url) delta_taxa_id &lt;- store(delta_taxa_url) delta_effort_id &lt;- store(delta_effort_url) delta_sites_id &lt;- store(delta_sites_url) print(c(delta_catch_id=delta_catch_id, delta_taxa_id=delta_taxa_id, delta_effort_id=delta_effort_id, delta_sites_id=delta_sites_id)) ## delta_catch_id ## &quot;hash://sha256/e0dc10d7f36cfc5ac147956abb91f24f2b2df9f914a004bbf1e85e7d9cf52f41&quot; ## delta_taxa_id ## &quot;hash://sha256/1473de800f3c5577da077507fb006be816a9194ddd417b1b98836be92eaea49d&quot; ## delta_effort_id ## &quot;hash://sha256/f2433efab802f55fa28c4aab628f3d529f4fdaf530bbc5c3a67ab92b5e8f71b2&quot; ## delta_sites_id ## &quot;hash://sha256/e25498ffc0208c3ae0e31a23204b856a9309f32ced2c87c8abcdd6f5cef55a9b&quot; 4.1.5 Loading data from a content identifier Once you have the content identifier for a data file of interest (e.g., delta_catch_id in this case), you can call contentid::resolve() to find the locations where that data is stored. Because you already have it stored locally, it returns the file path to the file on your local registry, which you can then use to load the data into a data frame or process the data as needed. delta_catch_file &lt;- contentid::resolve(delta_catch_id, store = TRUE) delta_catch &lt;- readr::read_csv(delta_catch_file, show_col_types=FALSE) ## Warning: One or more parsing issues, see `problems()` for details head(delta_catch) ## # A tibble: 6 × 32 ## SampleDate SampleTime StationCode MethodCode GearID CommonName ## &lt;chr&gt; &lt;time&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1/16/1998 14:05 YB BSEIN SEIN50 Threadfin Shad ## 2 1/16/1998 15:00 YB BSEIN SEIN50 Inland Silverside ## 3 1/19/1998 12:17 YB FNET FKNT Threadfin Shad ## 4 1/19/1998 12:17 YB FNET FKNT Chinook Salmon ## 5 1/19/1998 11:00 YB PSEIN PSEIN100 Threadfin Shad ## 6 1/19/1998 11:30 YB PSEIN PSEIN100 Inland Silverside ## # … with 26 more variables: GeneticallyConfirmed &lt;chr&gt;, GeneticID &lt;lgl&gt;, ## # Field_ID_CommonName &lt;chr&gt;, ForkLength &lt;dbl&gt;, Count &lt;dbl&gt;, FishSex &lt;chr&gt;, ## # Race &lt;chr&gt;, MarkCode &lt;chr&gt;, CWTSample &lt;lgl&gt;, FishTagID &lt;chr&gt;, ## # StageCode &lt;chr&gt;, Dead &lt;chr&gt;, GearConditionCode &lt;dbl&gt;, WeatherCode &lt;chr&gt;, ## # WaterTemperature &lt;dbl&gt;, Secchi &lt;dbl&gt;, Conductivity &lt;dbl&gt;, SpCnd &lt;dbl&gt;, ## # DO &lt;dbl&gt;, pH &lt;dbl&gt;, Turbidity &lt;dbl&gt;, SubstrateCode &lt;chr&gt;, Tide &lt;chr&gt;, ## # VolumeSeined &lt;dbl&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt; # And two more examples delta_taxa_file &lt;- contentid::resolve(delta_taxa_id, store = TRUE) delta_taxa &lt;- readr::read_csv(delta_taxa_file, show_col_types=FALSE) delta_sites_file &lt;- contentid::resolve(delta_sites_id, store = TRUE) delta_sites &lt;- readr::read_csv(delta_sites_file, show_col_types = FALSE) This approach is portable, as anyone can run it without having the data local beforehand. This is because resolve(id) will store the data locally if someone does not already have a copy of the data in their local cache. This works by consulting a number of well-know registries to discover the location of the files, including DataONE, Hash Archive, Zenodo, and Software Heritage. This approach is persistent, because it pulls data from these persistent archives, and can take advantage of archive redundancy. For example, here is the list of locations that can be currently used to retrieve this data file: contentid::query_sources(delta_catch_id, cols=c(&quot;identifier&quot;, &quot;source&quot;, &quot;date&quot;, &quot;status&quot;, &quot;sha1&quot;, &quot;sha256&quot;)) ## identifier ## 1 hash://sha256/e0dc10d7f36cfc5ac147956abb91f24f2b2df9f914a004bbf1e85e7d9cf52f41 ## 2 hash://sha256/e0dc10d7f36cfc5ac147956abb91f24f2b2df9f914a004bbf1e85e7d9cf52f41 ## source ## 1 /Users/runner/Library/Application Support/org.R-project.R/R/contentid/sha256/e0/dc/e0dc10d7f36cfc5ac147956abb91f24f2b2df9f914a004bbf1e85e7d9cf52f41 ## 2 https://gmn.edirepository.org/mn/v2/object/https%3A%2F%2Fpasta.lternet.edu%2Fpackage%2Fdata%2Feml%2Fedi%2F233%2F2%2F015e494911cf35c90089ced5a3127334 ## date status sha1 ## 1 2021-11-04 17:20:00 200 &lt;NA&gt; ## 2 2020-04-12 23:05:21 200 sha1-MX1/hA5Zj1875zKrDgTwCoBRxtA= ## sha256 ## 1 hash://sha256/e0dc10d7f36cfc5ac147956abb91f24f2b2df9f914a004bbf1e85e7d9cf52f41 ## 2 sha256-4NwQ1/Ns/FrBR5Vqu5HyTyst+fkUoAS78ehefZz1L0E= # [BUG FILED](https://github.com/cboettig/contentid/issues/81): `query_sources should not return an error on inaccessible repos -- it should skip them and produce a warning, so that the local repo will still work when disconnected from the internet This approach is reproducible, as the exact version of the data will be used every time (even if someone changes the data at the original web URI, which would require a new content identifier). This approach is traceable because there is a reference in the code to the specific data used based on in its content identifier, and the only way to change which data are used is to change the checksum that is being referenced to a new version. 4.1.6 Storing and using local data identifiers Because not all data are already published, it is also helpful to being working with content identifiers before the data are made public on the web. This is easily accomplished by storing a file in the local regitry, and then using its content identifier during analysis. # Store a local file vostok_co2 &lt;- system.file(&quot;extdata&quot;, &quot;vostok.icecore.co2&quot;, package = &quot;contentid&quot;) id &lt;- store(vostok_co2) vostok &lt;- retrieve(id) co2 &lt;- read.table(vostok, col.names = c(&quot;depth&quot;, &quot;age_ice&quot;, &quot;age_air&quot;, &quot;co2&quot;), skip = 21) head(co2) ## depth age_ice age_air co2 ## 1 149.1 5679 2342 284.7 ## 2 173.1 6828 3634 272.8 ## 3 177.4 7043 3833 268.1 ## 4 228.6 9523 6220 262.2 ## 5 250.3 10579 7327 254.6 ## 6 266.0 11334 8113 259.6 Later, when the data file is published to a DataONE repository, or registered in Hash Archive, the script will work for other people trying to access it via contentid::resolve(). 4.1.7 Improvements to content identifiers The contentid package is a work in progress, but holds tremendous potential to provide portable, persistent data access. However, some issues still remain to be worked out. First, content identifiers are not well-linked to DOIs. DOIs are the current standard for citing data, and carry the citation metadata for data packages (such as author, title, publication year, etc.). But the contentid package currently lacks a mechanism to determine the citation for a file that is used in a script. Because data networks like DataONE maintain the association between each content-identifier that is registered there with the assigned DOI for the Dataset that the object is part of, it would be technically feasible to extend the contentid package to retrieve the citation for a given content identifier. For example, if the data is on DataONE, we could write a function to print the citation: get_citation(id) ## [1] &quot;H.A.J. Meijer,M.H. Pertuisot,J. van der Plicht. 2006. High Accuracy 14C Measurements for Atmospheric CO2 Samples from the South Pole and Point Barrow, Alaska by Accelerator Mass Spectrometry. ESS-DIVE. ess-dive-8c0779e4f3ed341-20180716T234812410 https://search.dataone.org/view/ess-dive-8c0779e4f3ed341-20180716T234812410&quot; And this can even be displayed inline in the markdown text. For example, this chapter used data from: H.A.J. Meijer,M.H. Pertuisot,J. van der Plicht. 2006. High Accuracy 14C Measurements for Atmospheric CO2 Samples from the South Pole and Point Barrow, Alaska by Accelerator Mass Spectrometry. ESS-DIVE. ess-dive-8c0779e4f3ed341-20180716T234812410 https://search.dataone.org/view/ess-dive-8c0779e4f3ed341-20180716T234812410 Second, content identifiers are opaque, and not particularly transparent. For many researchers, seeing the long hash value in a script will not be very meaningful. The contentid package needs mechanisms to transparently indicate what a content identifier refers to. Again, we have this information available dynamically through the metadata collated in networks like DataONE. Another useful extension to contentid would be to provide functions for displaying detailed metadata about a content identifier when it is available, as well as a mechanism to provide more human-readable aliases for the identifier. These types of ideas for extension are how the R ecosystem advances. Anyone who sees a better way to extend packages like contentid can do so, or even create their own packages to ecplore new approaches to reproducible data access. 4.2 Programming Metadata and Data Publishing 4.2.1 Learning Objectives In this lesson, you will learn: How to write standardized metadata in R How to publish data packages to the Arctic Data Center programmatically For datasets with a relatively small number of files that do not have many columns, the Arctic Data Center web form is the most efficient way to create metadata. However, for datasets that have many hundreds of files with a similar structure, or a large number of attributes, programmatic metadata creation may be more efficient. Creating metadata and publishing programmatically also allows for more a streamlined approach to updating datasets that have been published. By incorporating this documentation and publishing approach into your scientific workflow, you can improve the reproducibility and transparency of your work. 4.2.2 Creating Metadata 4.2.2.1 About Ecological Metadata Language (EML) EML, the metadata standard that the Arctic Data Center uses, looks like this: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; Earlier in this course we learned how to create an EML document like this using the Arctic Data Center web form. Now, we will learn how to create it using R. This can be especially useful to decrease the repetitiveness of metadata creation when you have many files with the same format, files with many attributes, or many data packages with a similar format. When you create metadata using the web form, the form creates valid metadata for you. Valid, structured metadata is what enables computers to predictably parse the information in a metadata document, enabling search, display, and even meta-analysis. When we create metadata in R, there aren’t as many user-friendly checks in place to ensure we create valid EML, so we need to understand the structure of the document more completely in order to make sure that it will be compatible with the Arctic Data Center. Let’s look at a simplified version of the example above: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; EML is written in XML (eXtensisble Markup Language). One of the key concepts in XML is the element. An XML element is everything that is encompassed by a start tag (&lt;...&gt;) and an end tag (&lt;/...&gt;). So, a simple element in the example above is &lt;surName&gt;Willette&lt;/surName&gt;. The name of this element is surName and the value is simple text, “Willette”. Each element in EML has a specific (case sensitive!) name and description that is specified in the schema. The description of the surName element is: “The surname field is used for the last name of the individual associated with the resource. This is typically the family name of an individual…” The EML schema specifies not only the names and descriptions of all EML elements, but also certain requirements, like which elements must be included, what type of values are allowed within elements, and how the elements are organized. An EML document is valid when it adheres to all of the requirements speficied in the EML schema. You’ll notice that some elements, rather than having a simple value, instead contain other elements that are nested within them. Let’s look at individualName. &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; This element is a collection of other elements (sometimes referred to as child elements). In this example, the individualName element has a givenName and a surName child element. We can check the requirements of any particular element by looking at the schema documents, which includes some helpful diagrams. The diagram in the individualName section of the schema looks like this: This shows that within individualName there are 3 possible child elements: salutation, givenName, and surName. The yellow circle icon with stacked papers tells you that the elements come in series, so you can include one or more of the child elements (as opposed to a switch symbol, which means that you choose one element from a list of options). The bold line tells you that surName is required, and the 0..inf indicates that you can include 0 or more salultation or givenName elements. So, to summarize, EML is the metadata standard used by the Arctic Data Center. It is written in XML, which consists of a series of nested elements. The element definitions, required elements, and structure are all defined by a schema. When you write EML, in order for it to be valid, your EML document must conform to the requirements given in the schema. 4.2.2.2 Metadata in R: a simple example Now, let’s learn how the EML package can help us create EML in R. First, load the EML package in R. library(EML) The EML package relies on named list structures to generate name-value pairs for elements. “Named list structures” may sound unfamiliar, but they aren’t dissimilar from data.frames. A data.frame is just a named list of vectors of the same length, where the name of the vector is the column name. In this section, we will use the familiar $ operator to dig down into the named list structure that we generate. To show how this works, let’s create the individualName element, and save it as an object called me. Remember the schema requirements - indvidualName has child elements salutation, givenName, and surName. At least surName is required. me &lt;- list(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;) me ## $givenName ## [1] &quot;Jeanette&quot; ## ## $surName ## [1] &quot;Clark&quot; So we have created the contents of an individualName element, with child elements givenName and surName, and assigned the values of those child elements to my name. This might look confusing, hard to remember, and if you aren’t intimitely familiar with the EML schema, you are probably feeling a little intimidated. Luckily the EML package has a set of helper list constructor functions which tell you what child elements can be used in a parent element. The helper functions have the format eml$elementName(). When combined with the RStudio autocomplete functionality, the whole process gets a lot easier! me &lt;- eml$individualName(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;) We can then use this object me, which represents the individualName element, and insert it into a larger EML document. At the top level of a complete EML document are the packageId and system elements. This is how your document can be uniquely identified within whatever system manages it. The packageId element typically contains the DOI (Digital Object Identifier) or other identifier for the dataset. Nested within the top level is the dataset element. All EML documents must have, at a minimum, a title, creator, and contact, in addition to the packageId and system. Let’s create a minimal valid EML dataset, with an arbitrary packageId and system. doc &lt;- list(packageId = &quot;dataset-1&quot;, system = &quot;local&quot;, dataset = eml$dataset(title = &quot;A minimial valid EML dataset&quot;, creator = eml$creator(individualName = me), contact = eml$contact(individualName = me))) Unlike the web editor, in R there is nothing stopping you from inserting arbitrary elements into your EML document. A critical step to creating EML is validating your EML document to make sure it conforms to the schema requirements. In R this can be done using the eml_validate function. eml_validate(doc) ## [1] TRUE ## attr(,&quot;errors&quot;) ## character(0) We can write our EML using write_eml. write_eml(doc, &quot;../files/simple_example.xml&quot;) ## NULL Here is what the written file looks like: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:stmml=&quot;http://www.xml-cml.org/schema/stmml-1.1&quot; packageId=&quot;id&quot; system=&quot;system&quot; xsi:schemaLocation=&quot;eml://ecoinformatics.org/eml-2.1.1/ eml.xsd&quot;&gt; &lt;dataset&gt; &lt;title&gt;A minimal valid EML dataset&lt;/title&gt; &lt;creator&gt; &lt;individualName&gt; &lt;givenName&gt;Jeanette&lt;/givenName&gt; &lt;surName&gt;Clark&lt;/surName&gt; &lt;/individualName&gt; &lt;/creator&gt; &lt;contact&gt; &lt;individualName&gt; &lt;givenName&gt;Jeanette&lt;/givenName&gt; &lt;surName&gt;Clark&lt;/surName&gt; &lt;/individualName&gt; &lt;/contact&gt; &lt;/dataset&gt; &lt;/eml:eml&gt; 4.2.2.3 Validation Errors One of the hardest parts about creating EML in R is diagnosing validation errors. I won’t get into too much detail, but here is a simple example. The eml$... family of helpers can help prevent validation errors, since they have a set list of arguments which are allowed. Here, I bypass the eml$dataset() helper function to show what the error looks like. doc &lt;- list(packageId = &quot;dataset-1&quot;, system = &quot;local&quot;, dataset = list(title = &quot;A minimial valid EML dataset&quot;, creator = eml$creator(individualName = me), contact = eml$contact(individualName = me), arbitrary = &quot;This element isn&#39;t in the schema&quot;)) eml_validate(doc) ## [1] FALSE ## attr(,&quot;errors&quot;) ## [1] &quot;Element &#39;arbitrary&#39;: This element is not expected. Expected is one of ( references, alternateIdentifier, shortName, title ).&quot; This error essentially says that the element arbitrary is not expected, and gives you a hint of some elements that were expected. Validation errors can be tricky, especially when there are lots of them. Validate early and often! 4.2.2.4 Metadata in R: A more complete example As you might imagine, creating a complete metadata record like what is shown on this page would be pretty tedious if we did it just using the generic list or eml$... helpers, since the nesting structure can be very deep. The EML package has a set of higher level helper functions beginning with set_ that create some of the more complex elements. To demonstrate the use of these we are going to create an EML document that contains the following information: title creator and contact abstract methods geographic and temporal coverage description of a tabular data file and a script We will edit these elements using a mix of helpers and generic techniques. To get set up, navigate to this dataset and download the CSV file and the R script. Put them in a directory called files that is a sub-directory of the location of this RMarkdown file. 4.2.2.4.1 Title, creator, contact To start, lets create a basic EML skeleton using our example above, but with more information in the creator and contact information besides just my name. # eml creator and contact have identical schema requirements (both fall under `responsibleParty`) me &lt;- eml$creator(individualName = eml$individualName(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;), organizationName = &quot;National Center for Ecological Analysis and Synthesis&quot;, electronicMailAddress = &quot;jclark@nceas.ucsb.edu&quot;, userId = list(directory = &quot;https://orcid.org&quot;, userId = &quot;https://orcid.org/0000-0003-4703-1974&quot;)) doc &lt;- list(packageId = &quot;dataset-1&quot;, system = &quot;local&quot;, dataset = eml$dataset(title = &quot;A more robust valid EML dataset&quot;, creator = me, contact = me)) Because we have used the eml$dataset helper, all of the possible sub-elements have been populated in our EML document, allowing us to easily access and edit them using R autocomplete. 4.2.2.4.2 Abstract We can use this to dive down into sub-elements and edit them. Let’s do the abstract. This is a simple element so we can just assign the value of the abstract to a character string. doc$dataset$abstract &lt;- &quot;A brief but comprehensive description of the who, what, where, when, why, and how of my dataset.&quot; 4.2.2.4.3 Methods We can use the set_methods function to parse a markdown (or word) document and insert it into the methods section. This way of adding text is especially nice because it preserves formatting. doc$dataset$methods &lt;- set_methods(&quot;../files/methods.md&quot;) doc$dataset$methods ## $sampling ## NULL ## ## $methodStep ## $methodStep$instrumentation ## character(0) ## ## $methodStep$software ## NULL ## ## $methodStep$description ## $methodStep$description$section ## $methodStep$description$section[[1]] ## [1] &quot;&lt;title&gt;Data Collection&lt;/title&gt;\\n&lt;para&gt;\\n We collected some data and here is a description\\n &lt;/para&gt;&quot; ## ## $methodStep$description$section[[2]] ## [1] &quot;&lt;title&gt;Data processing&lt;/title&gt;\\n&lt;para&gt;\\n Here is how we processed the data\\n &lt;/para&gt;&quot; ## ## ## $methodStep$description$para ## list() 4.2.2.4.4 Coverage The geographic and temporal coverage can be set using set_coverage. doc$dataset$coverage &lt;- set_coverage(beginDate = 2001, endDate = 2010, geographicDescription = &quot;Alaska, United States&quot;, westBoundingCoordinate = -179.9, eastBoundingCoordinate = -120, northBoundingCoordinate = 75, southBoundingCoordinate = 55) 4.2.2.4.5 Data file: script Information about data files (or entity level information) can be added in child elements of the dataset element. Here we will use the element otherEntity (other options include spatialVector, spatialRaster, and dataTable) to represent the R script. First, some high level information. doc$dataset$otherEntity &lt;- eml$otherEntity(entityName = &quot;../files/datfiles_processing.R&quot;, entityDescription = &quot;Data processing script&quot;, entityType = &quot;application/R&quot;) We can use the set_physical helper to set more specific information about the file, like its size, delimiter, and checksum. This function automatically detects fundamental characteristics about the file if you give it a path to your file on your system. doc$dataset$otherEntity$physical &lt;- set_physical(&quot;../files/datfiles_processing.R&quot;) ## Automatically calculated file size using file.size(&quot;../files/datfiles_processing.R&quot;) ## Automatically calculated authentication size using digest::digest(&quot;../files/datfiles_processing.R&quot;, algo = &quot;md5&quot;, file = TRUE) 4.2.2.4.6 Data file: tabular Here we will use the element dataTable to describe the tabular data file. As before, we set the entityName, entityDescription, and the physical sections. doc$dataset$dataTable &lt;- eml$dataTable(entityName = &quot;../files/my-data.csv&quot;, entityDescription = &quot;Temperature data from in-situ loggers&quot;) doc$dataset$dataTable$physical &lt;- set_physical(&quot;../files/my-data.csv&quot;) ## Automatically calculated file size using file.size(&quot;../files/my-data.csv&quot;) ## Automatically calculated authentication size using digest::digest(&quot;../files/my-data.csv&quot;, algo = &quot;md5&quot;, file = TRUE) Next, perhaps the most important part of metadata, but frequently the most difficult to document in a metadata standard: attribute level information. An attribute is a variable in your dataset. For tabular data, this is information about columns within data tables, critical to understanding what kind of information is actually in the table! The set_attributes function will take a data.frame that gives required attribute information. This data.frame contains rows corresponding to column names (or attributes) in the dataset, and columns: attributeName (any text) attributeDefinition (any text) unit (required for numeric data, use get_unitList() to see a list of standard units) numberType (required for numeric data, one of: real, natural, whole, integer) formatString (required for dateTime data) definition (required for textDomain data) Two other sub-elements, the domain and measurementScale, can be inferred from the col_classes argument to set_attributes. Let’s create our attributes data.frame. atts &lt;- data.frame(attributeName = c(&quot;time&quot;, &quot;temperature&quot;, &quot;site&quot;), attributeDefinition = c(&quot;time of measurement&quot;, &quot;measured temperature in degrees Celsius&quot;, &quot;site identifier&quot;), unit = c(NA, &quot;celsius&quot;, NA), numberType = c(NA, &quot;real&quot;, NA), formatString = c(&quot;HH:MM:SS&quot;, NA, NA), definition = c(NA, NA, &quot;site identifier&quot;)) We will then use this in our set_attributes function, along with the col_classes argument, to generate a complete attributeList. doc$dataset$dataTable$attributeList &lt;- set_attributes(attributes = atts, col_classes = c(&quot;Date&quot;, &quot;numeric&quot;, &quot;character&quot;)) As you might imagine, this can get VERY tedious with wide data tables. The function shiny_attributes calls an interactive table that can not only automatically detect and attempt to fill in attribute information from a data.frame, but also helps with on the fly validation. Note: this requires that the shinyjs package is installed. atts_shiny &lt;- shiny_attributes(data = read.csv(&quot;../files/my-data.csv&quot;)) This produces a data.frame that you can insert into set_attributes as above. 4.2.2.4.7 Validating and writing the file Finally, we need to validate and write our file. eml_validate(doc) ## [1] TRUE ## attr(,&quot;errors&quot;) ## character(0) write_eml(doc, &quot;../files/complex_example.xml&quot;) ## NULL 4.2.3 Publish data to the Arctic Data Center test site 4.2.3.1 Setup and Introduction Now let’s see how to use the dataone and datapack R packages to upload data to DataONE member repositories like the KNB Data Repository and the Arctic Data Center. 4.2.3.1.1 The dataone package The dataone R package provides methods to enable R scripts to interact with DataONE to search for, download, upload and update data and metadata. The purpose of uploading data from R is to automate the repetitive tasks for large datasets with many files. For small datasets, the web submission form will certainly be simpler. The dataone package interacts with two major parts of the DataONE infrastructure: Coordinating Nodes (or cn) and Member Nodes (or mn). Coordinating nodes maintain a complete catalog of all data and provide the core DataONE services, including search and discovery. The cn that is exposed through search.dataone.org is called the production (or PROD) cn. Member Nodes expose their data and metadata through a common set of interfaces and services. The Arctic Data Center is an mn. To post data to the Arctic Data Center, we need to interact with both the coordinating and member nodes. In addition to the production cn, there are also several test coordinating node environments, and corresponding test member node environments. In this tutorial, we will be posting data to the test Arctic Data Center environment. 4.2.3.1.2 The datapack package The datapack R package represents the set of files in a dataset as a datapack::DataPackage. This DataPackage is just a special R object class that is specified in the datapack package. Each object in that DataPackage is represented as a DataObject, another R object class specified by datapack. When you are publishing your dataset, ideally you aren’t only publishing a set of observations. There are many other artifacts of your research, such as scripts, derived data files, and derived figures, that should be archived in a data package. Each of the types of files shown in the workflow below should be considered a data object in your package, including the metadata file that describes the individual components of your workflow. Each of the files in the diagram above has a relationship with the other files, and using datapack you can describe these relationships explicitly and unambiguously using controlled vocabularies and conceptual frameworks. For example, we know that the fine “Image 1” was generated by “Mapping Script.” We also know that both “Image 1” and “Mapping Script” are described by the file “Metadata.” Both of these relationsips are represented in datapack using speficic ontologies. Using datapack we will create a DataPackage that represents a (very simple) scientific workflow and the data objects within it, where the relationships between those objects are explicitly described. Then, we will upload this DataPackage to a test version of the Arctic Data Center repository using dataone. Before uploading any data to a DataONE repository, you must login to get an authentication token, which is a character string used to identify yourself. This token can be retrieved by logging into the test repository and copying the token into your R session. 4.2.3.1.3 Obtain an ORCID ORCID is a researcher identifier that provides a common way to link your researcher identity to your articles and data. An ORCID is to a researcher as a DOI is to a research article. To obtain an ORCID, register at https://orcid.org. 4.2.3.1.4 Log in to the test repository and copy your token We will be using a test server, so login and retrieve your token at https://test.arcticdata.io. Once you are logged in, navigate to your Profile Settings, and locate the “Authentication Token” section, and then copy the token for R to your clipboard. Finally, paste the token into the R Console to register it as an option for this R session. You are now logged in. But note that you need to keep this token private; don’t paste it into scripts or check it into git, as it is just as sensitive as your password. 4.2.3.2 Uploading A Package Using R with uploadDataPackage Datasets and metadata can be uploaded individually or as a collection. Such a collection, whether contained in local R objects or existing on a DataONE repository, will be informally referred to as a package. Load the libraries: library(dataone) library(datapack) First we need to create an R object that describes what coordinating and member nodes we will be uploading our dataset to. We do this with the dataone function D1Client (DataONE client). The first argument specifies the DataONE coordinating node (in this case a test node called STAGING) and the second specifices the member node. We’ll also create an object that only represents the member node, which is helpful for some functions. d1c &lt;- D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) mn &lt;- d1c@mn Next, we create a DataPackage as a container for our data, metadata, and scripts using the new function. This just creates an empty object with the class DataPackage dp &lt;- new(&quot;DataPackage&quot;) dp ## This package does not contain any DataObjects. We then need to add a metadata file and data file to this package. First we generate some identifiers for the objects. We’ll use the uuid scheme for all of our objects. If we were uploading to production, you would likely want use an identifier with the doi scheme for your metadata document. data_id &lt;- generateIdentifier(mn, scheme = &quot;uuid&quot;) script_id &lt;- generateIdentifier(mn, scheme = &quot;uuid&quot;) metadata_id &lt;- generateIdentifier(mn, scheme = &quot;uuid&quot;) Now we need to modify our EML document to include these identifiers. This increases the accessibility of the files in our dataset. First read in the EML that we created earlier. doc &lt;- read_eml(&quot;../files/complex_example.xml&quot;) Let’s replace the arbitrary packageId and system that we set in the example above to reflect the identifier we created for this package, and the system we are uploading the package to. doc$packageId &lt;- metadata_id doc$system &lt;- mn@identifier Now let’s add a distribution URL to the physical section of our entity information. All of the distribution URLs look something like this https://test.arcticdata.io/metacat/d1/mn/v2/object/urn:uuid:A0cc95b46-a318-4dd1-8f8a-0e6c280e1d95, where https://test.arcticdata.io/metacat/d1/mn/v2/ is the member node end point, and urn:uuid:A0cc95b46-a318-4dd1-8f8a-0e6c280e1d95 is the identifier of the object. We can easily construct this URL using the paste0 function, and insert it into the physical section of the dataTable element. # set url for csv doc$dataset$dataTable$physical$distribution$online$url &lt;- paste0(mn@endpoint, &quot;object/&quot;, data_id) # set url for script doc$dataset$otherEntity$physical$distribution$online$url &lt;- paste0(mn@endpoint, &quot;object/&quot;, script_id) write_eml(doc, &quot;../files/complex_example.xml&quot;) ## NULL Now we have a full metadata document ready to be uploaded. We now need to add our files to the DataPackage. First, let’s create a new DataObject, which is another object class specific to datapack. Our metadata file, data file, and script will all need to be added as a DataObject. Remember that all files in a package are considered data objects, not just the ones that we you traditionally think of as being “data”. The format argument specifies the type of file, and should match one of the list of DataONE formatIds (listed in the Id field) here. # Add the metadata document to the package metadataObj &lt;- new(&quot;DataObject&quot;, id = metadata_id, format =&quot;eml://ecoinformatics.org/eml-2.1.1&quot;, filename = &quot;../files/complex_example.xml&quot;) After creating the DataObject that represents the metadata file, we add it to the data package using addMember. dp &lt;- addMember(dp, metadataObj) dp ## Members: ## ## filename format mediaType size identifier modified local ## comple...le.xml eml...1.1 NA 5907 urn:uui...d0f7ffd1 n y ## ## Package identifier: NA ## RightsHolder: NA ## ## ## This package does not contain any provenance relationships. We add the data file and the script similarly. The only difference is in the addMember function we have to set the mo (metadata object) argument equal to the DataObject that represents the metadata file. Adding our csv file to the package this way not only adds the file to the data package, but it also specifies that the csv is described by the metadata. # Add our data file to the package sourceObj &lt;- new(&quot;DataObject&quot;, id = data_id, format = &quot;text/csv&quot;, filename = &quot;../files/my-data.csv&quot;) dp &lt;- addMember(dp, sourceObj, mo = metadataObj) dp ## Members: ## ## filename format mediaType size identifier modified local ## my-data.csv text/csv NA 45 urn:uui...34a4a4aa n y ## comple...le.xml eml...1.1 NA 5907 urn:uui...d0f7ffd1 n y ## ## Package identifier: NA ## RightsHolder: NA ## ## ## Relationships (updated): ## ## subject predicate object ## 1 complex_example.xml cito:documents my-data.csv ## 2 my-data.csv cito:isDocumentedBy complex_example.xml Next we add our script in the same way. # Add our script to the package scriptObj &lt;- new(&quot;DataObject&quot;, id = script_id, format = &quot;application/R&quot;, filename = &quot;../files/datfiles_processing.R&quot;) dp &lt;- addMember(dp, scriptObj, mo = metadataObj) dp ## Members: ## ## filename format mediaType size identifier modified local ## my-data.csv text/csv NA 45 urn:uui...34a4a4aa n y ## comple...le.xml eml...1.1 NA 5907 urn:uui...d0f7ffd1 n y ## datfil...sing.R app...n/R NA 5625 urn:uui...d7038ed9 n y ## ## Package identifier: NA ## RightsHolder: NA ## ## ## Relationships (updated): ## ## subject predicate object ## 1 complex_example.xml cito:documents my-data.csv ## 3 complex_example.xml cito:documents datfiles_processing.R ## 4 datfiles_processing.R cito:isDocumentedBy complex_example.xml ## 2 my-data.csv cito:isDocumentedBy complex_example.xml You can also specify other information about the relationships between files in your data package by adding provenance information. Here, we specify that the R script (program) uses the csv (sources) by including them as the specified arguments in the describeWorkflow function. dp &lt;- describeWorkflow(dp, sources = sourceObj, program = scriptObj) dp ## Members: ## ## filename format mediaType size identifier modified local ## my-data.csv text/csv NA 45 urn:uui...34a4a4aa n y ## comple...le.xml eml...1.1 NA 5907 urn:uui...d0f7ffd1 n y ## datfil...sing.R app...n/R NA 5625 urn:uui...d7038ed9 n y ## ## Package identifier: NA ## RightsHolder: NA ## ## ## Relationships (updated): ## ## subject predicate object ## 9 _3394ac4f-...345c1e8853 rdf:type prov:Association ## 8 _3394ac4f-...345c1e8853 prov:hadPlan datfiles_processing.R ## 1 complex_example.xml cito:documents my-data.csv ## 3 complex_example.xml cito:documents datfiles_processing.R ## 4 datfiles_processing.R cito:isDocumentedBy complex_example.xml ## 10 datfiles_processing.R rdf:type provone:Program ## 2 my-data.csv cito:isDocumentedBy complex_example.xml ## 5 my-data.csv rdf:type provone:Data ## 11 urn:uuid:d...3af39a958e dcterms:identifier urn:uuid:d...3af39a958e ## 7 urn:uuid:d...3af39a958e rdf:type provone:Execution ## 6 urn:uuid:d...3af39a958e prov:quali...ssociation _3394ac4f-...345c1e8853 ## 12 urn:uuid:d...3af39a958e prov:used my-data.csv Each object in a data package has an access policy. There are three levels of access you can grant either to individual files, or the package as a whole. read: the ability to view when published write: the ablity to edit after publishing changePermission: the ability to grant other people read, write, or changePermission access Here, I give my colleague (via his ORCID) “read” and “write” permission to my entire data package using addAccessRule. dp &lt;- addAccessRule(dp, subject = &quot;http://orcid.org/0000-0003-0077-4738&quot;, permission = c(&quot;read&quot;,&quot;write&quot;), getIdentifiers(dp)) Finally, we upload the package to the test server for the Arctic Data Center using uploadDataPackage. This function takes as arguments d1c (the DataONE client which specifies which coordinating and member nodes to use), dp (the data package itself), whether you want the package to include public read access (public = TRUE). packageId &lt;- uploadDataPackage(d1c, dp, public = TRUE) You can now search for and view the package at https://test.arcticdata.io: "],["session-5-model-analysis.html", "5 Session 5: Model Analysis 5.1 Learning Objectives 5.2 Comparing models to data 5.3 Simulation Models", " 5 Session 5: Model Analysis library(daymetr) library(dplyr) library(ggplot2) theme_set(theme_bw()) options(digits = 3) 5.1 Learning Objectives Today you will learn to Use visual and statistical approaches to compare model output to data Investigate the relationships between model inputs and outputs using sensitivity analysis and variance decomposition 5.2 Comparing models to data DayMet is a gridded weather data product. It provides daily weather statistics for North America on a 1 km grid. The primary input to DayMet is data from weather stations. Using a number of statistical methods, values for each square on the grid are imputed. Today we are going to use these as our ‘model’, and we are going to compare the output of this model to data from a weather station in Maricopa, AZ. Lets download the daily weather data from DayMet: mac_daymet_list &lt;- download_daymet(site = &quot;Maricopa Agricultural Center&quot;, lat = 33.07, lon = -111.97, start = 2020, end = 2020, internal = TRUE) ## Downloading DAYMET data for: Maricopa Agricultural Center at 33.07/-111.97 latitude/longitude ! ## Done ! mac_daymet &lt;- mac_daymet_list$data %&gt;% mutate(tmin = tmin..deg.c., tmax = tmax..deg.c., tmean = (tmin + tmax) / 2) %&gt;% select(doy = yday, tmean, tmax, tmin) Before we look at the ground truth, lets visualize these outputs and assess them for meaning. ggplot(data = mac_daymet, aes(doy, tmean)) + geom_point() + geom_line() Lets get some truth to compare our model to data measured at a weather station. The AZMet data format is described here: https://cals.arizona.edu/azmet/raw2003.htm. On that page you can see that these are the columns we are interested in: col variable 2 day of year 4 max temperature 5 min temperature 6 mean temperature We can read in the data and rename the columns of interest in the following steps: mac_azmet &lt;- read.csv(&#39;https://cals.arizona.edu/azmet/data/0621rd.txt&#39;, header = FALSE) %&gt;% select(doy = V2, tmean = V6, tmax = V4, tmin = V5) Now, lets join the data frames. We will use the suffix to identify model (DayMet) and observed station (AZMet) data. comparison &lt;- mac_daymet %&gt;% left_join(mac_azmet, by = &#39;doy&#39;, suffix = c(&#39;_model&#39;, &#39;_obs&#39;)) %&gt;% filter(!is.na(tmean_obs)) head(comparison) ## doy tmean_model tmax_model tmin_model tmean_obs tmax_obs tmin_obs ## 1 1 9.04 16.1 1.98 7.7 18.7 -1.1 ## 2 2 9.97 17.0 2.94 7.5 17.9 -1.9 ## 3 3 10.09 18.0 2.16 7.3 18.7 -1.6 ## 4 4 11.22 19.9 2.57 8.7 21.1 -1.1 ## 5 5 11.44 20.3 2.58 10.4 21.4 1.0 ## 6 6 12.25 21.3 3.23 9.4 20.7 -0.5 Just looking at the table, what do you notice about the observed and modeled temperatures? looks like the tmax observed is generally higher and tmin is generally lower! the station observed five days below freezing in the first six days of the year! Now, lets see how the model and data compare: ggplot(data = comparison) + geom_line(aes(doy, tmean_model)) + geom_line(aes(doy, tmin_model), color = &#39;grey&#39;) + geom_line(aes(doy, tmax_model), color = &#39;grey&#39;) + geom_line(aes(doy, tmean_obs), color = &#39;red&#39;) + geom_line(aes(doy, tmin_obs), color = &#39;pink&#39;) + geom_line(aes(doy, tmax_obs), color = &#39;pink&#39;) ggplot(data = comparison) + geom_point(aes(doy, tmean_model - tmean_obs)) ggplot(data = comparison) + geom_point(aes(tmean_model, tmean_obs)) + geom_abline(aes(intercept = 0, slope = 1)) + ylim(0, 40) + xlim(0, 40) 5.2.1 Statistical tests of model performance We are going to talk about a set of key metrics for comparing models to data: Slope, RMSE, R^2, Bias, SD ratio 5.2.1.1 Slope The slope of regression of predicted on observed: \\(Y_{obs}= bY_{mod}+\\epsilon\\). Does slope \\(b=1\\)? mod &lt;- comparison$tmean_model obs &lt;- comparison$tmean_obs reg &lt;- lm(obs~ mod) plot(mod, obs, xlab = &#39;model tmean&#39;, ylab = &#39;obs tmean&#39;) + abline(0,1) + abline(coef(reg), lty = 2) ## integer(0) summary(reg) ## ## Call: ## lm(formula = obs ~ mod) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.291 -2.661 -0.378 2.416 10.069 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.2091 0.7040 3.14 0.0019 ** ## mod 0.8514 0.0263 32.42 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.95 on 305 degrees of freedom ## Multiple R-squared: 0.775, Adjusted R-squared: 0.774 ## F-statistic: 1.05e+03 on 1 and 305 DF, p-value: &lt;2e-16 Does the regression model includes the 1:1 line? Why is the P-value so low? because the default hypothesis test is that the slope is different than 0, not different than 1 coef(reg) ## (Intercept) mod ## 2.209 0.851 confint(reg) ## 2.5 % 97.5 % ## (Intercept) 0.824 3.594 ## mod 0.800 0.903 Is the slope 1:1? What about for tmin and tmax? summary(lm(tmin_obs ~ tmin_model, data = comparison)) summary(lm(tmax_obs ~ tmax_model, data = comparison)) 5.2.1.2 RMSE Root Mean Square Error This is the most common model metric. It is the standard deviation of the residuals. \\[RMSE=\\sqrt{\\frac{1}{n}(Y_{model}-Y_{obs})^2}\\] RMSE &lt;- sqrt(mean((mod - obs)^2)) RMSE ## [1] 4.42 Sometimes you want a “Normalized” estimate of model error. That way the magnitude of the error is on the same scale as the observations. There are many ways to do this, a common approach is to divide by the mean of the observations. \\(NRMSE=RMSE/\\bar{Y}_{obs}\\) is equivalent to coefficient of variance \\(CV=\\frac{\\sigma}{\\bar{x}}\\). ## to normalize, divide by sd(obs) NRMSE &lt;- RMSE/sd(obs) Other approaches to normalization: \\(RMSE/sd(obs)\\) \\(RMSE/\\bar{Y_{obs}}\\) \\(RMSE/IQR\\) 5.2.1.3 \\(R^2\\) Corellation Coefficient When comparing models to data, it is common to calculate the Correlation Coefficient in terms of deviation from the 1:1 line \\[R^2=1-\\frac{\\sum{(Y_{mod}-Y_{obs})^2}}{\\sum{(Y_{obs}-\\bar{Y}_{obs})^2}}\\] cor(mod, obs) ## [1] 0.88 cor.test(mod, obs) ## ## Pearson&#39;s product-moment correlation ## ## data: mod and obs ## t = 32, df = 305, p-value &lt;2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.852 0.903 ## sample estimates: ## cor ## 0.88 5.2.1.4 Bias Do you over or under predict? \\[Y_{model}-Y_{obs}\\] mean(mod - obs) ## [1] 1.56 # normalized mean(mod - obs)/mean(obs) ## [1] 0.0656 It is also useful to visualize and compute statistics on these differences. hist(mod-obs) What fraction of the time is the model above the observation? sum(mod &gt; obs) / length(obs) ## [1] 0.681 5.2.1.5 Ratio of variances Does the model capture the observed variance? Often a model will have lower variances … why? \\[\\frac{SD_{Y_{model}}}{SD_{Y_{obs}}}\\] sd(obs) / sd(mod) ## [1] 0.967 5.2.1.6 Taylor Diagram - three metrics in two dimensions A Taylor Diagram allows you to plot three metrics of model performance - the RMSE, R^2, and SD ratio in two dimensions, taking advantage of the mathematical relationships among these scores. It is common to plot multiple points representing different models or model parameterizations, also different years or locations that the model is evaluated at. Plotting these as normalized values allows comparison across variables being predicted. library(plotrix) taylor.diagram(ref=obs, model = mod, normalize = TRUE, ref.sd = TRUE) 5.3 Simulation Models Yesterday I introduced the concept and application of simulation models, with examples related to simulating plant growth. In a regression model, it is often possible to see how each parameter relates to the model output (e.g. slope, intercept). For a complex model, it is often not possible to understand the high level dynamics of the system by analyzing the mathematics. Another approach is to treat the model as a ‘black box’. Then we can treat the black box as an experimental system, purturb it and see how it responds. Lets look at this in more detail. 5.3.1 Example: A leaf level model of photosynthesis Based on the coupled C4 photosynthesis - conductance model developed by Collatz and Ball Berry G. Collatz, M. Ribas-Carbo, J. Berry. (1992). Coupled photosynthesis-stomatal conductance model for leaves of C4 plants. Australian Journal of Plant Physiology 519–538. Water transpiration is coupled to photosynthesis through stomatal conductance, since plants need to regulate water loss as they uptake \\(CO_2\\): \\[g_s = m\\frac{A_n h_s}{c_a}p + b\\] Where \\(g_s\\) is stomatal conducatnce, \\(A_n\\) is net photosynthesis, \\(h_s\\) is relative humidity, \\(c_a\\) is \\(CO_2\\) at leaf surface. and Photosynthesis is \\[A_n=min(A_c, A_L)-R_d\\] Where Rubisco-limited rate is \\(A_c\\) and RuBP limited rate is \\(A_L\\) \\[A_c=V_m\\left[\\frac{c_i-\\Gamma}{c_i+K_c(1+O_2/K_o)}\\right]\\] \\[A_L=\\] This is a non-linear equation with key plant physiological traits: Parameter Description Vmax maximum carboxylation of Rubisco according to the Collatz model. alpha alpha parameter according to the Collatz model. Initial slope of the response to Irradiance. kparm k parameter according to the Collatz model. Initial slope of the response to CO2. theta theta parameter according to the Collatz model. Curvature for light response. beta beta parameter according to the Collatz model. Curvature for response to CO2. Rd Rd parameter according to the Collatz model. Dark respiration. b0 intercept for the Ball-Berry stomatal conductance model. b1 slope for the Ball-Berry stomatal conductance model. The rate of photosynthesis is determined by environmental factors: Parameter Description Tl temperature of the leaf (Celsius). RH relative humidity (as a fraction, i.e. 0-1). Qp quantum flux (direct light), (micro mol m-2 s-1). Catm Atmospheric CO2 in ppm (or micromol/mol). In the end: \\[[Gs, A, C_i]=f(T, RH, Light, CO_2, v_{max}, \\alpha, k, \\theta, R_d, b_0, b_1)\\] 5.3.2 Let’s run this model! First, lets load some meteorological data: library(BioCro) #devtools::install_github(&#39;ebimodeling/biocro&#39;) library(lubridate) library(ggplot2) data(&quot;weather04&quot;) time &lt;- ymd(&#39;2004-01-01&#39;) + days(weather04$doy-1) + hours(weather04$hour) par &lt;- weather04$solarR rh &lt;- weather04$RH temp &lt;- weather04$DailyTemp.C Now, estimate photosynthesis rate at each time step. This is isn’t the entire model - just what would be happening to the leaf of a plant with C4 photosynthesis in full sunlight. A &lt;- c4photo(Qp = par, Tl = temp, RH = rh)$Assim pairs(data.frame(A, par, temp, rh)) ## whoa this is wierd! What is going on? plot(temp, rh) a strange scaling algorithm, interpolating from daily to hourly see also https://rpubs.com/dlebauer/metdriver_comparisons library(ggplot2) ggplot()+ geom_line(aes(time, A)) + scale_x_datetime(limits = ymd_h(c(&#39;2004-05-01 0&#39;, &#39;2004-06-01 23&#39;))) ## Warning: Removed 7992 row(s) containing missing values (geom_path). ggplot()+ geom_line(aes(time, rh)) + scale_x_datetime(limits = ymd_h(c(&#39;2004-05-01 0&#39;, &#39;2004-06-01 23&#39;))) ## Warning: Removed 7992 row(s) containing missing values (geom_path). question: is f(mean(X)) = mean(f(X))? testQp &lt;- 11:20*100 testTl &lt;- 21:30 testRH &lt;- 21:30/50 A1 &lt;- c4photo(Qp = mean(testQp), Tl = mean(testTl), RH = mean(testRH)) A2 &lt;- lapply(c4photo(Qp = testQp, Tl = testTl, RH = testRH), mean) dplyr::bind_rows(A1 = A1, A2 = A2) ## # A tibble: 2 × 4 ## Gs Assim Ci GrossAssim ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 239. 31.3 171. 32.1 ## 2 243. 31.4 173. 32.2 Why are these different? For non-linear functions see Jensen’s Inequality (Wikipedia) This means - be careful when and how you use averages - everywhere!!! Spatial and temporal downscaling is how crop modelers deal with lower resolution atmospheric model forecasts. For example the most recent IPCC 100 y climate forecasts were generated on ~100km grids (Taylor et al 2012), thus one data point may simultaneously represent a month that is \\(60^o\\)F and foggy in San Fransicsco and \\(100^o\\)F and dry in Davis, CA. At the same time, crop models need to run on local hourly data while also capturing the uncertainty represented by within and across model variability. 5.3.2.1 Model sensitivity Next, we are going to look at the sensitivity of the model to each of the different weather parameters, holding the others at their mean meanQp &lt;- mean(par) meanTl &lt;- mean(temp) meanRH &lt;- mean(rh) plot(1:100/100, c4photo(Qp = rep(meanQp, 100), Tl = rep(meanTl, 100), RH = 1:100/100)$Assim, type = &#39;l&#39;, ylab = &#39;Assim&#39;, xlab = &#39;RH&#39;) plot(1:100/4, c4photo(Qp = rep(meanQp, 100), Tl = 1:100/4, RH = rep(meanRH, 100))$Assim, type = &#39;l&#39;, ylab = &#39;Assim&#39;, xlab = &#39;RH&#39;) 5.3.2.2 Monte Carlo Error propagation Given a model parameters represented as variables, e.g. \\[ V_{cmax}\\sim N(45,2)\\\\ Rd\\sim N(1, 0.1) b1 \\sim N(4,1) \\] What is expected uncertainty in model predictions of photosynthesis (A = assimilation) Lets take some samples from these distributions set.seed(100) n &lt;- 1000 vmax &lt;- rnorm(n, 45, 2) Rd &lt;- rnorm(n, 1, 0.10) b1 &lt;- rnorm(n, 4, 1) Now we can take a look at them x &lt;- 25:75 ggplot() + geom_histogram(aes(vmax, y = ..density..), proability = TRUE) + geom_line(aes(x, dnorm(x, 45, 2))) ## Warning: Ignoring unknown parameters: proability ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. x &lt;- 1:200/100 ggplot() + geom_histogram(aes(Rd, y = ..density..), proability = TRUE) + geom_line(aes(x, dnorm(x, 1, 0.1))) ## Warning: Ignoring unknown parameters: proability ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Next, we are going to propagate the parameter variance to see how much of an effect it has on the model output variance: ### sample given time series of met A &lt;- matrix(nrow = length(time), ncol = 1000) for(i in 1:1000){ A[,i] &lt;- c4photo(Qp = par, Tl = temp, RH = rh, vmax = vmax[i], Rd = Rd[i], b1=b1[i])$Assim } # take a look at the matrix .. # image(A, xlab = &#39;time&#39;, ylab = &#39;sample&#39;) ## shows an annual cycle of photosynthesis median &lt;- which.min(abs(quantile(colMeans(A), 0.50)-colMeans(A))) ucl &lt;- which.min(abs(quantile(colMeans(A), 0.975)-colMeans(A))) lcl &lt;- which.min(abs(quantile(colMeans(A), 0.025)-colMeans(A))) ggplot() + # geom_smooth(aes(time, A))+ geom_line(aes(time, A[,median])) + geom_line(aes(time, y = A[,lcl]), linetype = 2) + geom_line(aes(time, y = A[,ucl]), linetype = 2) + scale_x_datetime(limits = ymd_h(c(&#39;2004-05-01 0&#39;, &#39;2004-05-07 23&#39;))) ## Warning: Removed 8592 row(s) containing missing values (geom_path). ## Warning: Removed 8592 row(s) containing missing values (geom_path). ## Warning: Removed 8592 row(s) containing missing values (geom_path). 5.3.3 Variance Decomposition - which of these parameters are most important? Response of interest is the total assimilation a_total &lt;- colMeans(A) summary(aov(a_total ~ vmax + Rd + b1)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## vmax 1 22.66 22.66 8593 &lt;2e-16 *** ## Rd 1 2.55 2.55 966 &lt;2e-16 *** ## b1 1 0.40 0.40 153 &lt;2e-16 *** ## Residuals 996 2.63 0.00 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.3.4 Propagate met and Use met as a variable, sample over variation within the hour To answer the question - if I were to step out and look at the plants in the summer, how fast would they be growing? This time, instead of sampling from a parameterized distribution, I am going to sample over the middle (hours 10-4) of days in the middle of the year (approx. summer, day of year 120-240) ### sample over met variability A2 &lt;- Gs &lt;- Ci &lt;- Qp &lt;- Tl &lt;- RH &lt;- vector(length = 1000) for(i in 1:1000){ day &lt;- sample(120:240, size = 1) hour &lt;- sample(10:16, size = 1) j &lt;- day * 24 + hour Qp[i] &lt;- par[j] Tl[i] &lt;- temp[j] RH[i] &lt;- rh[j] res &lt;- c4photo(Qp = Qp[i], Tl = Tl[i], RH = RH[i], vmax = vmax[i], Rd = Rd[i], b1=b1[i]) A2[i] &lt;- res$Assim Gs[i] &lt;- res$Gs Ci[i] &lt;- res$Ci } hist(A2) Equivalent of sensitivity analysis: (where A2, Gs, Ci are response variables) pairs(data.frame(A2, Gs, Ci, vmax, Rd, b1, Qp, Tl, RH), pch = &#39;.&#39;) The lm is pretty much a sensitivity analysis: what is the slope of the effect of inputs on the output of the model. summary(lm(A2 ~ vmax + Rd + b1 + Qp + Tl + RH)) ## ## Call: ## lm(formula = A2 ~ vmax + Rd + b1 + Qp + Tl + RH) ## ## Residuals: ## Min 1Q Median 3Q Max ## -35.33 -1.35 0.71 1.61 6.75 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.61e+01 2.42e+00 -10.78 &lt; 2e-16 *** ## vmax 2.94e-01 3.94e-02 7.47 1.8e-13 *** ## Rd 5.76e-02 8.29e-01 0.07 0.94 ## b1 3.67e-01 7.75e-02 4.73 2.6e-06 *** ## Qp 1.05e-02 1.93e-04 54.54 &lt; 2e-16 *** ## Tl 1.08e+00 1.86e-02 57.99 &lt; 2e-16 *** ## RH 9.66e-01 1.28e+00 0.76 0.45 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.56 on 993 degrees of freedom ## Multiple R-squared: 0.895, Adjusted R-squared: 0.894 ## F-statistic: 1.41e+03 on 6 and 993 DF, p-value: &lt;2e-16 The analysis of variance partitions the variance - how much if the total variance in A2 is contributed by each of the following parameters (recall that the domain for met variables is &lt;&lt; the domain for physiological parameters … What would happen if we used a whole year of meteorological data instead of the one hour of met data that we used? summary(aov(A2 ~ vmax + Rd + b1 + Qp + Tl + RH)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## vmax 1 932 932 142.26 &lt;2e-16 *** ## Rd 1 21 21 3.21 0.074 . ## b1 1 33 33 5.01 0.025 * ## Qp 1 32153 32153 4906.97 &lt;2e-16 *** ## Tl 1 22380 22380 3415.44 &lt;2e-16 *** ## RH 1 4 4 0.57 0.450 ## Residuals 993 6507 7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 tidy(aov(Gs ~ vmax + Rd + b1 + Qp + Tl + RH)) tidy(aov(Ci ~ vmax + Rd + b1 + Qp + Tl + RH)) 5.3.5 References Taylor, K.E., R.J. Stouffer, G.A. Meehl: An Overview of CMIP5 and the experiment design.” Bull. Amer. Meteor. Soc., 93, 485-498, doi:10.1175/BAMS-D-11-00094.1, 2012. http://journals.ametsoc.org/doi/pdf/10.1175/BAMS-D-11-00094.1 Humphries S and Long SP (1995) WIMOVAC - a software package for modeling the dynamics of the plant leaf and canopy photosynthesis. Computer Applications in the Bioscience 11(4):361-371. Miguez FE, Zhu XG, Humphries S, Bollero GA, Long SP (2009) A semimechanistic model predicting the growth and production of the bioenergy crop Miscanthus × giganteus: description, parameterization and validation. Global Change Biology Bioenergy 1: 282-296. Wang D, Jaiswal D, Lebauer DS, Wertin TM, Bollero GA, Leakey ADB, Long SP (2015) A physiological and biophysical model of coppice willow (Salix spp.) production yields for the contiguous USA in current and future climate scenarios. Plant, cell &amp; environment, 38(9), 1850-1865. Thornton, M.M., R. Shrestha, Y. Wei, P.E. Thornton, S. Kao, and B.E. Wilson. 2020. Daymet: Daily Surface Weather Data on a 1-km Grid for North America, Version 4. ORNL DAAC, Oak Ridge, Tennessee, USA. https://doi.org/10.3334/ORNLDAAC/1840 Daymet: Daily Surface Weather Data on a 1-km Grid for North America, Version 4 https://doi.org/10.3334/ORNLDAAC/1840 "],["session-6-shiny.html", "6 Session 6: Shiny 6.1 Introduction to Shiny", " 6 Session 6: Shiny 6.1 Introduction to Shiny 6.1.1 Learning Objectives In this lesson we will: review the capabilities in Shiny applications learn about the basic layout for Shiny interfaces learn about the server component for Shiny applications build a simple shiny application for interactive plotting 6.1.2 Overview Shiny is an R package for creating interactive data visualizations embedded in a web application that you and your colleagues can view with just a web browser. Shiny apps are relatively easy to construct, and provide interactive features for letting others share and explore data and analyses. There are some really great examples of what Shiny can do on the RStudio webite like this one exploring movie metadata. A more scientific example is a tool from the SASAP project exploring proposal data from the Alaska Board of Fisheries. There is also an app for Delta monitoring efforts. Most any kind of analysis and visualization that you can do in R can be turned into a useful interactive visualization for the web that lets people explore your data more intuitively But, a Shiny application is not the best way to preserve or archive your data. Instead, for preservation use a repository that is archival in its mission like the KNB Data Repository, Zenodo, or Dryad. This will assign a citable identifier to the specific version of your data, which you can then read in an interactive visualiztion with Shiny. For example, the data for the Alaska Board of Fisheries application is published on the KNB and is citable as: Meagan Krupa, Molly Cunfer, and Jeanette Clark. 2017. Alaska Board of Fisheries Proposals 1959-2016. Knowledge Network for Biocomplexity. doi:10.5063/F1QN652R. While that is the best citation and archival location of the dataset, using Shiny, one can also provide an easy-to-use exploratory web application that you use to make your point that directly loads the data from the archival site. For example, the Board of Fisheries application above lets people who are not inherently familiar with the data to generate graphs showing the relationships between the variables in the dataset. We’re going to create a simple shiny app with two sliders so we can interactively control inputs to an R function. These sliders will allow us to interactively control a plot. 6.1.3 Create a sample shiny application File &gt; New &gt; Shiny Web App… Set some fields: Name it “myapp” or something else Select “Single File” Choose to create it in a new folder called ‘shiny-demo’ Click Create RStudio will create a new file called app.R that contains the Shiny application. Run it by choosing Run App from the RStudio editor header bar. This will bring up the default demo Shiny application, which plots a histogram and lets you control the number of bins in the plot. Note that you can drag the slider to change the number of bins in the histogram. 6.1.4 Shiny architecture A Shiny application consists of two functions, the ui and the server. The ui function is responsible for drawing the web page, while the server is responsible for any calculations and for creating any dynamic components to be rendered. Each time that a user makes a change to one of the interactive widgets, the ui grabs the new value (say, the new slider min and max) and sends a request to the server to re-render the output, passing it the new input values that the user had set. These interactions can sometimes happen on one computer (e.g., if the application is running in your local RStudio instance). Other times, the ui runs on the web browser on one computer, while the server runs on a remote computer somewhere else on the Internet (e.g., if the application is deployed to a web server). 6.1.5 Interactive scatterplots Let’s modify this application to plot Yolo bypass secchi disk data in a time-series, and allow aspects of the plot to be interactively changed. 6.1.5.1 Load data for the example Use this code to load the data at the top of your app.R script. Note we are using contentId again, and we have filtered for some species of interest. library(shiny) library(contentid) library(dplyr) library(ggplot2) library(lubridate) sha1 &lt;- &#39;hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0&#39; delta.file &lt;- contentid::resolve(sha1, registries=c(&quot;dataone&quot;), store = TRUE) # fix the sample date format, and filter for species of interest delta_data &lt;- read.csv(delta.file) %&gt;% mutate(SampleDate = mdy(SampleDate)) %&gt;% filter(grepl(&quot;Salmon|Striped Bass|Smelt|Sturgeon&quot;, CommonName)) names(delta_data) 6.1.5.2 Add a simple timeseries using ggplot We know there has been a lot of variation through time in the delta, so let’s plot a time-series of Secchi depth. We do so by switching out the histogram code for a simple ggplot, like so: server &lt;- function(input, output) { output$distPlot &lt;- renderPlot({ ggplot(delta_data, mapping = aes(SampleDate, Secchi)) + geom_point(colour=&quot;red&quot;, size=4) + theme_light() }) } If you now reload the app, it will display the simple time-series instead of the histogram. At this point, we haven’t added any interactivity. In a Shiny application, the server function provides the part of the application that creates our interactive components, and returns them to the user interface (ui) to be displayed on the page. 6.1.5.3 Add sliders to set the start and end date for the X axis To make the plot interactive, first we need to modify our user interface to include widgits that we’ll use to control the plot. Specifically, we will add a new slider for setting the minDate parameter, and modify the existing slider to be used for the maxDate parameter. To do so, modify the sidebarPanel() call to include two sliderInput() function calls: sidebarPanel( sliderInput(&quot;minDate&quot;, &quot;Min Date:&quot;, min = as.Date(&quot;1998-01-01&quot;), max = as.Date(&quot;2020-01-01&quot;), value = as.Date(&quot;1998-01-01&quot;)), sliderInput(&quot;maxDate&quot;, &quot;Max Date:&quot;, min = as.Date(&quot;1998-01-01&quot;), max = as.Date(&quot;2020-01-01&quot;), value = as.Date(&quot;2005-01-01&quot;)) ) If you reload the app, you’ll see two new sliders, but if you change them, they don’t make any changes to the plot. Let’s fix that. 6.1.5.4 Connect the slider values to the plot Finally, to make the plot interactive, we can use the input and output variables that are passed into the server function to access the current values of the sliders. In Shiny, each UI component is given an input identifier when it is created, which is used as the name of the value in the input list. So, we can access the minimum depth as input$minDate and the max as input$maxDate. Let’s use these values now by adding limits to our X axis in the ggplot: ggplot(delta_data, mapping = aes(SampleDate, Secchi)) + geom_point(colour=&quot;red&quot;, size=4) + xlim(c(input$minDate,input$maxDate)) + theme_light() At this point, we have a fully interactive plot, and the sliders can be used to change the min and max of the Depth axis. Looks so shiny! 6.1.5.5 Reversed Axes? What happens if a clever user sets the minimum for the X axis at a greater value than the maximum? You’ll see that the direction of the X axis becomes reversed, and the plotted points display right to left. This is really an error condition. Rather than use two independent sliders, we can modify the first slider to output a range of values, which will prevent the min from being greater than the max. You do so by setting the value of the slider to a vector of length 2, representing the default min and max date for the slider, such as c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")). So, delete the second slider, rename the first, and provide a vector for the value, like this: sliderInput(&quot;date&quot;, &quot;Date:&quot;, min = as.Date(&quot;1998-01-01&quot;), max = as.Date(&quot;2020-01-01&quot;), value = c(as.Date(&quot;1998-01-01&quot;), as.Date(&quot;2020-01-01&quot;))) ) Now, modify the ggplot to use this new date slider value, which now will be returned as a vector of length 2. The first element of the depth vector is the min, and the second is the max value on the slider. ggplot(delta_data, mapping = aes(SampleDate, Secchi)) + geom_point(colour=&quot;red&quot;, size=4) + xlim(c(input$date[1],input$date[2])) + theme_light() 6.1.6 Extending the user interface with dynamic plots If you want to display more than one plot in your application, and provide a different set of controls for each plot, the current layout would be too simple. Next we will extend the application to break the page up into vertical sections, and add a new plot in which the user can choose which variables are plotted. The current layout is set up such that the FluidPage contains the title element, and then a sidebarLayout, which is divided horizontally into a sidebarPanel and a mainPanel. 6.1.6.1 Vertical layout To extend the layout, we will first nest the existing sidebarLayout in a new verticalLayout, which simply flows components down the page vertically. Then we will add a new sidebarLayout to contain the bottom controls and graph. This mechanism of alternately nesting vertical and horizontal panels can be used to segment the screen into boxes with rules about how each of the panels is resized, and how the content flows when the browser window is resized. The sidebarLayout works to keep the sidebar about 1/3 of the box, and the main panel about 2/3, which is a good proportion for our controls and plots. Add the verticalLayout, and the second sidebarLayout for the second plot as follows: verticalLayout( # Sidebar with a slider input for depth axis sidebarLayout( sidebarPanel( sliderInput(&quot;date&quot;, &quot;Date:&quot;, min = as.Date(&quot;1998-01-01&quot;), max = as.Date(&quot;2020-01-01&quot;), value = c(as.Date(&quot;1998-01-01&quot;), as.Date(&quot;2020-01-01&quot;))) ), # Show a plot of the generated distribution mainPanel( plotOutput(&quot;distPlot&quot;) ) ), tags$hr(), sidebarLayout( sidebarPanel( selectInput(&quot;x_variable&quot;, &quot;X Variable&quot;, cols, selected = &quot;SampleDate&quot;), selectInput(&quot;y_variable&quot;, &quot;Y Variable&quot;, cols, selected = &quot;Count&quot;), selectInput(&quot;color_variable&quot;, &quot;Color&quot;, cols, selected = &quot;CommonName&quot;) ), # Show a plot with configurable axes mainPanel( plotOutput(&quot;varPlot&quot;) ) ), tags$hr() Note that the second sidebarPanel uses three selectInput elements to provide dropdown menus with the variable columns (cols) from our data frame. To manage that, we need to first set up the cols variable, which we do by saving the variables names from the delta_data data frame to a variable: sha1 &lt;- &#39;hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0&#39; delta.file &lt;- contentid::resolve(sha1, registries=c(&quot;dataone&quot;), store = TRUE) # fix the sample date format, and filter for species of interest delta_data &lt;- read.csv(delta.file) %&gt;% mutate(SampleDate = mdy(SampleDate)) %&gt;% filter(grepl(&quot;Salmon|Striped Bass|Smelt|Sturgeon&quot;, CommonName)) cols &lt;- names(delta_data) 6.1.6.2 Add the dynamic plot Because we named the second plot varPlot in our UI section, we now need to modify the server to produce this plot. Its very similar to the first plot, but this time we want to use the selected variables from the user controls to choose which variables are plotted. These variable names from the $input are character strings, and so would not be recognized as symbols in the aes mapping in ggplot. As recommended by the tidyverse authors, we use the non-standard evaluation syntax of .data[[\"colname\"]] to access the variables. output$varPlot &lt;- renderPlot({ ggplot(delta_data, aes(x = .data[[input$x_variable]], y = .data[[input$y_variable]], color = .data[[input$color_variable]])) + geom_point(size = 4)+ theme_light() }) 6.1.7 Finishing touches: data citation Citing the data that we used for this application is the right thing to do, and easy. You can add arbitrary HTML to the layout using utility functions in the tags list. # Application title titlePanel(&quot;Yolo Bypass Fish and Water Quality Data&quot;), p(&quot;Data for this application are from: &quot;), tags$ul( tags$li(&quot;Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.&quot;, tags$a(&quot;doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f&quot;, href=&quot;http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f&quot;) ) ), tags$br(), tags$hr(), The final application shows the data citation, the depth plot, and the configurable scatterplot in three distinct panels. 6.1.8 Publishing Shiny applications Once you’ve finished your app, you’ll want to share it with others. To do so, you need to publish it to a server that is set up to handle Shiny apps. Your main choices are: shinyapps.io (Hosted by RStudio) This is a service offered by RStudio, which is initially free for 5 or fewer apps and for limited run time, but has paid tiers to support more demanding apps. You can deploy your app using a single button push from within RStudio. Shiny server (On premises) This is an open source server which you can deploy for free on your own hardware. It requires more setup and configuration, but it can be used without a fee. RStudio connect (On premises) This is a paid product you install on your local hardware, and that contains the most advanced suite of services for hosting apps and RMarkdown reports. You can publish using a single button click from RStudio. A comparison of publishing features is available from RStudio. 6.1.8.1 Publishing to shinyapps.io The easiest path is to create an account on shinyapps.io, and then configure RStudio to use that account for publishing. Instructions for enabling your local RStudio to publish to your account are displayed when you first log into shinyapps.io: Once your account is configured locally, you can simply use the Publish button from the application window in RStudio, and your app will be live before you know it! 6.1.9 Summary Shiny is a fantastic way to quickly and efficiently provide data exploration for your data and code. We highly recommend it for its interactivity, but an archival-quality repository is the best long-term home for your data and products. In this example, we used data drawn directly from the EDI repository in our Shiny app, which offers both the preservation guarantees of an archive, plus the interactive data exploration from Shiny. You can utilize the full power of R and the tidyverse for writing your interactive applications. 6.1.10 Full source code for the final application library(shiny) library(contentid) library(dplyr) library(ggplot2) library(lubridate) # read in the data from EDI sha1 &lt;- &#39;hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0&#39; delta.file &lt;- contentid::resolve(sha1, registries=c(&quot;dataone&quot;), store = TRUE) # fix the sample date format, and filter for species of interest delta_data &lt;- read.csv(delta.file) %&gt;% mutate(SampleDate = mdy(SampleDate)) %&gt;% filter(grepl(&quot;Salmon|Striped Bass|Smelt|Sturgeon&quot;, CommonName)) cols &lt;- names(delta_data) # Define UI for application that draws a two plots ui &lt;- fluidPage( # Application title and data source titlePanel(&quot;Sacramento River floodplain fish and water quality dataa&quot;), p(&quot;Data for this application are from: &quot;), tags$ul( tags$li(&quot;Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.&quot;, tags$a(&quot;doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f&quot;, href=&quot;http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f&quot;) ) ), tags$br(), tags$hr(), verticalLayout( # Sidebar with a slider input for time axis sidebarLayout( sidebarPanel( sliderInput(&quot;date&quot;, &quot;Date:&quot;, min = as.Date(&quot;1998-01-01&quot;), max = as.Date(&quot;2020-01-01&quot;), value = c(as.Date(&quot;1998-01-01&quot;), as.Date(&quot;2020-01-01&quot;))) ), # Show a plot of the generated timeseries mainPanel( plotOutput(&quot;distPlot&quot;) ) ), tags$hr(), sidebarLayout( sidebarPanel( selectInput(&quot;x_variable&quot;, &quot;X Variable&quot;, cols, selected = &quot;SampleDate&quot;), selectInput(&quot;y_variable&quot;, &quot;Y Variable&quot;, cols, selected = &quot;Count&quot;), selectInput(&quot;color_variable&quot;, &quot;Color&quot;, cols, selected = &quot;CommonName&quot;) ), # Show a plot with configurable axes mainPanel( plotOutput(&quot;varPlot&quot;) ) ), tags$hr() ) ) # Define server logic required to draw the two plots server &lt;- function(input, output) { # turbidity plot output$distPlot &lt;- renderPlot({ ggplot(delta_data, mapping = aes(SampleDate, Secchi)) + geom_point(colour=&quot;red&quot;, size=4) + xlim(c(input$date[1],input$date[2])) + theme_light() }) # mix and match plot output$varPlot &lt;- renderPlot({ ggplot(delta_data, aes(x = .data[[input$x_variable]], y = .data[[input$y_variable]], color = .data[[input$color_variable]])) + geom_point(size = 4) + theme_light() }) } # Run the application shinyApp(ui = ui, server = server) 6.1.11 A shinier app with tabs and a map! library(shiny) library(contentid) library(dplyr) library(tidyr) library(ggplot2) library(lubridate) library(shinythemes) library(sf) library(leaflet) library(snakecase) # read in the data from EDI sha1 &lt;- &#39;hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0&#39; delta.file &lt;- contentid::resolve(sha1, registries=c(&quot;dataone&quot;), store = TRUE) # fix the sample date format, and filter for species of interest delta_data &lt;- read.csv(delta.file) %&gt;% mutate(SampleDate = mdy(SampleDate)) %&gt;% filter(grepl(&quot;Salmon|Striped Bass|Smelt|Sturgeon&quot;, CommonName)) %&gt;% rename(DissolvedOxygen = DO, Ph = pH, SpecificConductivity = SpCnd) cols &lt;- names(delta_data) sites &lt;- delta_data %&gt;% distinct(StationCode, Latitude, Longitude) %&gt;% drop_na() %&gt;% st_as_sf(coords = c(&#39;Longitude&#39;,&#39;Latitude&#39;), crs = 4269, remove = FALSE) # Define UI for application ui &lt;- fluidPage( navbarPage(theme = shinytheme(&quot;flatly&quot;), collapsible = TRUE, HTML(&#39;&lt;a style=&quot;text-decoration:none;cursor:default;color:#FFFFFF;&quot; class=&quot;active&quot; href=&quot;#&quot;&gt;Sacramento River Floodplain Data&lt;/a&gt;&#39;), id=&quot;nav&quot;, windowTitle = &quot;Sacramento River floodplain fish and water quality data&quot;, tabPanel(&quot;Data Sources&quot;, verticalLayout( # Application title and data source titlePanel(&quot;Sacramento River floodplain fish and water quality data&quot;), p(&quot;Data for this application are from: &quot;), tags$ul( tags$li(&quot;Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.&quot;, tags$a(&quot;doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f&quot;, href=&quot;http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f&quot;) ) ), tags$br(), tags$hr(), p(&quot;Map of sampling locations&quot;), mainPanel(leafletOutput(&quot;map&quot;)) ) ), tabPanel( &quot;Explore&quot;, verticalLayout( mainPanel( plotOutput(&quot;distPlot&quot;), width = 12, absolutePanel(id = &quot;controls&quot;, class = &quot;panel panel-default&quot;, top = 175, left = 75, width = 300, fixed=TRUE, draggable = TRUE, height = &quot;auto&quot;, sliderInput(&quot;date&quot;, &quot;Date:&quot;, min = as.Date(&quot;1998-01-01&quot;), max = as.Date(&quot;2020-01-01&quot;), value = c(as.Date(&quot;1998-01-01&quot;), as.Date(&quot;2020-01-01&quot;))) ) ), tags$hr(), sidebarLayout( sidebarPanel( selectInput(&quot;x_variable&quot;, &quot;X Variable&quot;, cols, selected = &quot;SampleDate&quot;), selectInput(&quot;y_variable&quot;, &quot;Y Variable&quot;, cols, selected = &quot;Count&quot;), selectInput(&quot;color_variable&quot;, &quot;Color&quot;, cols, selected = &quot;CommonName&quot;) ), # Show a plot with configurable axes mainPanel( plotOutput(&quot;varPlot&quot;) ) ), tags$hr() ) ) ) ) # Define server logic required to draw the two plots server &lt;- function(input, output) { output$map &lt;- renderLeaflet({leaflet(sites) %&gt;% addTiles() %&gt;% addCircleMarkers(data = sites, lat = ~Latitude, lng = ~Longitude, radius = 10, # arbitrary scaling fillColor = &quot;gray&quot;, fillOpacity = 1, weight = 0.25, color = &quot;black&quot;, label = ~StationCode) }) # turbidity plot output$distPlot &lt;- renderPlot({ ggplot(delta_data, mapping = aes(SampleDate, Secchi)) + geom_point(colour=&quot;red&quot;, size=4) + xlim(c(input$date[1],input$date[2])) + labs(x = &quot;Sample Date&quot;, y = &quot;Secchi Depth (m)&quot;) + theme_light() }) # mix and match plot output$varPlot &lt;- renderPlot({ ggplot(delta_data, mapping = aes(x = .data[[input$x_variable]], y = .data[[input$y_variable]], color = .data[[input$color_variable]])) + labs(x = to_any_case(input$x_variable, case = &quot;title&quot;), y = to_any_case(input$y_variable, case = &quot;title&quot;), color = to_any_case(input$color_variable, case = &quot;title&quot;)) + geom_point(size=4) + theme_light() }) } # Run the application shinyApp(ui = ui, server = server) 6.1.12 Resources Main Shiny site Official Shiny Tutorial "],["session-7-collaborative-synthesis.html", "7 Session 7: Collaborative synthesis", " 7 Session 7: Collaborative synthesis "],["session-8-reproducible-workflows-revisited.html", "8 Session 8: Reproducible workflows revisited 8.1 Workflows and Continuous Integration 8.2 Parallel Computing in R", " 8 Session 8: Reproducible workflows revisited 8.1 Workflows and Continuous Integration 8.1.1 Learning Outcomes Conceptualize workflows for reproducible processing Understand how workflow systems can simplify repetitive tasks Overview systems for executing workflows Understand the utility of continuous integration 8.1.2 Introduction Preparing data for running analysis, models, and visualization processes can be complex, with many dependencies among datasets, as well as complex needs for data cleaning, munging, and integration that need to occur before “analysis” can begin. Many research projects would benefit from a structured approach to organizing these processes into workflows. A research workflow is an ordered sequence of steps in which the outputs of one process are connected to the inputs of the next in a formal way. Steps are then chained together to typically create a directed, acyclic graph that represents the entire data processing pipeline. This hypothetical workflow shows three processing stages for downloading, integrating, and mapping the data, along with the outputs of each step. This is a simplified rendition of what is normally a much more complex process. Whether simple or complex, it is helpful to conceptualize your entire workflow as a directed graph, which helps to identify the explicit and implicit dependencies, and to plan work collaboratively. 8.1.3 Workflow dependencies and encapsulation While there are many thousands of details in any given analysis, the reason to create a workflow is to structure all of those details so that they are understandable and traceable. Being explicit about dependencies and building a hierarchical workflow that encapsulates the steps of the work as independent modules. So the idea is to focus the workflow on the major steps in the pipeline, and to articulate each of their dependencies. Workflows can be implemented in many ways, with various benefits: as a conceptual diagram As a series of functions that perform each step through a controlling script As a series functions managed by a workflow tool like targets many others… Here’s a simple, toy example of using functions to encapsulate a workflow. load_data &lt;- function() { delta_taxa_file &lt;- contentid::resolve(&quot;hash://sha256/1473de800f3c5577da077507fb006be816a9194ddd417b1b98836be92eaea49d&quot;) delta_taxa &lt;- readr::read_csv(delta_taxa_file, show_col_types = FALSE) print(&quot;Data loading complete.&quot;) return(delta_taxa) } clean_data &lt;- function(delta_taxa) { print(&quot;Data cleaning not implemented yet.&quot;) } plot_data &lt;- function(delta_taxa) { print(&quot;Plotting not implemented yet.&quot;) } run_workflow &lt;- function() { delta_taxa &lt;- load_data() delta_taxa_cleaned &lt;- clean_data(delta_taxa) plot_data(delta_taxa_cleaned) print(&quot;Worflow run completed.&quot;) } run_workflow() ## [1] &quot;Data loading complete.&quot; ## [1] &quot;Data cleaning not implemented yet.&quot; ## [1] &quot;Plotting not implemented yet.&quot; ## [1] &quot;Worflow run completed.&quot; This workflow modularizes the code so that it is reasonably understandable, and it makes the dependencies among the steps clear. But we can do more. Each time the workflow is run, all of the functions are run. We could improve efficiency by only running the functions for which a dependencies changed. Dependencies Dependencies are the data and processes that must have completed before a given step in the workflow can be run. In purely functional programming, all of the dependencies would be passed as arguments to the function. This makes it so that the function is able to run with only the information that is passed to it at runtime, and is very powerful. However, dependendencies can also be provided by writing files to disk, or into a daatbase. These are called side effects, because a change in the state of the application was made (e.g., a file was changed), but there is no signal in the main function calls that this has happened. Many workflow systems are simply trying to make it easier to manage both direct dependencies and side-effects so that execution of a workflow can be executed cleanly. 8.1.4 Benefits The benefits of conceptualizing work as a workflow include: Improved understanding Efficiency Automation Improved quality via modular testing Reproducibility 8.1.5 Organizing code in packages Utilizing functions is key to good workflow design. We also need mechanisms to organize these functions so that they are accessible to a workflow executor. In the toy example above I put all of the functions in a single code block in a single function. While this works, it would get unwieldy in larger projects. While there are various ways to include code that is present in multiple files (e.g., using source), R Packages are specifically designed to make it easy to encapsulate work into different functions and files, and have those be accessible to all parts of the workflow. They also provide a great mechanism for describing the installation dependencies of your code. The basic structure of an R package is just a series of R code in files in the R subdirectory, with metadata about the package: . ├── DESCRIPTION ├── LICENSE.md ├── NAMESPACE ├── R │   └── load_data.R │   └── load_data_taxa.R │   └── load_data_catch.R │   └── clean_taxa.R │   └── environment_info.R ├── man │   └── environment_info.Rd ├── mytools.Rproj └── tests ├── testthat │   └── test-enviroment_info.R └── testthat.R 8.1.6 Workflow systems While managing workflows solely as linked functions works, the presence of side-effects in a workflow can make it more difficult to efficiently run only the parts of the workflow where items have changed. Workflow systems like Make and [targets] have been created to provide a structured way to specify, analyze, and track dependencies, and to execute only the parts of the workflow that are needed. For example, here’s an example workflow from targets: In this workflow, each icon represents a target state of the application, and it is the job of the workflow executor to make sure that all of these “targets” are up-to-date. The final products are dependent on both a pre-processed data pipeline, and on the code for generating a plot. In this example, the dark green icons indicate parts of the workflow that have not changed. Whereas the blue create_plot box indicates that the function has changed, which then “taints” all of the downstream parts of the workflow that depend on it. So, in this case, the change in create_plot means that the hist target must be re-executed, but the data processing pipeline above it does not currently need to be re-run. Targets is configured by producing a special R script (_targets.R) that sets up the workflow to be executed. Here’s an example from the simple workflow example above: library(targets) library(tarchetypes) source(&quot;R/functions.R&quot;) options(tidyverse.quiet = TRUE) tar_option_set(packages = c(&quot;biglm&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;readr&quot;, &quot;tidyr&quot;)) list( tar_target( raw_data_file, &quot;data/raw_data.csv&quot;, format = &quot;file&quot; ), tar_target( raw_data, read_csv(raw_data_file, col_types = cols()) ), tar_target( data, raw_data %&gt;% filter(!is.na(Ozone)) ), tar_target(hist, create_plot(data)), tar_target(fit, biglm(Ozone ~ Wind + Temp, data)), tar_render(report, &quot;index.Rmd&quot;) ) This is really useful for being able to incrementally work through data loading and cleaning pipelines that feed downstream analytical functions that depend on using a consistent set of input data. 8.1.7 Exercise Take an analysis that you are familiar with, and: draw a diagram of the major steps and substeps of the workflow analyze the dependencies of these steps and substeps stub out a set of functions that would execute that workflow 8.1.8 Readings and tutorials The targets package 8.2 Parallel Computing in R 8.2.1 Learning Outcomes Understand what parallel computing is and when it may be useful Understand how parallelism can work Review sequential loops and *apply functions Understand and use the parallel package multicore functions Understand and use the foreach package functions 8.2.2 Introduction Processing large amounts of data with complex models can be time consuming. New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America: Levy et al. 2016. doi:10.5063/F1Z899CZ There are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization. Alternatively, think of remote sensing data. Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years. NEON Data Cube 8.2.3 Why parallelism? Much R code runs fast and fine on a single processor. But at times, computations can be: cpu-bound: Take too much cpu time memory-bound: Take too much memory I/O-bound: Take too much time to read/write from disk network-bound: Take too much time to transfer To help with cpu-bound computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire cluster of those computers. Plus, these machines also have large amounts of memory to avoid memory-bound computing jobs. 8.2.4 Processors (CPUs) and Cores A modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with mutliple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations. A computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time. A typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here we show four quad-core processors for a total of 16 cores in this machine. You can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later). Historically, R has only utilized one processor, which makes it single-threaded. Which is a shame, because the 2017 MacBook Pro that I am writing this on is much more powerful than that: jones@powder:~$ sysctl hw.ncpu hw.physicalcpu hw.ncpu: 8 hw.physicalcpu: 4 To interpret that output, this machine powder has 4 physical CPUs, each of which has two processing cores, for a total of 8 cores for computation. I’d sure like my R computations to use all of that processing power. Because its all on one machine, we can easily use multicore processing tools to make use of those cores. Now let’s look at the computational server aurora at NCEAS: jones@aurora:~$ lscpu | egrep &#39;CPU\\(s\\)|per core|per socket&#39; CPU(s): 88 On-line CPU(s) list: 0-87 Thread(s) per core: 2 Core(s) per socket: 22 NUMA node0 CPU(s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86 NUMA node1 CPU(s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87 Now that’s some compute power! Aurora has 384 GB of RAM, and ample storage. All still under the control of a single operating system. However, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now: JetStream 640 nodes, 15,360 cores, 80TB RAM Stampede2 at TACC is coming online in 2017 4200 nodes, 285,600 cores Note that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster. One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine. But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster. 8.2.5 When to parallelize It’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer! In addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced. Some propose that this may follow Amdahl’s Law, where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines): So, its important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done. With that, let’s do some parallel computing… 8.2.6 Loops and repetitive tasks using lapply When you have a list of repetitive tasks, you may be able to speed it up by adding more computing power. If each task is completely independent of the others, then it is a prime candidate for executing those tasks in parallel, each on its own core. For example, let’s build a simple loop that uses sample with replacement to do a bootstrap analysis. In this case, we select Sepal.Length and Species from the iris dataset, subset it to 100 observations, and then iterate across 10,000 trials, each time resampling the observations with replacement. We then run a logistic regression fitting species as a function of length, and record the coefficients for each trial to be returned. x &lt;- iris[which(iris[,5] != &quot;setosa&quot;), c(1,5)] trials &lt;- 10000 res &lt;- data.frame() system.time({ trial &lt;- 1 while(trial &lt;= trials) { ind &lt;- sample(100, 100, replace=TRUE) result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit)) r &lt;- coefficients(result1) res &lt;- rbind(res, r) trial &lt;- trial + 1 } }) ## user system elapsed ## 23.84 1.58 25.48 The issue with this loop is that we execute each trial sequentially, which means that only one of our 8 processors on this machine are in use. In order to exploit parallelism, we need to be able to dispatch our tasks as functions, with one task going to each processor. To do that, we need to convert our task to a function, and then use the *apply() family of R functions to apply that function to all of the members of a set. In R, using apply used to be faster than the equivalent code in a loop, but now they are similar due to optimizations in R loop handling. However, using the function allows us to later take advantage of other approaches to parallelization. Here’s the same code rewritten to use lapply(), which applies a function to each of the members of a list (in this case the trials we want to run): x &lt;- iris[which(iris[,5] != &quot;setosa&quot;), c(1,5)] trials &lt;- seq(1, 10000) boot_fx &lt;- function(trial) { ind &lt;- sample(100, 100, replace=TRUE) result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit)) r &lt;- coefficients(result1) res &lt;- rbind(data.frame(), r) } system.time({ results &lt;- lapply(trials, boot_fx) }) ## user system elapsed ## 25.894 0.756 26.711 8.2.7 Approaches to parallelization When parallelizing jobs, one can: Use the multiple cores on a local computer through mclapply Use multiple processors on local (and remote) machines using makeCluster and clusterApply In this approach, one has to manually copy data and code to each cluster member using clusterExport This is extra work, but sometimes gaining access to a large cluster is worth it 8.2.8 Parallelize using: mclapply The parallel library can be used to send tasks (encoded as function calls) to each of the processing cores on your machine in parallel. This is done by using the parallel::mclapply function, which is analogous to lapply, but distributes the tasks to multiple processors. mclapply gathers up the responses from each of these function calls, and returns a list of responses that is the same length as the list or vector of input data (one return per input item). library(parallel) library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select starts &lt;- rep(100, 40) fx &lt;- function(nstart) kmeans(Boston, 4, nstart=nstart) numCores &lt;- detectCores() numCores ## [1] 3 system.time( results &lt;- lapply(starts, fx) ) ## user system elapsed ## 1.248 0.118 1.374 system.time( results &lt;- mclapply(starts, fx, mc.cores = numCores) ) ## user system elapsed ## 0.902 0.273 0.733 Now let’s demonstrate with our bootstrap example: x &lt;- iris[which(iris[,5] != &quot;setosa&quot;), c(1,5)] trials &lt;- seq(1, 10000) boot_fx &lt;- function(trial) { ind &lt;- sample(100, 100, replace=TRUE) result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit)) r &lt;- coefficients(result1) res &lt;- rbind(data.frame(), r) } system.time({ results &lt;- mclapply(trials, boot_fx, mc.cores = numCores) }) ## user system elapsed ## 18.40 1.35 11.69 8.2.9 Parallelize using: foreach and doParallel The normal for loop in R looks like: for (i in 1:3) { print(sqrt(i)) } ## [1] 1 ## [1] 1.41 ## [1] 1.73 The foreach method is similar, but uses the sequential %do% operator to indicate an expression to run. Note the difference in the returned data structure. library(foreach) foreach (i=1:3) %do% { sqrt(i) } ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1.41 ## ## [[3]] ## [1] 1.73 In addition, foreach supports a parallelizable operator %dopar% from the doParallel package. This allows each iteration through the loop to use different cores or different machines in a cluster. Here, we demonstrate with using all the cores on the current machine: library(foreach) library(doParallel) ## Loading required package: iterators registerDoParallel(numCores) # use multicore, set to the number of our cores foreach (i=1:3) %dopar% { sqrt(i) } ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1.41 ## ## [[3]] ## [1] 1.73 # To simplify output, foreach has the .combine parameter that can simplify return values # Return a vector foreach (i=1:3, .combine=c) %dopar% { sqrt(i) } ## [1] 1.00 1.41 1.73 # Return a data frame foreach (i=1:3, .combine=rbind) %dopar% { sqrt(i) } ## [,1] ## result.1 1.00 ## result.2 1.41 ## result.3 1.73 The doParallel vignette on CRAN shows a much more realistic example, where one can use `%dopar% to parallelize a bootstrap analysis where a data set is resampled 10,000 times and the analysis is rerun on each sample, and then the results combined: # Let&#39;s use the iris data set to do a parallel bootstrap # From the doParallel vignette, but slightly modified x &lt;- iris[which(iris[,5] != &quot;setosa&quot;), c(1,5)] trials &lt;- 10000 system.time({ r &lt;- foreach(icount(trials), .combine=rbind) %dopar% { ind &lt;- sample(100, 100, replace=TRUE) result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit)) coefficients(result1) } }) ## user system elapsed ## 24.75 2.14 11.58 # And compare that to what it takes to do the same analysis in serial system.time({ r &lt;- foreach(icount(trials), .combine=rbind) %do% { ind &lt;- sample(100, 100, replace=TRUE) result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit)) coefficients(result1) } }) ## user system elapsed ## 23.830 0.759 24.651 # When you&#39;re done, clean up the cluster stopImplicitCluster() 8.2.10 Summary In this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores. Next, we reviewed the way in which traditional for loops in R can be rewritten as functions that are applied to a list serially using lapply, and then how the parallel package mclapply function can be substituted in order to utilize multiple cores on the local computer to speed up computations. Finally, we installed and reviewed the use of the foreach package with the %dopar operator to accomplish a similar parallelization using multiple cores. 8.2.11 Readings and tutorials Multicore Data Science with R and Python Beyond Single-Core R by Jonoathan Dursi (also see GitHub repo for slide source) The venerable Parallel R by McCallum and Weston (a bit dated on the tooling, but conceptually solid) The doParallel Vignette "],["session-9-communicating-your-research-the-message-box.html", "9 Session 9: Communicating Your Research: The Message Box 9.1 Communication Principles and Practices", " 9 Session 9: Communicating Your Research: The Message Box 9.1 Communication Principles and Practices “The ingredients of good science are obvious—novelty of research topic, comprehensive coverage of the relevant literature, good data, good analysis including strong statistical support, and a thought-provoking discussion. The ingredients of good science reporting are obvious—good organization, the appropriate use of tables and figures, the right length, writing to the intended audience— do not ignore the obvious” - Bourne 2005 9.1.1 Scholarly publications Peer-reviewed publication remains a primary mechanism for direct and efficient communication of research findings. Other scholarly communications include abstracts, technical reports, books and book chapters. These communications are largely directed towards students and peers; individuals learning about or engaged in the process of scientific research whether in a university, non-profit, agency, commercial or other setting. In this section we will focus less on peer-reviewed publications and more on messaging in general, whether for publications, reports, articles or social media. That said, the following tabling is a good summary of ‘10 Simple Rules’ for writing research papers (adapted from Zhang 2014, published in Budden and Michener, 2017) Make it a Driving Force “design a project with an ultimate paper firmly in mind” Less Is More “fewer but more significant papers serve both the research community and one’s career better than more papers of less significance” Pick the Right Audience “This is critical for determining the organization of the paper and the level of detail of the story, so as to write the paper with the audience in mind.” Be Logical “The foundation of ‘‘lively’’ writing for smooth reading is a sound and clear logic underlying the story of the paper.” “An effective tactic to help develop a sound logical flow is to imaginatively create a set of figures and tables, which will ultimately be developed from experimental results, and order them in a logical way based on the information flow through the experiments.” Be Thorough and Make It Complete Present the central underlying hypotheses; interpret the insights gleaned from figures and tables and discuss their implications; provide sufficient context so the paper is self-contained; provide explicit results so readers do not need to perform their own calculations; and include self-contained figures and tables that are described in clear legends Be Concise “the delivery of a message is more rigorous if the writing is precise and concise” Be Artistic “concentrate on spelling, grammar, usage, and a ‘‘lively’’ writing style that avoids successions of simple, boring, declarative sentences” Be Your Own Judge Review, revise and reiterate. “…put yourself completely in the shoes of a referee and scrutinize all the pieces—the significance of the work, the logic of the story, the correctness of the results and conclusions, the organization of the paper, and the presentation of the materials.” Test the Water in Your Own Backyard “…collect feedback and critiques from others, e.g., colleagues and collaborators.” Build a Virtual Team of Collaborators Treat reviewers as collaborators and respond objectively to their criticisms and recommendations. This may entail redoing research and thoroughly re-writing a paper. 9.1.2 Other communications Communicating your research outside of peer-reviewed journal articles is increasingly common, and important. These non academic communications can reach a more broad and diverse audience than traditional publications (and should be tailored for specific audience groups accordingly), are not subject to the same pay-walls as journal articles, and can augment and amplify scholarly publications. For example, this twitter announcement from Dr Heather Kropp, providing a synopsis of their synthesis publication in Environmental Research (note the reference to a repository where the data are located, though a DOI would be better). Whether this communication occurs through blogs, social media, or via interviews with others, developing practices to refine your messaging is critical for successful communication. One tool to support your communication practice is ‘The Message Box’ developed by COMPASS, an organization that helps scientists develop communications skills in order to share their knowledge and research across broad audiences without compromising the accuracy of their research. 9.1.3 The Message Box The Message Box is a tool that helps researchers take the information they hold about their research and communicate it in a way that resonates with the chosen audience. It can be used to help prepare for interviews with journalists or employers, plan for a presentation, outline a paper or lecture, prepare a grant proposal, or clearly, and with relevancy, communicate your work to others. While the message box can be used in all these ways, you must first identify the audience for your communication. The Message Box comprises five sections to help you sort and distill your knowledge in a way that will resonate with your (chosen) audience. How we communicate with other scientists (through scholarly publications) is not how the rest of the rest of the world typically communicates. In a scientific paper, we establish credibility in the introduction and methods, provide detailed data and results, and then share the significance of our work in the discussion and conclusions. But the rest of the world leads with the impact, the take home message. A quick glance of newspaper headlines demonstrates this. The five sections of the Message Box are provided below. For a detailed explanation of the sections and guidance on how to use the Message Box, work through the Message Box Workbook 9.1.3.1 Message Box Sections The Issue The Issue section in the center of the box identifies and describes the overarching issue or topic that you’re addressing in broad terms. It’s the big-picture context of your work. This should be very concise and clear; no more than a short phrase. You might find you revisit the Issue after you’ve filled out your Message Box, to see if your thinking on the overarching topic has changed since you started. Describes the overarching issue or topic: Big Picture Broad enough to cover key points Specific enough to set up what’s to come Concise and clear ‘Frames’ the rest of the message box The Problem The Problem is the chunk of the broader issue that you’re addressing in your area of expertise. It’s your piece of the pie, reflecting your work and expert knowledge. Think about your research questions and what aspect of the specific problem you’re addressing would matter to your audience. The Problem is also where you set up the So What and describe the situation you see and want to address. The part of the broader issue that your work is addressing Builds upon your work and expert knowledge Try to focus on one problem per audience Often the Problem is your research question This section sets you up for So What The So What The crux of the Message Box, and the critical question the COMPASS team seeks to help scientists answer, is “So what?” Why should your audience care? What about your research or work is important for them to know? Why are you talking to them about it? The answer to this question may change from audience to audience, and you’ll want to be able to adjust based on their interests and needs. We like to use the analogy of putting a message through a prism that clarifies the importance to different audiences. Each audience will be interested in different facets of your work, and you want your message to reflect their interests and accommodate their needs. The prism below includes a spectrum of audiences you might want to reach, and some of the questions they might have about your work. This is the crux of the message box Why should you audience care? What about your research is important for them to know? Why are you talking to them about it? The Solution The Solution section outlines the options for solving the problem you identified. When presenting possible solutions, consider whether they are something your audience can influence or act upon. And remind yourself of your communication goals: Why are you communicating with this audience? What do you want to accomplish? Outlines the options for solving the Problem Can your audience influence or act upon this? There may be multiple solutions Make sure your Solution relates back to the Problem. Edit one or both as needed The Benefit In the Benefit section, you list the benefits of addressing the Problem — all the good things that could happen if your Solution section is implemented. This ties into the So What of why your audience cares, but focuses on the positive results of taking action (the So What may be a negative thing — for example, inaction could lead to consequences that your audience cares about). If possible, it can be helpful to be specific here — concrete examples are more compelling than abstract. Who is likely to benefit, and where, and when? What are the benefits of addressing the Problem? What good things come from implementing your Solution? Make sure it connects with you So What Benefits and So What may be similar Additionally, to help make you message more memorable you should: Support your message with data Limit the use of numbers and statistics Use specific examples Compare numbers to concepts, help people relate Avoid jargon Lead with what you know In addition to the Message Box Workbook, COMPASS have resources on how to increase the impact of your message (include important statistics, draw comparisons, reduce jargon, use examples), exercises for practicing and refining your message and published examples. 9.1.4 Resources DataONE Webinar: Communication Strategies to Increase Your Impact from DataONE on Vimeo. Budden, AE and Michener, W (2017) Communicating and Disseminating Research Findings. In: Ecological Informatics, 3rd Edition. Recknagel, F, Michener, W (eds.) Springer-Verlag COMPASS Core Principles of Science Communication Example Message Boxes "],["session-10-collaborative-synthesis.html", "10 Session 10: Collaborative synthesis", " 10 Session 10: Collaborative synthesis "],["session-11-collaborative-synthesis.html", "11 Session 11: Collaborative Synthesis", " 11 Session 11: Collaborative Synthesis "],["session-12-collaborative-synthesis.html", "12 Session 12: Collaborative Synthesis", " 12 Session 12: Collaborative Synthesis "],["session-13-collaborative-synthesis-and-wrap-up.html", "13 Session 13: Collaborative Synthesis and Wrap-up", " 13 Session 13: Collaborative Synthesis and Wrap-up "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
