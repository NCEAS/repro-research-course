[["index.html", "Reproducible Practices for Arctic Research Using R Reproducible Practices for Arctic Research Using R 0.1 Schedule", " Reproducible Practices for Arctic Research Using R February 27 - March 3, 2023 Reproducible Practices for Arctic Research Using R The Arctic Data Center conducts training in data science and management, both of which are critical skills for stewardship of data, software, and other products of research that are preserved at the Arctic Data Center. 0.1 Schedule 0.1.1 Code of Conduct Please note that by participating in an NCEAS activity you agree to abide by our Code of Conduct 0.1.2 Logistics 0.1.2.1 Overview Welcome to the NCEAS Reproducible Research Techniques for Synthesis short course. In previous years, we have been able to run this course on site as a 5-day intensive training between 8:00 am and 5:00 pm. Participants traveled to NCEAS in Santa Barbara and we asked that participants were as engaged as possible during the instructional period. Although we have had to switch to a remote training model due to COVID-19, we ask participants for the same level of commitment in this remote setting, while understanding that some conflicts are unavoidable. We will be using the following tools to facilitate this training: Zoom (version 5.3.1) Slack (desktop app preferred) RStudio, accessed through a server on an up-to-date web browser: Firefox (version 80+) Chrome (version 80+) Safari (version 13+) Edge (version 81+) 0.1.2.2 Server You should receive a separate email prompting you to change your password using the NCEAS account service. Please change your password, and then ensure that you can log in at https://included-crab.nceas.ucsb.edu/. 0.1.2.3 Monitors If you have a second monitor or second device, it would be useful for this training. You’ll need enough screen real estate to handle the primary Zoom window, the participant pane in Zoom, Slack, and a browser with tabs for RStudio and our training curriculum. We recommend either using two monitors, or joining the Zoom room from a second device. If you must be on one machine for everything, here’s an example of what it could look like when you are following along with the class and how your screen will shift when you have a more detailed question that requires breakout assistance. When we’re in session, please turn your camera on, and mute your microphone unless you would like to ask a question or contribute to a discussion. 0.1.2.4 Working from Home If you are working from home, the appearance or sound of other adults, children, and pets in remote meetings such as this is completely normal and understandable. Having your video on and enabling the instructors and your fellow participants to see you brings some humanity to this physically distant workshop, and we believe that this is a crucial element of its success. If you would like to use the Zoom virtual background feature to hide your surroundings, please do provided your background of choice fits within the code of conduct (and here are some Arctic themed backgrounds if you need inspiration). 0.1.2.5 Non-Verbal Feedback We’ll be using the Zoom “Non Verbal Feedback” buttons throughout this course. We will ask you to put a green check by your name when you’re all set or you understand, and a red x by your name if you’re stuck or need assistance. These buttons can be found in the participants panel of the Zoom room. When you’re asked to answer using these buttons, please ensure that you select one so that the instructor has the feedback that they need to either continue the lesson or pause until everyone gets back on the same page. 0.1.2.6 Questions and Getting Help When you need to ask a question, please do so in one of the following ways: Turn your mic on and ask. If you are uncomfortable interrupting the instructor, you may also raise your virtual hand (in the participant panel) and the session facilitator will ask the instructor to pause and call upon you. Ask your question in the slack channel. If you have an issue/error and get stuck, you can ask for help in the following ways: Turn your mic on and ask for help. See also above regarding the use of a virtual raised hand. Let one of the instructors know in the slack channel. If prompted to do so, put a red X next to your name as your status in the participant window. When you have detailed questions or need one on one coding assistance, we will have zoom breakout rooms available with helpers. The helper will try to help you in Slack first. If the issue requires more in-depth troubleshooting, the helper will invite you to join their named Breakout Room. 0.1.2.7 The Power of Open To facilitate a lively and interactive learning environment, we’ll be calling on folks to share their code and to answer various questions posed by the instructor. It’s completely okay to say “Pass” or “I Don’t Know” - this is a supportive learning environment and we will all learn from each other. The instructors will be able to see your code as you go to help you if you get stuck, and the lead instructor may share participants’ code to show a successful example or illustrate a teaching moment. 0.1.3 About this book These written materials are the result of a continuous effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials. This work is licensed under a Creative Commons Attribution 4.0 International License. Citation: Jones, Matthew B., Amber E. Budden, Bryce Mecum, S. Jeanette Clark, Julien Brun, Julie Lowndes, Halina Do-Linh, Camila Vargas Poulsen, Daphne Virlar-Knight. 2023. Reproducible Research Techniques for Synthesis. NCEAS Learning Hub. Additional contributors: Erin McLean, Jessica S. Guo, David S. LeBauer, Ben Bolker, Stephanie Hampton, Samanta Katz, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Leah Wasser. "],["welcome-and-introductions.html", "1 Welcome and Introductions 1.1 Introduction to the Arctic Data Center and NSF Standards and Policies 1.2 Writing Good Data Management Plans", " 1 Welcome and Introductions This course is one of three that we are currently offering, covering fundamentals of open data sharing, reproducible research, ethical data use and reuse, and scalable computing for reusing large data sets. 1.1 Introduction to the Arctic Data Center and NSF Standards and Policies 1.1.1 Learning Objectives In this lesson, we will discuss: The mission and structure of the Arctic Data Center How the Arctic Data Center supports the research community About data policies from the NSF Arctic program 1.1.2 Arctic Data Center - History and Introduction The Arctic Data Center is the primary data and software repository for the Arctic section of National Science Foundation’s Office of Polar Programs (NSF OPP). We’re best known in the research community as a data archive – researchers upload their data to preserve it for the future and make it available for re-use. This isn’t the end of that data’s life, though. These data can then be downloaded for different analyses or synthesis projects. In addition to being a data discovery portal, we also offer top-notch tools, support services, and training opportunities. We also provide data rescue services. NSF has long had a commitment to data reuse and sharing. Since our start in 2016, we’ve grown substantially – from that original 4 TB of data from ACADIS to now over 76 TB at the start of 2023. In 2021 alone, we saw 16% growth in dataset count, and about 30% growth in data volume. This increase has come from advances in tools – both ours and of the scientific community, plus active community outreach and a strong culture of data preservation from NSF and from researchers. We plan to add more storage capacity in the coming months, as researchers are coming to us with datasets in the terabytes, and we’re excited to preserve these research products in our archive. We’re projecting our growth to be around several hundred TB this year, which has a big impact on processing time. Give us a heads up if you’re planning on having larger submissions so that we can work with you and be prepared for a large influx of data. The data that we have in the Arctic Data Center comes from a wide variety of disciplines. These different programs within NSF all have different focuses – the Arctic Observing Network supports scientific and community-based observations of biodiversity, ecosystems, human societies, land, ice, marine and freshwater systems, and the atmosphere as well as their social, natural, and/or physical environments, so that encompasses a lot right there in just that one program. We’re also working on a way right now to classify the datasets by discipline, so keep an eye out for that coming soon. Along with that diversity of disciplines comes a diversity of file types. The most common file type we have are image files in four different file types. Probably less than 200-300 of the datasets have the majority of those images – we have some large datasets that have image and/or audio files from drones. Most of those 6600+ datasets are tabular datasets. There’s a large diversity of data files, though, whether you want to look at remote sensing images, listen to passive acoustic audio files, or run applications – or something else entirely. We also cover a long period of time, at least by human standards. The data represented in our repository spans across centuries. We also have data that spans the entire Arctic, as well as the sub-Arctic, regions. 1.1.3 Data Discovery Portal To browse the data catalog, navigate to arcticdata.io. Go to the top of the page and under data, go to search. Right now, you’re looking at the whole catalog. You can narrow your search down by the map area, a general search, or searching by an attribute. Clicking on a dataset brings you to this page. You have the option to download all the files by clicking the green “Download All” button, which will zip together all the files in the dataset to your Downloads folder. You can also pick and choose to download just specific files. All the raw data is in open formats to make it easily accessible and compliant with FAIR principles – for example, tabular documents are in .csv (comma separated values) rather than Excel documents. The metrics at the top give info about the number of citations with this data, the number of downloads, and the number of views. This is what it looks like when you click on the Downloads tab for more information. Scroll down for more info about the dataset – abstract, keywords. Then you’ll see more info about the data itself. This shows the data with a description, as well as info about the attributes (or variables or parameters) that were measured. The green check mark indicates that those attributes have been annotated, which means the measurements have a precise definition to them. Scrolling further, we also see who collected the data, where they collected it, and when they collected it, as well as any funding information like a grant number. For biological data, there is the option to add taxa. 1.1.4 Tools and Infrastructure Across all our services and partnership, we are strongly aligned with the community principles of making data FAIR (Findable, Accesible, Interoperable and Reusable). We have a number of tools available to submitters and researchers who are there to download data. We also partner with other organizations, like Make Data Count and DataONE, and leverage those partnerships to create a better data experience. One of those tools is provenance tracking. With provenance tracking, users of the Arctic Data Center can see exactly what datasets led to what product, using the particular script that the researcher ran. Another tool are our Metadata Quality Checks. We know that data quality is important for researchers to find datasets and to have trust in them to use them for another analysis. For every submitted dataset, the metadata is run through a quality check to increase the completeness of submitted metadata records. These checks are seen by the submitter as well as are available to those that view the data, which helps to increase knowledge of how complete their metadata is before submission. That way, the metadata that is uploaded to the Arctic Data Center is as complete as possible, and close to following the guideline of being understandable to any reasonable scientist. 1.1.5 Support Services Metadata quality checks are the automatic way that we ensure quality of data in the repository, but the real quality and curation support is done by our curation team. The process by which data gets into the Arctic Data Center is iterative, meaning that our team works with the submitter to ensure good quality and completeness of data. When a submitter submits data, our team gets a notification and beings to evaluate the data for upload. They then go in and format it for input into the catalog, communicating back and forth with the researcher if anything is incomplete or not communicated well. This process can take anywhere from a few days or a few weeks, depending on the size of the dataset and how quickly the researcher gets back to us. Once that process has been completed, the dataset is published with a DOI (digital object identifier). 1.1.6 Training and Outreach In addition to the tools and support services, we also interact with the community via trainings like this one and outreach events. We run workshops at conferences like the American Geophysical Union, Arctic Science Summit Week and others. We also run an intern and fellows program, and webinars with different organizations. We’re invested in helping the Arctic science community learn reproducible techniques, since it facilitates a more open culture of data sharing and reuse. We strive to keep our fingers on the pulse of what researchers like yourselves are looking for in terms of support. We’re active on Twitter to share Arctic updates, data science updates, and specifically Arctic Data Center updates, but we’re also happy to feature new papers or successes that you all have had with working with the data. We can also take data science questions if you’re running into those in the course of your research, or how to make a quality data management plan. Follow us on Twitter and interact with us – we love to be involved in your research as it’s happening as well as after it’s completed. 1.1.7 Data Rescue We also run data rescue operations. We digitiazed Autin Post’s collection of glacier photos that were taken from 1964 to 1997. There were 100,000+ files and almost 5 TB of data to ingest, and we reconstructed flight paths, digitized the images of his notes, and documented image metadata, including the camera specifications. 1.1.8 Who Must Submit Projects that have to submit their data include all Arctic Research Opportunities through the NSF Office of Polar Programs. That data has to be uploaded within two years of collection. The Arctic Observing Network has a shorter timeline – their data products must be uploaded within 6 months of collection. Additionally, we have social science data, though that data often has special exceptions due to sensitive human subjects data. At the very least, the metadata has to be deposited with us. Arctic Research Opportunities (ARC) Complete metadata and all appropriate data and derived products Within 2 years of collection or before the end of the award, whichever comes first ARC Arctic Observation Network (AON) Complete metadata and all data Real-time data made public immediately Within 6 months of collection Arctic Social Sciences Program (ASSP) NSF policies include special exceptions for ASSP and other awards that contain sensitive data Human subjects, governed by an Institutional Review Board, ethically or legally sensitive, at risk of decontextualization Metadata record that documents non-sensitive aspects of the project and data Title, Contact information, Abstract, Methods For more complete information see our “Who Must Submit” webpage Recognizing the importance of sensitive data handling and of ethical treatment of all data, the Arctic Data Center submission system provides the opportunity for researchers to document the ethical treatment of data and how collection is aligned with community principles (such as the CARE principles). Submitters may also tag the metadata according to community develop data sensitivity tags. We will go over these features in more detail shortly. 1.1.9 Summary All the above informtion can be found on our website or if you need help, ask our support team at support@arcticdata.io or tweet us @arcticdatactr! 1.2 Writing Good Data Management Plans 1.2.1 Learning Objectives In this lesson, you will learn: Why create data management plans The major components of data management plans Tools that can help create a data management plan Features and functionality of the DMPTool 1.2.2 When to Plan: The Data Life Cycle Shown below is one version of the Data Life Cycle that was developed by DataONE. The data life cycle provides a high level overview of the stages involved in successful management and preservation of data for use and reuse. Multiple versions of the data life cycle exist with differences attributable to variation in practices across domains or communities. It is not necessary for researchers to move through the data life cycle in a cyclical fashion and some research activities might use only part of the life cycle. For instance, a project involving meta-analysis might focus on the Discover, Integrate, and Analyze steps, while a project focused on primary data collection and analysis might bypass the Discover and Integrate steps. However, Plan is at the top of the data life cycle as it is advisable to initiate your data management planning at the beginning of your research process, before any data has been collected. 1.2.3 Why Plan? Planning data management in advance provides a number of benefits to the researcher. Saves time and increases efficiency: Data management planning requires that a researcher think about data handling in advance of data collection, potentially raising any challenges before they occur. Engages your team: Being able to plan effectively will require conversation with multiple parties, engaging project participants from the outset. Allows you to stay organized: It will be easier to organize your data for analysis and reuse if you’ve made a plan about what analysis you want to run, future iterations, and more. Meet funder requirements: Most funders require a data management plan (DMP) as part of the proposal process. Share data: Information in the DMP is the foundation for archiving and sharing data with community. 1.2.4 How to Plan Make sure to plan from the start to avoid confusion, data loss, and increase efficiency. Given DMPs are a requirement of funding agencies, it is nearly always necessary to plan from the start. However, the same should apply to research that is being undertaken outside of a specific funded proposal. As indicated above, engaging your team is a benefit of data management planning. Collaborators involved in the data collection and processing of your research data bring diverse expertise. Therefore, plan in collaboration with these individuals. Make sure to utilize resources that are available to assist you in helping to write a good DMP. These might include your institutional library or organization data manager, online resources or education materials such as these. Use tools available to you; you don’t have to reinvent the wheel. Revise your plan as situations change or as you potentially adapt/alter your project. Like your research projects, DMPs are not static, they require changes and updates throughout the research project process. 1.2.5 What to include in a DMP If you are writing a DMP as part of a solicitation proposal, the funding agency will have guidelines for the information they want to be provided in the plan. However, in general, a good plan will provide information on the: study design data to be collected metadata policies for access sharing &amp; reuse long-term storage &amp; data management budget A note on Metadata: Both basic metadata (such as title and researcher contact information) and comprehensive metadata (such as complete methods of data collection) are critical for accurate interpretation and understanding. The full definitions of variables, especially units, inside each dataset are also critical as they relate to the methods used for creation. Knowing certain blocking or grouping methods, for example, would be necessary to understand studies for proper comparisons and synthesis. 1.2.6 NSF DMP requirements In the 2014 Proposal Preparation Instructions, Section J ‘Special Information and Supplementary Documentation’ NSF put forward the baseline requirements for a DMP. In addition, there are specific division and program requirements that provide additional detail. If you are working on a research project with funding that does not require a DMP, or are developing a plan for unfunded research, the NSF generic requirements are a good set of guidelines to follow. The following questions are the prompting questions in the Arctic Data Center DMP template for NSF projects, excluding the fairly straightforward personnel section. Five Sections of the NSF DMP Requirements 1. What types of data, samples, collections, software, materials, etc. will be produced during your project? Types of data, samples, physical collections, software, curriculum materials, other materials produced during project 2. What format(s) will data and metadata be collected, processed, and stored in? Standards to be used for data and metadata format and content (for initial data collection, as well as subsequent storage and processing) 3. How will data be accessed and shared during the course of the project? Provisions for appropriate protection of privacy, confidentiality, security, intellectual property, or other rights or requirements 4. How do you anticipate the data for this project will be used? Consider which bodies/groups are likely to be interested in the data, what and who are the intended or foreseeable uses/users of the data. Include re-distribution and the production of derivatives 5. What is the long-term strategy for maintaining, curating, and archiving the data? Plans for archiving data, samples, research products and for preservation of access 1.2.6.1 Individual Reflection Now that we’ve discussed the data life cycle, how to plan, what to generally include in a DMP, and the NSF DMP requirements - take five minutes to go through each required section for a NSF DMP and write down some initial thoughts on how you would approach completing those sections. What information would you include? How would you plan to answer the questions? What do you need to answer the questions in each section? After we’ll get into groups to further discuss. 1.2.6.2 Group Discussion Let’s split up into five groups; one group for each required section of a NSF DMP. As a group, share your initial thoughts about the section you’ve been assigned to and together as a group discuss how you would complete that section. Select someone in the group to share your approach to the whole class. Take the next 10-15 minutes for group discussion. Some guiding questions: What information do you need to complete the section? Think both broadly and detailed. Do you need to reference outside materials to complete the section? Is this information already known / found or is additional research required? What is the relevant, key information necessary for the research to be understood for either your future self or for someone new to the data? What information would you want to know if you were given a new project to work on? Being explicit and including details are important to think about for this question. What workflows, documentation, standards, maintenance, tools / software, or roles are required? 1.2.7 Tools in Support of Creating a DMP The DMPTool and DMP Online are both easy to use web based tools that support the development of a DMP. The tools are partnered and share a code base; the DMPTool incorporates templates from US funding agencies and the DMP Online is focused on EU requirements. 1.2.7.1 Quick Tips for DMPTool There is no requirement to answer all questions in one sitting. Completing a DMP can require information gathering from multiple sources. Saving the plan at any point does not submit the plan, it simply saves your edits. This means you can move between sections in any order or save as you go. You can collaborate in DMPTool which keeps all commentary together, saves time on collaboration, and makes it easy to access the most current version at any time since it is always available in DMPTool. 1.2.8 Arctic Data Center Support for DMPs To support researchers in creating DMPs that fulfills NSF template requirements and provides guidance on how to work with the Arctic Data Center for preservation, we have created an Arctic Data Center template within the DMPTool. This template walks researchers through the questions required by NSF and includes recommendations directly from the Arctic Data Center team. When creating a new plan, indicate that your funding agency is the National Science Foundation and you will then have the option to select a template. Here you can choose the Arctic Data Center. As you answer the questions posed, guidance information from the Arctic Data Center will be visible under the ‘NSF’ tab on the right hand side. An example answer is also provided at the bottom. It is not intended that you copy and paste this verbatim. Rather, this is example prose that you can refer to for answering the question. 1.2.9 Sharing Your DMP The DMPTool allows you to collaborate with authorized individuals on your plan and also to publish it (make it publicly accessible) via the website. If your research activity is funded, it is also useful to share your DMP with the Arctic Data Center. This is not an NSF requirement, but can greatly help the Arctic Data Center team prepare for your data submission. Especially if you are anticipating a high volume of data. 1.2.10 Additional Resources The article Ten Simple Rules for Creating a Good Data Management Plan is a great resource for thinking about writing a data management plan and the information you should include within the plan. "],["documenting-and-publishing-data.html", "2 Documenting and Publishing Data 2.1 Data Documentation and Publishing", " 2 Documenting and Publishing Data 2.1 Data Documentation and Publishing 2.1.1 Learning Objectives In this lesson, you will learn: About open data archives, especially the Arctic Data Center What science metadata are and how they can be used How data and code can be documented and published in open data archives Web-based submission 2.1.2 Data sharing and preservation 2.1.3 Data repositories: built for data (and code) GitHub is not an archival location Examples of dedicated data repositories: KNB, Arctic Data Center, tDAR, EDI, Zenodo Rich metadata Archival in their mission Certification for repositories: https://www.coretrustseal.org/ Data papers, e.g., Scientific Data List of data repositories: http://re3data.org Repository finder tool: https://repositoryfinder.datacite.org/ 2.1.4 Metadata Metadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where they were collected, and why they were collected. For consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the metadata for a sockeye salmon dataset: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; That same metadata document can be converted to HTML format and displayed in a much more readable form on the web: https://knb.ecoinformatics.org/#view/doi:10.5063/F1F18WN4 And, as you can see, the whole dataset or its components can be downloaded and reused. Also note that the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data. 2.1.5 Structure of a data package Note that the dataset above lists a collection of files that are contained within the dataset. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document. These data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4,and each of the individual files have their own identifiers as well. 2.1.6 DataONE Federation DataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses. DataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the (currently 44!) repositories one at a time. 2.1.7 Publishing data from the web Each data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the Arctic Data Center, we provide some easy to use web forms for editing and submitting a data package. This section provides a brief overview of some highlights within the data submission process, in advance of a more comprehensive hands-on activity. ORCiDs We will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button. ORCID is a non-profit organization made up of research institutions, funders, publishers and other stakeholders in the research space. ORCID stands for Open Researcher and Contributor ID. The purpose of ORCID is to give researchers a unique identifier which then helps highlight and give credit to researchers for their work. If you click on someone’s ORCID, their work and research contributions will show up (as long as the researcher used ORCID to publish or post their work). After signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. Sensitive Data Handling Underneath the Title field, you will see a section titled “Data Sensitivity”. As the primary repository for the NSF Office of Polar Programs Arctic Section, the Arctic Data Center accepts data from all disciplines. This includes data from social science research that may include sensitive data, meaning data that contains personal or identifiable information. Sharing sensitive data can pose challenges to researchers, however sharing metadata or anonymized data contributes to discovery, supports open science principles, and helps reduce duplicate research efforts. To help mitigate the challenges of sharing sensitive data, the Arctic Data Center has added new features to the data submission process influenced by the CARE Principles for Indigenous Data Governance (Collective benefit, Authority to control, Responsibility, Ethics). Researchers submitting data now have the option to choose between varying levels of sensitivity that best represent their dataset. Data submitters can select one of three sensitivity level data tags that best fit their data and/or metadata. Based on the level of sensitivity, guidelines for submission are provided. The data tags range from non-confidential information to maximally sensitive information. The purpose of these tags is to ethically contribute to open science by making the richest set of data available for future research. The first tag, “non-sensitive data”, represents data that does not contain potentially harmful information, and can be submitted without further precaution. Data or metadata that is “sensitive with minimal risk” means that either the sensitive data has been anonymized and shared with consent, or that publishing it will not cause any harm. The third option, “some or all data is sensitive with significant risk” represents data that contains potentially harmful or identifiable information, and the data submitter will be asked to hold off submitting the data until further notice. In the case where sharing anonymized sensitive data is not possible due to ethical considerations, sharing anonymized metadata still aligns with FAIR (Findable, Accessible, Interoperable, Reproducible) principles because it increases the visibility of the research which helps reduce duplicate research efforts. Hence, it is important to share metadata, and to publish or share sensitive data only when consent from participants is given, in alignment with the CARE principles and any IRB requirements. You will continue to be prompted to enter information about your research, and in doing so, create your metadata record. We recommend taking your time because the richer your metadata is, the more easily reproducible and usable your data and research will be for both your future self and other researchers. Detailed instructions are provided below for the hands-on activity. Research Methods Methods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software. As part of a recent update, researchers are now asked to describe the ethical data practices used throughout their research. The information provided will be visible as part of the metadata record. This feature was added to the data submission process to encourage transparency in data ethics. Transparency in data ethics is a vital part of open science and sharing ethical practices encourages deeper discussion about data reuse and ethics. We encourage you to think about the ethical data and research practices that were utilized during your research, even if they don’t seem obvious at first. File and Variable Level Metadata In addition to providing information about, (or a description of) your dataset, you can also provide information about each file and the variables within the file. By clicking the “Describe” button you can add comprehensive information about each of your measurements, such as the name, measurement type, standard units etc. Provenance The data submission system also provides the opportunity for you to provide provenance information, describe the relationship between package elements. When viewing your dataset followinng submission, After completing your data description and submitting your dataset you will see the option to add source data and code, and derived data and code. These are just some of the features and functionality of the Arctic Data Center submission system and we will go through them in more detail below as part of a hands-on activity. 2.1.7.1 Download the data to be used for the tutorial I’ve already uploaded the test data package, and so you can access the data here: https://demo.arcticdata.io/#view/urn:uuid:0702cc63-4483-4af4-a218-531ccc59069f Grab both CSV files, and the R script, and store them in a convenient folder. 2.1.7.2 Login via ORCID We will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button. When you sign in, you will be redirected to orcid.org, where you can either provide your existing ORCID credentials or create a new account. ORCID provides multiple ways to login, including using your email address, an institutional login from many universities, and/or a login from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login. 2.1.7.3 Create and submit the dataset After signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. 2.1.7.3.1 Click Add Files to choose the data files for your package You can select multiple files at a time to efficiently upload many files. The files will upload showing a progress indicator. You can continue editing metadata while they upload. 2.1.7.3.2 Enter Overview information This includes a descriptive title, abstract, and keywords. You also must enter a funding award number and choose a license. The funding field will search for an NSF award identifier based on words in its title or the number itself. The licensing options are CC-0 and CC-BY, which both allow your data to be downloaded and re-used by other researchers. CC-0 Public Domain Dedication: “…can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.” CC-BY: Attribution 4.0 International License: “…free to…copy,…redistribute,…remix, transform, and build upon the material for any purpose, even commercially,…[but] must give appropriate credit, provide a link to the license, and indicate if changes were made.” 2.1.7.3.3 People Information Information about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people: Creators - all the people who should be in the citation for the dataset Contacts - one is required, but defaults to the first Creator if omitted Principal Investigators Any others that are relevant For each, please provide their ORCID identifier, which helps link this dataset to their other scholarly works. 2.1.7.3.4 Location Information The geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that spatial search will be more precise. Note that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global: Mission Canyon, Santa Barbara, California, USA 2.1.7.3.5 Temporal Information Add the temporal coverage of the data, which represents the time period to which data apply. Again, use multiple date ranges if your sampling was discontinuous. 2.1.7.3.6 Methods Methods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data. 2.1.7.3.7 Save a first version with Submit When finished, click the Submit Dataset button at the bottom. If there are errors or missing fields, they will be highlighted. Correct those, and then try submitting again. When you are successful, you should see a large green banner with a link to the current dataset view. Click the X to close that banner if you want to continue editing metadata. Success! 2.1.7.4 File and variable level metadata The final major section of metadata concerns the structure and content of your data files. In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure. For example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information. The Attributes tab is where you enter variable (aka attribute) information, including: variable name (for programs) variable label (for display) - variable definition (be specific) - type of measurement - units &amp; code definitions You’ll need to add these definitions for every variable (column) in the file. When done, click Done. Now, the list of data files will show a green checkbox indicating that you have fully described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes. After you get the big green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes. 2.1.7.5 Add workflow provenance Understanding the relationships between files (aka provenance) in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, which are often then used in analysis and visualization code to produce final outputs. In the DataONE network, we support structured descriptions of these relationships, so researchers can see the flow of data from raw data to derived to outputs. You add provenance by navigating to the data table descriptions and selecting the Add buttons to link the data and scripts that were used in your computational workflow. On the left side, select the Add circle to add an input data source to the filteredSpecies.csv file. This starts building the provenance graph to explain the origin and history of each data object. The linkage to the source dataset should appear. Then you can add the link to the source code that handled the conversion between the data files by clicking on Add arrow and selecting the R script: The diagram now shows the relationships among the data files and the R script, so click Submit to save another version of the package. Et voilà! A beautifully preserved data package! "],["literate-analysis-with-rmarkdown.html", "3 Literate Analysis with RMarkdown 3.1 RStudio Setup 3.2 Literate Analysis with RMarkdown", " 3 Literate Analysis with RMarkdown 3.1 RStudio Setup 3.1.1 Learning Objectives In this lesson, you will learn: Creating an R project and how to organize your work in a project Supplemental objective: How to make sure your local RStudio environment is set up for analysis Supplemental objective: How to set up Git and GitHub 3.1.2 Logging into the RStudio server To prevent us from spending most of this lesson troubleshooting the myriad of issues that can arise when setting up the R, RStudio, and git environments, we have chosen to have everyone work on a remote server with all of the software you need installed. We will be using a special kind of RStudio just for servers called RStudio Server. If you have never worked on a remote server before, you can think of it like working on a different computer via the internet. Note that the server has no knowledge of the files on your local filesystem, but it is easy to transfer files from the server to your local computer, and vice-versa, using the RStudio server interface. Here are the instructions for logging in and getting set up: Server Setup You should have received an email prompting you to change your password for your server account. If you did not, please put up a post-it and someone will help you. If you were able to successfully change your password, you can log in at: https://included-crab.nceas.ucsb.edu/ 3.1.3 Why use an R project? In this workshop, we are going to be using R project to organize our work. An R project is tied to a directory on your local computer, and makes organizing your work and collaborating with others easier. The Big Idea: using an R project is a reproducible research best practice because it bundles all your work within a working directory. Consider your current data analysis workflow. Where do you import you data? Where do you clean and wrangle it? Where do you create graphs, and ultimately, a final report? Are you going back and forth between multiple software tools like Microsoft Excel, JMP, and Google Docs? An R project and the tools in R that we will talk about today will consolidate this process because it can all be done (and updated) in using one software tool, RStudio, and within one R project. We are going to be doing nearly all of the work in this course in one R project. Our version of RStudio Server allows you to share projects with others. Sharing your project with the instructors of the course will allow for them to jump into your session and type along with you, should you encounter an error you cannot fix. Creating your project In your RStudio server session, follow these steps to set up your shared project: In the “File” menu, select “New Project” Click “New Directory” Click “New Project” Under “Directory name” type: training_{USERNAME}, eg: training_do-linh Leave “Create Project as subdirectory of:” set to ~ Click “Create Project” Your RStudio should open your project automatically after creating it. One way to check this is by looking at the top right corner and checking for the project name. Sharing your project To share your project with the instructor team, locate the “project switcher” dropdown menu in the upper right of your RStudio window. This dropdown has the name of your project (eg: training_do-linh), and a dropdown arrow. Click the dropdown menu, then “Share Project.” When the dialog box pops up, add the following usernames to your project: do-linh jclark jones vargas-pouslen virlar-knight Once those names show up in the list, click “OK”. 3.1.4 Understand how to use paths and working directories Artwork by Allison Horst. A cartoon of a cracked glass cube looking frustrated with casts on its arm and leg, with bandaids on it, containing “setwd”, looks on at a metal riveted cube labeled “R Proj” holding a skateboard looking sympathetic, and a smaller cube with a helmet on labeled “here” doing a trick on a skateboard. Now that we have your project created (and notice we know it’s an R Project because we see a .Rproj file in our Files pane), let’s learn how to move in a project. We do this using paths. There are two types of paths in computing: absolute paths and relative paths. An absolute path always starts with the root of your file system and locates files from there. The absolute path to my project directory is: /home/do-linh/training_do-linh Relative paths start from some location in your file system that is below the root. Relative paths are combined with the path of that location to locate files on your system. R (and some other languages like MATLAB) refer to the location where the relative path starts as our working directory. RStudio projects automatically set the working directory to the directory of the project. This means that you can reference files from within the project without worrying about where the project directory itself is. If I want to read in a file from the data directory within my project, I can simply type read.csv(\"data/samples.csv\") as opposed to read.csv(\"/home/do-linh/training_do-linh/data/samples.csv\") This is not only convenient for you, but also when working collaboratively. We will talk more about this later, but if Matt makes a copy of my R project that I have published on GitHub, and I am using relative paths, he can run my code exactly as I have written it, without going back and changing \"/home/do-linh/training_do-linh/data/samples.csv\" to \"/home/jones/training_jones/data/samples.csv\" Note that once you start working in projects you should basically never need to run the setwd() command. If you are in the habit of doing this, stop and take a look at where and why you do it. Could leveraging the working directory concept of R projects eliminate this need? Almost definitely! Similarly, think about how you work with absolute paths. Could you leverage the working directory of your R project to replace these with relative paths and make your code more portable? Probably! 3.1.5 Organizing your project When starting a new research project, one of the first things I do is create an R project for it (just like we have here!). The next step is to then populate that project with relevant directories. There are many tools out there that can do this automatically. Some examples are rrtools or usethis::create_package(). The goal is to organize your project so that it is a compendium of your research. This means that the project has all of the digital parts needed to replicate your analysis, like code, figures, the manuscript, and data access. There are lots of good examples out there of research compendia. Here is one from a friend of NCEAS, Carl Boettiger, which he put together for a paper he wrote. The complexity of this project reflects years of work. Perhaps more representative of the situation we are in at the start of our course is a project that looks like this one, which we have just started at NCEAS. Currently, the only file in your project is your .Rproj file. Let’s add some directories and start a file folder structure. Some common directories are: data: where we store our data (often contains subdirectories for raw, processed, and metadata data) R: contains scripts for cleaning or wrangling, etc. (some find this name misleading if their work has other scripts beyond the R programming language, in which case they call this directory scripts) plots or figs: generated plots, graphs, and figures doc: summaries or reports of analysis or other relevant project information Directory organization will vary from project to project, but the ultimate goal is to create a well organized project for both reproducibility and collaboration. 3.1.6 Summary organize your research into projects using R projects use R project working directories instead of setwd() use relative paths from those working directories, not absolute paths structure your R project as a compendium 3.1.7 Supplemental Objectives 3.1.7.1 Preparing to work in RStudio The default RStudio setup has a few panes that you will use. Here they are with their default locations: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) You can change the default location of the panes, among many other things: Customizing RStudio. One key question to ask whenever we open up RStudio is “where am I?” Because we like to work in RStudio projects, often this question is synonymous with “what project am I in?” In our setup we have already worked with R projects a little, but haven’t explained much about what they are or why we use them. An R project is really a special kind of working directory, which has its own workspace, history, and settings. Even though it isn’t much more than a special folder, it is a powerful way to organize your work. There are two places that can indicate what project we are in. The first is the project switcher menu in the upper right hand corner of your RStudio window. The second is the working directory path, in the top bar of your console. Note that by default, your working directory is set to the top level of your R project directory unless you change it using the setwd() function. Setting up git Before using git, you need to tell it who you are, also known as setting the global options. The only way to do this is through the command line. Newer versions of RStudio have a nice feature where you can open a terminal window in your RStudio session. Do this by selecting Tools -&gt; Terminal -&gt; New Terminal. A terminal tab should now be open where your console usually is. To set the global options, type the following into the command prompt, with your actual name, and press enter: git config --global user.name &quot;Halina Do-Linh&quot; Note that if it runs successfully, it will look like nothing happened. We will check at the end to make sure it worked. Next, enter the following line, with the email address you used when you created your account on github.com: git config --global user.email &quot;username@github.com&quot; Note that these lines need to be run one at a time. Next, we will set our credentials to not time out for a very long time. This is related to the way that our server operating system handles credentials - not doing this will make your PAT (which we will set up soon) expire immediately on the system, even though it is actually valid for a month. git config --global credential.helper &#39;cache --timeout=10000000&#39; Finally, check to make sure everything looks correct by entering this command, which will return the options that you have set. git config --global --list 3.1.7.2 GitHub Authentication GitHub recently deprecated password authentication for accessing repositories, so we need to set up a secure way to authenticate. The book Happy git with R has a wealth of information related to working with git in R, and these instructions are based off of section 10.1. We will be using a PAT (Personal Access Token) in this course, because it is easy to set up. For better security and long term use, we recommend taking the extra steps to set up SSH keys. Steps: Run usethis::create_github_token() in the console In the browser window that pops up, scroll to the bottom and click “generate token.” You may need to log into GitHub first. Copy the token from the green box on the next page Back in RStudio, run credentials::set_github_pat() Paste your token into the dialog box that pops up. 3.1.8 Setting up the R environment on your local computer R Version We will use R version 4.0.5, which you can download and install from CRAN. To check your version, run this in your RStudio console: R.version$version.string If you have R version 4.0.0 that will likely work fine as well. RStudio Version We will be using RStudio version 1.4 or later, which you can download and install here To check your RStudio version, run the following in your RStudio console: RStudio.Version()$version If the output of this does not say 1.4 or higher, you should update your RStudio. Do this by selecting Help -&gt; Check for Updates and follow the prompts. Package installation Run the following lines to check that all of the packages we need for the training are installed on your computer. packages &lt;- c(&quot;dplyr&quot;, &quot;tidyr&quot;, &quot;readr&quot;, &quot;devtools&quot;, &quot;usethis&quot;, &quot;roxygen2&quot;, &quot;leaflet&quot;, &quot;ggplot2&quot;, &quot;DT&quot;, &quot;scales&quot;, &quot;shiny&quot;, &quot;sf&quot;, &quot;ggmap&quot;, &quot;broom&quot;, &quot;captioner&quot;, &quot;MASS&quot;) for (package in packages) { if (!(package %in% installed.packages())) { install.packages(package) } } rm(packages) # remove variable from workspace # Now upgrade any out-of-date packages update.packages(ask=FALSE) If you haven’t installed all of the packages, this will automatically start installing them. If they are installed, it won’t do anything. Next, create a new R Markdown (File -&gt; New File -&gt; R Markdown). If you have never made an R Markdown document before, a dialog box will pop up asking if you wish to install the required packages. Click yes. At this point, RStudio and R should be all set up. Setting up git locally If you haven’t downloaded git already, you can do so here. If you haven’t already, go to github.com and create an account. Then you can follow the instructions that we used above to set your email address and user name. Note for Windows Users If you get “command not found” (or similar) when you try these steps through the RStudio terminal tab, you may need to set the type of terminal that gets launched by RStudio. Under some git install scenarios, the git executable may not be available to the default terminal type. Follow the instructions on the RStudio site for Windows specific terminal options. In particular, you should choose “New Terminals open with Git Bash” in the Terminal options (Tools-&gt;Global Options-&gt;Terminal). In addition, some versions of windows have difficulty with the command line if you are using an account name with spaces in it (such as “Matt Jones”, rather than something like “mbjones”). You may need to use an account name without spaces. Updating a previous R installation This is useful for users who already have R with some packages installed and need to upgrade R, but don’t want to lose packages. If you have never installed R or any R packages before, you can skip this section. If you already have R installed, but need to update, and don’t want to lose your packages, these two R functions can help you. The first will save all of your packages to a file. The second loads the packages from the file and installs packages that are missing. Save this script to a file (e.g. package_update.R). #&#39; Save R packages to a file. Useful when updating R version #&#39; #&#39; @param path path to rda file to save packages to. eg: installed_old.rda save_packages &lt;- function(path){ tmp &lt;- installed.packages() installedpkgs &lt;- as.vector(tmp[is.na(tmp[,&quot;Priority&quot;]), 1]) save(installedpkgs, file = path) } #&#39; Update packages from a file. Useful when updating R version #&#39; #&#39; @param path path to rda file where packages were saved update_packages &lt;- function(path){ tmp &lt;- new.env() installedpkgs &lt;- load(file = path, envir = tmp) installedpkgs &lt;- tmp[[ls(tmp)[1]]] tmp &lt;- installed.packages() installedpkgs.new &lt;- as.vector(tmp[is.na(tmp[,&quot;Priority&quot;]), 1]) missing &lt;- setdiff(installedpkgs, installedpkgs.new) install.packages(missing) update.packages(ask=FALSE) } Source the file that you saved above (eg: source(package_update.R)). Then, run the save_packages function. save_packages(&quot;installed.rda&quot;) Then quit R, go to CRAN, and install the latest version of R. Source the R script that you saved above again (eg: source(package_update.R)), and then run: update_packages(&quot;installed.rda&quot;) This should install all of your R packages that you had before you upgraded. 3.2 Literate Analysis with RMarkdown 3.2.1 Learning Objectives In this lesson we will: explore an example of RMarkdown as literate analysis learn markdown syntax write and run R code in RMarkdown build and knit an example document 3.2.2 Introduction and motivation The concept of literate analysis dates to a 1984 article by Donald Knuth. In this article, Knuth proposes a reversal of the programming paradigm. Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do. If our aim is to make scientific research more transparent, the appeal of this paradigm reversal is immediately apparent. All too often, computational methods are written in such a way as to be borderline incomprehensible - even to the person who originally wrote the code! The reason for this is obvious, computers interpret information very differently than people do. By switching to a literate analysis model, you help enable human understanding of what the computer is doing. As Knuth describes, in the literate analysis model, the author is an “essayist” who chooses variable names carefully, explains what they mean, and introduces concepts in the analysis in a way that facilitates understanding. RMarkdown is an excellent way to generate literate analysis, and a reproducible workflow. RMarkdown is a combination of two things - R, the programming language, and markdown, a set of text formatting directives. In an R script, the language assumes that you are writing R code, unless you specify that you are writing prose (using a comment, designated by #). The paradigm shift of literate analysis comes in the switch to RMarkdown, where instead of assuming you are writing code, Rmarkdown assumes that you are writing prose unless you specify that you are writing code. This, along with the formatting provided by markdown, encourages the “essayist” to write understandable prose to accompany the code that explains to the human-beings reading the document what the author told the computer to do. This is in contrast to writing just R code, where the author telling to the computer what to do with maybe a smattering of terse comments explaining the code to a reader. Before we dive in deeper, let’s look at an example of what a rendered literate analysis with RMarkdown can look like using a real example. Here is an example of a real analysis workflow written using RMarkdown. There are a few things to notice about this document, which assembles a set of similar data sources on salmon brood tables with different formatting into a single data source. It introduces the data sources using in-line images, links, interactive tables, and interactive maps. An example of data formatting from one source using R is shown. The document executes a set of formatting scripts in a directory to generate a single merged file. Some simple quality checks are performed (and their output shown) on the merged data. Simple analysis and plots are shown. In addition to achieving literate analysis, this document also represents a reproducible analysis. Because the entire merging and quality control of the data is done using the R code in the RMarkdown, if a new data source and formatting script are added, the document can be run all at once with a single click to re-generate the quality control, plots, and analysis of the updated data. RMarkdown is an amazing tool to use for collaborative research, so we will spend some time learning it well now, and use it through the rest of the course. 3.2.3 Basic RMarkdown syntax An RMarkdown file has three main components: - YAML metadata to guide the RMarkdown build process - Text to display - Code chunks to run Today we are going to use Rmarkdown to run some analysis on data. We are specifically going to focus on the code chunk and text components. We will discuss more about the YAML part on of an RMarkdown later in the course. For now, you just need to know that every RMarkdown file has a YAML and this sets some general guidelines on how your want the output of your document to look like. Setup Open a new RMarkdown file using the following prompts: File -&gt; New File -&gt; RMarkdown A popup window will appear. You can just click the OK button here, or give your file a new title if you wish. Leave the output format as HTML. The first thing to notice is that by opening a file, we are again seeing the 4th pane of the RStudio console, which is essentially a text editor. Let’s have a look at this file — It looks a little different than a R script. It’s not blank; there is some initial text already provided for you. Lets identify the three main components in the image above. We have the YAML a the top, in between the two sets of dashed lines. Then we also see white and grey sections. The gray sections are R code chunks and the white sections are plain text. Let’s go ahead and render this file by clicking the “Knit” button, the blue yarn at the top of the RMarkdown file. When you first click this button, RStudio will prompt you to save this file. Save it in the top level of your home directory on the server, and name it something that you will remember (like rmarkdown-intro.Rmd). What do you notice between the two? First, the knit process produced a second file (an HTML file) that popped up in a second window. You’ll also see this file in your directory with the same name as your Rmd, but with the html extension. In it’s simplest format, RMarkdown files come in pairs - the RMarkdown file, and its rendered version. In this case, we are knitting, or rendering, the file into HTML. You can also knit to PDF or Word files. Notice how the grey R code chunks are surrounded by 3 back-ticks and {r LABEL}. These are evaluated and return the output text in the case of summary(cars) and the output plot in the case of plot(pressure). The label next to the letter r in the code chunk syntax is a chunk label - this can help you navigate your RMarkdown document using the drop-down menu at the bottom of the editor pane. Notice how the code plot(pressure) is not shown in the HTML output because of the R code chunk option echo = FALSE. RMarkdown has lots of chunk options: - allow for code to be run but not shown (echo = FALSE), - code to be shown but not run (eval = FALSE), - code to be run, but results not shown (results = 'hide'), - or any combination of those. It is important to emphasize one more time that in an RMarkdown document, the gray areas of the document are code, in this case R code because that is what it is indicated in the ```{r} syntax at the start of this gray area. And the white areas of this Rmd are in Markdown language. Let’s talk about Markdown first, before going deeper into the R side. Markdown is a formatting language for plain text, and there are only around 15 rules to know. Notice the syntax in the document we just knitted: headers get rendered at multiple levels: #, ## bold: **word** There are some good cheatsheets to get you started, and here is one built into RStudio: Go to Help &gt; Markdown Quick Reference . Important: note that the hash symbol # is used differently in Markdown and in R: in an R script or inside an R code chunk, a hash indicates a comment that will not be evaluated. You can use as many as you want: # is equivalent to ######. It’s just a matter of style. in Markdown, a hash indicates a level of a header. And the number you use matters: # is a “level one header”, meaning the biggest font and the top of the hierarchy. ### is a level three header, and will show up nested below the # and ## headers. Challenge In Markdown, Write some italic text, make a numbered list, and add a few sub-headers. Use the Markdown Quick Reference (in the menu bar: Help &gt; Markdown Quick Reference). Re-knit your html file and observe your edits. 3.2.3.1 Rmarkdown editing tools Recent versions of RStudio, now have a ‘what you see is what you get’ (wysiwyg) editor or Visual editor, which can be a nice way to write markdown without remembering all of the markdown rules. Since there aren’t many rules for markdown, I recommend just learning them - especially since markdown is used in many, many other contexts besides RMarkdown (formatting GitHub comments, for example). To access the editor, click the Visual button in the upper left hand corner of your editor pane. You’ll notice that your document is now formatted as you type, and you can change elements of the formatting using the row of icons in the top of the editor pane. Although I don’t really recommend doing all of your markdown composition in the Visual editor, there are two features to this editor that I find immensely helpful, adding citations, and adding tables. From the insert drop down, select “citation.” In the window that appears, there are several options in the left hand panel for the source of your citation. If you had a citation manager, such as Zotero, installed, this would be included in that list. For now, select “From DOI”, and in the search bar enter a DOI of your choice (eg: 10.1038/s41467-020-17726-z), then select “Insert.” After selecting insert, a couple of things happen. First, the citation reference is inserted into your markdown text as [@oke2020]. Second, a file called references.bib containing the BibTex format of the citation is created. Third, that file is added to the YAML header of your RMarkdown document (bibliography: references.bib). Adding another citation will automatically update your references.bib file. So easy! The second task that the markdown editor is convenient for is generating tables. Markdown tables are a bit finicky and annoying to type, and there are a number of formatting options that are difficult to remember if you don’t use them often. In the top icon bar, the “table” drop down gives several options for inserting, editing, and formatting tables. Experiment with this menu to insert a small table. 3.2.4 Code chunks Every time I open a new RMarkdown I delete everything below the “setup chunk” (line 10). The setup chunk is the one that looks like this: knitr::opts_chunk$set(echo = TRUE) This is a very useful chunk that will set the default R chunk options for your entire document. I like keeping it in my document so that I can easily modify default chunk options based on the audience for my RMarkdown. For example, if I know my document is going to be a report for a non-technical audience, I might set echo = FALSE in my setup chunk, that way all of the text, plots, and tables appear in the knitted document. The code, on the other hand, is still run, but doesn’t display in the final document. Now let’s practice with some R chunks. You can Create a new chunk in your RMarkdown in one of these ways: click “Insert &gt; R” at the top of the editor pane type by hand ```{r} ``` use the keyboard shortcut Command + Option + i (for windows, Ctrl + Alt + i) Now, let’s write some R code. x &lt;- 4*3 x ## [1] 12 Hitting return does not execute this command; remember, it’s just a text file. To execute it, we need to get what we typed in the the R chunk (the grey R code) down into the console. How do we do it? There are several ways (let’s do each of them): copy-paste this line into the console (generally not recommended as a primary method) select the line (or simply put the cursor there), and click ‘Run’. This is available from the bar above the file (green arrow) the menu bar: Code &gt; Run Selected Line(s) keyboard shortcut: command-return click the green arrow at the right of the code chunk Challenge Add a few more commands to your code chunk. Execute them by trying the three ways above. Question: What is the difference between running code using the green arrow in the chunk and the command-return keyboard shortcut? 3.2.5 Literate analysis practice Now that we have gone over the basics, let’s go a little deeper by building a simple, small RMarkdown document that represents a literate analysis using real data. We are going to work with the same data we download yesterday. You should all have the BGchem2008data.csv inside your data folder. If you don’t, please follow the steps below to download the data and then upload to your RStudio Server. Setup Navigate to the following dataset: https://doi.org/10.18739/A25T3FZ8X Download the file “BGchem2008data.csv” Click the “Upload” button in your RStudio server file browser. In the dialog box, make sure the destination directory is the data directory in your R project, click “choose file,” and locate the BGchem2008data.csv file. Press “ok” to upload the file. 3.2.5.1 Developing code in RMarkdown Experienced R users who have never used RMarkdown often struggle a bit in the transition to developing analysis in RMarkdown - which makes sense! It is switching the code paradigm to a new way of thinking. Rather than starting an R chunk and putting all of your code in that single chunk, here I describe what I think is a better way. Open a document and block out the high-level sections you know you’ll need to include using top level headers. Add bullet points for some high level pseudo-code steps you know you’ll need to take. Start filling in under each bullet point the code that accomplishes each step. As you write your code, transform your bullet points into prose, and add new bullet points or sections as needed. For this mini-analysis, we will just have the following sections and code steps: Introduction read in data Analysis calculate summary statistics calculate mean Redfield ratio plot Redfield ratio Challenge Create the ‘outline’ of your document with the information above. Top level bullet points should be top level sections. The second level points should be a list within each section. Next, write a sentence saying where your dataset came from, including a hyperlink, in the introduction section. Hint: Navigate to Help &gt; Markdown Quick Reference to lookup the hyperlink syntax. 3.2.5.2 Read in the data Now that we have outlined our document, we can start writing code! To read the data into our environment, we will use a function from the readr package. To use a package in our analysis, we need to load it into our environment using library(package_name). Even though we have installed it, we haven’t yet told our R session to access it. Because there are so many packages (many with conflicting namespaces) R cannot automatically load every single package you have installed. Instead, you load only the ones you need for a particular analysis. Loading the package is a key part of the reproducible aspect of our Rmarkdown, so we will include it as an R chunk. It is generally good practice to include all of your library calls in a single, dedicated R chunk near the top of your document. This lets collaborators know what packages they might need to install before they start running your code. You should have already installed readr as part of the setup for this course, so add a new R chunk below your setup chunk that calls the readr library, and run it. It should look like this: library(readr) Now, below the introduction that you wrote, add a code chunk that uses the read_csv function to read in your data file. 3.2.5.2.1 Recap on file paths In computing, a path specifies the unique location of a file on the filesystem. A path can come in one of two forms: absolute or relative. Absolute paths start at the very top of your file system, and work their way down the directory tree to the file. Relative paths start at an arbitrary point in the file system. In R, this point is set by your working directory. 3.2.5.2.2 File path in RMarkdown RMarkdown has a special way of handling relative paths that can be very handy. When working in an RMarkdown document, R will set all paths relative to the location of the RMarkdown file. This way, you don’t have to worry about setting a working directory, or changing your colleagues absolute path structure with the correct user name, etc. If your RMarkdown is stored near where the data it analyses are stored (good practice, generally), setting paths becomes much easier! If you saved your “BGchem2008data.csv” data file in the same location as your Rmd, you can just write the following to read it in. Checkout the help page by typing ?read_csv in the console. This tells you that for this function the first argument should be a pointer to the file. Rstudio has some nice helpers to help you navigate paths. If you open quotes and press ‘tab’ with your cursor between the quotes, a popup menu will appear showing you some options. bg_chem &lt;- read_csv(&quot;../data/BGchem2008data.csv&quot;) Parsed with column specification: cols( Date = col_date(format = &quot;&quot;), Time = col_datetime(format = &quot;&quot;), Station = col_character(), Latitude = col_double(), Longitude = col_double(), Target_Depth = col_double(), CTD_Depth = col_double(), CTD_Salinity = col_double(), CTD_Temperature = col_double(), Bottle_Salinity = col_double(), d18O = col_double(), Ba = col_double(), Si = col_double(), NO3 = col_double(), NO2 = col_double(), NH4 = col_double(), P = col_double(), TA = col_double(), O2 = col_double() ) Warning messages: 1: In get_engine(options$engine) : Unknown language engine &#39;markdown&#39; (must be registered via knit_engines$set()). 2: Problem with `mutate()` input `Lower`. ℹ NAs introduced by coercion ℹ Input `Lower` is `as.integer(Lower)`. 3: In mask$eval_all_mutate(dots[[i]]) : NAs introduced by coercion If you run this line in your RMarkdown document, you should see the bg_chem object populate in your environment pane. It also spits out lots of text explaining what types the function parsed each column into. This text is important, and should be examined, but we might not want it in our final document. Challenge Use one of two methods to figure out how to suppress warning and message text in your chunk output: The gear icon in the chunk, next to the play button The RMarkdown reference guide (also under Help &gt; Cheatsheets) Aside Why not use read.csv from base R? We chose to show read_csv from the readr package for a few reasons. One is to introduce the concept of packages and showing how to load them, but read_csv has several advantages over read.csv. more reasonable function defaults (no stringsAsFactors!) smarter column type parsing, especially for dates it is much faster than read.csv, which is helpful for large files 3.2.5.3 Calculate Summary Statistics As our “analysis” we are going to calculate some very simple summary statistics and generate a single plot. In this dataset of oceanographic water samples, we will be examining the ratio of nitrogen to phosphate to see how closely the data match the Redfield ratio, which is the consistent 16:1 ratio of nitrogen to phosphorous atoms found in marine phytoplankton. Let’s start by exploring the data we just read. Every time I read a new data set, I like to familiarize my self with it and make sure that the data looks as expected. There are a couple of functions I use to do this. We first create a new R chunk and run the following functions. Because this just an exploration and we do not want this chunk to be part of our report, we will indicate that by adding eval = FALSE and echo = FLASE in the setup of the chunk, that way, the code in this chunk will not run and not be displayed when I knit the final document. ## Prints the column names of my data frame colnames(bg_chem) ## General structure of the data frame - shows class of each column str(bg_chem) ## First 6 lines of the data frame head(bg_chem) ## Summary of each column of data summary(bg_chem) ## Opens data frame in its own tab to see each row and column of the data View(bg_chem) Now that we know a more about the data set we are working with lets do some analyses. Under the appropriate bullet point in your analysis section, create a new R chunk, and use it to calculate the mean nitrate (NO3), nitrite (NO2), ammonium (NH4), and phosphorous (P) measured. Save these mean values as new variables with easily understandable names, and write a (brief) description of your operation using markdown above the chunk. Remember that the $ indicates which column of your data to look into. nitrate &lt;- mean(bg_chem$NO3) nitrite &lt;- mean(bg_chem$NO2) amm &lt;- mean(bg_chem$NH4) phos &lt;- mean(bg_chem$P) In another chunk, use those variables to calculate the nitrogen:phosphate ratio (Redfield ratio). ratio &lt;- (nitrate + nitrite + amm)/phos You can access this variable in your Markdown text by using R in-line in your text. The syntax to call R in-line (as opposed to as a chunk) is a single backtick `, the letter “r”, whatever your simple R command is - here we will use round(ratio) to print the calculated ratio, and a closing backtick `. So: ` 6 `. This allows us to access the value stored in this variable in our explanatory text without resorting to the evaluate-copy-paste method so commonly used for this type of task. The text as it looks in your RMrakdown will look like this: The Redfield ratio for this dataset is approximately `r round(ratio)`. And the rendered text like this: The Redfield ratio for this dataset is approximately 6. Finally, create a simple plot using base R that plots the ratio of the individual measurements, as opposed to looking at mean ratio. plot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4) Challenge Decide whether or not you want the plotting code above to show up in your knitted document along with the plot, and implement your decision as a chunk option. “Knit” your RMarkdown document (by pressing the Knit button) to observe the results. Aside How do I decide when to make a new chunk? Like many of life’s great questions, there is no clear cut answer. My preference is to have one chunk per functional unit of analysis. This functional unit could be 50 lines of code or it could be 1 line, but typically it only does one “thing.” This could be reading in data, making a plot, or defining a function. It could also mean calculating a series of related summary statistics (as above). Ultimately the choice is one related to personal preference and style, but generally you should ensure that code is divided up such that it is easily explainable in a literate analysis as the code is run. 3.2.6 RMarkdown and environments Let’s walk through an exercise with the document you built together to demonstrate how RMarkdown handles environments. We will be deliberately inducing some errors here for demonstration purposes. First, follow these steps: Restart your R session (Session &gt; Restart R) Run the last chunk in your Rmarkdown by pressing the play button on the chunk Perhaps not surprisingly, we get an error: Error in plot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4) : object &#39;bg_chem&#39; not found This is because we have not run the chunk of code that reads in the bg_chem data. The R part of Rmarkdown works just like a regular R script. You have to execute the code, and the order that you run it in matters. It is relatively easy to get mixed up in a large RMarkdown document - running chunks out of order, or forgetting to run chunks. To resolve this, follow the next step: Select from the “Run” menu (top right of Rmarkdown editor) “Restart R and run all chunks” Observe the bg_chem variable in your environment. This is one of my favorite ways to reset and re-run my code when things seem to have gone sideways. This is great practice to do periodically since it helps ensure you are writing code that actually runs. For the next demonstration: Restart your R session (Session &gt; Restart R) Press Knit to run all of the code in your document Observe the state of your environment pane Assuming your document knitted and produced an html page, your code ran. Yet the environment pane is empty. What happened? The Knit button is rather special - it doesn’t just run all of the code in your document. It actually spins up a fresh R environment separate from the one you have been working in, runs all of the code in your document, generates the output, and then closes the environment. This is one of the best ways RMarkdown helps ensure you have built a reproducible workflow. If, while you were developing your code, you ran a line in the console as opposed to adding it to your RMarkdown document, the code you develop while working actively in your environment will still work. However, when you knit your document, the environment RStudio spins up doesn’t know anything about that working environment you were in. Thus, your code may error because it doesn’t have that extra piece of information. Commonly, library calls are the source of this kind of frustration when the author runs it in the console, but forgets to add it to the script. To further clarify the point on environments, perform the following steps: Select from the “Run” menu (top right of Rmarkdown editor) “Run All” Observe all of the variables in your environment. Aside What about all my R scripts? Some pieces of R code are better suited for R scripts than RMarkdown. A function you wrote yourself that you use in many different analyses is probably better to define in an R script than repeated across many RMarkdown documents. Some analyses have mundane or repetitive tasks that don’t need to be explained very much. For example, in the document shown in the beginning of this lesson, 15 different excel files needed to be reformatted in slightly different, mundane ways, like renaming columns and removing header text. Instead of including these tasks in the primary markdown, I instead chose to write one R script per file and stored them all in a directory. I took the contents of one script and included it in my literate analysis, using it as an example to explain what the scripts did, and then used the source function to run them all from within my RMarkdown. So, just because you know RMarkdown now, doesn’t mean you won’t be using R scripts anymore. Both .R and .Rmd have their roles to play in analysis. With practice, it will become more clear what works well in RMarkdown, and what belongs in a regular R script. 3.2.7 Go Further Create an RMarkdown document with some of your own data. If you don’t have a good dataset handy, use the example dataset here: Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X. Your document might contain the following sections: Introduction to your dataset Include an external link Simple analysis Presentation of a result A table An in-line R command 3.2.8 Resources RMarkdown Reference Guide RMarkdown Home Page RMarkdown Cheat Sheet What is RMardown Video 3.2.9 Troubleshooting 3.2.9.1 My RMarkdown won’t knit to PDF If you get an error when trying to knit to PDF that says your computer doesn’t have a LaTeX installation, one of two things is likely happening: Your computer doesn’t have LaTeX installed You have an installation of LaTeX but RStudio cannot find it (it is not on the path) If you already use LaTeX (like to write papers), you fall in the second category. Solving this requires directing RStudio to your installation - and isn’t covered here. If you fall in the first category - you are sure you don’t have LaTeX installed - can use the R package tinytex to easily get an installation recognized by RStudio, as long as you have administrative rights to your computer. To install tinytex run: install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() If you get an error that looks like destination /usr/local/bin not writable, you need to give yourself permission to write to this directory (again, only possible if you have administrative rights). To do this, run this command in the terminal: sudo chown -R `whoami`:admin /usr/local/bin and then try the above install instructions again. More information about tinytex can be found here "],["version-control-with-git-and-github.html", "4 Version Control with git and GitHub 4.1 Learning Objectives 4.2 Introduction to git 4.3 Create a remote repository on GitHub 4.4 Working locally with Git via RStudio 4.5 Setting up git on an existing project 4.6 Go Further", " 4 Version Control with git and GitHub 4.1 Learning Objectives In this lesson, you will learn: Why git is useful for reproducible analysis How to use git to track changes to your work over time How to use GitHub to collaborate with others How to structure your commits so your changes are clear to others How to write effective commit messages 4.2 Introduction to git Every file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when problems are discovered. Data files get combined together, then errors are fixed, and then they are split and combined again. In the course of a single analysis, one can expect thousands of changes to files. And yet, all we use to track this are simplistic filenames. You might think there is a better way, and you’d be right: version control. 4.2.1 A Motivating Example Before diving into the details of git and how to use it, let’s start with a motivating example that’s representative of the types of problems git can help us solve. Say, for example, you’re working on an analysis in R and you’ve got it into a state you’re pretty happy with. We’ll call this version 1: You come into the office the following day and you have an email from your boss, “Hey, you know what this model needs?” You’re not entirely sure what she means but you figure there’s only one thing she could be talking about: more cowbell. So you add it to the model in order to really explore the space. But you’re worried about losing track of the old model so, instead of editing the code in place, you comment out the old code and put as serious a warning as you can muster in a comment above it. Commenting out code you don’t want to lose is something probably all of us have done at one point or another but it’s really hard to understand why you did this when you come back years later or you when you send your script to a colleague. Luckily, there’s a better way: Version control. Instead of commenting out the old code, we can change the code in place and tell git to commit our change. So now we have two distinct versions of our analysis and we can always see what the previous version(s) look like. You may have noticed something else in the diagram above: Not only can we save a new version of our analysis, we can also write as much text as we like about the change in the commit message. In addition to the commit message, git also tracks who, when, and where the change was made. Imagine that some time has gone by and you’ve committed a third version of your analysis, version 3, and a colleague emails with an idea: What if you used machine learning instead? Maybe you’re not so sure the idea will work out and this is where a tool like git shines. Without a tool like git, we might copy analysis.R to another file called analysis-ml.R which might end up having mostly the same code except for a few lines. This isn’t particularly problematic until you want to make a change to a bit of shared code and now you have to make changes in two files, if you even remember to. Instead, with git, we can start a branch. Branches allow us to confidently experiment on our code, all while leaving the old code in tact and recoverable. So you’ve been working in a branch and have made a few commits on it and your boss emails again asking you to update the model in some way. If you weren’t using a tool like git, you might panic at this point because you’ve rewritten much of your analysis to use a different method but your boss wants change to the old method. But with git and branches, we can continue developing our main analysis at the same time as we are working on any experimental branches. Branches are great for experiments but also great for organizing your work generally. After all that hard work on the machine learning experiment, you and your colleague could decide to scrap it. It’s perfectly fine to leave branches around and switch back to the main line of development but we can also delete them to tidy up. If, instead, you and your colleague had decided you liked the machine learning experiment, you could also merge the branch with your main development line. Merging branches is analogous to accepting a change in Word’s Track Changes feature but way more powerful and useful. A key takeaway here is that git can drastically increase your confidence and willingness to make changes to your code and help you avoid problems down the road. Analysis rarely follows a linear path and we need a tool that respects this. Finally, imagine that, years later, your colleague asks you to make sure the model you reported in a paper you published together was actually the one you used. Another really powerful feature of git is tags which allow us to record a particular state of our analysis with a meaningful name. In this case, we are lucky because we tagged the version of our code we used to run the analysis. Even if we continued to develop beyond commit 5 (above) after we submitted our manuscript, we can always go back and run the analysis as it was in the past. 4.2.1.1 Summary With git, we can: Avoid using cryptic filenames and comments to keep track of our work Describe our changes with as much information as we like so it’s easier to understand why our code changed (commits) Work on multiple, simultaneous development (branches) of our code at the same time and, optionally, merge them together Go back in time to look at (and even run) older versions of our code Tag specific versions of our code as meaningful (tags) And, as we’ll see below, git has one extra superpower available to us: It’s distributed. Multiple people can work on the same analysis at the same time on their own computer and everyone’s changes can eventually merged together. Version control and Collaboration using Git and GitHub First, just what are git and GitHub? git: version control software used to track files in a folder (a repository) git creates the versioned history of a repository GitHub: web site that allows users to store their git repositories and share them with others Let’s look at a GitHub repository This screen shows the copy of a repository stored on GitHub, with its list of files, when the files and directories were last modified, and some information on who made the most recent changes. If we drill into the “commits” for the repository, we can see the history of changes made to all of the files. Looks like kellijohnson and seananderson were fixing things in June and July: And finally, if we drill into the changes made on June 13, we can see exactly what was changed in each file: Tracking these changes, how they relate to released versions of software and files is exactly what Git and GitHub are good for. And we will show how they can really be effective for tracking versions of scientific code, figures, and manuscripts to accomplish a reproducible workflow. The Git lifecycle As a git user, you’ll need to understand the basic concepts associated with versioned sets of changes, and how they are stored and moved across repositories. Any given git repository can be cloned so that it exist both locally, and remotely. But each of these cloned repositories is simply a copy of all of the files and change history for those files, stored in git’s particular format. For our purposes, we can consider a git repository just a folder with a bunch of additional version-related metadata. In a local git-enabled folder, the folder contains a workspace containing the current version of all files in the repository. These working files are linked to a hidden folder containing the ‘Local repository’, which contains all of the other changes made to the files, along with the version metadata. So, when working with files using git, you can use git commands to indicate specifically which changes to the local working files should be staged for versioning (using the git add command), and when to record those changes as a version in the local repository (using the command git commit). The remaining concepts are involved in synchronizing the changes in your local repository with changes in a remote repository. The git push command is used to send local changes up to a remote repository (possibly on GitHub), and the git pull command is used to fetch changes from a remote repository and merge them into the local repository. git clone: to copy a whole remote repository to local git add (stage): notify git to track particular changes git commit: store those changes as a version git pull: merge changes from a remote repository to our local repository git push: copy changes from our local repository to a remote repository git status: determine the state of all files in the local repository git log: print the history of changes in a repository Those seven commands are the majority of what you need to successfully use git. But this is all super abstract, so let’s explore with some real examples. 4.3 Create a remote repository on GitHub Let’s start by creating a repository on GitHub, then we’ll edit some files. Setup Log into GitHub Click the New repository button Name it training-test Create a README.md Set the LICENSE to Apache 2.0 Add a .gitignore file for R If you were successful, it should look something like this: You’ve now created your first repository! It has a couple of files that GitHub created for you, like the README.md file, and the LICENSE file, and the .gitignore file. For simple changes to text files, you can make edits right in the GitHub web interface. Challenge Navigate to the README.md file in the file listing, and edit it by clicking on the pencil icon. This is a regular Markdown file, so you can just add markdown text. When done, add a commit message, and hit the Commit changes button. Congratulations, you’ve now authored your first versioned commit. If you navigate back to the GitHub page for the repository, you’ll see your commit listed there, as well as the rendered README.md file. Let’s point out a few things about this window. It represents a view of the repository that you created, showing all of the files in the repository so far. For each file, it shows when the file was last modified, and the commit message that was used to last change each file. This is why it is important to write good, descriptive commit messages. In addition, the blue header above the file listing shows the most recent commit, along with its commit message, and its SHA identifier. That SHA identifier is the key to this set of versioned changes. If you click on the SHA identifier (810f314), it will display the set of changes made in that particular commit. In the next section we’ll use the GitHub URL for the GitHub repository you created to clone the repository onto your local machine so that you can edit the files in RStudio. To do so, start by copying the GitHub URL, which represents the repository location: 4.4 Working locally with Git via RStudio For convenience, it would be nice to be able to edit the files locally on our computer using RStudio rather than working on the GitHub website. We can do this by using the clone command to copy the repository from GitHub to our local computer, which then allows us to push changes up to GitHub and pull down any changes that have occurred. We refer to the remote copy of the repository that is on GitHub as the origin repository (the one that we cloned from), and the copy on our local computer as the local copy. RStudio knows how to work with files under version control with Git, but only if you are working within an RStudio project folder. In this next section, we will clone the repository that you created on GitHub into a local repository as an RStudio project. Here’s what we’re going to do: Create the new project Inspect the Git tab and version history Commit a change to the README.md file Commit the changes that RStudio made Inspect the version history Add and commit an Rmd file Push these changes to GitHub View the change history on GitHub Setup In the File menu, select “New Project.” In the dialog that pops up, select the “Version Control” option, and paste the GitHub URL that you copied into the field for the remote repository Repository URL. While you can name the local copy of the repository anything, it’s typical to use the same name as the GitHub repository to maintain the correspondence. Once you hit `Create Project, a new RStudio window will open with all of the files from the remote repository copied locally. Depending on how your version of RStudio is configured, the location and size of the panes may differ, but they should all be present, including a Git tab and the normal Files tab listing the files that had been created in the remote repository. You’ll note that there is one new file training-test.Rproj, and three files that we created earlier on GitHub (.gitignore, LICENSE, and README.md). In the Git tab, you’ll note that two files are listed. This is the status pane that shows the current modification status of all of the files in the repository. In this case, the .gitignore file is listed as M for Modified, and training-test.Rproj is listed with a ? ? to indicate that the file is untracked. This means that git has not stored any versions of this file, and knows nothing about the file. As you make version control decisions in RStudio, these icons will change to reflect the current version status of each of the files. Inspect the history. For now, let’s click on the History button in the Git tab, which will show the log of changes that occurred, and will be identical to what we viewed on GitHub. By clicking on each row of the history, you can see exactly what was added and changed in each of the two commits in this repository. Challenge Commit a README.md change. Next let’s make a change to the README.md file, this time from RStudio. Add a new section to your markdown using a header, and under it include a list three advantages to using git. Once you save, you’ll immediately see the README.md file show up in the Git tab, marked as a modification. You can select the file in the Git tab, and click Diff to see the differences that you saved (but which are not yet committed to your local repository). And here’s what the newly made changes look like compared to the original file. New lines are highlighted in green, while removed lines are in red. Commit the RStudio changes. To commit the changes you made to the README.md file, check the Staged checkbox next to the file (which tells Git which changes you want included in the commit), then provide a descriptive Commit message, and then click Commit. Note that some of the changes in the repository, namely training-test.Rproj are still listed as having not been committed. This means there are still pending changes to the repository. You can also see the note that says: Your branch is ahead of ‘origin/main’ by 1 commit. This means that we have committed 1 change in the local repository, but that commit has not yet been pushed up to the origin repository, where origin is the typical name for our remote repository on GitHub. So, let’s commit the remaining project files by staging them and adding a commit message. When finished, you’ll see that no changes remain in the Git tab, and the repository is clean. Inspect the history. Note that the message now says: Your branch is ahead of ‘origin/main’ by 2 commits. These 2 commits are the two we just made, and have not yet been pushed to GitHub. By clicking on the History button, we can see that there are now a total of four commits in the local repository (while there had only been two on GitHub). Push these changes to GitHub. Now that everything has been changed as desired locally, you can push the changes to GitHub using the Push button. This will prompt you for your GitHub username and password, and upload the changes, leaving your repository in a totally clean and synchronized state. When finished, looking at the history shows all four commits, including the two that were done on GitHub and the two that were done locally on RStudio. And note that the labels indicate that both the local repository (HEAD) and the remote repository (origin/HEAD) are pointing at the same version in the history. So, if we go look at the commit history on GitHub, all the commits will be shown there as well. Aside What should I write in my commit message? Clearly, good documentation of what you’ve done is critical to making the version history of your repository meaningful and helpful. Its tempting to skip the commit message altogether, or to add some stock blurb like ‘Updates’. It’s better to use messages that will be helpful to your future self in deducing not just what you did, but why you did it. Also, commit messaged are best understood if they follow the active verb convention. For example, you can see that my commit messages all started with a past tense verb, and then explained what was changed. While some of the changes we illustrated here were simple and so easily explained in a short phrase, for more complex changes, its best to provide a more complete message. The convention, however, is to always have a short, terse first sentence, followed by a more verbose explanation of the details and rationale for the change. This keeps the high level details readable in the version log. I can’t count the number of times I’ve looked at the commit log from 2, 3, or 10 years prior and been so grateful for diligence of my past self and collaborators. Collaboration and conflict free workflows Up to now, we have been focused on using Git and GitHub for yourself, which is a great use. But equally powerful is to share a GitHub repository with other researchers so that you can work on code, analyses, and models together. When working together, you will need to pay careful attention to the state of the remote repository to avoid and handle merge conflicts. A merge conflict occurs when two collaborators make two separate commits that change the same lines of the same file. When this happens, git can’t merge the changes together automatically, and will give you back an error asking you to resolve the conflict. Don’t be afraid of merge conflicts, they are pretty easy to handle. and there are some great guides. That said, its truly painless if you can avoid merge conflicts in the first place. You can minimize conflicts by: Ensure that you pull down changes just before you commit Ensures that you have the most recent changes But you may have to fix your code if conflict would have occurred Coordinate with your collaborators on who is touching which files You still need to communicate to collaborate 4.5 Setting up git on an existing project Now you have two projects set up in your RStudio environment, training_{USERNAME} and training_test. We set you up with the training_test project since we think it is an easy way to introduce you to git, but more commonly researchers will have an existing directory of code that they then want to make a git repository out of. For the last exercise of this session, we will do this with your training_{USERNAME} project. First, switch to your training_{USERNAME} project using the RStudio project switcher. The project switcher is in the upper right corner of your RStudio pane. Click the dropdown next to your project name (training_test), and then select the training_{USERNAME} project from the “recent projects” list. Next, from the Tools menu, select “Project Options.” In the dialog that pops up, select “Git/SVN” from the menu on the left. In the dropdown at the top of this page, select “Git” and click “Yes” in the confirmation box. Click “Yes” again to restart RStudio. When RStudio restarts, you should have a git tab, with two untracked files (.gitignore and training_{USERNAME}.Rproj). Challenge Add and commit these two files to your git repository Now we have your local repository all set up. You can make as many commits as you want on this repository, and it will likely still be helpful to you, but the power in git and GitHub is really in collaboration. As discussed, GitHub facilitates this, so let’s get this repository on GitHub. Go back to GitHub, and click on the “New Repository” button. In the repository name field, enter the same name as your RProject. So for me, this would be training_jclark. Add a description, keep the repository public, and, most importantly: DO NOT INITIALIZE THE REPOSITORY WITH ANY FILES. We already have the repository set up locally so we don’t need to do this. Initializing the repository will only cause merge issues. Here is what your page should look like: Once your page looks like that, click the “create repository” button. This will open your empty repository with a page that conveniently gives you exactly the instructions you need. In our case, we are “pushing an existing repository from the command line.” Click the clipboard icon to copy the code for the middle option of the three on this page. It should have three lines and look like this: git remote add origin https://github.com/jeanetteclark/training_jclark.git git branch -M main git push -u origin main Back in RStudio, open the terminal by clicking the tab labeled as such next to the console tab. The prompt should look something like this: jclark@included-crab:~/training_jclark$ In the prompt, paste the code that you copied from the GitHub page and press return. You will be prompted to type your GitHub username and password. The code that you copied and pasted did three things: added the GitHub repository as the remote repository renamed the default branch to main pushed the main branch to the remote GitHub repository If you go back to your browser and refresh your GitHub repository page, you should now see your files appear. Challenge On your repository page, GitHub has a button that will help you add a readme file. Click the “Add a README” button and use markdown syntax to create a simple readme. Commit the changes to your repository. Go to your local repository (in RStudio) and pull the changes you made. 4.6 Go Further There’s a lot we haven’t covered in this brief tutorial. There are some great and much longer tutorials that cover advanced topics, such as: Using git on the command line Resolving conflicts Branching and merging Pull requests versus direct contributions for collaboration Using .gitignore to protect sensitive data GitHub Issues and why they are useful and much, much more Try Git is a great interactive tutorial Software Carpentry Version Control with Git Codecademy Learn Git (some paid) "],["git-collaboration-and-conflict-management.html", "5 git Collaboration and Conflict Management 5.1 Learning Objectives 5.2 Introduction 5.3 Collaborating with a trusted colleague without conflicts 5.4 Merge conflicts 5.5 How to resolve a conflict 5.6 Workflows to avoid merge conflicts", " 5 git Collaboration and Conflict Management 5.1 Learning Objectives In this lesson, you will learn: How to use Git and GitHub to collaborate with colleagues on code What typically causes conflicts when collaborating Workflows to avoid conflicts How to resolve a conflict 5.2 Introduction Git is a great tool for working on your own, but even better for working with friends and colleagues. Git allows you to work with confidence on your own local copy of files with the confidence that you will be able to successfully synchronize your changes with the changes made by others. The simplest way to collaborate with Git is to use a shared repository on a hosting service such as GitHub, and use this shared repository as the mechanism to move changes from one collaborator to another. While there are other more advanced ways to sync git repositories, this “hub and spoke” model works really well due to its simplicity. In this model, the collaborator will clone a copy of the owner’s repository from GitHub, and the owner will grant them collaborator status, enabling the collaborator to directly pull and push from the owner’s GitHub repository. 5.3 Collaborating with a trusted colleague without conflicts We start by enabling collaboration with a trusted colleague. We will designate the Owner as the person who owns the shared repository, and the Collaborator as the person that they wish to grant the ability to make changes to their repository. We start by giving that person access to our GitHub repository. Setup We will break you into pairs, so choose one person as the Owner and one as the Collaborator Log into GitHub as the `Owner Navigate to the Owner’s training repository (e.g., training_jones) Then, have the Owner visit their training repository created earlier, and visit the Settings page, and select the Manage access screen, and add the username of your Collaborator in the box. Once the collaborator has been added, they should check their email for an invitation from GitHub, and click on the acceptance link, which will enable them to collaborate on the repository. We will start by having the collaborator make some changes and share those with the Owner without generating any conflicts, In an ideal world, this would be the normal workflow. Here are the typical steps. 5.3.1 Step 1: Collaborator clone To be able to contribute to a repository, the collaborator must clone the repository from the Owner’s github account. To do this, the Collaborator should visit the github page for the Owner’s repository, and then copy the clone URL. In R Studio, the Collaborator will create a new project from version control by pasting this clone URL into the appropriate dialog (see the earlier chapter introducing GitHub). 5.3.2 Step 2: Collaborator Edits With a clone copied locally, the Collaborator can now make changes to the index.Rmd file in the repository, adding a line or statement somewhere noticeable near the top. Save your changes. 5.3.3 Step 3: Collaborator commit and push To sync changes, the collaborator will need to add, commit, and push their changes to the Owner’s repository. But before doing so, it’s good practice to pull immediately before committing to ensure you have the most recent changes from the owner. So, in R Studio’s Git tab, first click the “Diff” button to open the git window, and then press the green “Pull” down arrow button. This will fetch any recent changes from the origin repository and merge them. Next, add the changed index.Rmd file to be committed by clicking the checkbox next to it, type in a commit message, and click ‘Commit.’ Once that finishes, then the collaborator can immediately click ‘Push’ to send the commits to the Owner’s GitHub repository. 5.3.4 Step 4: Owner pull Now, the owner can open their local working copy of the code in RStudio, and pull those changes down to their local copy. Congrats, the owner now has your changes! 5.3.5 Step 5: Owner edits, commit, and push Next, the owner should do the same. Make changes to a file in the repository, save it, pull to make sure no new changes have been made while editing, and then add, commit, and push the Owner changes to GitHub. 5.3.6 Step 6: Collaborator pull The collaborator can now pull down those owner changes, and all copies are once again fully synced. And you’re off to collaborating. Challenge Now that the instructors have demonstrated this conflict-free process, break into pairs and try the same with your partner. Start by designating one person as the Owner and one as the Collborator, and then repeat the steps described above: Step 0: Setup permissions for your collaborator Step 1: Collaborator clones the Owner repository Step 2: Collaborator Edits the README file Step 3: Collaborator commits and pushes the file to GitHub Step 4: Owner pulls the changes that the Collaborator made Step 5: Owner edits, commits, and pushes some new changes Step 6: Collaborator pulls the owners changes from GitHub 5.4 Merge conflicts So things can go wrong, which usually starts with a merge conflict, due to both collaborators making incompatible changes to a file. While the error messages from merge conflicts can be daunting, getting things back to a normal state can be straightforward once you’ve got an idea where the problem lies. A merge conflict occurs when both the owner and collaborator change the same lines in the same file without first pulling the changes that the other has made. This is most easily avoided by good communication about who is working on various sections of each file, and trying to avoid overlaps. But sometimes it happens, and git is there to warn you about potential problems. And git will not allow you to overwrite one person’s changes to a file with another’s changes to the same file if they were based on the same version. The main problem with merge conflicts is that, when the Owner and Collaborator both make changes to the same line of a file, git doesn’t know whose changes take precedence. You have to tell git whose changes to use for that line. 5.5 How to resolve a conflict Abort, abort, abort… Sometimes you just made a mistake. When you get a merge conflict, the repository is placed in a ‘Merging’ state until you resolve it. There’s a commandline command to abort doing the merge altogether: git merge --abort Of course, after doing that you still haven’t synced with your collaborator’s changes, so things are still unresolved. But at least your repository is now usable on your local machine. Checkout The simplest way to resolve a conflict, given that you know whose version of the file you want to keep, is to use the commandline git program to tell git to use either your changes (the person doing the merge), or their changes (the other collaborator). keep your collaborators file: git checkout --theirs conflicted_file.Rmd keep your own file: git checkout --ours conflicted_file.Rmd Once you have run that command, then run add, commit, and push the changes as normal. Pull and edit the file But that requires the commandline. If you want to resolve from RStudio, or if you want to pick and choose some of your changes and some of your collaborator’s, then instead you can manually edit and fix the file. When you pulled the file with a conflict, git notices that there is a conflict and modifies the file to show both your own changes and your collaborator’s changes in the file. It also shows the file in the Git tab with an orange U icon, which indicates that the file is Unmerged, and therefore awaiting you help to resolve the conflict. It delimits these blocks with a series of less than and greater than signs, so they are easy to find: To resolve the conficts, simply find all of these blocks, and edit them so that the file looks how you want (either pick your lines, your collaborators lines, some combination, or something altogether new), and save. Be sure you removed the delimiter lines that started with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. Once you have made those changes, you simply add, commit, and push the files to resolve the conflict. 5.5.1 Producing and resolving merge conflicts To illustrate this process, we’re going to carefully create a merge conflict step by step, show how to resolve it, and show how to see the results of the successful merge after it is complete. First, we will walk through the exercise to demonstrate the issues. 5.5.1.1 Owner and collaborator ensure all changes are updated First, start the exercise by ensuring that both the Owner and Collaborator have all of the changes synced to their local copies of the Owner’s repository in RStudio. This includes doing a git pull to ensure that you have all changes local, and make sure that the Git tab in RStudio doesn’t show any changes needing to be committed. 5.5.1.2 Owner makes a change and commits From that clean slate, the Owner first modifies and commits a small change including their name on a specific line of the README.md file (we will change line 4). Work to only change that one line, and add your username to the line in some form and commit the changes (but DO NOT push). We are now in a situation where the owner has unpushed changes that the collaborator can not yet see. 5.5.1.3 Collaborator makes a change and commits on the same line Now the collaborator also makes changes to the same (line 4) of the README.md file in their RStudio copy of the project, adding their name to the line. They then commit. At this point, both the owner and collaborator have committed changes based on their shared version of the README.md file, but neither has tried to share their changes via GitHub. 5.5.1.4 Collaborator pushes the file to GitHub Sharing starts when the Collaborator pushes their changes to the GitHub repo, which updates GitHub to their version of the file. The owner is now one revision behind, but doesn’t yet know it. 5.5.1.5 Owner pushes their changes and gets an error At this point, the owner tries to push their change to the repository, which triggers an error from GitHub. While the error message is long, it basically tells you everything needed (that the owner’s repository doesn’t reflect the changes on GitHub, and that they need to pull before they can push). 5.5.1.6 Owner pulls from GitHub to get Collaborator changes Doing what the message says, the Owner pulls the changes from GitHub, and gets another, different error message. In this case, it indicates that there is a merge conflict because of the conflicting lines. In the Git pane of RStudio, the file is also flagged with an orange ‘U’, which stands for an unresolved merge conflict. 5.5.1.7 Owner edits the file to resolve the conflict To resolve the conflict, the Owner now needs to edit the file. Again, as indicated above, git has flagged the locations in the file where a conflict occurred with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. The Owner should edit the file, merging whatever changes are appropriate until the conflicting lines read how they should, and eliminate all of the marker lines with with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. Of course, for scripts and programs, resolving the changes means more than just merging the text – whoever is doing the merging should make sure that the code runs properly and none of the logic of the program has been broken. 5.5.1.8 Owner commits the resolved changes From this point forward, things proceed as normal. The owner first ‘Adds’ the file changes to be made, which changes the orange U to a blue M for modified, and then commits the changes locally. The owner now has a resolved version of the file on their system. 5.5.1.9 Owner pushes the resolved changes to GitHub Have the Owner push the changes, and it should replicate the changes to GitHub without error. 5.5.1.10 Collaborator pulls the resolved changes from GitHub Finally, the Collaborator can pull from GitHub to get the changes the owner made. 5.5.1.11 Both can view commit history When either the Collaborator or the Owner view the history, the conflict, associated branch, and the merged changes are clearly visible in the history. Merge Conflict Challenge Now it’s your turn. In pairs, intentionally create a merge conflict, and then go through the steps needed to resolve the issues and continue developing with the merged files. See the sections above for help with each of these steps: Step 0: Owner and collaborator ensure all changes are updated Step 1: Owner makes a change and commits Step 2: Collaborator makes a change and commits on the same line Step 3: Collaborator pushes the file to GitHub Step 4: Owner pushes their changes and gets an error Step 5: Owner pulls from GitHub to get Collaborator changes Step 6: Owner edits the file to resolve the conflict Step 7: Owner commits the resolved changes Step 8: Owner pushes the resolved changes to GitHub Step 9: Collaborator pulls the resolved changes from GitHub Step 10: Both can view commit history 5.6 Workflows to avoid merge conflicts Some basic rules of thumb can avoid the vast majority of merge conflicts, saving a lot of time and frustration. These are words our teams live by: Communicate often Tell each other what you are working on Pull immediately before you commit or push Commit often in small chunks. A good workflow is encapsulated as follows: Pull -&gt; Edit -&gt; Add -&gt; Pull -&gt; Commit -&gt; Push Always start your working sessions with a pull to get any outstanding changes, then start doing your editing and work. Stage your changes, but before you commit, Pull again to see if any new changes have arrived. If so, they should merge in easily if you are working in different parts of the program. You can then Commit and immediately Push your changes safely. Good luck, and try to not get frustrated. Once you figure out how to handle merge conflicts, they can be avoided or dispatched when they occur, but it does take a bit of practice. "],["social-aspects-of-collaboration.html", "6 Social Aspects of Collaboration 6.1 Thinking preferences 6.2 Developing a Code of Conduct 6.3 Authorship and Credit Policies 6.4 Data Sharing and Reuse Policies 6.5 Research Data Publishing Ethics 6.6 Extra Reading", " 6 Social Aspects of Collaboration 6.1 Thinking preferences 6.1.1 Learning Objectives An activity and discussion that will provide: Opportunity to get to know fellow participants and trainers An introduction to variation in thinking preferences 6.1.2 Thinking Preferences Activity Step 1: Read through the statements contained within this document and determine which descriptors are most like you. Make a note of them. Review the descriptors again and determine which are quite like you. You are working towards identifying your top 20. If you have more than 20, discard the descriptors that resonate the least. Using the letter codes in the right hand column, count the number of descriptors that fall into the categories A B C and D. Step 2: Scroll to the second page and copy the graphic onto a piece of paper, completing the quadrant with your scores for A, B, C and D. Step 3: Reflect and share out: Do you have a dominant letter? Were some of the statements you included in your top 20 easier to resonate with than others? Were you answering based on how you are or how you wish to be? 6.1.3 About the Whole Brain Thinking System Everyone thinks differently. The way individuals think guides the way they work, and the way groups of individuals think guides how teams work. Understanding thinking preferences facilitates effective collaboration and team work. The Whole Brain Model, developed by Ned Herrmann, builds upon early conceptualizations of brain functioning. For example, the left and right hemispheres were thought to be associated with different types of information processing while our neocortex and limbic system would regulate different functions and behaviours. The Herrmann Brain Dominance Instrument (HBDI) provides insight into dominant characteristics based on thinking preferences. There are four major thinking styles that reflect the left cerebral, left limbic, right cerebral and right limbic. Analytical (Blue) Practical (Green) Relational (Red) Experimental (Yellow) These four thinking styles are characterized by different traits. Those in the BLUE quadrant have a strong logical and rational side. They analyze information and may be technical in their approach to problems. They are interested in the ‘what’ of a situation. Those in the GREEN quadrant have a strong organizational and sequential side. They like to plan details and are methodical in their approach. They are interested in the ‘when’ of a situation. The RED quadrant includes those that are feelings-based in their apporach. They have strong interpersonal skills and are good communicators. They are interested in the ‘who’ of a situation. Those in the YELLOW quadrant are ideas people. They are imaginative, conceptual thinkers that explore outside the box. Yellows are interested in the ‘why’ of a situation. Most of us identify with thinking styles in more than one quadrant and these different thinking preferences reflect a complex self made up of our rational, theoretical self; our ordered, safekeeping self; our emotional, interpersonal self; and our imaginitive, experimental self. Undertsanding the complexity of how people think and process information helps us understand not only our own approach to problem solving, but also how individuals within a team can contribute. There is great value in diversity of thinking styles within collaborative teams, each type bringing stengths to different aspects of project development. 6.2 Developing a Code of Conduct Whether you are joining a lab group or establishing a new collaboration, articulating a set of shared agreements about how people in the group will treat each other will help create the conditions for successful collaboration. If agreements or a code of conduct do not yet exist, invite a conversation among all members to create them. Co-creation of a code of conduct will foster collaboration and engagement as a process in and of itself, and is important to ensure all voices heard such that your code of conduct represents the perspectives of your community. If a code of conduct already exists, and your community will be a long-acting collaboration, you might consider revising the code of conduct. Having your group ‘sign off’ on the code of conduct, whether revised or not, supports adoption of the principles. When creating a code of conduct, consider both the behaviors you want to encourage and those that will not be tolerated. For example, the Openscapes code of conduct includes Be respectful, honest, inclusive, accommodating, appreciative, and open to learning from everyone else. Do not attack, demean, disrupt, harass, or threaten others or encourage such behavior. Below are other example codes of conduct: NCEAS Code of Conduct Carpentries Code of Conduct Arctic Data Center Code of Conduct Mozilla Community Participation Guidelines Ecological Society of America Code of Conduct 6.3 Authorship and Credit Policies Navigating issues of intellectual property and credit can be a challenge, particularly for early career researchers. Open communication is critical to avoiding misunderstandings and conflicts. Talk to your coauthors and collaborators about authorship, credit, and data sharing early and often. This is particularly important when working with new collaborators and across lab groups or disciplines which may have divergent views on authorship and data sharing. If you feel uncomfortable talking about issues surrounding credit or intellectual property, seek the advice or assistance of a mentor to support you in having these important conversations. The “Publication” section of the Ecological Society of America’s Code of Ethics is a useful starting point for discussions about co-authorship, as are the International Committee of Medical Journal Editors guidelines for authorship and contribution. You should also check guidelines published by the journal(s) to which you anticipate submitting your work. For collaborative research projects, develop an authorship agreement for your group early in the project and refer to it for each product. This example authorship agreement from the Arctic Data Center provides a useful template. It builds from information contained within Weltzin et al (2006) and provides a rubric for inclusion of individuals as authors. Your collaborative team may not choose to adopt the agreement in the current form, however it will prompt thought and discussion in advance of developing a consensus. Some key questions to consider as you are working with your team to develop the agreement: What roles do we anticipate contributors will play? e.g., the NISO Contributor Roles Taxonomy (CRediT) identifies 14 distinct roles: Conceptualization Data curation Formal Analysis Funding acquisition Investigation Methodology Project administration Resources Software Supervision Validation Visualization Writing – original draft Writing – review &amp; editing What are our criteria for authorship? (See the ICMJE guidelines for potential criteria) Will we extend the opportunity for authorship to all group members on every paper or product? Do we want to have an opt in or opt out policy? (In an opt out policy, all group members are considered authors from the outset and must request removal from the paper if they don’t want think they meet the criteria for authorship) Who has the authority to make decisions about authorship? Lead author? PI? Group? How will we decide authorship order? In what other ways will we acknowledge contributions and extend credit to collaborators? How will we resolve conflicts if they arise? 6.4 Data Sharing and Reuse Policies As with authorship agreements, it is valuable to establish a shared agreement around handling of data when embarking on collaborative projects. Data collected as part of a funded research activity will typically have been managed as part of the Data Management Plan (DMP) associated with that project. However, collaborative research brings together data from across research projects with different data management plans and can include publicly accessible data from repositories where no management plan is available. For these reasons, a discussion and agreement around the handling of data brought into and resulting from the collaboration is warranted and management of this new data may benefit from going through a data management planning process. Below we discuss example data agreements. The example data policy template provided by the Arctic Data Center addresses three categories of data. Individual data not in the public domain Individual data with public access Derived data resulting from the project For the first category, the agreement considers conditions under which those data may be used and permissions associated with use. It also addresses access and sharing. In the case of individual, publicly accessible data, the agreement stipulates that the team will abide by the attribution and usage policies that the data were published under, noting how those requirements we met. In the case of derived data, the agreement reads similar to a DMP with consideration of making the data public; management, documentation and archiving; pre-publication sharing; and public sharing and attribution. As research data objects receive a persistent identifier (PID), often a DOI, there are citable objects and consideration should be given to authorship of data, as with articles. The following example lab policy from the Wolkovich Lab combines data management practices with authorship guidelines and data sharing agreements. It provides a lot of detail about how this lab approaches data use, attribution and authorship. For example: Section 6: Co-authorship &amp; data If you agree to take on existing data you cannot offer co-authorship for use of the data unless four criteria are met: The co-author agrees to (and does) make substantial intellectual contribution to the work, which includes the reading and editing of all manuscripts on which you are a co-author through the submission-for-publication stage. This includes helping with interpretation of the data, system, study questions. Agreement of co-authorship is made at the start of the project. Agreement is approved of by Lizzie. All data-sharers are given an equal opportunity at authorship. It is not allowed to offer or give authorship to one data-sharer unless all other data-sharers are offered an equal opportunity at authorship—this includes data that are publicly-available, meaning if you offer authorship to one data-sharer and were planning to use publicly-available data you must reach out to the owner of the publicly-available data and strongly offer equivalent authorship as offered to the other data-sharer. As an example, if five people share data freely with you for a meta-analysis and and a sixth wants authorship you either must strongly offer equivalent authorship to all five or deny authorship to the sixth person. Note that the above requirements must also be met in this situation. If one or more datasets are more central or critical to a paper to warrant selective authorship this must be discussed and approved by Lizzie (and has not, to date, occurred within the lab). 6.4.0.1 Policy Preview This policy is communicated with all incoming lab members, from undergraduate to postdocs and visiting scholars, and is shared here with permission from Dr Elizabeth Wolkovich. 6.4.1 Community Principles: CARE and FAIR The CARE and FAIR Principles were introduced previously in the context of introducing the Arctic Data Center and our data submission and documentation process. In this section we will dive a little deeper. To recap, the Arctic Data Center is an openly-accessible data repository and the data published through the repository is open for anyone to reuse, subject to conditions of the license (at the Arctic Data Center, data is released under one of two licenses: CC-0 Public Domain and CC-By Attribution 4.0). In facilitating use of data resources, the data stewardship community have converged on principles surrounding best practices for open data management One set of these principles is the FAIR principles. FAIR stands for Findable, Accessible, Interoperable, and Reproducible. The “Fostering FAIR Data Practices in Europe” project found that it is more monetarily and timely expensive when FAIR principles are not used, and it was estimated that 10.2 billion dollars per years are spent through “storage and license costs to more qualitative costs related to the time spent by researchers on creation, collection and management of data, and the risks of research duplication.” FAIR principles and open science are overlapping concepts, but are distinctive concepts. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. Another set of community developed principles surrounding open data are the CARE Principles. The CARE principles for Indigenous Data Governance complement the more data-centric approach of the FAIR principles, introducing social responsibility to open data management practices. The CARE Principles stand for: Collective Benefit - Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data Authority to Control - Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data. Responsibility - Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples. Ethics - Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem. The CARE principles align with the FAIR principles by outlining guidelines for publishing data that is findable, accessible, interoperable, and reproducible while at the same time, accounts for Indigenous’ Peoples rights and interests. Initially designed to support Indigenous data sovereignty, CARE principles are now being adopted across domains, and many researchers argue they are relevant for both Indigenous Knowledge and data, as well as data from all disciplines (Carroll et al., 2021). These principles introduce a “game changing perspective” that encourages transparency in data ethics, and encourages data reuse that is purposeful and intentional that aligns with human well-being aligns with human well-being (Carroll et al., 2021). 6.5 Research Data Publishing Ethics For over 20 years, the Committee on Publication Ethics (COPE) has provided trusted guidance on ethical practices for scholarly publishing. The COPE guidelines have been broadly adopted by academic publishers across disciplines, and represent a common approach to identify, classify, and adjudicate potential breaches of ethics in publication such as authorship conflicts, peer review manipulation, and falsified findings, among many other areas. Despite these guidelines, there has been a lack of ethics standards, guidelines, or recommendations for data publications, even while some groups have begun to evaluate and act upon reported issues in data publication ethics. Data retractions To address this gap, the Force 11 Working Group on Research Data Publishing Ethics was formed as a collaboration among research data professionals and the Committee on Publication Ethics (COPE) “to develop industry-leading guidance and recommended best practices to support repositories, journal publishers, and institutions in handling the ethical responsibilities associated with publishing research data.” The group released the “Joint FORCE11 &amp; COPE Research Data Publishing Ethics Working Group Recommendations” (Puebla, Lowenberg, and WG 2021), which outlines recommendations for four categories of potential data ethics issues: Force11/COPE Authorship and Contribution Conflicts Authorship omissions Authorship ordering changes / conflicts Institutional investigation of author finds misconduct Legal/regulatory restrictions Copyright violation Insufficient rights for deposit Breaches of national privacy laws (GPDR, CCPA) Breaches of biosafety and biosecurity protocols Breaches of contract law governing data redistribution Risks of publication or release Risks to human subjects Lack of consent Breaches of himan rights Release of personally identifiable information (PII) Risks to species, ecosystems, historical sites Locations of endangered species or historical sites Risks to communities or societies Data harvested for profit or surveillance Breaches of data sovereignty Rigor of published data Unintentional errors in collection, calculation, display Un-interpretable data due to lack of adequate documentation Errors of of study design and inference Data manipulation or fabrication Guidelines cover what actions need to be taken, depending on whether the data are already published or not, as well as who should be involved in decisions, who should be notified of actions, and when the public should be notified. The group has also published templates for use by publishers and repositories to announce the extent to which they plan to conform to the data ethics guidelines. Discussion: Data publishing policies At the Arctic Data Center, we need to develop policies and procedures governing how we react to potential breaches of data publication ethics. In this exercise, break into groups to provide advice on how the Arctic Data Center should respond to reports of data ethics issues, and whether we should adopt the Joint FORCE11 &amp; COPE Research Data Publishing Ethics Working Group Policy Templates for repositories. In your discussion, consider: Should the repository adopt the repository policy templates from Force11? Who should be involved in evaluation of the merits of ethical cases reported to ADC? Who should be involved in deciding the actions to take? What are the range of responses that the repository should consider for ethical breaches? Who should be notified when a determination has been made that a breach has occurred? You might consider a hypothetical scenario such as the following in considering your response. The data coordinator at the Arctic Data Center receives an email in 2022 from a prior postdoctoral fellow who was employed as part of an NSF-funded project on microbial diversity in Alaskan tundra ecosystems. The email states that a dataset from 2014 in the Arctic Data Center was published with the project PI as author, but omits two people, the postdoc and an undergraduate student, as co-authors on the dataset. The PI retired in 2019, and the postdoc asks that they be added to the author list of the dataset to correct the historical record and provide credit. 6.6 Extra Reading Cheruvelil, K. S., Soranno, P. A., Weathers, K. C., Hanson, P. C., Goring, S. J., Filstrup, C. T., &amp; Read, E. K. (2014). Creating and maintaining high-performing collaborative research teams: The importance of diversity and interpersonal skills. Frontiers in Ecology and the Environment, 12(1), 31-38. DOI: 10.1890/130001 Carroll, S. R., Garba, I., Figueroa-Rodríguez, O. L., Holbrook, J., Lovett, R., Materechera, S., … Hudson, M. (2020). The CARE Principles for Indigenous Data Governance. Data Science Journal, 19(1), 43. DOI: http://doi.org/10.5334/dsj-2020-043 "],["data-modeling-essentials.html", "7 Data Modeling Essentials 7.1 Data Modeling &amp; Tidy Data", " 7 Data Modeling Essentials 7.1 Data Modeling &amp; Tidy Data 7.1.1 Learning Objectives Understand basics of relational data models aka tidy data Learn how to design and create effective data tables 7.1.2 Introduction In this lesson we are going to learn what relational data models are, and how they can be used to manage and analyze data efficiently. Relational data models are what relational databases use to organize tables. However, you don’t have to be using a relational database (like mySQL, MariaDB, Oracle, or Microsoft Access) to enjoy the benefits of using a relational data model. Additionally, your data don’t have to be large or complex for you to benefit. Here are a few of the benefits of using a relational data model: Powerful search and filtering Handle large, complex data sets Enforce data integrity Decrease errors from redundant updates Simple guidelines for data management A great paper called ‘Some Simple Guidelines for Effective Data Management’ (Borer et al. 2009) lays out exactly that - guidelines that make your data management, and your reproducible research, more effective. The first six guidelines are straightforward, but worth mentioning here: Use a scripted program (like R!) Non-proprietary file formats are preferred (eg: csv, txt) Keep a raw version of data Use descriptive file and variable names (without spaces!) Include a header line in your tabular data files Use plain ASCII text The next three are a little more complex, but all are characteristics of the relational data model: Design tables to add rows, not columns Each column should contain only one type of information Record a single piece of data only once; separate information collected at different scales into different tables. 7.1.3 Recognizing untidy data Before we learn how to create a relational data model, let’s look at how to recognize data that does not conform to the model. Data Organization This is a screenshot of an actual dataset that came across NCEAS. We have all seen spreadsheets that look like this - and it is fairly obvious that whatever this is, it isn’t very tidy. Let’s dive deeper in to exactly why we wouldn’t consider it tidy. Multiple tables Your human brain can see from the way this sheet is laid out that it has three tables within it. Although it is easy for us to see and interpret this, it is extremely difficult to get a computer to see it this way, which will create headaches down the road should you try to read in this information to R or another programming language. Inconsistent observations Rows correspond to observations. If you look across a single row, and you notice that there are clearly multiple observations in one row, the data are likely not tidy. Inconsistent variables Columns correspond to variables. If you look down a column, and see that multiple variables exist in the table, the data are not tidy. A good test for this can be to see if you think the column consists of only one unit type. Marginal sums and statistics Marginal sums and statistics also are not considered tidy, and they are not the same type of observation as the other rows. Instead, they are a combination of observations. 7.1.4 Good enough data modeling Denormalized data When data are “denormalized” it means that observations about different entities are combined. In the above example, each row has measurements about both the site at which observations occurred, as well as observations of two individual plants of possibly different species found at that site. This is not normalized data. People often refer to this as wide format, because the observations are spread across a wide number of columns. Note that, should one encounter a new species in the survey, we would have to add new columns to the table. This is difficult to analyze, understand, and maintain. Tabular data Observations. A better way to model data is to organize the observations about each type of entity in its own table. This results in: Separate tables for each type of entity measured Each row represents a single observed entity Observations (rows) are all unique This is normalized data (aka tidy data) Variables. In addition, for normalized data, we expect the variables to be organized such that: All values in a column are of the same type All columns pertain to the same observed entity (e.g., row) Each column represents either an identifying variable or a measured variable Challenge Try to answer the following questions: What are the observed entities in the example above? What are the measured variables associated with those observations? Answer: If we use these questions to tidy our data, we should end up with: one table for each entity observed one column for each measured variable additional columns for identifying variables (such as site ID) Here is what our tidy data look like: Note that this normalized version of the data meets the three guidelines set by (Borer et al. 2009): Design tables to add rows, not columns Each column should contain only one type of information Record a single piece of data only once; separate information collected at different scales into different tables. 7.1.5 Using normalized data Normalizing data by separating it into multiple tables often makes researchers really uncomfortable. This is understandable! The person who designed this study collected all of these measurements for a reason - so that they could analyze the measurements together. Now that our site and species information are in separate tables, how would we use site elevation as a predictor variable for species composition, for example? The answer is keys - and they are the cornerstone of relational data models. When one has normalized data, we often use unique identifiers to reference particular observations, which allows us to link across tables. Two types of identifiers are common within relational data: Primary Key: unique identifier for each observed entity, one per row Foreign Key: reference to a primary key in another table (linkage) Challenge In our normalized tables above, identify the following: the primary key for each table any foreign keys that exist Answer The primary key of the top table is id. The primary key of the bottom table is site. The site column is the primary key of that table because it uniquely identifies each row of the table as a unique observation of a site. In the first table, however, the site column is a foreign key that references the primary key from the second table. This linkage tells us that the first height measurement for the DAPU observation occurred at the site with the name Taku. Entity-Relationship Model (ER) An Entity-Relationship model allows us to compactly draw the structure of the tables in a relational database, including the primary and foreign keys in the tables. In the above model, one can see that each site in the sites observations table must have one or more observations in the species observations table, whereas each species opservation has one and only one site. Merging data Frequently, analysis of data will require merging these separately managed tables back together. There are multiple ways to join the observations in two tables, based on how the rows of one table are merged with the rows of the other. When conceptualizing merges, one can think of two tables, one on the left and one on the right. The most common (and often useful) join is when you merge the subset of rows that have matches in both the left table and the right table: this is called an INNER JOIN. Other types of join are possible as well. A LEFT JOIN takes all of the rows from the left table, and merges on the data from matching rows in the right table. Keys that don’t match from the left table are still provided with a missing value (na) from the right table. A RIGHT JOIN is the same, except that all of the rows from the right table are included with matching data from the left, or a missing value. Finally, a FULL OUTER JOIN includes all data from all rows in both tables, and includes missing values wherever necessary. Sometimes people represent these as Venn diagrams showing which parts of the left and right tables are included in the results for each join. These however, miss part of the story related to where the missing value come from in each result. In the figure above, the blue regions show the set of rows that are included in the result. For the INNER join, the rows returned are all rows in A that have a matching row in B. 7.1.6 Data modeling exercise Break into groups, 1 per table To demonstrate, we’ll be working with a tidied up version of a dataset from ADF&amp;G containing commercial catch data from 1878-1997. The dataset and reference to the original source can be viewed at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2. That site includes metadata describing the full data set, including column definitions. Here’s the first catch table: And here’s the region_defs table: Challenge Using the Invision live session your instructor starts, work through the following tasks: Draw an ER model for the tables Indicate the primary and foreign keys Is the catch table in normal (aka tidy) form? If so, what single type of entity was observed? If not, how might you restructure the data table to make it tidy? Draw a new ER diagram showing this re-designed data structure Optional Extra Challenge If you have time, take on this extra challenge with your group. Navigate to this dataset: Richard Lanctot and Sarah Saalfeld. 2019. Utqiaġvik shorebird breeding ecology study, Utqiaġvik, Alaska, 2003-2018. Arctic Data Center. doi:10.18739/A23R0PT35 Try to determine the primary and foreign keys for the following tables: Utqiagvik_adult_shorebird_banding.csv Utqiagvik_egg_measurements.csv Utqiagvik_nest_data.csv Utqiagvik_shorebird_resightings.csv 7.1.7 Resources Borer et al. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America. White et al. 2013. Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution 6. Software Carpentry SQL tutorial Tidy Data "],["cleaning-and-manipulating-data.html", "8 Cleaning and Manipulating Data 8.1 Data Cleaning and Manipulation", " 8 Cleaning and Manipulating Data 8.1 Data Cleaning and Manipulation 8.1.1 Learning Objectives In this lesson, you will learn: What the Split-Apply-Combine strategy is and how it applies to data The difference between wide vs. tall table formats and how to convert between them How to use dplyr and tidyr to clean and manipulate data for analysis How to join multiple data.frames together using dplyr 8.1.2 Introduction The data we get to work with are rarely, if ever, in the format we need to do our analyses. It’s often the case that one package requires data in one format, while another package requires the data to be in another format. To be efficient analysts, we should have good tools for reformatting data for our needs so we can do our actual work like making plots and fitting models. The dplyr and tidyr R packages provide a fairly complete and extremely powerful set of functions for us to do this reformatting quickly and learning these tools well will greatly increase your efficiency as an analyst. Analyses take many shapes, but they often conform to what is known as the Split-Apply-Combine strategy. This strategy follows a usual set of steps: Split: Split the data into logical groups (e.g., area, stock, year) Apply: Calculate some summary statistic on each group (e.g. mean total length by year) Combine: Combine the groups back together into a single table Figure 1: diagram of the split apply combine strategy As shown above (Figure 1), our original table is split into groups by year, we calculate the mean length for each group, and finally combine the per-year means into a single table. dplyr provides a fast and powerful way to express this. Let’s look at a simple example of how this is done: Assuming our length data is already loaded in a data.frame called length_data: year length_cm 1991 5.673318 1991 3.081224 1991 4.592696 1992 4.381523 1992 5.597777 1992 4.900052 1992 4.139282 1992 5.422823 1992 5.905247 1992 5.098922 We can do this calculation using dplyr like this: length_data %&gt;% group_by(year) %&gt;% summarise(mean_length_cm = mean(length_cm)) Another exceedingly common thing we need to do is “reshape” our data. Let’s look at an example table that is in what we will call “wide” format: site 1990 1991 … 1993 gold 100 118 … 112 lake 100 118 … 112 … … … … … dredge 100 118 … 112 You are probably quite familiar with data in the above format, where values of the variable being observed are spread out across columns (Here: columns for each year). Another way of describing this is that there is more than one measurement per row. This wide format works well for data entry and sometimes works well for analysis but we quickly outgrow it when using R. For example, how would you fit a model with year as a predictor variable? In an ideal world, we’d be able to just run: lm(length ~ year) But this won’t work on our wide data because lm needs length and year to be columns in our table. Or how would we make a separate plot for each year? We could call plot one time for each year but this is tedious if we have many years of data and hard to maintain as we add more years of data to our dataset. The tidyr package allows us to quickly switch between wide format and what is called tall format using the pivot_longer function: site_data %&gt;% pivot_longer(-site, names_to = &quot;year&quot;, values_to = &quot;length&quot;) site year length gold 1990 101 lake 1990 104 dredge 1990 144 … … … dredge 1993 145 In this lesson we’re going to walk through the functions you’ll most commonly use from the dplyr and tidyr packages: dplyr mutate() group_by() summarise() select() filter() arrange() left_join() rename() tidyr pivot_longer() pivot_wider() unite() separate() 8.1.3 Data Cleaning Basics To demonstrate, we’ll be working with a tidied up version of a dataset from ADF&amp;G containing commercial catch data from 1878-1997. The dataset and reference to the original source can be found at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2. First, open a new RMarkdown document. Delete everything below the setup chunk, and add a library chunk that calls dplyr, tidyr, and readr: library(dplyr) library(tidyr) library(readr) Aside A note on loading packages. You may have noticed the following warning messages pop up when you ran your library chunk. Attaching package: ‘dplyr’ The following objects are masked from ‘package:stats’: filter, lag The following objects are masked from ‘package:base’: intersect, setdiff, setequal, union These are important warnings. They are letting you know that certain functions from the stats and base packages (which are loaded by default when you start R) are masked by different functions with the same name in the dplyr package. It turns out, the order that you load the packages in matters. Since we loaded dplyr after stats, R will assume that if you call filter, you mean the dplyr version unless you specify otherwise. Being specific about which version of filter, for example, you call is easy. To explicitly call a function by its unambiguous name, you use the syntax package_name::function_name(...). So, if I wanted to call the stats version of filter in this Rmarkdown document, I would use the syntax stats::filter(...). Challenge The warnings above are important, but we might not want them in our final document. After you have read them, adjust the chunk settings on your library chunk to suppress warnings and messages. Now that we have learned a little mini-lesson on functions, let’s get the data that we are going to use for this lesson. Setup Navigate to the salmon catch dataset: https://knb.ecoinformatics.org/#view/df35b.304.2 Right click the “download” button for the file “byerlySalmonByRegion.csv” Select “copy link address” from the dropdown menu Paste the URL into a read.csv call like below The code chunk you use to read in the data should look something like this: catch_original &lt;- read.csv(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1&quot;) Aside Note that windows users who want to use this method locally also need to use the url function here with the argument method = \"libcurl\" as below: catch_original &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1&quot;, method = &quot;libcurl&quot;)) This dataset is relatively clean and easy to interpret as-is. But while it may be clean, it’s in a shape that makes it hard to use for some types of analyses so we’ll want to fix that first. Challenge Before we get too much further, spend a minute or two outlining your RMarkdown document so that it includes the following sections and steps: Data Sources read in the data Clean and Reshape data remove unnecessary columns check column typing reshape data Join to Regions dataset About the pipe (%&gt;%) operator Before we jump into learning tidyr and dplyr, we first need to explain the %&gt;%. Both the tidyr and the dplyr packages use the pipe operator - %&gt;%, which may look unfamiliar. The pipe is a powerful way to efficiently chain together operations. The pipe will take the output of a previous statement, and use it as the input to the next statement. Say you want to both filter out rows of a dataset, and select certain columns. Instead of writing df_filtered &lt;- filter(df, ...) df_selected &lt;- select(df_filtered, ...) You can write df_cleaned &lt;- df %&gt;% filter(...) %&gt;% select(...) If you think of the assignment operator (&lt;-) as reading like “gets”, then the pipe operator would read like “then.” So you might think of the above chunk being translated as: The cleaned dataframe gets the original data, and then a filter (of the original data), and then a select (of the filtered data). The benefits to using pipes are that you don’t have to keep track of (or overwrite) intermediate data frames. The drawbacks are that it can be more difficult to explain the reasoning behind each step, especially when many operations are chained together. It is good to strike a balance between writing efficient code (chaining operations), while ensuring that you are still clearly explaining, both to your future self and others, what you are doing and why you are doing it. RStudio has a keyboard shortcut for %&gt;% : Ctrl + Shift + M (Windows), Cmd + Shift + M (Mac). Selecting/removing columns: select() The first issue is the extra columns All and notesRegCode. Let’s select only the columns we want, and assign this to a variable called catch_data. catch_data &lt;- catch_original %&gt;% select(Region, Year, Chinook, Sockeye, Coho, Pink, Chum) head(catch_data) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 Much better! select also allows you to say which columns you don’t want, by passing unquoted column names preceded by minus (-) signs: catch_data &lt;- catch_original %&gt;% select(-All, -notesRegCode) head(catch_data) Quality Check Now that we have the data we are interested in using, we should do a little quality check to see that it seems as expected. One nice way of doing this is the glimpse function. dplyr::glimpse(catch_data) ## Rows: 1,708 ## Columns: 7 ## $ Region &lt;chr&gt; &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;,… ## $ Year &lt;int&gt; 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 18… ## $ Chinook &lt;chr&gt; &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;9… ## $ Sockeye &lt;int&gt; 5, 155, 224, 182, 251, 274, 207, 189, 253, 408, 989, 791, 708,… ## $ Coho &lt;int&gt; 0, 0, 16, 11, 42, 24, 11, 1, 5, 8, 192, 161, 132, 139, 84, 107… ## $ Pink &lt;int&gt; 0, 0, 0, 92, 0, 0, 8, 187, 529, 606, 996, 2218, 673, 1545, 204… ## $ Chum &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 2, 0, 0, 0, 102, 343… Challenge Notice the output of the glimpse function call. Does anything seem amiss with this dataset that might warrant fixing? Answer: The Chinook catch data are character class. Let’s fix it using the function mutate before moving on. Changing column content: mutate() We can use the mutate function to change a column, or to create a new column. First Let’s try to just convert the Chinook catch values to numeric type using the as.numeric() function, and overwrite the old Chinook column. catch_clean &lt;- catch_data %&gt;% mutate(Chinook = as.numeric(Chinook)) ## Warning: There was 1 warning in `mutate()`. ## ℹ In argument: `Chinook = as.numeric(Chinook)`. ## Caused by warning: ## ! NAs introduced by coercion head(catch_clean) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 We get a warning “NAs introduced by coercion” which is R telling us that it couldn’t convert every value to an integer and, for those values it couldn’t convert, it put an NA in its place. This is behavior we commonly experience when cleaning datasets and it’s important to have the skills to deal with it when it crops up. To investigate, let’s isolate the issue. We can find out which values are NAs with a combination of is.na() and which(), and save that to a variable called i. i &lt;- which(is.na(catch_clean$Chinook)) i ## [1] 401 It looks like there is only one problem row, lets have a look at it in the original data. catch_data[i,] ## Region Year Chinook Sockeye Coho Pink Chum ## 401 GSE 1955 I 66 0 0 1 Well that’s odd: The value in catch_thousands is I. It turns out that this dataset is from a PDF which was automatically converted into a CSV and this value of I is actually a 1. Let’s fix it by incorporating the ifelse function to our mutate call, which will change the value of the Chinook column to 1 if the value is equal to I, otherwise it will use as.numeric to turn the character representations of numbers into numeric typed values. catch_clean &lt;- catch_data %&gt;% mutate(Chinook = if_else(Chinook == &quot;I&quot;, &quot;1&quot;, Chinook)) %&gt;% mutate(Chinook = as.integer(Chinook)) head(catch_clean) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 Changing shape: pivot_longer() and pivot_wider() The next issue is that the data are in a wide format and, we want the data in a tall format instead. pivot_longer() from the tidyr package helps us do just this conversion: catch_long &lt;- catch_clean %&gt;% pivot_longer(cols = -c(Region, Year), names_to = &quot;species&quot;, values_to = &quot;catch&quot;) head(catch_long) ## # A tibble: 6 × 4 ## Region Year species catch ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 The syntax we used above for pivot_longer() might be a bit confusing so let’s walk though it. The first argument to pivot_longer is the columns over which we are pivoting. You can select these by listing either the names of the columns you do want to pivot, or the names of the columns you are not pivoting over. The names_to argument takes the name of the column that you are creating from the column names you are pivoting over. The values_to argument takes the name of the column that you are creating from the values in the columns you are pivoting over. The opposite of pivot_longer(), pivot_wider(), works in a similar declarative fashion: catch_wide &lt;- catch_long %&gt;% pivot_wider(names_from = species, values_from = catch) head(catch_wide) ## # A tibble: 6 × 7 ## Region Year Chinook Sockeye Coho Pink Chum ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 Renaming columns with rename() If you scan through the data, you may notice the values in the catch column are very small (these are supposed to be annual catches). If we look at the metadata we can see that the catch column is in thousands of fish so let’s convert it before moving on. Let’s first rename the catch column to be called catch_thousands: catch_long &lt;- catch_long %&gt;% rename(catch_thousands = catch) head(catch_long) ## # A tibble: 6 × 4 ## Region Year species catch_thousands ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 Aside Many people use the base R function names to rename columns, often in combination with column indexing that relies on columns being in a particular order. Column indexing is often also used to select columns instead of the select function from dplyr. Although these methods both work just fine, they do have one major drawback: in most implementations they rely on you knowing exactly the column order your data is in. To illustrate why your knowledge of column order isn’t reliable enough for these operations, considering the following scenario: Your colleague emails you letting you know that she has an updated version of the conductivity-temperature-depth data from this year’s research cruise, and sends it along. Excited, you re-run your scripts that use this data for your phytoplankton research. You run the script and suddenly all of your numbers seem off. You spend hours trying to figure out what is going on. Unbeknownst to you, your colleagues bought a new sensor this year that measures dissolved oxygen. Because of the new variables in the dataset, the column order is different. Your script which previously renamed the 4th column, SAL_PSU to salinity now renames the 4th column, O2_MGpL to salinity. No wonder your results looked so weird, good thing you caught it! If you had written your code so that it doesn’t rely on column order, but instead renames columns using the rename function, the code would have run just fine (assuming the name of the original salinity column didn’t change, in which case the code would have errored in an obvious way). This is an example of a defensive coding strategy, where you try to anticipate issues before they arise, and write your code in such a way as to keep the issues from happening. Adding columns: mutate() Now let’s use mutate again to create a new column called catch with units of fish (instead of thousands of fish). catch_long &lt;- catch_long %&gt;% mutate(catch = catch_thousands * 1000) head(catch_long) Now let’s remove the catch_thousands column for now since we don’t need it. Note that here we have added to the expression we wrote above by adding another function call (mutate) to our expression. This takes advantage of the pipe operator by grouping together a similar set of statements, which all aim to clean up the catch_long data.frame. catch_long &lt;- catch_long %&gt;% mutate(catch = catch_thousands * 1000) %&gt;% select(-catch_thousands) head(catch_long) ## # A tibble: 6 × 4 ## Region Year species catch ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5000 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 We’re now ready to start analyzing the data. group_by and summarise As I outlined in the Introduction, dplyr lets us employ the Split-Apply-Combine strategy and this is exemplified through the use of the group_by() and summarise() functions: mean_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarise(catch_mean = mean(catch)) head(mean_region) ## # A tibble: 6 × 2 ## Region catch_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 ALU 40384. ## 2 BER 16373. ## 3 BRB 2709796. ## 4 CHG 315487. ## 5 CKI 683571. ## 6 COP 179223. Another common use of group_by() followed by summarize() is to count the number of rows in each group. We have to use a special function from dplyr, n(). n_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarize(n = n()) head(n_region) ## # A tibble: 6 × 2 ## Region n ## &lt;chr&gt; &lt;int&gt; ## 1 ALU 435 ## 2 BER 510 ## 3 BRB 570 ## 4 CHG 550 ## 5 CKI 525 ## 6 COP 470 Aside If you are finding that you are reaching for this combination of group_by(), summarise() and n() a lot, there is a helpful dplyr function count() that accomplishes this in one function! Challenge Find another grouping and statistic to calculate for each group. Find out if you can group by multiple variables. Filtering rows: filter() filter() is the verb we use to filter our data.frame to rows matching some condition. It’s similar to subset() from base R. Let’s go back to our original data.frame and do some filter()ing: SSE_catch &lt;- catch_long %&gt;% filter(Region == &quot;SSE&quot;) head(SSE_catch) ## # A tibble: 6 × 4 ## Region Year species catch ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5000 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 Challenge Filter to just catches of over one million fish. Filter to just Chinook from the SSE region Sorting your data: arrange() arrange() is how we sort the rows of a data.frame. In my experience, I use arrange() in two common cases: When I want to calculate a cumulative sum (with cumsum()) so row order matters When I want to display a table (like in an .Rmd document) in sorted order Let’s re-calculate mean catch by region, and then arrange() the output by mean catch: mean_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarise(mean_catch = mean(catch)) %&gt;% arrange(mean_catch) head(mean_region) ## # A tibble: 6 × 2 ## Region mean_catch ## &lt;chr&gt; &lt;dbl&gt; ## 1 BER 16373. ## 2 KTZ 18836. ## 3 ALU 40384. ## 4 NRS 51503. ## 5 KSK 67642. ## 6 YUK 68646. The default sorting order of arrange() is to sort in ascending order. To reverse the sort order, wrap the column name inside the desc() function: mean_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarise(mean_catch = mean(catch)) %&gt;% arrange(desc(mean_catch)) head(mean_region) ## # A tibble: 6 × 2 ## Region mean_catch ## &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 3184661. ## 2 BRB 2709796. ## 3 NSE 1825021. ## 4 KOD 1528350 ## 5 PWS 1419237. ## 6 SOP 1110942. 8.1.4 Joins in dplyr So now that we’re awesome at manipulating a single data.frame, where do we go from here? Manipulating more than one data.frame. If you’ve ever used a database, you may have heard of or used what’s called a “join”, which allows us to to intelligently merge two tables together into a single table based upon a shared column between the two. We’ve already covered joins in Data Modeling &amp; Tidy Data so let’s see how it’s done with dplyr. The dataset we’re working with, https://knb.ecoinformatics.org/#view/df35b.304.2, contains a second CSV which has the definition of each Region code. This is a really common way of storing auxiliary information about our dataset of interest (catch) but, for analylitcal purposes, we often want them in the same data.frame. Joins let us do that easily. Let’s look at a preview of what our join will do by looking at a simplified version of our data: Visualisation of our left_join First, let’s read in the region definitions data table and select only the columns we want. Note that I have piped my read.csv result into a select call, creating a tidy chunk that reads and selects the data that we need. region_defs &lt;- read.csv(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1&quot;) %&gt;% select(code, mgmtArea) head(region_defs) ## code mgmtArea ## 1 GSE Unallocated Southeast Alaska ## 2 NSE Northern Southeast Alaska ## 3 SSE Southern Southeast Alaska ## 4 YAK Yakutat ## 5 PWSmgmt Prince William Sound Management Area ## 6 BER Bering River Subarea Copper River Subarea If you examine the region_defs data.frame, you’ll see that the column names don’t exactly match the image above. If the names of the key columns are not the same, you can explicitly specify which are the key columns in the left and right side as shown below: catch_joined &lt;- left_join(catch_long, region_defs, by = c(&quot;Region&quot; = &quot;code&quot;)) head(catch_joined) ## # A tibble: 6 × 5 ## Region Year species catch mgmtArea ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 SSE 1886 Chinook 0 Southern Southeast Alaska ## 2 SSE 1886 Sockeye 5000 Southern Southeast Alaska ## 3 SSE 1886 Coho 0 Southern Southeast Alaska ## 4 SSE 1886 Pink 0 Southern Southeast Alaska ## 5 SSE 1886 Chum 0 Southern Southeast Alaska ## 6 SSE 1887 Chinook 0 Southern Southeast Alaska Notice that I have deviated from our usual pipe syntax (although it does work here!) because I prefer to see the data.frames that I am joining side by side in the syntax. Another way you can do this join is to use rename to change the column name code to Region in the region_defs data.frame, and run the left_join this way: region_defs &lt;- region_defs %&gt;% rename(Region = code, Region_Name = mgmtArea) catch_joined &lt;- left_join(catch_long, region_defs, by = c(&quot;Region&quot;)) head(catch_joined) Now our catches have the auxiliary information from the region definitions file alongside them. Note: dplyr provides a complete set of joins: inner, left, right, full, semi, anti, not just left_join. separate() and unite() separate() and its complement, unite() allow us to easily split a single column into numerous (or numerous into a single). This can come in really handle when we need to split a column into two pieces by a consistent separator (like a dash). Let’s make a new data.frame with fake data to illustrate this. Here we have a set of site identification codes. with information about the island where the site is (the first 3 letters) and a site number (the 3 numbers). If we want to group and summarize by island, we need a column with just the island information. sites_df &lt;- data.frame(site = c(&quot;HAW-101&quot;, &quot;HAW-103&quot;, &quot;OAH-320&quot;, &quot;OAH-219&quot;, &quot;MAI-039&quot;)) sites_df %&gt;% separate(site, c(&quot;island&quot;, &quot;site_number&quot;), &quot;-&quot;) ## island site_number ## 1 HAW 101 ## 2 HAW 103 ## 3 OAH 320 ## 4 OAH 219 ## 5 MAI 039 Exercise: Split the city column in the following data.frame into city and state_code columns: cities_df &lt;- data.frame(city = c(&quot;Juneau AK&quot;, &quot;Sitka AK&quot;, &quot;Anchorage AK&quot;)) # Write your solution here unite() does just the reverse of separate(). If we have a data.frame that contains columns for year, month, and day, we might want to unite these into a single date column. dates_df &lt;- data.frame(year = c(&quot;1930&quot;, &quot;1930&quot;, &quot;1930&quot;), month = c(&quot;12&quot;, &quot;12&quot;, &quot;12&quot;), day = c(&quot;14&quot;, &quot;15&quot;, &quot;16&quot;)) dates_df %&gt;% unite(date, year, month, day, sep = &quot;-&quot;) ## date ## 1 1930-12-14 ## 2 1930-12-15 ## 3 1930-12-16 Exercise: Use unite() on your solution above to combine the cities_df back to its original form with just one column, city: # Write your solution here Summary We just ran through the various things we can do with dplyr and tidyr but if you’re wondering how this might look in a real analysis. Let’s look at that now: catch_original &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1&quot;, method = &quot;libcurl&quot;)) region_defs &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1&quot;, method = &quot;libcurl&quot;)) %&gt;% select(code, mgmtArea) mean_region &lt;- catch_original %&gt;% select(-All, -notesRegCode) %&gt;% mutate(Chinook = ifelse(Chinook == &quot;I&quot;, 1, Chinook)) %&gt;% mutate(Chinook = as.numeric(Chinook)) %&gt;% pivot_longer(-c(Region, Year), names_to = &quot;species&quot;, values_to = &quot;catch&quot;) %&gt;% mutate(catch = catch*1000) %&gt;% group_by(Region) %&gt;% summarize(mean_catch = mean(catch)) %&gt;% left_join(region_defs, by = c(&quot;Region&quot; = &quot;code&quot;)) head(mean_region) ## # A tibble: 6 × 3 ## Region mean_catch mgmtArea ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ALU 40384. Aleutian Islands Subarea ## 2 BER 16373. Bering River Subarea Copper River Subarea ## 3 BRB 2709796. Bristol Bay Management Area ## 4 CHG 315487. Chignik Management Area ## 5 CKI 683571. Cook Inlet Management Area ## 6 COP 179223. Copper River Subarea "],["data-visualization-and-publishing-to-the-web.html", "9 Data Visualization and Publishing to the Web 9.1 Publishing Analyses to the Web 9.2 Publication Graphics", " 9 Data Visualization and Publishing to the Web 9.1 Publishing Analyses to the Web 9.1.1 Learning Objectives In this lesson, you will learn: How to use git, GitHub (+Pages), and (R)Markdown to publish an analysis to the web 9.1.2 Introduction Sharing your work with others in engaging ways is an important part of the scientific process. So far in this course, we’ve introduced a small set of powerful tools for doing open science: R and its many packages RStudio git GiHub RMarkdown RMarkdown, in particular, is amazingly powerful for creating scientific reports but, so far, we haven’t tapped its full potential for sharing our work with others. In this lesson, we’re going to take an existing GitHub repository and turn it into a beautiful and easy to read web page using the tools listed above. 9.1.3 A Minimal Example Use your existing training_username repository Add a new file at the top level called index.Rmd. The easiest way to do this is through the RStudio menu. Choose File -&gt; New File -&gt; RMarkdown… This will bring up a dialog box. You should create a “Document” in “HTML” format. These are the default options. Be sure to use the exact capitalization (lower case ‘index’) as different operating systems handle capitalization differently and it can interfere with loading your web page later. Open index.Rmd (if it isn’t already open) Press Knit Observe the rendered output Notice the new file in the same directory index.html. This is our RMarkdown file rendered as HTML (a web page) Commit your changes (to both index.Rmd and index.html) and push to GitHub Open your web browser to the GitHub.com page for your repository Go to Settings &gt; GitHub Pages and turn on GitHub Pages for the main branch Now, the rendered website version of your repo will show up at a special URL. GitHub Pages follows a convention like this: github pages url pattern Note that it will no longer be at github.com but github.io Go to https://{username}.github.io/{repo_name}/ (Note the trailing /) Observe the awesome rendered output Now that we’ve successfully published a web page from an RMarkdown document, let’s make a change to our RMarkdown document and follow the steps to actually publish the change on the web: Go back to our index.Rmd Delete all the content, except the YAML frontmatter Type “Hello world” Commit, push Go back to https://{username}.github.io/{repo_name}/ 9.1.4 Exercise: Sharing your work RMarkdown web pages are a great way to share work in progress with your colleagues. To do so simply requires thinking through your presentation so that it highlights the workflow to be reviewed. You can also include multiple pages and build a simple web site for walking through your work that is accessible to people who aren’t all set up to open your content in R. In this exercise, we’ll publish another RMarkdown page, and create a table of contents on the main page to guide people to the main page. First, in your training repository, create a new RMarkdown file that describes some piece of your work and note the name. I’ll use an RMarkdown named data-cleaning.Rmd. Once you have an RMarkdown created, Knit the document which will create the HTML version of the file, which in this case will be named data-cleaning.html. Now, return to editing your index.Rmd file from the beginning of this lesson. The index file represents the ‘default’ file for a web site, and is returned whenever you visit the web site but don’t specify an explicit file to be returned. Let’s modify the index page, adding a bulleted list, and in that list, include a link to the new markdown page that we created: ## Analysis plan - [Data Cleaning](data-cleaning.html) - Data Interpolation and Gap filling - Linear models - Animal movement models based on telemetry - Data visualization Commit and push the web page to GitHub. Now when you visit your web site, you’ll see the table of contents, and can navigate to the new data cleaning page. 9.2 Publication Graphics 9.2.1 Learning Objectives In this lesson, you will learn: The basics of the ggplot2 package to create static plots How to use ggplot2’s theming abilities to create publication-grade graphics The basics of the leaflet package to create interactive maps 9.2.2 Overview ggplot2 is a popular package for visualizing data in R. From the home page: ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. It’s been around for years and has pretty good documentation and tons of example code around the web (like on StackOverflow). This lesson will introduce you to the basic components of working with ggplot2. ggplot vs base vs lattice vs XYZ… R provides many ways to get your data into a plot. Three common ones are, “base graphics” (plot(), hist(), etc`) lattice ggplot2 All of them work! I use base graphics for simple, quick and dirty plots. I use ggplot2 for most everything else. ggplot2 excels at making complicated plots easy and easy plots simple enough. Setup Open a new RMarkdown document and remove the filler text. First, let’s load the packages we’ll need: library(leaflet) library(dplyr) library(tidyr) library(ggplot2) library(DT) library(scales) # install.packages(&quot;scales&quot;) Load the data table directly from the KNB Data Repository, if it isn’t already present on your local computer. This technique only downloads the file if you need it. data_url &lt;- &quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Af119a05b-bbe7-4aea-93c6-85434dcb1c5e&quot; esc &lt;- tryCatch( read.csv(&quot;data/escapement.csv&quot;), error=function(cond) { message(paste(&quot;Escapement file does not seem to exist, so get it from the KNB.&quot;)) esc &lt;- read.csv(url(data_url, method = &quot;libcurl&quot;)) return(esc) } ) head(esc) Challenge Now that we have the data loaded, use your dplyr and tidyr skills to calculate annual escapement by species and region. Hint: try to use separate to extract the year, month, and day from the date column. Here is the solution: annual_esc &lt;- esc %&gt;% separate(sampleDate, c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;), sep = &quot;-&quot;) %&gt;% mutate(Year = as.numeric(Year)) %&gt;% group_by(Species, SASAP.Region, Year) %&gt;% summarize(escapement = sum(DailyCount)) %&gt;% filter(Species %in% c(&quot;Chinook&quot;, &quot;Sockeye&quot;, &quot;Chum&quot;, &quot;Coho&quot;, &quot;Pink&quot;)) ## `summarise()` has grouped output by &#39;Species&#39;, &#39;SASAP.Region&#39;. You can override ## using the `.groups` argument. head(annual_esc) ## # A tibble: 6 × 4 ## # Groups: Species, SASAP.Region [1] ## Species SASAP.Region Year escapement ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Chinook Alaska Peninsula and Aleutian Islands 1974 1092 ## 2 Chinook Alaska Peninsula and Aleutian Islands 1975 1917 ## 3 Chinook Alaska Peninsula and Aleutian Islands 1976 3045 ## 4 Chinook Alaska Peninsula and Aleutian Islands 1977 4844 ## 5 Chinook Alaska Peninsula and Aleutian Islands 1978 3901 ## 6 Chinook Alaska Peninsula and Aleutian Islands 1979 10463 That command used a lot of the dplyr commands that we’ve used, and some that are new. The separate function is used to divide the sampleDate column up into Year, Month, and Day columns, and then we use group_by to indicate that we want to calculate our results for the unique combinations of species, region, and year. We next use summarize to calculate an escapement value for each of these groups. Finally, we use a filter and the %in% operator to select only the salmon species. 9.2.3 Static figures using ggplot2 Every graphic you make in ggplot2 will have at least one aesthetic and at least one geom (layer). The aesthetic maps your data to your geometry (layer). Your geometry specifies the type of plot we’re making (point, bar, etc.). Now, let’s plot our results using ggplot. ggplot uses a mapping aesthetic (set using aes()) and a geometry to create your plot. Additional geometries/aesthetics and theme elements can be added to a ggplot object using +. ggplot(annual_esc, aes(x = Species, y = escapement)) + geom_col() What if we want our bars to be blue instad of gray? You might think we could run this: ggplot(annual_esc, aes(x = Species, y = escapement, fill = &quot;blue&quot;)) + geom_col() Why did that happen? Notice that we tried to set the fill color of the plot inside the mapping aesthetic call. What we have done, behind the scenes, is create a column filled with the word “blue” in our dataframe, and then mapped it to the fill aesthetic, which then chose the default fill color of red. What we really wanted to do was just change the color of the bars. If we want do do that, we can call the color option in the geom_bar function, outside of the mapping aesthetics function call. ggplot(annual_esc, aes(x = Species, y = escapement)) + geom_col(fill = &quot;blue&quot;) What if we did want to map the color of the bars to a variable, such as region. ggplot is really powerful because we can easily get this plot to visualize more aspects of our data. ggplot(annual_esc, aes(x = Species, y = escapement, fill = SASAP.Region)) + geom_col() Aside ggplot2 and the pipe operator Just like in dplyr and tidyr, we can also pipe a data.frame directly into the first argument of the ggplot function using the %&gt;% operator. Let’s look at an example using a different geometry. Here, we use the pipe operator to pass in a filtered version of annual_esc, and make a line plot with points at each observation. annual_esc %&gt;% filter(SASAP.Region == &quot;Kodiak&quot;) %&gt;% ggplot(aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() This can certainly be convenient, especially for cases like the above, but use it carefully! Combining too many data-tidying or subsetting operations with your ggplot call can make your code more difficult to debug and understand. Setting ggplot themes Now let’s work on making this plot look a bit nicer. Add a title using ggtitle(), adjust labels using ylab(), and include a built in theme using theme_bw(). There are a wide variety of built in themes in ggplot that help quickly set the look of the plot. Use the RStudio autocomplete theme_ &lt;TAB&gt; to view a list of theme functions. For clarity in the next section, I’ll save the filtered version of the annual escapement data.frame to it’s own object. kodiak_esc &lt;- annual_esc %&gt;% filter(SASAP.Region == &quot;Kodiak&quot;) ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + ylab(&quot;Escapement&quot;) + ggtitle(&quot;Kodiak Salmon Escapement&quot;) + theme_bw() You can see that the theme_bw() function changed a lot of the aspects of our plot! The background is white, the grid is a different color, etc. There are lots of other built in themes like this that come with the ggplot2 package. Challenge Use the RStudio autocomplete, the ggplot2 documentation, a cheatsheet, or good old google to find other built in themes. Pick out your favorite one and add it to your plot. The built in theme functions change the default settings for many elements that can also be changed invididually using thetheme() function. The theme() function is a way to further fine-tune the look of your plot. This function takes MANY arguments (just have a look at ?theme). Luckily there are many great ggplot resources online so we don’t have to remember all of these, just google “ggplot cheatsheet” and find one you like. Let’s look at an example of a theme call, where we change the position of our plot above from the right side to the bottom, and remove the title from the legend. ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + ylab(&quot;Escapement&quot;) + ggtitle(&quot;Kodiak Salmon Escapement&quot;) + theme_bw() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) Note that the theme() call needs to come after any built in themes like theme_bw() are used. Otherwise, theme_bw() will likely override any theme elements that you changed using theme(). You can also save the result of a series of theme() function calls to an object to use on multiple plots. This prevents needing to copy paste the same lines over and over again! my_theme &lt;- theme_bw() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + ylab(&quot;Escapement&quot;) + ggtitle(&quot;Kodiak Salmon Escapement&quot;) + my_theme Challenge Using whatever method you like, figure out how to rotate the x-axis tick labels to a 45 degree angle. Smarter tick labels using scales Fixing tick labels in ggplot can be super annoying. The y-axis labels in the plot above don’t look great. We could manually fix them, but it would likely be tedious and error prone. The scales package provides some nice helper functions to easily rescale and relabel your plots. Here, we use scale_y_continuous from ggplot2, with the argument labels, which is assigned to the function name comma, from the scales package. This will format all of the labels on the y-axis of our plot with comma-formatted numbers. ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + scale_y_continuous(labels = comma) + ylab(&quot;Escapement&quot;) + ggtitle(&quot;Kodiak Salmon Escapement&quot;) + my_theme 9.2.3.1 Saving plots Saving plots using ggplot is easy! The ggsave function will save either the last plot you created, or any plot that you have saved to a variable. You can specify what output format you want, size, resolution, etc. ggsave(&quot;kodiak_esc.png&quot;, width = 3, height = 3, units = &quot;in&quot;) Creating multiple plots What if we wanted to generate a plot for every region? A fast way to do this uses the function facet_wrap(). This function takes a mapping to a variable using the syntax ~variable_name. The ~ (tilde) is a model operator which tells facet_wrap to model each unique value within variable_name to a facet in the plot. The default behaviour of facet wrap is to put all facets on the same x and y scale. You can use the scales argument to specify whether to allow different scales between facet plots. Here, we free the y scale. You can also specify the number of columns using the n_col argument. ggplot(annual_esc, aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + scale_y_continuous(labels = comma) + facet_wrap(~SASAP.Region, scales = &quot;free_y&quot;, ncol = 2) + ylab(&quot;Escapement&quot;) + my_theme 9.2.4 Interactive visualization using leaflet and DT Tables Now that we know how to make great static visualizations, lets introduce two other packages that allow us to display our data in interactive ways. These packages really shine when used with GitHub pages, so at the end of this lesson we will publish our figures to the website created earlier in the week during this lesson. First let’s show an interactive table of unique sampling locations using DT. Write a data.frame containing unique sampling locations with no missing values using two new functions from dplyr and tidyr: distinct() and drop_na(). locations &lt;- esc %&gt;% distinct(Location, Latitude, Longitude) %&gt;% drop_na() And display it as an interactive table using datatable() from the DT package. datatable(locations) Maps Similar to ggplot2, you can make a basic leaflet map using just a couple lines of code. Note that unlike ggplot2, the leaflet package uses pipe operators (%&gt;%) and not the additive operator (+). The addTiles() function without arguments will add base tiles to your map from OpenStreetMap. addMarkers() will add a marker at each location specified by the latitude and longitude arguments. Note that the ~ symbol is used here to model the coordinates to the map (similar to facet_wrap in ggplot). leaflet(locations) %&gt;% addTiles() %&gt;% addMarkers(lng = ~Longitude, lat = ~Latitude, popup = ~ Location) You can also use leaflet to import Web Map Service (WMS) tiles. Here is an example that utilizes the General Bathymetric Map of the Oceans (GEBCO) WMS tiles. In this example, we also demonstrate how to create a more simple circle marker, the look of which is explicitly set using a series of style-related arguments.. leaflet(locations) %&gt;% addWMSTiles(&quot;https://www.gebco.net/data_and_products/gebco_web_services/web_map_service/mapserv?&quot;, layers = &#39;GEBCO_LATEST&#39;, attribution = &quot;Imagery reproduced from the GEBCO_2014 Grid, version 20150318, www.gebco.net&quot;) %&gt;% addCircleMarkers(lng = ~Longitude, lat = ~Latitude, popup = ~ Location, radius = 5, # set fill properties fillColor = &quot;salmon&quot;, fillOpacity = 1, # set stroke properties stroke = T, weight = 0.5, color = &quot;white&quot;, opacity = 1) Leaflet has a ton of functionality that can enable you to create some beautiful, functional maps with relative ease. Here is an example of some we created as part of the SASAP project, created using the same tools we showed you here. This map hopefully gives you an idea of how powerful the combination of RMarkdown and GitHub pages can be. 9.2.5 Resources Lisa Charlotte Rost. (2018) Why not to use two axes, and what to use instead: The case against dual axis charts "],["functions-and-packages.html", "10 Functions and Packages 10.1 Creating R Functions 10.2 Creating R Packages", " 10 Functions and Packages 10.1 Creating R Functions Many people write R code as a single, continuous stream of commands, often drawn from the R Console itself and simply pasted into a script. While any script brings benefits over non-scripted solutions, there are advantages to breaking code into small, reusable modules. This is the role of a function in R. In this lesson, we will review the advantages of coding with functions, practice by creating some functions and show how to call them, and then do some exercises to build other simple functions. 10.1.0.1 Learning outcomes Learn why we should write code in small functions Write code for one or more functions Document functions to improve understanding and code communication 10.1.1 Why functions? In a word: DRY: Don’t Repeat Yourself By creating small functions that only one logical task and do it well, we quickly gain: Improved understanding Reuse via decomposing tasks into bite-sized chunks Improved error testing Temperature conversion Imagine you have a bunch of data measured in Fahrenheit and you want to convert that for analytical purposes to Celsius. You might have an R script that does this for you. airtemps &lt;- c(212, 30.3, 78, 32) celsius1 &lt;- (airtemps[1]-32)*5/9 celsius2 &lt;- (airtemps[2]-32)*5/9 celsius3 &lt;- (airtemps[3]-32)*5/9 Note the duplicated code, where the same formula is repeated three times. This code would be both more compact and more reliable if we didn’t repeat ourselves. Creating a function Functions in R are a mechanism to process some input and return a value. Similarly to other variables, functions can be assigned to a variable so that they can be used throughout code by reference. To create a function in R, you use the function function (so meta!) and assign its result to a variable. Let’s create a function that calculates celsius temperature outputs from fahrenheit temperature inputs. fahr_to_celsius &lt;- function(fahr) { celsius &lt;- (fahr-32)*5/9 return(celsius) } By running this code, we have created a function and stored it in R’s global environment. The fahr argument to the function function indicates that the function we are creating takes a single parameter (the temperature in fahrenheit), and the return statement indicates that the function should return the value in the celsius variable that was calculated inside the function. Let’s use it, and check if we got the same value as before: celsius4 &lt;- fahr_to_celsius(airtemps[1]) celsius4 ## [1] 100 celsius1 == celsius4 ## [1] TRUE Excellent. So now we have a conversion function we can use. Note that, because most operations in R can take multiple types as inputs, we can also pass the original vector of airtemps, and calculate all of the results at once: celsius &lt;- fahr_to_celsius(airtemps) celsius ## [1] 100.0000000 -0.9444444 25.5555556 0.0000000 This takes a vector of temperatures in fahrenheit, and returns a vector of temperatures in celsius. Challenge Now, create a function named celsius_to_fahr that does the reverse, it takes temperature data in celsius as input, and returns the data converted to fahrenheit. Then use that formula to convert the celsius vector back into a vector of fahrenheit values, and compare it to the original airtemps vector to ensure that your answers are correct. Hint: the formula for C to F conversions is celsius*9/5 + 32. # Your code goes here Did you encounter any issues with rounding or precision? 10.1.2 Documenting R functions Functions need documentation so that we can communicate what they do, and why. The roxygen2 package provides a simple means to document your functions so that you can explain what the function does, the assumptions about the input values, a description of the value that is returned, and the rationale for decisions made about implementation. Documentation in ROxygen is placed immediately before the function definition, and is indicated by a special comment line that always starts with the characters #'. Here’s a documented version of a function: #&#39; Convert temperature data from Fahrenheit to Celsius #&#39; #&#39; @param fahr Temperature data in degrees Fahrenheit to be converted #&#39; @return temperature value in degrees Celsius #&#39; @keywords conversion #&#39; @export #&#39; @examples #&#39; fahr_to_celsius(32) #&#39; fahr_to_celsius(c(32, 212, 72)) fahr_to_celsius &lt;- function(fahr) { celsius &lt;- (fahr-32)*5/9 return(celsius) } Note the use of the @param keyword to define the expectations of input data, and the @return keyword for defining the value that is returned from the function. The @examples function is useful as a reminder as to how to use the function. Finally, the @export keyword indicates that, if this function were added to a package, then the function should be available to other code and packages to utilize. 10.1.3 Summary Functions are useful to reduce redundancy, reuse code, and reduce errors Build functions with the function function Document functions with roxygen2 comments Spoiler – the exercise answered Don’t peek until you write your own… # Your code goes here celsius_to_fahr &lt;- function(celsius) { fahr &lt;- celsius*9/5 + 32 return(fahr) } result &lt;- celsius_to_fahr(celsius) airtemps == result ## [1] TRUE TRUE TRUE TRUE 10.1.4 Examples: Minimizing work with functions Functions can of course be as simple or complex as needed. They can be be very effective in repeatedly performing calculations, or for bundling a group of commands that are used on many different input data sources. For example, we might create a simple function that takes fahrenheit temperatures as input, and calculates both celsius and Kelvin temperatures. All three values are then returned in a list, making it very easy to create a comparison table among the three scales. convert_temps &lt;- function(fahr) { celsius &lt;- (fahr-32)*5/9 kelvin &lt;- celsius + 273.15 return(list(fahr=fahr, celsius=celsius, kelvin=kelvin)) } temps_df &lt;- data.frame(convert_temps(seq(-100,100,10))) Once we have a dataset like that, we might want to plot it. One thing that we do repeatedly is set a consistent set of display elements for creating graphs and plots. By using a function to create a custom ggplot theme, we can enable to keep key parts of the formatting flexible. FOr example, in the custom_theme function, we provide a base_size argument that defaults to using a font size of 9 points. Because it has a default set, it can safely be omitted. But if it is provided, then that value is used to set the base font size for the plot. custom_theme &lt;- function(base_size = 9) { ggplot2::theme( axis.ticks = ggplot2::element_blank(), text = ggplot2::element_text(family = &#39;Helvetica&#39;, color = &#39;gray30&#39;, size = base_size), plot.title = ggplot2::element_text(size = ggplot2::rel(1.25), hjust = 0.5, face = &#39;bold&#39;), panel.background = ggplot2::element_blank(), legend.position = &#39;right&#39;, panel.border = ggplot2::element_blank(), panel.grid.minor = ggplot2::element_blank(), panel.grid.major = ggplot2::element_line(colour = &#39;grey90&#39;, size = .25), legend.key = ggplot2::element_rect(colour = NA, fill = NA), axis.line = ggplot2::element_blank() ) } library(ggplot2) ggplot(temps_df, mapping=aes(x=fahr, y=celsius, color=kelvin)) + geom_point() + custom_theme(10) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. In this case, we set the font size to 10, and plotted the air temperatures. The custom_theme function can be used anywhere that one needs to consistently format a plot. But we can go further. One can wrap the entire call to ggplot in a function, enabling one to create many plots of the same type with a consistent structure. For example, we can create a scatterplot function that takes a data frame as input, along with a point_size for the points on the plot, and a font_size for the text. scatterplot &lt;- function(df, point_size = 2, font_size=9) { ggplot(df, mapping=aes(x=fahr, y=celsius, color=kelvin)) + geom_point(size=point_size) + custom_theme(font_size) } Calling that let’s us, in a single line of code, create a highly customized plot but maintain flexibiity via the arguments passed in to the function. Let’s set the point size to 3 and font to 16 to make the plot more legible. scatterplot(temps_df, point_size=3, font_size = 16) Once these functions are set up, all of the plots built with them can be reformatted by changing the settings in just the functions, whether they were used to create 1, 10, or 100 plots. 10.2 Creating R Packages 10.2.1 Learning Objectives In this lesson, you will learn: The advantages of using R packages for organizing code Simple techniques for creating R packages Approaches to documenting code in packages 10.2.2 Why packages? Most R users are familiar with loading and utilizing packages in their work. And they know how rich CRAN is in providing for many conceivable needs. Most people have never created a package for their own work, and most think the process is too complicated. Really it’s pretty straighforward and super useful in your personal work. Creating packages serves two main use cases: Mechanism to redistribute reusable code (even if just for yourself) Mechanism to reproducibly document analysis and models and their results Even if you don’t plan on writing a package with such broad appeal such as, say, ggplot2 or dplyr, you still might consider creating a package to contain: Useful utility functions you write i.e. a Personal Package. Having a place to put these functions makes it much easier to find and use them later. A set of shared routines for your lab or research group, making it easier to remain consistent within your team and also to save time. The analysis accompanying a thesis or manuscript, making it all that much easier for others to reproduce your results. The usethis, devtools and roxygen2 packages make creating and maintining a package to be a straightforward experience. 10.2.3 Install and load packages library(devtools) library(usethis) library(roxygen2) 10.2.4 Create a basic package Thanks to the great usethis package, it only takes one function call to create the skeleton of an R package using create_package(). Which eliminates pretty much all reasons for procrastination. To create a package called mytools, all you do is: setwd(&#39;..&#39;) create_package(&quot;mytools&quot;) ✔ Setting active project to &#39;/Users/jones/development/mytools&#39; ✔ Creating &#39;R/&#39; ✔ Creating &#39;man/&#39; ✔ Writing &#39;DESCRIPTION&#39; ✔ Writing &#39;NAMESPACE&#39; ✔ Writing &#39;mytools.Rproj&#39; ✔ Adding &#39;.Rproj.user&#39; to &#39;.gitignore&#39; ✔ Adding &#39;^mytools\\\\.Rproj$&#39;, &#39;^\\\\.Rproj\\\\.user$&#39; to &#39;.Rbuildignore&#39; ✔ Opening new project &#39;mytools&#39; in RStudio Note that this will open a new project (mytools) and a new session in RStudio server. The create_package function created a top-level directory structure, including a number of critical files under the standard R package structure. The most important of which is the DESCRIPTION file, which provides metadata about your package. Edit the DESCRIPTION file to provide reasonable values for each of the fields, including your own contact information. Information about choosing a LICENSE is provided in the Extending R documentation. The DESCRIPTION file expects the license to be chose from a predefined list, but you can use it’s various utility methods for setting a specific license file, such as the Apacxhe 2 license: usethis::use_apache_license() ✔ Setting License field in DESCRIPTION to &#39;Apache License (&gt;= 2.0)&#39; ✔ Writing &#39;LICENSE.md&#39; ✔ Adding &#39;^LICENSE\\\\.md$&#39; to &#39;.Rbuildignore&#39; Once your license has been chosen, and you’ve edited your DESCRIPTION file with your contact information, a title, and a description, it will look like this: Package: mytools Title: Utility Functions Created by Matt Jones Version: 0.1 Authors@R: &quot;Matthew Jones &lt;jones@nceas.ucsb.edu&gt; [aut, cre]&quot; Description: Package mytools contains a suite of utility functions useful whenever I need stuff to get done. Depends: R (&gt;= 3.5.0) License: Apache License (&gt;= 2.0) LazyData: true 10.2.5 Add your code The skeleton package created contains a directory R which should contain your source files. Add your functions and classes in files to this directory, attempting to choose names that don’t conflict with existing packages. For example, you might add a file cutsom_theme that contains a function custom_theme() that you might want to reuse. The usethis::use_r() function will help set up you files in the right places. For example, running: usethis::use_r(&quot;custom_theme&quot;) ● Modify &#39;R/custom_theme&#39; creates the file R/custom_theme, which you can then modify to add the implementation fo the following function from the functions lesson: custom_theme &lt;- function(base_size = 9) { ggplot2::theme( axis.ticks = ggplot2::element_blank(), text = ggplot2::element_text(family = &#39;Helvetica&#39;, color = &#39;gray30&#39;, size = base_size), plot.title = ggplot2::element_text(size = ggplot2::rel(1.25), hjust = 0.5, face = &#39;bold&#39;), panel.background = ggplot2::element_blank(), legend.position = &#39;right&#39;, panel.border = ggplot2::element_blank(), panel.grid.minor = ggplot2::element_blank(), panel.grid.major = ggplot2::element_line(colour = &#39;grey90&#39;, size = .25), legend.key = ggplot2::element_rect(colour = NA, fill = NA), axis.line = ggplot2::element_blank() ) } If your R code depends on functions from another package, then you must declare so in the Imports list in the DESCRIPTION file for your package. In our example above, we depend on the ggplot2 package, and so we need to list it as a dependency. Once again, usethis provides a handy helper method: usethis::use_package(&quot;ggplot2&quot;) ✔ Adding &#39;ggplot2&#39; to Imports field in DESCRIPTION ● Refer to functions with `devtools::fun()` 10.2.6 Add documentation You should provide documentation for each of your functions and classes. This is done in the roxygen2 approach of providing embedded comments in the source code files, which are in turn converted into manual pages and other R documentation artifacts. Be sure to define the overall purpose of the function, and each of its parameters. #&#39; A function set a custom ggplot theme. #&#39; #&#39; This function sets ggplot theme elements that I like, with the ability to change #&#39; the base size of the text. #&#39; #&#39; @param base_size Base size of plot text #&#39; #&#39; @keywords plotting #&#39; #&#39; @export #&#39; #&#39; @examples #&#39; library(ggplot2) #&#39; #&#39; ggplot(iris, aes(Sepal.Length, Sepal.Width)) + #&#39; geom_point() + #&#39; custom_theme(base_size = 10) #&#39; custom_theme &lt;- function(base_size = 9) { ggplot2::theme( axis.ticks = ggplot2::element_blank(), text = ggplot2::element_text(family = &#39;Helvetica&#39;, color = &#39;gray30&#39;, size = base_size), plot.title = ggplot2::element_text(size = ggplot2::rel(1.25), hjust = 0.5, face = &#39;bold&#39;), panel.background = ggplot2::element_blank(), legend.position = &#39;right&#39;, panel.border = ggplot2::element_blank(), panel.grid.minor = ggplot2::element_blank(), panel.grid.major = ggplot2::element_line(colour = &#39;grey90&#39;, size = .25), legend.key = ggplot2::element_rect(colour = NA, fill = NA), axis.line = ggplot2::element_blank() ) } Once your files are documented, you can then process the documentation using the document() function to generate the appropriate .Rd files that your package needs. devtools::document() Updating mytools documentation Updating roxygen version in /Users/jones/development/mytools/DESCRIPTION Writing NAMESPACE Loading mytools Writing NAMESPACE Writing custom_theme.Rd That’s really it. You now have a package that you can check() and install() and release(). See below for these helper utilities. 10.2.7 Test your package You can test your code using the tetsthat testing framework. The ussethis::use_testthat() function will set up your package for testing, and then you can use the use_test() function to setup individual test files. For example, in the functions lesson we created some tests for our fahr_to_celsius functions but ran them line by line in the console. First, lets add that function to our package. Run the use_r function in the console: usethis::use_r(&quot;fahr_to_celsius&quot;) Then copy the function and documentation into the R script that opens and save the file. #&#39; Convert temperature data from Fahrenheit to Celsius #&#39; #&#39; @param fahr Temperature data in degrees Fahrenheit to be converted #&#39; @return temperature value in degrees Celsius #&#39; @keywords conversion #&#39; @export #&#39; @examples #&#39; fahr_to_celsius(32) #&#39; fahr_to_celsius(c(32, 212, 72)) fahr_to_celsius &lt;- function(fahr) { celsius &lt;- (fahr-32)*5/9 return(celsius) } Now, set up your package for testing: usethis::use_testthat() ✔ Adding &#39;testthat&#39; to Suggests field in DESCRIPTION ✔ Creating &#39;tests/testthat/&#39; ✔ Writing &#39;tests/testthat.R&#39; Then write a test for fahr_to_celsius: usethis::use_test(&quot;fahr_to_celsius&quot;) ✔ Writing &#39;tests/testthat/test-fahr_to_celsius.R&#39; ● Modify &#39;tests/testthat/test-fahr_to_celsius.R&#39; You can now add tests to the test-fahr_to_celsius.R, and you can run all of the tests using devtools::test(). For example, if you add a test to the test-fahr_to_celsius.R file: test_that(&quot;fahr_to_celsius works&quot;, { expect_equal(fahr_to_celsius(32), 0) expect_equal(fahr_to_celsius(212), 100) }) Then you can run the tests to be sure all of your functions are working using devtools::test(): devtools::test() Loading mytools Testing mytools ✔ | OK F W S | Context ✔ | 2 | test-fahr_to_celsius [0.1 s] ══ Results ════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════ Duration: 0.1 s OK: 2 Failed: 0 Warnings: 0 Skipped: 0 Yay, all tests passed! 10.2.8 Checking and installing your package Now that your package is built, you can check it for consistency and completeness using check(), and then you can install it locally using install(), which needs to be run from the parent directory of your module. devtools::check() devtools::install() Your package is now available for use in your local environment. 10.2.9 Sharing and releasing your package The simplest way to share your package with others is to upload it to a GitHub repository, which allows others to install your package using the install_github('mytools','github_username') function from devtools. If your package might be broadly useful, also consider releasing it to CRAN, using the release() method from `devtools(). Releasing a package to CRAN requires a significant amoutn of work to ensure it follows the standards set by the R community, but it is entirely tractable and a valuable contribution to the science community. If you are considering releasing a package more broadly, you may find that the supportive community at ROpenSci provides incredible help and valuable feeback through their onboarding process. Challenge Add the other temperature conversion functions with full documentation to your package, write tests to ensure the functions work properly, and then document(), check(), and install() the new version of the package. Don’t forget to update the version number before you install! 10.2.10 More reading Hadley Wickham’s awesome book: R Packages Thomas Westlake’s blog Writing an R package from scratch "],["data-portals.html", "11 Data Portals 11.1 What is a Portal? 11.2 Portal Uses 11.3 Portal Features 11.4 Enhancing Access to Social Science Research Data 11.5 Relationship between Arctic Data Center portals and DataONE 11.6 Creating Portals 11.7 How to Publish Portals 11.8 Sharing Portals 11.9 Tutorial Videos 11.10 Acknowledgements", " 11 Data Portals Data portals are a new feature on the Arctic Data Center. Researchers can now easily view project information and datasets all in one place. 11.1 What is a Portal? A portal is a collection of Arctic Data Center data packages on a unique webpage. Typically, a research project’s website won’t be maintained beyond the life of the project and all the information on the website that provides context for the data collection is lost. Arctic Data Center portals can provide a means to preserve information regarding the projects’ objectives, scopes, and organization and couple this with the data files so it’s clear how to use and interpret the data for years to come. Portals also leverage Arctic Data Center’s metric features, which create statistics describing the project’s data packages. Information such as total size of data, proportion of data file types, and data collection periods are immediately available from the portal webpage. 11.2 Portal Uses Portals allow users to bundle supplementary information about their group, data, or project along with the data packages. Data contributors can organize their project specific data packages into a unique portal and customize the portal’s theme and structure according to the needs of that project. Researchers can also use portals to compare their public data packages and highlight and share them with other teams, as well as the broader Arctic research audience. To see an example of a portal, please view the Toolik Field Station’s portal. 11.3 Portal Features Published portals vary in their design according to the needs and preferences of the individual or group. However, when constructing a portal there are three core elements: a data page, a metrics page, and customizable free-form pages. Flexible Content Creation Portals can be constructed as a website providing information about the research and products. Using our flexible ‘free-form’ pages (written in markdown), you can add and re-order pages to meet your needs. These pages might be used as a home, or ‘landing’ page with general project information. They are also used to showcase research products, and communicate news and upcoming events. Curated Collections of Data The data page is arguably the most important component of the Arctic Data Center portal system. This is where users will display the data packages of their choice. Whether these data reflect research products from your research group (collated based on contributor IDs), or thematic areas of research interest (collated based on keyword and search terms), will depend upon the intended use of the portal but in all cases, you are refining the suite of data viewed by your audience. The data page looks and performs just like the main Arctic Data Center catalog - with some added bonuses, see below. Customized Search Capabilities You can also build more precise search capabilities into your portal, leveraging the rich metadata associated with data products preserved at the Arctic Data Center. For example, in the example below the research group have identified seven primary search categories and within these, enable users to search within specific metadata fields or else select from a drop down list of options. In doing so, your audience can construct refined search queries drilling down to the data of interest. (Note that although the SASAP portal is hosted by DataONE, the same functionality exists at the Arctic Data Center. More on this later). Metrics, Metrics, Metrics As with the full Arctic Data Center catalog, we aggregate metrics for the collection of data packages within a portal. This page is not customizable - it comes as a default with the portal - but you can choose to delete it. The metrics provided include a summary of the holdings: number, volume, time period, format of datasets, metadata assessment scores, citations across all packages, and counts of downloads and views. These latter metrics can be particularly useful if wanting to track the usage or reach of your project or group’s activities. 11.4 Enhancing Access to Social Science Research Data Many of the portal examples provided above are organizational, individual or project portals created by members of the Arctic research community. The ability to group relevant datasets and customize search criteria, increases data discoverability and accessibility among target audiences. The Arctic Data Center has leveraged these features to create a specific portal for social science data. Within this portal users can subset by social science discipline, data type and other metadata fields built into the portal. These search features depend on sufficient user contributed metadata describing the dataset, and as can be seen from the ‘with data’ toggle, the portal does not require the data themselves to be uploaded to the Arctic Data Center. 11.5 Relationship between Arctic Data Center portals and DataONE Both DataONE and the Arctic Data Center use Metacat and MetacatUI software and both have the capability for individuals and groups to develop portals. This is true of other repositories running this software. The difference between a portal at teh Arctic Data Center and one through DataONE is the corpus of data that can be pulled into your portal. Arctic Data Center portals expose only data held in the Arctic Data Center repository. A portal in DataONE can expose data from across the full network of data repositories - including DataONE. This is particularly useful for interdisciplinary research projects, labs that have published data to multiple repositories etc. However, unlike at the Arctic Data Center, there is a cost associated with a DataONE portal as they are part of the organizations sustainability model. 11.6 Creating Portals A step-by-step guide on how to navigate the Arctic Data Center and create a new portal. For video tutorials on how to create your first portal, please visit the Arctic Data Center’s website. Getting Started with Portals If you are on the Arctic Data Center’s primary website, select the button on the top right titled ‘Create Portal’, this will take you to sign in with your ORCID id if you are not already signed in. Sign in with your ORCID, which will then take you directly to the page where you can start customizing your portal. You can also get to the page to create a new portal by clicking on your name in the upper right hand corner when you are signed in to the Arctic Data Center with your ORCID. A dropdown will appear, and you would select ‘My Portals’. On your profile settings page, select ‘My Portals’. After the page loads select the green button ‘+ New Portal’ to add a new portal, you’ll automatically be directed to a fresh edit session. Portal Settings Page In a new edit session, the first page you’ll be taken to is the settings page where you’ll be able to add details about your portal. Portal URL Identify a short name for your portal that will become part of the URL. If the name is available, a label will indicate it’s available and if the name is taken already, it will note that the name is already taken. This feature ensures the portals are unique. Portal description Sharing options For the purposes of this training, please leave your portal in ‘Private’ status. You are welcome to return and make the portal public when the portal is complete and is useful to you and others. Permissions Adding collaborators to help you create your portal is as straightforward as copying and pasting in their ORCID into the box below the permissions section. You can choose whether the collaborator can view, edit, or is an owner of the portal. You can have multiples of each role. Partner Logos Adding Data to Portals When selecting the data tab you will see a page with two sections. These are titled with instructive statements to help explain their function: Add filters to help people find data within your collection build search features for others to use within your portal Add data to your collection construct a search to populate your portal with data We’re going to start with the second section. When adding data to your collection, you can include any of the datasets that are available at the Arctic Data Center. You build rules based on metadata to define which datasets should be included. Of course, where metadata is incomplete or abbreviated, this will impact your results. Data added to the network in the future that match these rules will also be added to your collection. The first thing we notice is the ability to include/exclude data based on all/any metadata criteria. This setting applies across all rules. In the default view you have the starrtig point for a single rule, and a view showing 0 datasets. As we build rules, the page will refresh to show how many datasets are included based on your rule structure. You can continue to add rules (and rule groups) to create complex queries that are specific to your needs. The metadata fields available for rule construction are easily visible in the dropdown option, and grouped for ease of use. You also have the option to define ‘any metadata field’ though your results may be less precise. Building Search Options for Your Audience This section covers the first part of the ‘data’ page. “Add filters to help people find data within your collection”. Although it appears first, I recommend constructing these filters after you have defined your data as you will have a better undertsanding of the metadata field that are relevant to your portal collection. It appears at the top of this editor page as when published, it will be at the top for users, hence the editor page reflects the layout of the published page. When selecting “add a search filter” you will be presented with a pop-up that comprises three primary elements. The metadata filed you will be querying The way in which you want the user to interact with that metadata Language settings for the filter you are building As you select options for number 1 - the metadata field, the pop-up will refresh to show only those relevant options. Save this filter to close the pop-up, return to the main editor and add another search filter. Data Package Metrics As stated above, the metrics page is a default function provided by the Arctic Data Center. This page cannot be edited and cannot be viewed while editing. Users do have the option to delete the page if they’d like. To delete the page, select the arrow next to the word ‘Metrics’ in the tab and choose ‘Delete’ from the dropdown list. You can alwys change your mind and add a metrics page with the ‘+’ tab. To see metric summaries, navigate to your portal in view mode. See Saving and Editing Portals for more information on how to view portals. Creating Unique Freeform Pages To watch a tutorial on creating a new freeform page see this video:Creating a Freeform Text Page To add a freeform page to a portal, select the ‘+’ tab next to the data and metric tabs and then choose the freeform option that appears on screen. A freeform page will then populate. Easily customize your banner with a unique image, title, and page description. To change the name of the tab, click on the arrow in the ‘Untitled’ tab and select ‘Rename’ from the dropdown list. Below the banner, there is a markdown text box with some examples on how to use the markdown formatting directives to customize the text display. There is also a formatting header at the top to assist if you’re unfamiliar with markdown. As you write, toggle through the Edit and Preview modes in the markdown text box to make sure your information is displaying as intended. Portals are flexible and can accommodate as many additional freeform pages as needed. The markdown header structure helps to generate the table of contents for the page. Please see these additional resources for help with markdown: Markdown reference Ten minute tutorial For a longer example where you can also preview the results, checkout the Showdown Live Editor Saving and Editing Portals Be sure to save your portal when you complete a page to ensure your progress is retained. Whenever a portal is saved, a dialogue box will pop up at the top of the page prompting users to view their private portal in view mode. You can choose to ignore this and continue editing. To delete a page from your portal, select the arrow in the tab and choose ‘Delete’ from the dropdown. Users can view and edit their portal from their ‘My Portals’ tab. First, click the arrow your name in the top-right corner to drop down your menu options. Then, select ‘My Portals’ from the dropdown underneath your name. See the section on Getting Started with Portals for more details. Click on the portal title to view it or select the edit button to make changes. 11.7 How to Publish Portals New portals are automatically set to private and only visible to the portal creator. The portal will remain private until the owner decides to make it public. To make your portal public, go into the settings of your portal. Under the description, you’ll see a new section called ‘Sharing Options’. You can toggle between your portal being private and your portal being public there. 11.8 Sharing Portals In order to share a published portal, users can simply direct recipients to their unique identifier. Portal identifiers are embedded into the Arctic Data Center’s portal URL: https://arcticdata.io/catalog/portals/portal-identifier To view or edit your portal identifier, go into edit mode in your portal. In the settings page, your portal URL will be the first item on the page. 11.9 Tutorial Videos For video tutorials on how to create your first portal, please visit the Arctic Data Center video tutorial page. 11.10 Acknowledgements Much of this documentation was composed by ESS-DIVE, which can be found here. "],["exercise-cleaning-and-manipulating-data.html", "12 Exercise: Cleaning and Manipulating Data 12.1 Hands On: Clean and Integrate Datasets", " 12 Exercise: Cleaning and Manipulating Data 12.1 Hands On: Clean and Integrate Datasets 12.1.1 Learning Objectives In this lesson, you will: Clean and integrate a dataset using dplyr and tidyr Make use of previously-learned knowledge of dplyr, tidyr, and writing functions 12.1.2 Outline In this session, you will load data from the following dataset: Richard Lanctot and Sarah Saalfeld. 2019. Utqiaġvik shorebird breeding ecology study, Utqiaġvik, Alaska, 2003-2018. Arctic Data Center. doi:10.18739/A23R0PT35 Using our knowledge of R, we are going to try to answer the following two questions: What species of predator is the most abundant and has this changed through time? Does the number of eggs predated increase with the total number of predators for all species laying nests? One of the features if this dataset is that it has many files with similar formatting, most of which contain the column species which is comprised of the Bird Banding Laboratory species codes. These four letter codes aren’t very intuitive to most people, so one of the things we will do is write a function that can be used on any file in this dataset that contains a species code. High-level steps The goal here is for you to have to come up with the functions to do the analysis with minimal guidance. This is supposed to be challenging if you are new to dplyr/tidyr. Below is a set of high-level steps you can follow to answer our research question. After the list is a schematic of the steps in table form which I expect will be useful in guiding your code. Load the species table using the code in the Setup block below. Read the following two files into your environment. Utqiagvik_predator_surveys.csv Utqiagvik_nest_data.csv Write a function that will translate species codes into common names. Hint: The fastest way to do this involves adding a column to the data.frame. Your function will have two arguments Optional Extra Challenge: For a little extra challenge, try to incorporate an if statement that looks for NA values in the common name field you are adding. What other conditionals might you include to make your function smarter? Calculate total number of predators by year and species, and plot the result. Calculate total number of eggs predated by year and species. Calculate total number of predators by year, join to summarized egg predation table, and plot the result. Setup Use the following two code chunks to set up your species table. The code I use here incorporates two new packages that I thought would be worth showing. The first, rvest, is a package that enables easy scraping and handling of information from websites. It requires a moderate amount of knowledge of html to use, but can be very handy. In this case, I use it because I couldn’t find a plain text version of the BBL species codes anywhere. To build a reproducible and long lived workflow, I would want to run this code and then store a plain text version of the data in a long lived location, which cites the original source appropriately. The second package, janitor, is a great package that has simple tools for cleaning up data. It is especially handy for handling improperly named variables in a data.frame. In this case, I use it to handle a bunch of spaces in the column names so that our lives are a little easier later on. library(rvest) library(readr) library(dplyr) library(janitor) webpage &lt;- read_html(&quot;https://www.pwrc.usgs.gov/bbl/manual/speclist.cfm&quot;) tbls &lt;- html_nodes(webpage, &quot;table&quot;) %&gt;% html_table(fill = TRUE) species &lt;- tbls[[1]] %&gt;% clean_names() %&gt;% select(alpha_code, common_name) %&gt;% mutate(alpha_code = tolower(alpha_code)) Visual schematic of data Make this: year common_name pred_count 2003 Glaucous Gull 54 2003 Parasitic Jaeger 2 2003 Pomarine Jaeger 6 2004 Glaucous Gull 69 2004 Long-tailed Jaeger 13 And then make this: common_name year total_predated pred_count American Golden-Plover 2003 4 62 American Golden-Plover 2004 22 93 American Golden-Plover 2005 0 72 American Golden-Plover 2006 0 193 American Golden-Plover 2007 12 154 American Golden-Plover 2008 22 328 American Golden-Plover 2009 92 443 Aside Why do we need to use a function for this task? You will likely at some point realize that the function we asked you to write is pretty simple. The code can in fact be accomplished in a single line. So why write your own function for this? There are a couple of answers. The first and most obvious is that we want to you practice writing function syntax with simple examples. But there are other reasons why this operation might benefit from a function: Follow the DRY principles! If you find yourself doing the same cleaning steps on many of your data files, over and over again, those operations are good candidates for functions. This falls into that category, since we need to do the same transformation on both of the files we use here, and if we incorporated more files from this dataset it would come in even more use. Add custom warnings and quality control. Functions allow you to incorporate quality control through conditional statements coupled with warnings. Instead of checking for NA’s or duplicated rows after you run a join, you can check within the function and return a warning if any are found. Check your function input more carefully Similar to custom warnings, functions allow you to create custom errors too. Writing functions is a good way to incorporate defensive coding practices, where potential issues are looked for and the process is stopped if they are found. 12.1.3 Full solution. Warning, spoilers ahead! Load in the libraries and the species table, as described above. library(rvest) library(readr) library(dplyr) library(janitor) library(ggplot2) webpage &lt;- read_html(&quot;https://www.pwrc.usgs.gov/bbl/manual/speclist.cfm&quot;) tbls &lt;- html_nodes(webpage, &quot;table&quot;) %&gt;% html_table(fill = TRUE) species &lt;- tbls[[1]] %&gt;% clean_names() %&gt;% select(alpha_code, common_name) %&gt;% mutate(alpha_code = tolower(alpha_code)) Read in the two data files. pred &lt;- read_csv(&quot;https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A9ffec04c-7e2d-41dd-9e88-b6c2e8c4375e&quot;) nests &lt;- read_csv(&quot;https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A982bd2fc-4edf-4da7-96ef-0d11b853102d&quot;) Define a function to join the species table to an arbirary table with a species column in this dataset. Note that this version has some extra conditionals for the optional challenge. #&#39; Function to add common name to data.frame according to the BBL list of species codes #&#39; @param df A data frame containing BBL species codes in column `species` #&#39; @param species A data frame defining BBL species codes with columns `alpha_code` and `common_name` #&#39; @return A data frame with original data df, plus the common name of species assign_species_name &lt;- function(df, species){ if (!(&quot;alpha_code&quot; %in% names(species)) | !(&quot;species&quot; %in% names(df)) | !(&quot;common_name&quot; %in% names(species))){ stop(&quot;Tables appear to be formatted incorrectly.&quot;) } return_df &lt;- left_join(df, species, by = c(&quot;species&quot; = &quot;alpha_code&quot;)) if (nrow(return_df) &gt; nrow(df)){ warning(&quot;Joined table has more rows than original table. Check species table for duplicated code values.&quot;) } if (length(which(is.na(return_df$common_name))) &gt; 0){ x &lt;- length(which(is.na(return_df$common_name))) warning(paste(&quot;Common name has&quot;, x, &quot;rows containing NA&quot;)) } return(return_df) } Or a simple version without the extra challenges added: #&#39; Function to add common name to data.frame according to the BBL list of species codes #&#39; @param df A data frame containing BBL species codes in column `species` #&#39; @param species A data frame defining BBL species codes with columns `alpha_code` and `common_name` #&#39; @return A data frame with original data df, plus the common name of species assign_species_name &lt;- function(df, species){ return_df &lt;- left_join(df, species, by = c(&quot;species&quot; = &quot;alpha_code&quot;)) return(return_df) } Question 1: What species of predator is the most abundant and has this changed through time? Calculate the number of each species by year. Species counts with no common name are removed after examining the data and determining that the species code in these cases was “none”, as in, no predators were observed that day. pred_species &lt;- assign_species_name(pred, species) %&gt;% group_by(year, common_name) %&gt;% summarise(pred_count = sum(count, na.rm = T), .groups = &quot;drop&quot;) %&gt;% filter(!is.na(common_name)) ## Warning in assign_species_name(pred, species): Common name has 268 rows ## containing NA Plot the result. ggplot(pred_species, aes(x = year, y = pred_count, color = common_name)) + geom_line() + geom_point() + labs(x = &quot;Year&quot;, y = &quot;Number of Predators&quot;, color = &quot;Species&quot;) + theme_bw() Question 2: Does the number of eggs predated increase with the total number of predators for all species laying nests? Calculate the number of eggs predated by year and species. Species with no common name were examined, and found to have missing values in the species code as well. This is likely a data entry error that should be examined more closely, but for simplicity here we can drop these rows. nests_species &lt;- assign_species_name(nests, species) %&gt;% group_by(common_name, year) %&gt;% summarise(total_predated = sum(number_eggs_predated, na.rm = T), .groups = &quot;drop&quot;) %&gt;% filter(!is.na(common_name)) ## Warning in assign_species_name(nests, species): Common name has 4 rows ## containing NA Calculate total number of predators across all species by year. pred_total &lt;- pred_species %&gt;% group_by(year) %&gt;% summarise(pred_count = sum(pred_count, na.rm = T), .groups = &quot;drop&quot;) Join egg predation data to total predator data. nest_pred &lt;- left_join(nests_species, pred_total, by = &quot;year&quot;) Plot the number of eggs predated by total predators, faceted over species. ggplot(nest_pred, aes(x = pred_count, y = total_predated)) + geom_point() + facet_wrap(~common_name, scales = &quot;free_y&quot;, ncol = 2) + labs(x = &quot;Number of Predators&quot;, y = &quot;Number of Eggs Predated&quot;) + theme_bw() "],["geospatial-vector-analysis.html", "13 Geospatial Vector Analysis 13.1 Spatial vector analysis using sf", " 13 Geospatial Vector Analysis 13.1 Spatial vector analysis using sf 13.1.1 Learning Objectives In this lesson, you will learn: How to use the sf package to analyze geospatial data Static mapping with ggplot interactive mapping with leaflet 13.1.2 Introduction From the sf vignette: Simple features or simple feature access refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects. It also describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them. The sf package is an R implementation of Simple Features. This package incorporates: a new spatial data class system in R functions for reading and writing data tools for spatial operations on vectors Most of the functions in this package starts with prefix st_ which stands for spatial and temporal. In this tutorial, our goal is to use a shapefile of Alaska regions and data on population in Alaska by community to create a map that looks like this: The data we will be using to create the map are: Alaska regional boundaries Community locations and population Alaksa rivers 13.1.3 Working with geospatial data All of the data used in this tutorial are simplified versions of real datasets available on the KNB. I’ve simplified the original high-resolution geospatial datasets to ease the processing burden on your computers while learning how to do the analysis. These simplified versions of the datasets may contain topological errors. The original version of the datasets are indicated throughout the chapter. Setup For convience, I’ve hosted a zipped copy of all of the files on our test site. Follow these steps to get ready for the next exercise: Navigate to this dataset and download the zip folder. Move the downloaded zip into the training directory and unzip it. The first file we will use is a shapefile of regional boundaries in alaska derived from: Jared Kibele and Jeanette Clark. 2018. State of Alaska’s Salmon and People Regional Boundaries. Knowledge Network for Biocomplexity. doi:10.5063/F1125QWP. Now we can load the libraries we need: library(sf) library(ggplot2) library(leaflet) library(scales) library(ggmap) library(dplyr) Read in the data and look at a plot of it. ## Read in shapefile using sf ak_regions &lt;- read_sf(&quot;shapefiles/ak_regions_simp.shp&quot;) plot(ak_regions) We can also examine it’s class. class(ak_regions) ## [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; sf objects usually have two types - sf and data.frame. Two main differences comparing to a regular data.frame object are spatial metadata (geometry type, dimension, bbox, epsg (SRID), proj4string) and additional column - typically named geometry. Since our shapefile object has the data.frame class, viewing the contents of the object using the head function shows similar results to data we read in using read.csv. head(ak_regions) ## Simple feature collection with 6 features and 3 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -179.2296 ymin: 51.15702 xmax: 179.8567 ymax: 71.43957 ## Geodetic CRS: WGS 84 ## # A tibble: 6 × 4 ## region_id region mgmt_area geometry ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 1 Aleutian Islands 3 (((-171.1345 52.44974, -171.1686 52.4174… ## 2 2 Arctic 4 (((-139.9552 68.70597, -139.9893 68.7051… ## 3 3 Bristol Bay 3 (((-159.8745 58.62778, -159.8654 58.6137… ## 4 4 Chignik 3 (((-155.8282 55.84638, -155.8049 55.8655… ## 5 5 Copper River 2 (((-143.8874 59.93931, -143.9165 59.9403… ## 6 6 Kodiak 3 (((-151.9997 58.83077, -152.0358 58.8271… Coordinate Reference System Every sf object needs a coordinate reference system (or crs) defined in order to work with it correctly. A coordinate reference system contains both a datum and a projection. The datum is how you georeference your points (in 3 dimensions!) onto a spheroid. The projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. All coordinate reference systems require a datum. However, some coordinate reference systems are “unprojected” (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984. ESRI has a blog post that explains these concepts in more detail with very helpful diagrams and examples. You can view what crs is set by using the function st_crs st_crs(ak_regions) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;latitude&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;longitude&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] This is pretty confusing looking. Without getting into the details, that long string says that this data has a greographic coordinate system (WGS84) with no projection. A convenient way to reference crs quickly is by using the EPSG code, a number that represents a standard projection and datum. You can check out a list of (lots!) of EPSG codes here. We will use several EPSG codes in this lesson. Here they are, along with their more readable names: 3338: Alaska Albers 4326: WGS84 (World Geodetic System 1984), used in GPS 3857: Pseudo-Mercator, used in Google Maps, OpenStreetMap, Bing, ArcGIS, ESRI You will often need to transform your geospatial data from one coordinate system to another. The st_transform function does this quickly for us. You may have noticed the maps above looked wonky because of the dateline. We might want to set a different projection for this data so it plots nicer. A good one for Alaska is called the Alaska Albers projection, with an EPSG code of 3338. ak_regions_3338 &lt;- ak_regions %&gt;% st_transform(crs = 3338) st_crs(ak_regions_3338) ## Coordinate Reference System: ## User input: EPSG:3338 ## wkt: ## PROJCRS[&quot;NAD83 / Alaska Albers&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;Alaska Albers (meters)&quot;, ## METHOD[&quot;Albers Equal Area&quot;, ## ID[&quot;EPSG&quot;,9822]], ## PARAMETER[&quot;Latitude of false origin&quot;,50, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8821]], ## PARAMETER[&quot;Longitude of false origin&quot;,-154, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8822]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,55, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,65, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8824]], ## PARAMETER[&quot;Easting at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8826]], ## PARAMETER[&quot;Northing at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8827]]], ## CS[Cartesian,2], ## AXIS[&quot;easting (X)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;northing (Y)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Topographic mapping (small scale).&quot;], ## AREA[&quot;United States (USA) - Alaska.&quot;], ## BBOX[51.3,172.42,71.4,-129.99]], ## ID[&quot;EPSG&quot;,3338]] plot(ak_regions_3338) Much better! 13.1.4 sf &amp; the Tidyverse sf objects can be used as a regular data.frame object in many operations. We already saw the results of plot and head. Challenge Try running some other functions you might use to explore a regular data.frame on your sf flavored data.frame. Since sf objects are data.frames, they play nicely with packages in the tidyverse. Here are a couple of simple examples: select() ak_regions_3338 %&gt;% select(region) ## Simple feature collection with 13 features and 1 field ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -2175328 ymin: 405653 xmax: 1579226 ymax: 2383770 ## Projected CRS: NAD83 / Alaska Albers ## # A tibble: 13 × 2 ## region geometry ## &lt;chr&gt; &lt;MULTIPOLYGON [m]&gt; ## 1 Aleutian Islands (((-1156666 420855.1, -1159837 417990.3, -1161898 41694… ## 2 Arctic (((571289.9 2143072, 569941.5 2142691, 569158.2 2142146… ## 3 Bristol Bay (((-339688.6 973904.9, -339302 972297.3, -339229.2 9710… ## 4 Chignik (((-114381.9 649966.8, -112866.8 652065.8, -108836.8 65… ## 5 Copper River (((561012 1148301, 559393.7 1148169, 557797.7 1148492, … ## 6 Kodiak (((115112.5 983293, 113051.3 982825.9, 110801.3 983211.… ## 7 Kotzebue (((-678815.3 1819519, -677555.2 1820698, -675557.8 1821… ## 8 Kuskokwim (((-1030125 1281198, -1029858 1282333, -1028980 1284032… ## 9 Cook Inlet (((35214.98 1002457, 36660.3 1002038, 36953.11 1001186,… ## 10 Norton Sound (((-848357 1636692, -846510 1635203, -840513.7 1632225,… ## 11 Prince William Sound (((426007.1 1087250, 426562.5 1088591, 427711.6 1089991… ## 12 Southeast (((1287777 744574.1, 1290183 745970.8, 1292940 746262.7… ## 13 Yukon (((-375318 1473998, -373723.9 1473487, -373064.8 147393… Note the sticky geometry column! The geometry column will stay with your sf object even if it is not called explicitly. filter() ak_regions_3338 %&gt;% filter(region == &quot;Southeast&quot;) ## Simple feature collection with 1 feature and 3 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 559475.7 ymin: 722450 xmax: 1579226 ymax: 1410576 ## Projected CRS: NAD83 / Alaska Albers ## # A tibble: 1 × 4 ## region_id region mgmt_area geometry ## * &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [m]&gt; ## 1 12 Southeast 1 (((1287777 744574.1, 1290183 745970.8, 1292940 … Joins You can also use the sf package to create spatial joins, useful for when you want to utilize two datasets together. As an example, let’s ask a question: how many people live in each of these Alaska regions? We have some population data, but it gives the number of people by city, not by region. To determine the number of people per region we will need to: read in the city data from a csv and turn it into an sf object use a spatial join (st_join) to assign each city to a region use group_by and summarize to calculate the total population by region First, read in the population data as a regular data.frame. This data is derived from: Jeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018. Languages used in Alaskan households, 1990-2015. Knowledge Network for Biocomplexity. doi:10.5063/F11G0JHX. Unnecessary columns were removed and the most recent year of data was selected. pop &lt;- read.csv(&quot;shapefiles/alaska_population.csv&quot;) The st_join function is a spatial left join. The arguments for both the left and right tables are objects of class sf which means we will first need to turn our population data.frame with latitude and longitude coordinates into an sf object. We can do this easily using the st_as_sf function, which takes as arguments the coordinates and the crs. The remove = F specification here ensures that when we create our geometry column, we retain our original lat lng columns, which we will need later for plotting. Although it isn’t said anywhere explicitly in the file, let’s assume that the coordinate system used to reference the latitude longitude coordinates is WGS84, which has a crs number of 4236. pop_4326 &lt;- st_as_sf(pop, coords = c(&#39;lng&#39;, &#39;lat&#39;), crs = 4326, remove = F) head(pop_4326) ## Simple feature collection with 6 features and 5 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -176.6581 ymin: 51.88 xmax: -154.1703 ymax: 62.68889 ## Geodetic CRS: WGS 84 ## year city lat lng population geometry ## 1 2015 Adak 51.88000 -176.6581 122 POINT (-176.6581 51.88) ## 2 2015 Akhiok 56.94556 -154.1703 84 POINT (-154.1703 56.94556) ## 3 2015 Akiachak 60.90944 -161.4314 562 POINT (-161.4314 60.90944) ## 4 2015 Akiak 60.91222 -161.2139 399 POINT (-161.2139 60.91222) ## 5 2015 Akutan 54.13556 -165.7731 899 POINT (-165.7731 54.13556) ## 6 2015 Alakanuk 62.68889 -164.6153 777 POINT (-164.6153 62.68889) Now we can do our spatial join! You can specify what geometry function the join uses (st_intersects, st_within, st_crosses, st_is_within_distance, …) in the join argument. The geometry function you use will depend on what kind of operation you want to do, and the geometries of your shapefiles. In this case, we want to find what region each city falls within, so we will use st_within. pop_joined &lt;- st_join(pop_4326, ak_regions_3338, join = st_within) This gives an error! Error: st_crs(x) == st_crs(y) is not TRUE Turns out, this won’t work right now because our coordinate reference systems are not the same. Luckily, this is easily resolved using st_transform, and projecting our population object into Alaska Albers. pop_3338 &lt;- st_transform(pop_4326, crs = 3338) pop_joined &lt;- st_join(pop_3338, ak_regions_3338, join = st_within) head(pop_joined) ## Simple feature collection with 6 features and 8 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -1537925 ymin: 472626.9 xmax: -10340.71 ymax: 1456223 ## Projected CRS: NAD83 / Alaska Albers ## year city lat lng population region_id region ## 1 2015 Adak 51.88000 -176.6581 122 1 Aleutian Islands ## 2 2015 Akhiok 56.94556 -154.1703 84 6 Kodiak ## 3 2015 Akiachak 60.90944 -161.4314 562 8 Kuskokwim ## 4 2015 Akiak 60.91222 -161.2139 399 8 Kuskokwim ## 5 2015 Akutan 54.13556 -165.7731 899 1 Aleutian Islands ## 6 2015 Alakanuk 62.68889 -164.6153 777 13 Yukon ## mgmt_area geometry ## 1 3 POINT (-1537925 472626.9) ## 2 3 POINT (-10340.71 770998.4) ## 3 4 POINT (-400885.5 1236460) ## 4 4 POINT (-389165.7 1235475) ## 5 3 POINT (-766425.7 526057.8) ## 6 4 POINT (-539724.9 1456223) Challenge Like we mentioned above, there are many different types of joins you can do with geospatial data. Examine the help page for these joins (?st_within will get you there). What other joins types might be appropriate for examining the relationship between points and polygyons? What about two sets of polygons? Group and summarize Next we compute the total population for each region. In this case, we want to do a group_by and summarise as this were a regular data.frame - otherwise all of our point geometries would be included in the aggreation, which is not what we want. Our goal is just to get the total population by region. We remove the sticky geometry using as.data.frame, on the advice of the sf::tidyverse help page. pop_region &lt;- pop_joined %&gt;% as.data.frame() %&gt;% group_by(region) %&gt;% summarise(total_pop = sum(population)) head(pop_region) ## # A tibble: 6 × 2 ## region total_pop ## &lt;chr&gt; &lt;int&gt; ## 1 Aleutian Islands 8840 ## 2 Arctic 8419 ## 3 Bristol Bay 6947 ## 4 Chignik 311 ## 5 Cook Inlet 408254 ## 6 Copper River 2294 And use a regular left_join to get the information back to the Alaska region shapefile. Note that we need this step in order to regain our region geometries so that we can make some maps. pop_region_3338 &lt;- left_join(ak_regions_3338, pop_region) ## Joining with `by = join_by(region)` #plot to check plot(pop_region_3338[&quot;total_pop&quot;]) So far, we have learned how to use sf and dplyr to use a spatial join on two datasets and calculate a summary metric from the result of that join. The group_by and summarize functions can also be used on sf objects to summarize within a dataset and combine geometries. Many of the tidyverse functions have methods specific for sf objects, some of which have additional arguments that wouldn’t be relevant to the data.frame methods. You can run ?sf::tidyverse to get documentation on the tidyverse sf methods. Let’s try some out. Say we want to calculate the population by Alaska management area, as opposed to region. pop_mgmt_338 &lt;- pop_region_3338 %&gt;% group_by(mgmt_area) %&gt;% summarize(total_pop = sum(total_pop)) plot(pop_mgmt_338[&quot;total_pop&quot;]) Notice that the region geometries were combined into a single polygon for each management area. If we don’t want to combine geometries, we can specify do_union = F as an argument. pop_mgmt_3338 &lt;- pop_region_3338 %&gt;% group_by(mgmt_area) %&gt;% summarize(total_pop = sum(total_pop), do_union = F) plot(pop_mgmt_3338[&quot;total_pop&quot;]) Writing the file Save the spatial object to disk using write_sf() and specifying the filename. Writing your file with the extension .shp will assume an ESRI driver driver, but there are many other format options available. write_sf(pop_region_3338, &quot;shapefiles/ak_regions_population.shp&quot;, delete_layer = TRUE) 13.1.5 Visualize with ggplot ggplot2 now has integrated functionality to plot sf objects using geom_sf(). We can plot sf objects just like regular data.frames using geom_sf. ggplot(pop_region_3338) + geom_sf(aes(fill = total_pop)) + theme_bw() + labs(fill = &quot;Total Population&quot;) + scale_fill_continuous(low = &quot;khaki&quot;, high = &quot;firebrick&quot;, labels = comma) We can also plot multiple shapefiles in the same plot. Say if we want to visualize rivers in Alaska, in addition to the location of communities, since many communities in Alaska are on rivers. We can read in a rivers shapefile, doublecheck the crs to make sure it is what we need, and then plot all three shapefiles - the regional population (polygons), the locations of cities (points), and the rivers (linestrings). The rivers shapefile is a simplified version of Jared Kibele and Jeanette Clark. Rivers of Alaska grouped by SASAP region, 2018. Knowledge Network for Biocomplexity. doi:10.5063/F1SJ1HVW. rivers_3338 &lt;- read_sf(&quot;shapefiles/ak_rivers_simp.shp&quot;) st_crs(rivers_3338) ## Coordinate Reference System: ## User input: Albers ## wkt: ## PROJCRS[&quot;Albers&quot;, ## BASEGEOGCRS[&quot;GCS_GRS 1980(IUGG, 1980)&quot;, ## DATUM[&quot;D_unknown&quot;, ## ELLIPSOID[&quot;GRS80&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433]]], ## CONVERSION[&quot;unnamed&quot;, ## METHOD[&quot;Albers Equal Area&quot;, ## ID[&quot;EPSG&quot;,9822]], ## PARAMETER[&quot;Latitude of false origin&quot;,50, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8821]], ## PARAMETER[&quot;Longitude of false origin&quot;,-154, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8822]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,55, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,65, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8824]], ## PARAMETER[&quot;Easting at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8826]], ## PARAMETER[&quot;Northing at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8827]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]] Note that although no EPSG code is set explicitly, with some sluething we can determine that this is EPSG:3338. This site is helpful for looking up EPSG codes. ggplot() + geom_sf(data = pop_region_3338, aes(fill = total_pop)) + geom_sf(data = rivers_3338, aes(size = StrOrder), color = &quot;black&quot;) + geom_sf(data = pop_3338, aes(), size = .5) + scale_size(range = c(0.01, 0.2), guide = F) + theme_bw() + labs(fill = &quot;Total Population&quot;) + scale_fill_continuous(low = &quot;khaki&quot;, high = &quot;firebrick&quot;, labels = comma) ## Warning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in ## ggplot2 3.3.4. ## ℹ Please use &quot;none&quot; instead. Incorporate base maps into static maps using ggmap The ggmap package has some functions that can render base maps (as raster objects) from open tile servers like Google Maps, Stamen, OpenStreetMap, and others. We’ll need to transform our shapefile with population data by community to EPSG:3857 which is the CRS used for rendering maps in Google Maps, Stamen, and OpenStreetMap, among others. pop_3857 &lt;- pop_3338 %&gt;% st_transform(crs = 3857) Next, let’s grab a base map from the Stamen map tile server covering the region of interest. First we include a function that transforms the bounding box (which starts in EPSG:4326) to also be in the EPSG:3857 CRS, which is the projection that the map raster is returned in from Stamen. This is an issue with ggmap described in more detail here # Define a function to fix the bbox to be in EPSG:3857 # See https://github.com/dkahle/ggmap/issues/160#issuecomment-397055208 ggmap_bbox_to_3857 &lt;- function(map) { if (!inherits(map, &quot;ggmap&quot;)) stop(&quot;map must be a ggmap object&quot;) # Extract the bounding box (in lat/lon) from the ggmap to a numeric vector, # and set the names to what sf::st_bbox expects: map_bbox &lt;- setNames(unlist(attr(map, &quot;bb&quot;)), c(&quot;ymin&quot;, &quot;xmin&quot;, &quot;ymax&quot;, &quot;xmax&quot;)) # Coonvert the bbox to an sf polygon, transform it to 3857, # and convert back to a bbox (convoluted, but it works) bbox_3857 &lt;- st_bbox(st_transform(st_as_sfc(st_bbox(map_bbox, crs = 4326)), 3857)) # Overwrite the bbox of the ggmap object with the transformed coordinates attr(map, &quot;bb&quot;)$ll.lat &lt;- bbox_3857[&quot;ymin&quot;] attr(map, &quot;bb&quot;)$ll.lon &lt;- bbox_3857[&quot;xmin&quot;] attr(map, &quot;bb&quot;)$ur.lat &lt;- bbox_3857[&quot;ymax&quot;] attr(map, &quot;bb&quot;)$ur.lon &lt;- bbox_3857[&quot;xmax&quot;] map } Next, we define the bounding box of interest, and use get_stamenmap() to get the basemap. Then we run our function defined above on the result of the get_stamenmap() call. bbox &lt;- c(-170, 52, -130, 64) # This is roughly southern Alaska ak_map &lt;- get_stamenmap(bbox, zoom = 4) ak_map_3857 &lt;- ggmap_bbox_to_3857(ak_map) Finally, plot both the base raster map with the population data overlayed, which is easy now that everything is in the same projection (3857): ggmap(ak_map_3857) + geom_sf(data = pop_3857, aes(color = population), inherit.aes = F) + scale_color_continuous(low = &quot;khaki&quot;, high = &quot;firebrick&quot;, labels = comma) 13.1.6 Visualize sf objects with leaflet We can also make an interactive map from our data above using leaflet. Leaflet (unlike ggplot) will project data for you. The catch is that you have to give it both a projection (like Alaska Albers), and that your shapefile must use a geographic coordinate system. This means that we need to use our shapefile with the 4326 EPSG code. Remember you can always check what crs you have set using st_crs. Here we define a leaflet projection for Alaska Albers, and save it as a variable to use later. epsg3338 &lt;- leaflet::leafletCRS( crsClass = &quot;L.Proj.CRS&quot;, code = &quot;EPSG:3338&quot;, proj4def = &quot;+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot;, resolutions = 2^(16:7)) You might notice that this looks familiar! The syntax is a bit different, but most of this information is also contained within the crs of our shapefile: st_crs(pop_region_3338) ## Coordinate Reference System: ## User input: EPSG:3338 ## wkt: ## PROJCRS[&quot;NAD83 / Alaska Albers&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;Alaska Albers (meters)&quot;, ## METHOD[&quot;Albers Equal Area&quot;, ## ID[&quot;EPSG&quot;,9822]], ## PARAMETER[&quot;Latitude of false origin&quot;,50, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8821]], ## PARAMETER[&quot;Longitude of false origin&quot;,-154, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8822]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,55, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,65, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8824]], ## PARAMETER[&quot;Easting at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8826]], ## PARAMETER[&quot;Northing at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8827]]], ## CS[Cartesian,2], ## AXIS[&quot;easting (X)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;northing (Y)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Topographic mapping (small scale).&quot;], ## AREA[&quot;United States (USA) - Alaska.&quot;], ## BBOX[51.3,172.42,71.4,-129.99]], ## ID[&quot;EPSG&quot;,3338]] Since leaflet requires that we use an unprojected coordinate system, let’s use st_transform yet again to get back to WGS84. pop_region_4326 &lt;- pop_region_3338 %&gt;% st_transform(crs = 4326) m &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;% addPolygons(data = pop_region_4326, fillColor = &quot;gray&quot;, weight = 1) m We can add labels, legends, and a color scale. pal &lt;- colorNumeric(palette = &quot;Reds&quot;, domain = pop_region_4326$total_pop) m &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;% addPolygons(data = pop_region_4326, fillColor = ~pal(total_pop), weight = 1, color = &quot;black&quot;, fillOpacity = 1, label = ~region) %&gt;% addLegend(position = &quot;bottomleft&quot;, pal = pal, values = range(pop_region_4326$total_pop), title = &quot;Total Population&quot;) m We can also add the individual communities, with popup labels showing their population, on top of that! pal &lt;- colorNumeric(palette = &quot;Reds&quot;, domain = pop_region_4326$total_pop) m &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;% addPolygons(data = pop_region_4326, fillColor = ~pal(total_pop), weight = 1, color = &quot;black&quot;, fillOpacity = 1) %&gt;% addCircleMarkers(data = pop_4326, lat = ~lat, lng = ~lng, radius = ~log(population/500), # arbitrary scaling fillColor = &quot;gray&quot;, fillOpacity = 1, weight = 0.25, color = &quot;black&quot;, label = ~paste0(pop_4326$city, &quot;, population &quot;, comma(pop_4326$population))) %&gt;% addLegend(position = &quot;bottomleft&quot;, pal = pal, values = range(pop_region_4326$total_pop), title = &quot;Total Population&quot;) m There is a lot more functionality to sf including the ability to intersect polygons, calculate distance, create a buffer, and more. Here are some more great resources and tutorials for a deeper dive into this great package: Raster analysis in R Spatial analysis in R with the sf package Intro to Spatial Analysis sf github repo Tidy spatial data in R: using dplyr, tidyr, and ggplot2 with sf mapping-fall-foliage-with-sf "],["reproducibility-and-provenance.html", "14 Reproducibility and Provenance", " 14 Reproducibility and Provenance 14.0.1 Learning Objectives In this lesson, we will: Discuss the concept of reproducible workflows Review the importance of computational reproducibility Review the utility of provenance metadata Overview how R packages are great ways to package work reproducibly Learn how to build a reproducible paper in RMarkdown Review tools and techniques for reproducibility supported by the NCEAS and DataONE Reproducible Research: Recap Working in a reproducible manner: increases research efficiency, accelerating the pace of your research and collaborations provides transparency by capturing and communicating scientific workflows enables research to stand on the shoulders of giants (build on work that came before) allows credit for secondary usage and supports easy attribution increases trust in science To enable others to fully interpret, reproduce or build upon our research, we need to provide more comprehensive information than is typically included in a figure or publication. The methods sections of papers are typically inadequate to fully reproduce the work described in the paper. What data were used in this study? What methods applied? What were the parameter settings? What documentation or code are available to us to evaluate the results? Can we trust these data and methods? Are the results reproducible? Computational reproducibility is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions. Practically speaking, reproducibility includes: Preserving the data Preserving the software workflow Documenting what you did Describing how to interpret it all Computational Provenance and Workflows Computational provenance refers to the origin and processing history of data including: Input data Workflow/scripts Output data Figures Methods, dataflow, and dependencies When we put these all together with formal documentation, we create a computational workflow that captures all of the steps from inital data cleaning and integration, through analysis, modeling, and visualization. Here’s an example of a computational workflow from Mark Carls: Mark Carls. Analysis of hydrocarbons following the Exxon Valdez oil spill, Gulf of Alaska, 1989 - 2014. Gulf of Alaska Data Portal. urn:uuid:3249ada0-afe3-4dd6-875e-0f7928a4c171., that represents a three step workflow comprising four source data files and two output visualizations. This screenshot of the dataset page shows how DataONE renders the workflow model information as part of our interactive user interface. You can clearly see which data files were inputs to the process, the scripts that are used to process and visualize the data, and the final output objects that are produced, in this case two graphical maps of Prince William Sound in Alaska. From Provenance to Reproducibility At DataONE we facilitate reproducible science through provenance by: Tracking data derivation history Tracking data inputs and outputs of analyses Preserving and documenting software workflows Tracking analysis and model executions Linking all of these to publications Introducing ProvONE, an extension of W3C PROV ProvONE provides the fundamental information required to understand and analyze scientific workflow-based computational experiments. It covers the main aspects that have been identified as relevant in the provenance literature including data structure. This addresses the most relevant aspects of how the data, both used and produced by a computational process, is organized and represented. For scientific workflows this implies the inputs and outputs of the various tasks that form part of the workflow. 14.0.2 Data Citation and Transitive Credit We want to move towards a model such that when a user cites a research publication we will also know: Which data produced it What software produced it What was derived from it Who to credit down the attribution stack This is transitive credit. And it changes the way in which we think about science communication and traditional publications. 14.0.3 Reproducible Papers with rrtools A great overview of this approach to reproducible papers comes from: Ben Marwick, Carl Boettiger &amp; Lincoln Mullen (2018) Packaging Data Analytical Work Reproducibly Using R (and Friends), The American Statistician, 72:1, 80-88, doi:10.1080/00031305.2017.1375986 This lesson will draw from existing materials: rrtools Reproducible papers with RMarkdown The key idea in Marwick et al. (2018) is that of the “research compendium”: A single container for not just the journal article associated with your research but also the underlying analysis, data, and even the required software environment required to reproduce your work. Research compendia make it easy for researchers to do their work but also for others to inspect or even reproduce the work because all necessary materials are readily at hand due to being kept in one place. Rather than a constrained set of rules, the research compendium is a scaffold upon which to conduct reproducible research using open science tools such as: R RMarkdown git and GitHub Fortunately for us, Ben Marwick (and others) have written an R package called rrtools that helps us create a research compendium from scratch. To start a reproducible paper with rrtools, run: # install `tinytex` (See below) # Upgrade contentid to 0.12.0 #remotes::install_github(&quot;cboettig/contentid&quot;) # Upgrade readr from 1.3.1 to 2.0.1 #install.packages(c(&quot;readr&quot;)) #remotes::install_github(&quot;benmarwick/rrtools&quot;) setwd(&quot;..&quot;) rrtools::use_compendium(&quot;mypaper&quot;) rrtools has created the beginnings of a research compendium for us. At this point, it looks mostly the same as an R package. That’s because it uses the same underlying folder structure and metadata and therefore it technically is an R package. And this means our research compendium will be easy to install, just like an R package. rrtools also helps you set up some key information like: Add a license (always a good idea) Set up a README file in the RMarkdown format Create an analysis folder to hold our reproducible paper usethis::use_apache_license() rrtools::use_readme_rmd() rrtools::use_analysis() This creates a standard, predictable layout for our code and data and outputs that multiple people can understand. At this point, we’re ready to start writing the paper. To follow the structure rrtools has put in place for us, here are some next steps you can take: Edit ./analysis/paper/paper.Rmd to begin writing your paper and your analysis in the same document Add any citations to ./analysis/paper/references.bib Add any longer R scripts that don’t fit in your paper in an R folder at the top level Add raw data to ./data/raw_data Write out any derived data (generated in paper.Rmd) to ./data/derived_data Write out any figures in ./analysis/figures You can then write all of your R code in your RMarkdown, and generate your manuscript all in the format needed for your journal (using it’s .csl file). Here is a look at the beginning of the Rmd: And the rendered PDF: 14.0.4 Reproducible Papers with rticles Note that the rticles package provides a lot of other great templates for formatting your paper specifically to the requirements of many journals. In addition to a custom CSL file for reference customization, rticles supports custom LATeX templates that fit the formatting requirements of each journals. After installing rticles with a command like remotes::install_github(\"rstudio/rticles\") and restarting your RStudio session, you will be able to create articles from these custom templates using the File | New File | R Markdown... menu, which shows the following dialog: Select the “PNAS” template, give the file a name and choose a location for the files, and click “OK”. You can now Knit the Rmd file to see a highly-targeted article format, like this one for PNAS: While rticles doesn’t produce the detailed folder layout of rrtools, it is relatively simple to use the two packages together. Use rrtools to generate the core directory layout and approach to data handling, and then use articles to create the structure of the paper itself. The combination is incredibly flexible. The 5th Generation of Reproducible Papers Whole Tale simplifies computational reproducibility. It enables researchers to easily package and share ‘tales’. Tales are executable research objects captured in a standards-based tale format complete with metadata. They can contain: data (references) code (computational methods) narrative (traditional science story) compute environment (e.g. RStudio, Jupyter) By combining data, code and the compute environment, tales allow researchers to: re-create the computational results from a scientific study achieve computational reproducibility “set the default to reproducible.” They also empower users to verify and extend results with different data, methods, and environments. You can browse existing tales, run and interact with published tales and create new tales via the Whole Tale Dashboard. By integrating with DataONE and Dataverse, Whole Tale includes over 90 major research repositories from which a user can select datasets to make those datasets the starting point of an interactive data exploration and analysis inside of one of the Whole Tale environments. Within DataONE, we are adding functionality to work with data in the Whole Tale environment directly from the dataset landing page. Full circle reproducibility can be achieved by publishing data, code AND the environment. "],["appendix.html", "15 Appendix 15.1 Collaborating using Git 15.2 Programming Metadata and Data Publishing", " 15 Appendix 15.1 Collaborating using Git 15.1.1 Learning Objectives In this lesson, you will learn: New mechanisms to collaborate using git What is a Pull Request in Github? How to contribute code to colleague’s repository using Pull Requests What is a branch in git? How to use a branch to organize code What is a tag in git and how is it useful for collaboration? 15.1.2 Pull requests We’ve shown in other chapters how to directly collaborate on a repository with colleagues by granting them write privileges as a collaborator to your repository. This is useful with close collaborators, but also grants them tremendous latitude to change files and analyses, to remove files from the working copy, and to modify all files in the repository. Pull requests represent a mechanism to more judiciously collaborate, one in which a collaborator can suggest changes to a repository, the owner and collaborator can discuss those changes in a structured way, and the owner can then review and accept all or only some of those changes to the repository. This is useful with open source code where a community is contributing to shared analytical software, to students in a lab working on related but not identical projects, and to others who want the capability to review changes as they are submitted. To use pull requests, the general procedure is as follows. The collaborator first creates a fork of the owner’s repository, which is a cloned copy of the original that is linked to the original. This cloned copy is in the collaborator’s GitHub account, which means they have the ability to make changes to it. But they don’t have the right to change the original owner’s copy. So instead, they clone their GitHub copy onto their local machine, which makes the collaborator’s GitHub copy the origin as far as they are concerned. In this scenario, we generally refer to the Collaborator’s repository as the remote origin, and the Owner’s repository as upstream. Pull requests are a mechanism for someone that has a forked copy of a repository to request that the original owner review and pull in their changes. This allows them to collaborate, but keeps the owner in control of exactly what changed. 15.1.3 Exercise: Create and merge pull requests In this exercise, work in pairs. Each pair should create a fork of their partner’s training repository, and then clone that onto their local machine. Then they can make changes to that forked repository, and, from the GitHub interface, create a pull request that the owner can incorporate. We’ll walk through the process from both the owner and the collaborator’s perspectives. In the following example, mbjones will be the repository owner, and metamattj will be the collaborator. Change settings (Owner): Edit the github settings file for your training-test repository, and ensure that the collaborator does not have editing permission. Also, be sure that all changes in your repository are committed and pushed to the origin server. Fork (Collaborator): Visit the GitHub page for the owner’s GitHub repository on which you’d like to make changes, and click the Fork button. This will create a clone of that repository in your own GitHub account. You will be able to make changes to this forked copy of the repository, but you will not be able to make direct changes to the owner’s copy. After you have forked the repository, visit your GitHub page for your forked repository, copy the url, and create a new RStudio project using that repository url. Edit README.md (Collaborator): The collaborator should make one or more changes to the README.md file from their cloned copy of the repository, commit the changes, and push them to their forked copy. At this point, their local repo and github copy both have the changes that they made, but the owner’s repository has not yet been changed. When you now visit your forked copy of the repository on Github, you will now see your change has been made, and it will say that This branch is 1 commit ahead of mbjones:main. Create Pull Request (Collaborator): At this point, click the aptly named Pull Request button to create a pull request which will be used to ask that the owner pull in your changes to their copy. When you click Create pull request, provide a brief summary of the request, and a more detailed message to start a conversation about what you are requesting. It’s helpful to be polite and concise while providing adequate context for your request. This will start a conversation with the owner in which you can discuss your changes, they can easily review the changes, and they can ask for further changes before the accept and pull them in. The owner of the repository is in control and determines if and when the changes are merged. Review pull request (Owner): The owner will get an email notification that the Pull Request was created, and can see the PR listed in their Pull requests tab of their repsoitory. The owner can now initiate a conversation about the change, requesting further changes. The interface indicates whether there are any conflicts with the changes, and if not, gives the owner the option to Merge pull request. Merge pull request (Owner): Once the owner thinks the changes look good, they can click the Merge pull request button to accept the changes and pull them into their repository copy. Edit the message, and then click Confirm merge. Congratulations, the PR request has now been merged into the owner’s copy, and has been closed with a note indicating that the changes have been made. Sync with owner (Collaborator): Now that the pull request has been merged, there is a new merge commit in the Owner’s repository that is not present in either of the collaborator’s repositories. To fix that, one needs to pull changes from the upstream repository into the collaborator’s local repository, and then push those changes from that local repository to the collaborator’s origin repository. To add a reference to the upstream remote (the repository you made your fork from), in the terminal, run: git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git Then to pull from the main branch of the upstream repository, in the terminal, run: git pull upstream main At this point, the collaborator is fully up to date. 15.1.4 Branches Branches are a mechanism to isolate a set of changes in their own thread, allowing multiple types of work to happen in parallel on a repository at the same time. These are most often used for trying out experimental work, or for managing bug fixes for historical releases of software. Here’s an example graph showing a branch2.1 that has changes in parallel to the main branch of development: The default branch in almost all repositories is called main, and it is the branch that is typically shown in the GitHub interface and elsewhere. There are many mechanisms to create branches. The one we will try is through RStudio, in which we use the branch dialog to create and switch between branches. 15.1.4.1 Exercise: Create a new branch in your training repository called exp-1, and then make changes to the RMarkdown files in the directory. Commit and push those changes to the branch. Now you can switch between branches using the github interface. 15.2 Programming Metadata and Data Publishing 15.2.1 Learning Objectives In this lesson, you will learn: How to write standardized metadata in R How to publish data packages to the Arctic Data Center programmatically For datasets with a relatively small number of files that do not have many columns, the Arctic Data Center web form is the most efficient way to create metadata. However, for datasets that have many hundreds of files with a similar structure, or a large number of attributes, programmatic metadata creation may be more efficient. Creating metadata and publishing programmatically also allows for more a streamlined approach to updating datasets that have been published. By incorporating this documentation and publishing approach into your scientific workflow, you can improve the reproducibility and transparency of your work. 15.2.2 Creating Metadata 15.2.2.1 About Ecological Metadata Language (EML) EML, the metadata standard that the Arctic Data Center uses, looks like this: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; Earlier in this course we learned how to create an EML document like this using the Arctic Data Center web form. Now, we will learn how to create it using R. This can be especially useful to decrease the repetitiveness of metadata creation when you have many files with the same format, files with many attributes, or many data packages with a similar format. When you create metadata using the web form, the form creates valid metadata for you. Valid, structured metadata is what enables computers to predictably parse the information in a metadata document, enabling search, display, and even meta-analysis. When we create metadata in R, there aren’t as many user-friendly checks in place to ensure we create valid EML, so we need to understand the structure of the document more completely in order to make sure that it will be compatible with the Arctic Data Center. Let’s look at a simplified version of the example above: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; EML is written in XML (eXtensisble Markup Language). One of the key concepts in XML is the element. An XML element is everything that is encompassed by a start tag (&lt;...&gt;) and an end tag (&lt;/...&gt;). So, a simple element in the example above is &lt;surName&gt;Willette&lt;/surName&gt;. The name of this element is surName and the value is simple text, “Willette”. Each element in EML has a specific (case sensitive!) name and description that is specified in the schema. The description of the surName element is: “The surname field is used for the last name of the individual associated with the resource. This is typically the family name of an individual…” The EML schema specifies not only the names and descriptions of all EML elements, but also certain requirements, like which elements must be included, what type of values are allowed within elements, and how the elements are organized. An EML document is valid when it adheres to all of the requirements speficied in the EML schema. You’ll notice that some elements, rather than having a simple value, instead contain other elements that are nested within them. Let’s look at individualName. &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; This element is a collection of other elements (sometimes referred to as child elements). In this example, the individualName element has a givenName and a surName child element. We can check the requirements of any particular element by looking at the schema documents, which includes some helpful diagrams. The diagram in the individualName section of the schema looks like this: This shows that within individualName there are 3 possible child elements: salutation, givenName, and surName. The yellow circle icon with stacked papers tells you that the elements come in series, so you can include one or more of the child elements (as opposed to a switch symbol, which means that you choose one element from a list of options). The bold line tells you that surName is required, and the 0..inf indicates that you can include 0 or more salultation or givenName elements. So, to summarize, EML is the metadata standard used by the Arctic Data Center. It is written in XML, which consists of a series of nested elements. The element definitions, required elements, and structure are all defined by a schema. When you write EML, in order for it to be valid, your EML document must conform to the requirements given in the schema. 15.2.2.2 Metadata in R: a simple example Now, let’s learn how the EML package can help us create EML in R. First, load the EML package in R. library(EML) The EML package relies on named list structures to generate name-value pairs for elements. “Named list structures” may sound unfamiliar, but they aren’t dissimilar from data.frames. A data.frame is just a named list of vectors of the same length, where the name of the vector is the column name. In this section, we will use the familiar $ operator to dig down into the named list structure that we generate. To show how this works, let’s create the individualName element, and save it as an object called me. Remember the schema requirements - indvidualName has child elements salutation, givenName, and surName. At least surName is required. me &lt;- list(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;) me ## $givenName ## [1] &quot;Jeanette&quot; ## ## $surName ## [1] &quot;Clark&quot; So we have created the contents of an individualName element, with child elements givenName and surName, and assigned the values of those child elements to my name. This might look confusing, hard to remember, and if you aren’t intimitely familiar with the EML schema, you are probably feeling a little intimidated. Luckily the EML package has a set of helper list constructor functions which tell you what child elements can be used in a parent element. The helper functions have the format eml$elementName(). When combined with the RStudio autocomplete functionality, the whole process gets a lot easier! me &lt;- eml$individualName(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;) We can then use this object me, which represents the individualName element, and insert it into a larger EML document. At the top level of a complete EML document are the packageId and system elements. This is how your document can be uniquely identified within whatever system manages it. The packageId element typically contains the DOI (Digital Object Identifier) or other identifier for the dataset. Nested within the top level is the dataset element. All EML documents must have, at a minimum, a title, creator, and contact, in addition to the packageId and system. Let’s create a minimal valid EML dataset, with an arbitrary packageId and system. doc &lt;- list(packageId = &quot;dataset-1&quot;, system = &quot;local&quot;, dataset = eml$dataset(title = &quot;A minimial valid EML dataset&quot;, creator = eml$creator(individualName = me), contact = eml$contact(individualName = me))) Unlike the web editor, in R there is nothing stopping you from inserting arbitrary elements into your EML document. A critical step to creating EML is validating your EML document to make sure it conforms to the schema requirements. In R this can be done using the eml_validate function. eml_validate(doc) ## [1] TRUE ## attr(,&quot;errors&quot;) ## character(0) We can write our EML using write_eml. write_eml(doc, &quot;../files/simple_example.xml&quot;) Here is what the written file looks like: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:stmml=&quot;http://www.xml-cml.org/schema/stmml-1.1&quot; packageId=&quot;id&quot; system=&quot;system&quot; xsi:schemaLocation=&quot;eml://ecoinformatics.org/eml-2.1.1/ eml.xsd&quot;&gt; &lt;dataset&gt; &lt;title&gt;A minimal valid EML dataset&lt;/title&gt; &lt;creator&gt; &lt;individualName&gt; &lt;givenName&gt;Jeanette&lt;/givenName&gt; &lt;surName&gt;Clark&lt;/surName&gt; &lt;/individualName&gt; &lt;/creator&gt; &lt;contact&gt; &lt;individualName&gt; &lt;givenName&gt;Jeanette&lt;/givenName&gt; &lt;surName&gt;Clark&lt;/surName&gt; &lt;/individualName&gt; &lt;/contact&gt; &lt;/dataset&gt; &lt;/eml:eml&gt; 15.2.2.3 Validation Errors One of the hardest parts about creating EML in R is diagnosing validation errors. I won’t get into too much detail, but here is a simple example. The eml$... family of helpers can help prevent validation errors, since they have a set list of arguments which are allowed. Here, I bypass the eml$dataset() helper function to show what the error looks like. doc &lt;- list(packageId = &quot;dataset-1&quot;, system = &quot;local&quot;, dataset = list(title = &quot;A minimial valid EML dataset&quot;, creator = eml$creator(individualName = me), contact = eml$contact(individualName = me), arbitrary = &quot;This element isn&#39;t in the schema&quot;)) eml_validate(doc) ## [1] FALSE ## attr(,&quot;errors&quot;) ## [1] &quot;Element &#39;arbitrary&#39;: This element is not expected. Expected is one of ( references, alternateIdentifier, shortName, title ).&quot; This error essentially says that the element arbitrary is not expected, and gives you a hint of some elements that were expected. Validation errors can be tricky, especially when there are lots of them. Validate early and often! 15.2.2.4 Metadata in R: A more complete example As you might imagine, creating a complete metadata record like what is shown on this page would be pretty tedious if we did it just using the generic list or eml$... helpers, since the nesting structure can be very deep. The EML package has a set of higher level helper functions beginning with set_ that create some of the more complex elements. To demonstrate the use of these we are going to create an EML document that contains the following information: title creator and contact abstract methods geographic and temporal coverage description of a tabular data file and a script We will edit these elements using a mix of helpers and generic techniques. To get set up, navigate to this dataset and download the CSV file and the R script. Put them in a directory called files that is a sub-directory of the location of this RMarkdown file. 15.2.2.4.1 Title, creator, contact To start, lets create a basic EML skeleton using our example above, but with more information in the creator and contact information besides just my name. # eml creator and contact have identical schema requirements (both fall under `responsibleParty`) me &lt;- eml$creator(individualName = eml$individualName(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;), organizationName = &quot;National Center for Ecological Analysis and Synthesis&quot;, electronicMailAddress = &quot;jclark@nceas.ucsb.edu&quot;, userId = list(directory = &quot;https://orcid.org&quot;, userId = &quot;https://orcid.org/0000-0003-4703-1974&quot;)) doc &lt;- list(packageId = &quot;dataset-1&quot;, system = &quot;local&quot;, dataset = eml$dataset(title = &quot;A more robust valid EML dataset&quot;, creator = me, contact = me)) Because we have used the eml$dataset helper, all of the possible sub-elements have been populated in our EML document, allowing us to easily access and edit them using R autocomplete. 15.2.2.4.2 Abstract We can use this to dive down into sub-elements and edit them. Let’s do the abstract. This is a simple element so we can just assign the value of the abstract to a character string. doc$dataset$abstract &lt;- &quot;A brief but comprehensive description of the who, what, where, when, why, and how of my dataset.&quot; 15.2.2.4.3 Methods We can use the set_methods function to parse a markdown (or word) document and insert it into the methods section. This way of adding text is especially nice because it preserves formatting. doc$dataset$methods &lt;- set_methods(&quot;../files/methods.md&quot;) doc$dataset$methods ## $sampling ## NULL ## ## $methodStep ## $methodStep$instrumentation ## character(0) ## ## $methodStep$software ## NULL ## ## $methodStep$description ## $methodStep$description$section ## $methodStep$description$section[[1]] ## [1] &quot;&lt;title&gt;Data Collection&lt;/title&gt;\\n&lt;para&gt;\\n We collected some data and here is a description\\n &lt;/para&gt;&quot; ## ## $methodStep$description$section[[2]] ## [1] &quot;&lt;title&gt;Data processing&lt;/title&gt;\\n&lt;para&gt;\\n Here is how we processed the data\\n &lt;/para&gt;&quot; ## ## ## $methodStep$description$para ## list() 15.2.2.4.4 Coverage The geographic and temporal coverage can be set using set_coverage. doc$dataset$coverage &lt;- set_coverage(beginDate = 2001, endDate = 2010, geographicDescription = &quot;Alaska, United States&quot;, westBoundingCoordinate = -179.9, eastBoundingCoordinate = -120, northBoundingCoordinate = 75, southBoundingCoordinate = 55) 15.2.2.4.5 Data file: script Information about data files (or entity level information) can be added in child elements of the dataset element. Here we will use the element otherEntity (other options include spatialVector, spatialRaster, and dataTable) to represent the R script. First, some high level information. doc$dataset$otherEntity &lt;- eml$otherEntity(entityName = &quot;../files/datfiles_processing.R&quot;, entityDescription = &quot;Data processing script&quot;, entityType = &quot;application/R&quot;) We can use the set_physical helper to set more specific information about the file, like its size, delimiter, and checksum. This function automatically detects fundamental characteristics about the file if you give it a path to your file on your system. doc$dataset$otherEntity$physical &lt;- set_physical(&quot;../files/datfiles_processing.R&quot;) ## Automatically calculated file size using file.size(&quot;../files/datfiles_processing.R&quot;) ## Automatically calculated authentication size using digest::digest(&quot;../files/datfiles_processing.R&quot;, algo = &quot;md5&quot;, file = TRUE) 15.2.2.4.6 Data file: tabular Here we will use the element dataTable to describe the tabular data file. As before, we set the entityName, entityDescription, and the physical sections. doc$dataset$dataTable &lt;- eml$dataTable(entityName = &quot;../files/my-data.csv&quot;, entityDescription = &quot;Temperature data from in-situ loggers&quot;) doc$dataset$dataTable$physical &lt;- set_physical(&quot;../files/my-data.csv&quot;) ## Automatically calculated file size using file.size(&quot;../files/my-data.csv&quot;) ## Automatically calculated authentication size using digest::digest(&quot;../files/my-data.csv&quot;, algo = &quot;md5&quot;, file = TRUE) Next, perhaps the most important part of metadata, but frequently the most difficult to document in a metadata standard: attribute level information. An attribute is a variable in your dataset. For tabular data, this is information about columns within data tables, critical to understanding what kind of information is actually in the table! The set_attributes function will take a data.frame that gives required attribute information. This data.frame contains rows corresponding to column names (or attributes) in the dataset, and columns: attributeName (any text) attributeDefinition (any text) unit (required for numeric data, use get_unitList() to see a list of standard units) numberType (required for numeric data, one of: real, natural, whole, integer) formatString (required for dateTime data) definition (required for textDomain data) Two other sub-elements, the domain and measurementScale, can be inferred from the col_classes argument to set_attributes. Let’s create our attributes data.frame. atts &lt;- data.frame(attributeName = c(&quot;time&quot;, &quot;temperature&quot;, &quot;site&quot;), attributeDefinition = c(&quot;time of measurement&quot;, &quot;measured temperature in degrees Celsius&quot;, &quot;site identifier&quot;), unit = c(NA, &quot;celsius&quot;, NA), numberType = c(NA, &quot;real&quot;, NA), formatString = c(&quot;HH:MM:SS&quot;, NA, NA), definition = c(NA, NA, &quot;site identifier&quot;)) We will then use this in our set_attributes function, along with the col_classes argument, to generate a complete attributeList. doc$dataset$dataTable$attributeList &lt;- set_attributes(attributes = atts, col_classes = c(&quot;Date&quot;, &quot;numeric&quot;, &quot;character&quot;)) As you might imagine, this can get VERY tedious with wide data tables. The function shiny_attributes calls an interactive table that can not only automatically detect and attempt to fill in attribute information from a data.frame, but also helps with on the fly validation. Note: this requires that the shinyjs package is installed. atts_shiny &lt;- shiny_attributes(data = read.csv(&quot;../files/my-data.csv&quot;)) This produces a data.frame that you can insert into set_attributes as above. 15.2.2.4.7 Validating and writing the file Finally, we need to validate and write our file. eml_validate(doc) ## [1] TRUE ## attr(,&quot;errors&quot;) ## character(0) write_eml(doc, &quot;../files/complex_example.xml&quot;) 15.2.3 Publish data to the Arctic Data Center test site 15.2.3.1 Setup and Introduction Now let’s see how to use the dataone and datapack R packages to upload data to DataONE member repositories like the KNB Data Repository and the Arctic Data Center. 15.2.3.1.1 The dataone package The dataone R package provides methods to enable R scripts to interact with DataONE to search for, download, upload and update data and metadata. The purpose of uploading data from R is to automate the repetitive tasks for large datasets with many files. For small datasets, the web submission form will certainly be simpler. The dataone package interacts with two major parts of the DataONE infrastructure: Coordinating Nodes (or cn) and Member Nodes (or mn). Coordinating nodes maintain a complete catalog of all data and provide the core DataONE services, including search and discovery. The cn that is exposed through search.dataone.org is called the production (or PROD) cn. Member Nodes expose their data and metadata through a common set of interfaces and services. The Arctic Data Center is an mn. To post data to the Arctic Data Center, we need to interact with both the coordinating and member nodes. In addition to the production cn, there are also several test coordinating node environments, and corresponding test member node environments. In this tutorial, we will be posting data to the test Arctic Data Center environment. 15.2.3.1.2 The datapack package The datapack R package represents the set of files in a dataset as a datapack::DataPackage. This DataPackage is just a special R object class that is specified in the datapack package. Each object in that DataPackage is represented as a DataObject, another R object class specified by datapack. When you are publishing your dataset, ideally you aren’t only publishing a set of observations. There are many other artifacts of your research, such as scripts, derived data files, and derived figures, that should be archived in a data package. Each of the types of files shown in the workflow below should be considered a data object in your package, including the metadata file that describes the individual components of your workflow. Each of the files in the diagram above has a relationship with the other files, and using datapack you can describe these relationships explicitly and unambiguously using controlled vocabularies and conceptual frameworks. For example, we know that the fine “Image 1” was generated by “Mapping Script.” We also know that both “Image 1” and “Mapping Script” are described by the file “Metadata.” Both of these relationsips are represented in datapack using speficic ontologies. Using datapack we will create a DataPackage that represents a (very simple) scientific workflow and the data objects within it, where the relationships between those objects are explicitly described. Then, we will upload this DataPackage to a test version of the Arctic Data Center repository using dataone. Before uploading any data to a DataONE repository, you must login to get an authentication token, which is a character string used to identify yourself. This token can be retrieved by logging into the test repository and copying the token into your R session. 15.2.3.1.3 Obtain an ORCID ORCID is a researcher identifier that provides a common way to link your researcher identity to your articles and data. An ORCID is to a researcher as a DOI is to a research article. To obtain an ORCID, register at https://orcid.org. 15.2.3.1.4 Log in to the test repository and copy your token We will be using a test server, so login and retrieve your token at https://test.arcticdata.io. Once you are logged in, navigate to your Profile Settings, and locate the “Authentication Token” section, and then copy the token for R to your clipboard. Finally, paste the token into the R Console to register it as an option for this R session. You are now logged in. But note that you need to keep this token private; don’t paste it into scripts or check it into git, as it is just as sensitive as your password. 15.2.3.2 Uploading A Package Using R with uploadDataPackage Datasets and metadata can be uploaded individually or as a collection. Such a collection, whether contained in local R objects or existing on a DataONE repository, will be informally referred to as a package. Load the libraries: library(dataone) library(datapack) First we need to create an R object that describes what coordinating and member nodes we will be uploading our dataset to. We do this with the dataone function D1Client (DataONE client). The first argument specifies the DataONE coordinating node (in this case a test node called STAGING) and the second specifices the member node. We’ll also create an object that only represents the member node, which is helpful for some functions. d1c &lt;- D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) mn &lt;- d1c@mn Next, we create a DataPackage as a container for our data, metadata, and scripts using the new function. This just creates an empty object with the class DataPackage dp &lt;- new(&quot;DataPackage&quot;) dp ## This package does not contain any DataObjects. We then need to add a metadata file and data file to this package. First we generate some identifiers for the objects. We’ll use the uuid scheme for all of our objects. If we were uploading to production, you would likely want use an identifier with the doi scheme for your metadata document. data_id &lt;- generateIdentifier(mn, scheme = &quot;uuid&quot;) script_id &lt;- generateIdentifier(mn, scheme = &quot;uuid&quot;) metadata_id &lt;- generateIdentifier(mn, scheme = &quot;uuid&quot;) Now we need to modify our EML document to include these identifiers. This increases the accessibility of the files in our dataset. First read in the EML that we created earlier. doc &lt;- read_eml(&quot;../files/complex_example.xml&quot;) Let’s replace the arbitrary packageId and system that we set in the example above to reflect the identifier we created for this package, and the system we are uploading the package to. doc$packageId &lt;- metadata_id doc$system &lt;- mn@identifier Now let’s add a distribution URL to the physical section of our entity information. All of the distribution URLs look something like this https://test.arcticdata.io/metacat/d1/mn/v2/object/urn:uuid:A0cc95b46-a318-4dd1-8f8a-0e6c280e1d95, where https://test.arcticdata.io/metacat/d1/mn/v2/ is the member node end point, and urn:uuid:A0cc95b46-a318-4dd1-8f8a-0e6c280e1d95 is the identifier of the object. We can easily construct this URL using the paste0 function, and insert it into the physical section of the dataTable element. # set url for csv doc$dataset$dataTable$physical$distribution$online$url &lt;- paste0(mn@endpoint, &quot;object/&quot;, data_id) # set url for script doc$dataset$otherEntity$physical$distribution$online$url &lt;- paste0(mn@endpoint, &quot;object/&quot;, script_id) write_eml(doc, &quot;../files/complex_example.xml&quot;) Now we have a full metadata document ready to be uploaded. We now need to add our files to the DataPackage. First, let’s create a new DataObject, which is another object class specific to datapack. Our metadata file, data file, and script will all need to be added as a DataObject. Remember that all files in a package are considered data objects, not just the ones that we you traditionally think of as being “data”. The format argument specifies the type of file, and should match one of the list of DataONE formatIds (listed in the Id field) here. # Add the metadata document to the package metadataObj &lt;- new(&quot;DataObject&quot;, id = metadata_id, format =&quot;eml://ecoinformatics.org/eml-2.1.1&quot;, filename = &quot;../files/complex_example.xml&quot;) After creating the DataObject that represents the metadata file, we add it to the data package using addMember. dp &lt;- addMember(dp, metadataObj) dp ## Members: ## ## filename format mediaType size identifier modified local ## comple...le.xml eml...1.1 NA 6078 urn:uui...d0f7ffd1 n y ## ## Package identifier: NA ## RightsHolder: NA ## ## ## This package does not contain any provenance relationships. We add the data file and the script similarly. The only difference is in the addMember function we have to set the mo (metadata object) argument equal to the DataObject that represents the metadata file. Adding our csv file to the package this way not only adds the file to the data package, but it also specifies that the csv is described by the metadata. # Add our data file to the package sourceObj &lt;- new(&quot;DataObject&quot;, id = data_id, format = &quot;text/csv&quot;, filename = &quot;../files/my-data.csv&quot;) dp &lt;- addMember(dp, sourceObj, mo = metadataObj) dp ## Members: ## ## filename format mediaType size identifier modified local ## comple...le.xml eml...1.1 NA 6078 urn:uui...d0f7ffd1 n y ## my-data.csv text/csv NA 45 urn:uui...34a4a4aa n y ## ## Package identifier: NA ## RightsHolder: NA ## ## ## Relationships (updated): ## ## subject predicate object ## 1 complex_example.xml cito:documents my-data.csv ## 2 my-data.csv cito:isDocumentedBy complex_example.xml Next we add our script in the same way. # Add our script to the package scriptObj &lt;- new(&quot;DataObject&quot;, id = script_id, format = &quot;application/R&quot;, filename = &quot;../files/datfiles_processing.R&quot;) dp &lt;- addMember(dp, scriptObj, mo = metadataObj) dp ## Members: ## ## filename format mediaType size identifier modified local ## comple...le.xml eml...1.1 NA 6078 urn:uui...d0f7ffd1 n y ## my-data.csv text/csv NA 45 urn:uui...34a4a4aa n y ## datfil...sing.R app...n/R NA 5625 urn:uui...d7038ed9 n y ## ## Package identifier: NA ## RightsHolder: NA ## ## ## Relationships (updated): ## ## subject predicate object ## 1 complex_example.xml cito:documents my-data.csv ## 3 complex_example.xml cito:documents datfiles_processing.R ## 4 datfiles_processing.R cito:isDocumentedBy complex_example.xml ## 2 my-data.csv cito:isDocumentedBy complex_example.xml You can also specify other information about the relationships between files in your data package by adding provenance information. Here, we specify that the R script (program) uses the csv (sources) by including them as the specified arguments in the describeWorkflow function. dp &lt;- describeWorkflow(dp, sources = sourceObj, program = scriptObj) dp ## Members: ## ## filename format mediaType size identifier modified local ## comple...le.xml eml...1.1 NA 6078 urn:uui...d0f7ffd1 n y ## my-data.csv text/csv NA 45 urn:uui...34a4a4aa n y ## datfil...sing.R app...n/R NA 5625 urn:uui...d7038ed9 n y ## ## Package identifier: NA ## RightsHolder: NA ## ## ## Relationships (updated): ## ## subject predicate object ## 9 _e3641e2f-...b0d938f99f rdf:type prov:Association ## 8 _e3641e2f-...b0d938f99f prov:hadPlan datfiles_processing.R ## 1 complex_example.xml cito:documents my-data.csv ## 3 complex_example.xml cito:documents datfiles_processing.R ## 4 datfiles_processing.R cito:isDocumentedBy complex_example.xml ## 10 datfiles_processing.R rdf:type provone:Program ## 2 my-data.csv cito:isDocumentedBy complex_example.xml ## 5 my-data.csv rdf:type provone:Data ## 11 urn:uuid:6...b35073d80d dcterms:identifier urn:uuid:6...b35073d80d ## 7 urn:uuid:6...b35073d80d rdf:type provone:Execution ## 6 urn:uuid:6...b35073d80d prov:quali...ssociation _e3641e2f-...b0d938f99f ## 12 urn:uuid:6...b35073d80d prov:used my-data.csv Each object in a data package has an access policy. There are three levels of access you can grant either to individual files, or the package as a whole. read: the ability to view when published write: the ablity to edit after publishing changePermission: the ability to grant other people read, write, or changePermission access Here, I give my colleague (via his ORCID) “read” and “write” permission to my entire data package using addAccessRule. dp &lt;- addAccessRule(dp, subject = &quot;http://orcid.org/0000-0003-0077-4738&quot;, permission = c(&quot;read&quot;,&quot;write&quot;), getIdentifiers(dp)) Finally, we upload the package to the test server for the Arctic Data Center using uploadDataPackage. This function takes as arguments d1c (the DataONE client which specifies which coordinating and member nodes to use), dp (the data package itself), whether you want the package to include public read access (public = TRUE). packageId &lt;- uploadDataPackage(d1c, dp, public = TRUE) You can now search for and view the package at https://test.arcticdata.io: "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
