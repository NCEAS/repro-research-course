[["index.html", "Open Science Synthesis for the Delta Science Program: Week 1 Open Science Synthesis for the Delta Science Program: Week 1 0.1 Schedule", " Open Science Synthesis for the Delta Science Program: Week 1 September 13-17, 2021 Open Science Synthesis for the Delta Science Program: Week 1 This book is for week 1 of 3, one-week facilitated synthesis and training events for Delta researchers that will revolve around scientific computing and scientific software for reproducible science. Week 1 will focus on introducing concepts and tooling for conducting reproducible research synthesis projects. Groups will also work on synthesis development using a logic model framework. 0.1 Schedule 0.1.1 Code of Conduct Please note that by participating in an NCEAS activity you agree to abide by our Code of Conduct 0.1.2 Logistics 0.1.2.1 Overview Welcome to week 1 of Open Science Synthesis for the Delta Science Program! Although we have had to switch to a remote training model due to COVID-19, we ask participants for the same level of commitment in this remote setting, while understanding that some conflicts are unavoidable. We will be using the following tools to facilitate this training: Zoom (version 5.5.2+) Slack (desktop app preferred) RStudio, accessed through a server on an up-to-date web browser: Firefox (version 80+) Chrome (version 80+) Safari (version 13+) Edge (version 81+) 0.1.2.2 Server You should receive a separate email prompting you to change your password using the NCEAS account service. Please change your password, and then ensure that you can log in at https://included-crab.nceas.ucsb.edu/. 0.1.2.3 Monitors If you have a second monitor or second device, it would be useful for this training. You’ll need enough screen real estate to handle the primary Zoom window, the participant pane in Zoom, Slack, and a browser with tabs for RStudio and our training curriculum. We recommend either using two monitors, or joining the Zoom room from a second device. If you must be on one machine for everything, here’s an example of what it could look like when you are following along with the class and how your screen will shift when you have a more detailed question that requires breakout assistance. When we’re in session, please turn your camera on, and mute your microphone unless you would like to ask a question or contribute to a discussion. 0.1.2.4 Working from Home We recognize that working from home during the pandemic comes with challenges. The appearance or sound of other adults, children, and pets in remote meetings such as this is completely normal and understandable. Having your video on and enabling the instructors and your fellow participants to see you brings some humanity to this physically distant workshop, and we believe that this is a crucial element of its success. If you would like to use the Zoom virtual background feature to hide your surroundings, please do provided your background of choice fits within the code of conduct. 0.1.2.5 Non-Verbal Feedback We’ll be using the Zoom “Non Verbal Feedback” buttons throughout this course. We will ask you to put a green check by your name when you’re all set or you understand, and a red x by your name if you’re stuck or need assistance. These buttons can be found in the participants panel of the Zoom room. When you’re asked to answer using these buttons, please ensure that you select one so that the instructor has the feedback that they need to either continue the lesson or pause until everyone gets back on the same page. 0.1.2.6 Questions and Getting Help When you need to ask a question, please do so in one of the following ways: Turn your mic on and ask. If you are uncomfortable interrupting the instructor, you may also raise your virtual hand (in the participant panel) and the session facilitator will ask the instructor to pause and call upon you. Ask your question in the slack channel. If you have an issue/error and get stuck, you can ask for help in the following ways: Turn your mic on and ask for help. See also above regarding the use of a virtual raised hand. Let one of the instructors know in the slack channel. If prompted to do so, put a red X next to your name as your status in the participant window. When you have detailed questions or need one on one coding assistance, we will have zoom breakout rooms available with helpers. The helper will try to help you in Slack first. If the issue requires more in-depth troubleshooting, the helper will invite you to join their named Breakout Room. 0.1.2.7 The Power of Open To facilitate a lively and interactive learning environment, we’ll be calling on folks to share their code and to answer various questions posed by the instructor. It’s completely okay to say “Pass” or “I Don’t Know” - this is a supportive learning environment and we will all learn from each other. The instructors will be able to see your code as you go to help you if you get stuck, and the lead instructor may share participants’ code to show a successful example or illustrate a teaching moment. 0.1.3 About this book These written materials are the result of a continuous effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials. This work is licensed under a Creative Commons Attribution 4.0 International License. Citation: Jones, Matthew B., Amber E. Budden, Bryce Mecum, S. Jeanette Clark, Julien Brun, Julie Lowndes, and Erin McLean. 2021. Reproducible Research Techniques for Synthesis. NCEAS Learning Hub. Additional contributors: Ben Bolker, Stephanie Hampton, Samanta Katz, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Leah Wasser. "],["session-1-setup-and-introduction-to-rmarkdown.html", "1 Session 1: Setup and Introduction to RMarkdown 1.1 RStudio and Git/GitHub Setup 1.2 Introduction to R 1.3 Literate Analysis with RMarkdown", " 1 Session 1: Setup and Introduction to RMarkdown These chapters will help make sure your computer is set up appropriately for the workshop, and help anyone who is new to R or wants a base R refresher. Please email jclark@nceas.ucsb.edu with questions or issues. 1.1 RStudio and Git/GitHub Setup 1.1.1 Learning Objectives In this lesson, you will learn: How to check to make sure your RStudio environment is set up properly for analysis How to set up git 1.1.2 Logging into the RStudio server To help prevent us from spending most of this lesson remotely troubleshooting the myriad of issues that can arise when setting up the R, RStudio, and git environments, we have chosen to have everyone work on a remote server with all of the software you need installed. We will be using a special kind of RStudio just for servers called, aptly, RStudio Server. If you have never worked on a remove server before, you can think of it like working on a different computer via the internet. Note that the server has no knowledge of the files on your local filesystem, but it is easy to transfer files from the server to your local computer, and vice-versa, using the RStudio server interface. Here are the instructions for logging in and getting set up: Setup You should have received an email prompting you to change your password for your server account. If you did not, please let one of the helpers know on Slack. If you were able to successfully change your password, you can log in at: https://included-crab.nceas.ucsb.edu/ In this workshop, we are going to be using R projects to organize our work. An R project is tied to a directory on your local computer, and makes organizing your work and collaborating with others easier. We are going to be doing nearly all of the work in this course in one project. Our version of RStudio server allows you to share projects with others. Sharing your project with the instructors of the course will allow for them to jump into your session and type along with you, should you encounter an error you cannot fix. Setup In your RStudio server session, follow these steps to set up your shared project: In the “File” menu, select “New Project” Click “New Directory” Click “New Project” Under “Directory name” type: training_{USERNAME}, eg: training_jclark Leave “Create Project as subdirectory of:” set to ~ Click “Create Project” Your Rstudio should now open your project. To share your project with the instructor team, locate the “project switcher” dropdown menu in the upper right of your RStudio window. This dropdown has the name if your project (eg: training_clark), and a dropdown arrow. Click the dropdown menu, then “Share Project.” When the dialog box pops up, add the following usernames to your project: jclark aebudden jones brun Once those names show up in the list, click “OK”. Setting up git Before using git, you need to tell it who you are, also known as setting the global options. The only way to do this is through the command line. Newer versions of RStudio have a nice feature where you can open a terminal window in your RStudio session. Do this by selecting Tools -&gt; Terminal -&gt; New Terminal. A terminal tab should now be open where your console usually is. To set the global options, type the following into the command prompt, with your actual name, and press enter: git config --global user.name &quot;Matt Jones&quot; Note that if it ran successfully, it will look like nothing happened. We will check at the end to makre sure it worked. Next, enter the following line, with the email address you used when you created your account on github.com: git config --global user.email &quot;gitcode@magisa.org&quot; Note that these lines need to be run one at a time. Finally, check to make sure everything looks correct by entering these commands, which will return the options that you have set. git config --global user.name git config --global user.email 1.1.2.1 GitHub Authentication GitHub recently deprecated password authentication for accessing repositories, so we need to set up a secure way to authenticate. The book Happy git with R has a wealth of information related to working with git in R, and these instructions are based off of section 10.1. We will be using a PAT (Personal Access Token) in this course, because it is easy to set up. For better security and long term use, we recommend taking the extra steps to set up SSH keys. Steps: Run usethis::create_github_token() in the console In the browser window that pops up, scroll to the bottom and click “generate token.” You may need to log into GitHub first. Copy the token from the green box on the next page Back in RStudio, run credentials::set_github_pat() Paste your token into the dialog box that pops up. 1.1.3 Preparing to work in RStudio The default RStudio setup has a few panes that you will use. Here they are with their default locations: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) You can change the default location of the panes, among many other things: Customizing RStudio. One key question to ask whenever we open up RStudio is “where am I?” Because we like to work in RStudio projects, often this question is synonymous with “what project am I in?” In our setup we have already worked with R projects a little, but haven’t explained much about what they are or why we use them. An R project is really a special kind of working directory, which has its own workspace, history, and settings. Even though it isn’t much more than a special folder, it is a powerful way to organize your work. There are two places that can indicate what project we are in. The first is the project switcher menu in the upper right hand corner of your RStudio window. The second is the working directory path, in the top bar of your console. Note that by default, your working directory is set to the top level of your R project directory unless you change it using the setwd() function. About paths and working directories There are two types of paths in computing: absolute paths and relative paths. An absolute path always starts with the root of your file system and locates files from there. The absolute path to my R project directory is: /home/jclark/training_jclark Relative paths start from some location in your file system that is below the root. Relative paths are combined with the path of that location to locate files on your system. R (and some other languages like MATLAB) refer to the location where the relative path starts as our working directory. RStudio projects automatically set the working directory to the directory of the project. This means that you can reference files from within the project without worrying about where the project directory itself is. If I want to read in a file from the data directory within my project, I can simply type read.csv(\"data/samples.csv\") as opposed to read.csv(\"/home/jclark/training_jclark/data/samples.csv\") This is not only convenient for you, but also when working collaboratively. We will talk more about this later, but if Bryce makes a copy of my R project that I have published on GitHub, and I am using relative paths, he can run my code exactly as I have written it, without going back and changing \"/home/jclark/training_jclark/data/samples.csv\" to \"/home/mecum/training_jclark/data/samples.csv\" Note that once you start working in projects you should basically never need to run the setwd() command. If you are in the habit of doing this, stop and take a look at where and why you do it. Could leveraging the working directory concept of R projects eliminate this need? Almost definitely! Similarly, think about how you work with absolute paths. Could you leverage the working directory of your R project to replace these with relative paths and make your code more portable? Probably! Organizing your project When starting a new research project, one of the first things I do is set up an R project for it (just like we have here!) The next step is to then populate that project with relevant directories. There are many tools out there that can do this automatically. Some examples are rrtools or usethis::create_package(). The goal is to organize your project so that it is a compendium of your research. This means that the project has all of the digital parts needed to replicate your analysis, like code, figures, the manuscript, and data access. There are lots of good examples out there of research compendia. Here is one from a friend of NCEAS, Carl Boettiger, which he put together for a paper he wrote. The complexity of this project reflects years of work. Perhaps more representative of the situation we are in at the start of our course is a project that looks like this one, which we have just started at NCEAS. Currently, the only file in your project is your .Rproj file. This file is what contains the settings specific to your R project. We haven’t changed anything here yet, but will when we learn git. Summary organize your research into projects using R projects use R project working directories instead of setwd use relative paths from those working directories, not absolute paths structure your R project as a compendium 1.1.4 Setting up the R environment on your local computer R Version We will use R version 4.0.5, which you can download and install from CRAN. To check your version, run this in your RStudio console: R.version$version.string If you have R version 4.0.0 that will likely work fine as well. RStudio Version We will be using RStudio version 1.4 or later, which you can download and install here To check your RStudio version, run the following in your RStudio console: RStudio.Version()$version If the output of this does not say 1.4 or higher, you should update your RStudio. Do this by selecting Help -&gt; Check for Updates and follow the prompts. Package installation Run the following lines to check that all of the packages we need for the training are installed on your computer. packages &lt;- c(&quot;dplyr&quot;, &quot;tidyr&quot;, &quot;readr&quot;, &quot;devtools&quot;, &quot;usethis&quot;, &quot;roxygen2&quot;, &quot;leaflet&quot;, &quot;ggplot2&quot;, &quot;DT&quot;, &quot;scales&quot;, &quot;shiny&quot;, &quot;sf&quot;, &quot;ggmap&quot;, &quot;broom&quot;, &quot;captioner&quot;, &quot;MASS&quot;) for (package in packages) { if (!(package %in% installed.packages())) { install.packages(package) } } rm(packages) #remove variable from workspace # Now upgrade any out-of-date packages update.packages(ask=FALSE) If you haven’t installed all of the packages, this will automatically start installing them. If they are installed, it won’t do anything. Next, create a new R Markdown (File -&gt; New File -&gt; R Markdown). If you have never made an R Markdown document before, a dialog box will pop up asking if you wish to install the required packages. Click yes. At this point, RStudio and R should be all set up. Setting up git locally If you haven’t downloaded git already, you can do so here. If you haven’t already, go to github.com and create an account. Then you can follow the instructions that we used above to set your email address and user name. Note for Windows Users If you get “command not found” (or similar) when you try these steps through the RStudio terminal tab, you may need to set the type of terminal that gets launched by RStudio. Under some git install scenarios, the git executable may not be available to the default terminal type. Follow the instructions on the RStudio site for Windows specific terminal options. In particular, you should choose “New Terminals open with Git Bash” in the Terminal options (Tools-&gt;Global Options-&gt;Terminal). In addition, some versions of windows have difficulty with the command line if you are using an account name with spaces in it (such as “Matt Jones”, rather than something like “mbjones”). You may need to use an account name without spaces. Updating a previous R installation This is useful for users who already have R with some packages installed and need to upgrade R, but don’t want to lose packages. If you have never installed R or any R packages before, you can skip this section. If you already have R installed, but need to update, and don’t want to lose your packages, these two R functions can help you. The first will save all of your packages to a file. The second loads the packages from the file and installs packages that are missing. Save this script to a file (eg package_update.R). #&#39; Save R packages to a file. Useful when updating R version #&#39; #&#39; @param path path to rda file to save packages to. eg: installed_old.rda save_packages &lt;- function(path){ tmp &lt;- installed.packages() installedpkgs &lt;- as.vector(tmp[is.na(tmp[,&quot;Priority&quot;]), 1]) save(installedpkgs, file = path) } #&#39; Update packages from a file. Useful when updating R version #&#39; #&#39; @param path path to rda file where packages were saved update_packages &lt;- function(path){ tmp &lt;- new.env() installedpkgs &lt;- load(file = path, envir = tmp) installedpkgs &lt;- tmp[[ls(tmp)[1]]] tmp &lt;- installed.packages() installedpkgs.new &lt;- as.vector(tmp[is.na(tmp[,&quot;Priority&quot;]), 1]) missing &lt;- setdiff(installedpkgs, installedpkgs.new) install.packages(missing) update.packages(ask=FALSE) } Source the file that you saved above (eg: source(package_update.R)). Then, run the save_packages function. save_packages(&quot;installed.rda&quot;) Then quit R, go to CRAN, and install the latest version of R. Source the R script that you saved above again (eg: source(package_update.R)), and then run: update_packages(&quot;installed.rda&quot;) This should install all of your R packages that you had before you upgraded. 1.2 Introduction to R 1.2.1 Learning Objectives In this lesson we will: get oriented to the RStudio interface work with R in the console be introduced to built-in R functions learn to use the help pages 1.2.2 Introduction and Motivation There is a vibrant community out there that is collectively developing increasingly easy to use and powerful open source programming tools. The changing landscape of programming is making learning how to code easier than it ever has been. Incorporating programming into analysis workflows not only makes science more efficient, but also more computationally reproducible. In this course, we will use the programming language R, and the accompanying integrated development environment (IDE) RStudio. R is a great language to learn for data-oriented programming because it is widely adopted, user-friendly, and (most importantly) open source! So what is the difference between R and RStudio? Here is an analogy to start us off. If you were a chef, R is a knife. You have food to prepare, and the knife is one of the tools that you’ll use to accomplish your task. And if R were a knife, RStudio is the kitchen. RStudio provides a place to do your work! Other tools, communication, community, it makes your life as a chef easier. RStudio makes your life as a researcher easier by bringing together other tools you need to do your work efficiently - like a file browser, data viewer, help pages, terminal, community, support, the list goes on. So it’s not just the infrastructure (the user interface or IDE), although it is a great way to learn and interact with your variables, files, and interact directly with git. It’s also data science philosophy, R packages, community, and more. So although you can prepare food without a kitchen and we could learn R without RStudio, that’s not what we’re going to do. We are going to take advantage of the great RStudio support, and learn R and RStudio together. Something else to start us off is to mention that you are learning a new language here. It’s an ongoing process, it takes time, you’ll make mistakes, it can be frustrating, but it will be overwhelmingly awesome in the long run. We all speak at least one language; it’s a similar process, really. And no matter how fluent you are, you’ll always be learning, you’ll be trying things in new contexts, learning words that mean the same as others, etc, just like everybody else. And just like any form of communication, there will be miscommunications that can be frustrating, but hands down we are all better off because of it. While language is a familiar concept, programming languages are in a different context from spoken languages, but you will get to know this context with time. For example: you have a concept that there is a first meal of the day, and there is a name for that: in English it’s “breakfast”. So if you’re learning Spanish, you could expect there is a word for this concept of a first meal. (And you’d be right: ‘desayuno’). We will get you to expect that programming languages also have words (called functions in R) for concepts as well. You’ll soon expect that there is a way to order values numerically. Or alphabetically. Or search for patterns in text. Or calculate the median. Or reorganize columns to rows. Or subset exactly what you want. We will get you increase your expectations and learn to ask and find what you’re looking for. 1.2.2.1 Resources This lesson is a combination of excellent lessons by others. Huge thanks to Julie Lowndes for writing most of this content and letting us build on her material, which in turn was built on Jenny Bryan’s materials. I definitely recommend reading through the original lessons and using them as reference: Julie Lowndes’ Data Science Training for the Ocean Health Index R, RStudio, RMarkdown Programming in R Jenny Bryan’s lectures from STAT545 at UBC Here are some other resources that we like for learning R: Learn R in the console with swirl The Introduction to R lesson in Data Carpentry’s R for data analysis course The Stat 545 course materials The QCBS Introduction to R lesson (in French) Other resources: LaTeX Equation Formatting Base R Cheatsheet MATLAB/R Translation Cheat Sheet 1.2.3 R at the console Launch RStudio/R. Notice the default panes: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) FYI: you can change the default location of the panes, among many other things: Customizing RStudio. An important first question: where are we? If you’ve just opened RStudio for the first time, you’ll be in your Home directory. This is noted by the ~/ at the top of the console. You can see too that the Files pane in the lower right shows what is in the Home directory where you are. You can navigate around within that Files pane and explore, but note that you won’t change where you are: even as you click through you’ll still be Home: ~/. OK let’s go into the Console, where we interact with the live R process. We use R to calculate things for us, so let’s do some simple math. 3*4 ## [1] 12 You can assign the value of that mathematic operation to a variable, or object, in R. You do this using the assignment operator, &lt;-. Make an assignment and then inspect the object you just created. x &lt;- 3 * 4 x ## [1] 12 In my head I hear, e.g., “x gets 12”. All R statements where you create objects – “assignments” – have this form: objectName &lt;- value. I’ll write it in the console with a hash #, which is the way R comments so it won’t be evaluated. ## objectName &lt;- value ## This is also how you write notes in your code to explain what you are doing. Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a convention for demarcating words in names. # i_use_snake_case # other.people.use.periods # evenOthersUseCamelCase Make an assignment this_is_a_really_long_name &lt;- 2.5 To inspect this variable, instead of typing it, we can press the up arrow key and call your command history, with the most recent commands first. Let’s do that, and then delete the assignment: this_is_a_really_long_name ## [1] 2.5 Another way to inspect this variable is to begin typing this_…and RStudio will automagically have suggested completions for you that you can select by hitting the tab key, then press return. One more: science_rocks &lt;- &quot;yes it does!&quot; You can see that we can assign an object to be a word, not a number. In R, this is called a “string”, and R knows it’s a word and not a number because it has quotes \" \". You can work with strings in your data in R pretty easily, thanks to the stringr and tidytext packages. We won’t talk about strings very much specifically, but know that R can handle text, and it can work with text and numbers together. Strings and numbers lead us to an important concept in programming: that there are different “classes” or types of objects. An object is a variable, function, data structure, or method that you have written to your environment. You can see what objects you have loaded by looking in the “environment” pane in RStudio. The operations you can do with an object will depend on what type of object it is. This makes sense! Just like you wouldn’t do certain things with your car (like use it to eat soup), you won’t do certain operations with character objects (strings), for example. Try running the following line in your console: &quot;Hello world!&quot; * 3 What happened? Why? You may have noticed that when assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name: weight_kg &lt;- 55 # doesn&#39;t print anything (weight_kg &lt;- 55) # but putting parenthesis around the call prints the value of `weight_kg` ## [1] 55 weight_kg # and so does typing the name of the object ## [1] 55 Now that R has weight_kg in memory, we can do arithmetic with it. For instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg): 2.2 * weight_kg ## [1] 121 We can also change a variable’s value by assigning it a new one: weight_kg &lt;- 57.5 2.2 * weight_kg ## [1] 126.5 This means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a new variable, weight_lb: weight_lb &lt;- 2.2 * weight_kg and then change weight_kg to 100. weight_kg &lt;- 100 What do you think is the current content of the object weight_lb? 126.5 or 220? Why? You can also store more than one value in a single object. Storing a series of weights in a single object is a convenient way to perform the same operation on multiple values at the same time. One way to create such an object is the function c(), which stands for combine or concatenate. Here we will create a vector of weights in kilograms, and convert them to pounds, saving the weight in pounds as a new object. weight_kg &lt;- c(55, 25, 12) weight_kg ## [1] 55 25 12 weight_lb &lt;- weight_kg * 2.2 weight_lb ## [1] 121.0 55.0 26.4 1.2.3.1 Error messages are your friends Implicit contract with the computer/scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Pay attention to how you type. Remember that this is a language, not unsimilar to English! There are times you aren’t understood – it’s going to happen. There are different ways this can happen. Sometimes you’ll get an error. This is like someone saying ‘What?’ or ‘Pardon’? Error messages can also be more useful, like when they say ‘I didn’t understand this specific part of what you said, I was expecting something else’. That is a great type of error message. Error messages are your friend. Google them (copy-and-paste!) to figure out what they mean. And also know that there are errors that can creep in more subtly, without an error message right away, when you are giving information that is understood, but not in the way you meant. Like if I’m telling a story about tables and you’re picturing where you eat breakfast and I’m talking about data. This can leave me thinking I’ve gotten something across that the listener (or R) interpreted very differently. And as I continue telling my story you get more and more confused… So write clean code and check your work as you go to minimize these circumstances! 1.2.3.2 Logical operators and expressions A moment about logical operators and expressions. We can ask questions about the objects we just made. == means ‘is equal to’ != means ‘is not equal to’ &lt; means ` is less than’ &gt; means ` is greater than’ &lt;= means ` is less than or equal to’ &gt;= means ` is greater than or equal to’ weight_kg == 2 ## [1] FALSE FALSE FALSE weight_kg &gt;= 30 ## [1] TRUE FALSE FALSE weight_kg != 5 ## [1] TRUE TRUE TRUE Shortcuts You will make lots of assignments and the operator &lt;- is a pain to type. Don’t be lazy and use =, although it would work, because it will just sow confusion later. Instead, utilize RStudio’s keyboard shortcut: Alt + - (the minus sign). Notice that RStudio automagically surrounds &lt;- with spaces, which demonstrates a useful code formatting practice. Code is miserable to read on a good day. Give your eyes a break and use spaces. RStudio offers many handy keyboard shortcuts. Also, Alt+Shift+K brings up a keyboard shortcut reference card. 1.2.3.3 Clearing the environment Now look at the objects in your environment (workspace) – in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with a few different R commands: objects() ## [1] &quot;science_rocks&quot; &quot;this_is_a_really_long_name&quot; ## [3] &quot;weight_kg&quot; &quot;weight_lb&quot; ## [5] &quot;x&quot; ls() ## [1] &quot;science_rocks&quot; &quot;this_is_a_really_long_name&quot; ## [3] &quot;weight_kg&quot; &quot;weight_lb&quot; ## [5] &quot;x&quot; If you want to remove the object named weight_kg, you can do this: rm(weight_kg) To remove everything: rm(list = ls()) or click the broom in RStudio’s Environment pane. 1.2.4 R functions, help pages So far we’ve learned some of the basic syntax and concepts of R programming, and how to navigate RStudio, but we haven’t done any complicated or interesting programming processes yet. This is where functions come in! A function is a way to group a set of commands together to undertake a task in a reusable way. When a function is executed, it produces a return value. We often say that we are “calling” a function when it is executed. Functions can be user defined and saved to an object using the assignment operator, so you can write whatever functions you need, but R also has a mind-blowing collection of built-in functions ready to use. To start, we will be using some built in R functions. All functions are called using the same syntax: function name with parentheses around what the function needs in order to do what it was built to do. The pieces of information that the function needs to do its job are called arguments. So the syntax will look something like: result_value &lt;- function_name(argument1 = value1, argument2 = value2, ...). 1.2.4.1 A simple example To take a very simple example, let’s look at the mean() function. As you might expect, this is a function that will take the mean of a set of numbers. Very convenient! Let’s create our vector of weights again: weight_kg &lt;- c(55, 25, 12) and use the mean function to calculate the mean weight. mean(weight_kg) ## [1] 30.66667 1.2.4.2 Getting help What if you know the name of the function that you want to use, but don’t know exactly how to use it? Thankfully RStudio provides an easy way to access the help documentation for functions. To access the help page for mean, enter the following into your console: ?mean The help pane will show up in the lower right hand corner of your RStudio. The help page is broken down into sections: Description: An extended description of what the function does. Usage: The arguments of the function(s) and their default values. Arguments: An explanation of the data each argument is expecting. Details: Any important details to be aware of. Value: The data the function returns. See Also: Any related functions you might find useful. Examples: Some examples for how to use the function. 1.2.4.3 Your turn Exercise: Talk to your neighbor(s) and look up the help file for a function that you know or expect to exist. Here are some ideas: ?getwd(), ?plot(), min(), max(), ?log()). And there’s also help for when you only sort of remember the function name: double-questionmark: ??install Not all functions have (or require) arguments: date() ## [1] &quot;Mon Sep 13 04:39:35 2021&quot; 1.2.4.4 Use a function to read a file into R So far we have learned how to assign values to objects in R, and what a function is, but we haven’t quite put it all together yet with real data yet. To do this, we will introduce the function read.csv, which will be in the first lines of many of your future scripts. It does exactly what it says, it reads in a csv file to R. Since this is our first time using this function, first access the help page for read.csv. This has a lot of information in it, as this function has a lot of arguments, and the first one is especially important - we have to tell it what file to look for. Let’s get a file! 1.2.4.4.1 Download a file from the Arctic Data Center Follow these steps to get set up for the next exercise: Navigate to this dataset by Craig Tweedie that is published on the Arctic Data Center. Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X. Download the first csv file called “BGchem2008data.csv” by clicking the “download” button next to the file. Move this file from your Downloads folder into a place you can more easily find it. Eg: a folder called data in your previously-created directory arctic_training_files. Now we have to tell read.csv how to find the file. We do this using the file argument which you can see in usage section in the help page. In R, you can either use absolute paths (which will start with your home directory ~/) or paths relative to your current working directory. RStudio has some great autocomplete capabilities when using relative paths, so we will go that route. Assuming you have moved your file to a folder within arctic_training_files called data, and your working directory is your home directory (~/) your read.csv call will look like this: bg_chem &lt;- read.csv(&quot;Documents/arctic_training_files/data/BGchem2008data.csv&quot;) You should now have an object of the class data.frame in your environment called bg_chem. Check your environment pane to ensure this is true. Note that in the help page there are a whole bunch of arguments that we didn’t use in the call above. Some of the arguments in function calls are optional, and some are required. Optional arguments will be shown in the usage section with a name = value pair, with the default value shown. If you do not specify a name = value pair for that argument in your function call, the function will assume the default value (example: header = TRUE for read.csv). Required arguments will only show the name of the argument, without a value. Note that the only required argument for read.csv is file. You can always specify arguments in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want file = \"data/BGchem2008data.csv\", since file is the first argument. If we wanted to add another argument, say stringsAsFactors, we need to specify it explicitly using the name = value pair, since the second argument is header. For functions I call often, I use this resolve by position for the first argument or maybe the first two. After that, I always use name = value. Many R users (including myself) will override the default stringsAsFactors argument using the following call: bg_chem &lt;- read.csv(&quot;Documents/arctic_training_files/data/BGchem2008data.csv&quot;, stringsAsFactors = FALSE) 1.2.5 Using data.frames A data.frame is a two dimensional data structure in R that mimics spreadsheet behavior. It is a collection of rows and columns of data, where each column has a name and represents a variable, and each row represents a measurement of that variable. When we ran read.csv, the object bg_chem that we created is a data.frame. There are a a bunch of ways R and RStudio help you explore data frames. Here are a few, give them each a try: click on the word bg_chem in the environment pane click on the arrow next to bg_chem in the environment pane execute head(bg_chem) in the console execute View(bg_chem) in the console Usually we will want to run functions on individual columns in a data.frame. To call a specific column, we use the list subset operator $. Say you want to look at the first few rows of the Date column only. This would do the trick: head(bg_chem$Date) ## [1] &quot;2008-03-21&quot; &quot;2008-03-21&quot; &quot;2008-03-21&quot; &quot;2008-03-21&quot; &quot;2008-03-21&quot; ## [6] &quot;2008-03-22&quot; How about calculating the mean temperature of all the CTD samples? mean(bg_chem$CTD_Temperature) ## [1] -0.9646915 Or, if we want to save this to a variable to use later: mean_temp &lt;- mean(bg_chem$CTD_Temperature) You can also create basic plots using the list subset operator. plot(bg_chem$CTD_Depth, bg_chem$CTD_Temperature) There are many more advancted tools and functions in R that will enable you to make better plots using cleaner syntax, we will cover some of these later in the course. 1.2.5.1 Your Turn Exercise: Spend a few minutes exploring this dataset. Try out different functions on columns using the list subset operator and experiment with different plots. 1.2.5.2 I just entered a command and nothing is happening It may be because you didn’t complete a command: is there a little + in your console? R is saying that it is waiting for you to finish. In the example below, I need to close that parenthesis. &gt; x &lt;- seq(1, 10 + You can either just type the closing parentheses here and push return, or push the esc button twice. 1.2.5.3 R says my object is not found New users will frequently see errors that look like this: Error in mean(myobject) : object 'myobject' not found This means that you do not have an object called myobject saved in your environment. The common reasons for this are: typo: make sure your object name is spelled exactly like what shows up in the console. Remember R is case sensitive. not writing to a variable: note that the object is only saved in the environment if you use the assignment operator, eg: myobject &lt;- read.csv(...) not executing the line in your script: remember that writing a line of code in a script or RMarkdown document is not the same as writing in the console, you have to execute the line of code using command + enter or using one of the several ways in the RStudio graphical user interface. 1.3 Literate Analysis with RMarkdown 1.3.1 Learning Objectives In this lesson we will: explore an example of RMarkdown as literate analysis learn markdown syntax write and run R code in RMarkdown build and knit an example document 1.3.2 Introduction and motivation The concept of literate analysis dates to a 1984 article by Donald Knuth. In this article, Knuth proposes a reversal of the programming paradigm. Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do. If our aim is to make scientific research more transparent, the appeal of this paradigm reversal is immediately apparent. All too often, computational methods are written in such a way as to be borderline incomprehensible - even to the person who originally wrote the code! The reason for this is obvious, computers interpret information very differently than people do. By switching to a literate analysis model, you help enable human understanding of what the computer is doing. As Knuth describes, in the literate analysis model, the author is an “essayist” who chooses variable names carefully, explains what they mean, and introduces concepts in the analysis in a way that facilitates understanding. RMarkdown is an excellent way to generate literate analysis, and a reproducible workflow. RMarkdown is a combination of two things - R, the programming language, and markdown, a set of text formatting directives. In R, the language assumes that you are writing R code, unless you specify that you are writing prose (using a comment, designated by #). The paradigm shift of literate analysis comes in the switch to RMarkdown, where instead of assuming you are writing code, Rmarkdown assumes that you are writing prose unless you specify that you are writing code. This, along with the formatting provided by markdown, encourages the “essayist” to write understandable prose to accompany the code that explains to the human-beings reading the document what the author told the computer to do. This is in contrast to writing just R code, where the author telling to the computer what to do with maybe a smattering of terse comments explaining the code to a reader. Before we dive in deeper, let’s look at an example of what literate analysis with RMarkdown can look like using a real example. Here is an example of a real analysis workflow written using RMarkdown. There are a few things to notice about this document, which assembles a set of similar data sources on salmon brood tables with different formatting into a single data source. It introduces the data sources using in-line images, links, interactive tables, and interactive maps. An example of data formatting from one source using R is shown. The document executes a set of formatting scripts in a directory to generate a single merged file. Some simple quality checks are performed (and their output shown) on the merged data. Simple analysis and plots are shown. In addition to achieving literate analysis, this document also represents a reproducible analysis. Because the entire merging and quality control of the data is done using the R code in the RMarkdown, if a new data source and formatting script are added, the document can be run all at once with a single click to re-generate the quality control, plots, and analysis of the updated data. RMarkdown is an amazing tool to use for collaborative research, so we will spend some time learning it well now, and use it through the rest of the course. Setup Open a new RMarkdown file using the following prompts: File -&gt; New File -&gt; RMarkdown A popup window will appear. You can just click the OK button here, or give your file a new title if you wish. Leave the output format as HTML. 1.3.3 Basic RMarkdown syntax The first thing to notice is that by opening a file, we are seeing the 4th pane of the RStudio console, which is essentially a text editor. Let’s have a look at this file — it’s not blank; there is some initial text already provided for you. Notice a few things about it: There are white and grey sections. R code is in grey sections, and other text is in white. Let’s go ahead and “Knit” the document by clicking the blue yarn at the top of the RMarkdown file. When you first click this button, RStudio will prompt you to save this file. Save it in the top level of your home directory on the server, and name it something that you will remember (like rmarkdown-intro.Rmd). What do you notice between the two? First, the knit process produced a second file (an HTML file) that popped up in a second window. You’ll also see this file in your directory with the same name as your Rmd, but with the html extension. In it’s simplest format, RMarkdown files come in pairs - the RMarkdown file, and its rendered version. In this case, we are knitting, or rendering, the file into HTML. You can also knit to PDF or Word files. Notice how the grey R code chunks are surrounded by 3 backticks and {r LABEL}. These are evaluated and return the output text in the case of summary(cars) and the output plot in the case of plot(pressure). The label next to the letter r in the code chunk syntax is a chunk label - this can help you navigate your RMarkdown document using the dropdown menu at the bottom of the editor pane. Notice how the code plot(pressure) is not shown in the HTML output because of the R code chunk option echo = FALSE. RMarkdown has lots of chunk options, including ones that allow for code to be run but not shown (echo = FALSE), code to be shown but not run (eval = FALSE), code to be run, but results not shown (results = 'hide'), or any combination of those. Before we get too deeply into the R side, let’s talk about Markdown. Markdown is a formatting language for plain text, and there are only around 15 rules to know. Notice the syntax in the document we just knitted: headers get rendered at multiple levels: #, ## bold: **word** There are some good cheatsheets to get you started, and here is one built into RStudio: Go to Help &gt; Markdown Quick Reference . Important: note that the hash symbol # is used differently in Markdown and in R: in R, a hash indicates a comment that will not be evaluated. You can use as many as you want: # is equivalent to ######. It’s just a matter of style. in Markdown, a hash indicates a level of a header. And the number you use matters: # is a “level one header”, meaning the biggest font and the top of the hierarchy. ### is a level three header, and will show up nested below the # and ## headers. Challenge In Markdown, Write some italic text, make a numbered list, and add a few sub-headers. Use the Markdown Quick Reference (in the menu bar: Help &gt; Markdown Quick Reference). Re-knit your html file and observe your edits. 1.3.3.1 New Rmarkdown editing tools The new version of RStudio (1.4) has a ‘what you see is what you get’ (wysiwyg) editor, which can be a nice way to write markdown without remembering all of the markdown rules. Since there aren’t many rules for markdown, I recommend just learning them - especially since markdown is used in many, many other contexts besides RMarkdown (formatting GitHub comments, for example). To access the editor, click the compass icon in the upper right hand corner of your editor pane. You’ll notice that your document is now formatted as you type, and you can change elements of the formatting using the row of icons in the top of the editor pane. Although I don’t really recommend doing all of your markdown composition in this format, there are two features to this editor that I find immensely helpful - adding citations, and adding tables. From the insert drop down, select “citation.” In the window that appears, there are several options in the left hand panel for the source of your citation. If you had a citation manager, such as Zotero, installed, this would be included in that list. For now, select “From DOI”, and in the search bar enter a DOI of your choice (eg: 10.1038/s41467-020-17726-z), then select “Insert.” After selecting insert, a couple of things happen. First, the citation reference is inserted into your markdown text as [@oke2020]. Second, a file called references.bib containing the BibTex format of the citation is created. Third, that file is added to the YAML header of your RMarkdown document (bibliography: references.bib). Adding another citation will automatically update your references.bib file. So easy! The second task that the markdown editor is convenient for is generating tables. Markdown tables are a bit finicky and annoying to type, and there are a number of formatting options that are difficult to remember if you don’t use them often. In the top icon bar, the “table” drop down gives several options for inserting, editing, and formatting tables. Experiment with this menu to insert a small table. 1.3.4 Code chunks Next, do what I do every time I open a new RMarkdown: delete everything below the “setup chunk” (line 10). The setup chunk is the one that looks like this: knitr::opts_chunk$set(echo = TRUE) This is a very useful chunk that will set the default R chunk options for your entire document. I like keeping it in my document so that I can easily modify default chunk options based on the audience for my RMarkdown. For example, if I know my document is going to be a report for a non-technical audience, I might set echo = FALSE in my setup chunk, that way all of the text, plots, and tables appear in the knitted document. The code, on the other hand, is still run, but doesn’t display in the final document. Now let’s practice with some R chunks. You can Create a new chunk in your RMarkdown in one of these ways: click “Insert &gt; R” at the top of the editor pane type by hand ```{r} ``` use the keyboard shortcut Command + Option + i (for windows, Ctrl + Alt + i) Now, let’s write some R code. x &lt;- 4*3 x Hitting return does not execute this command; remember, it’s just a text file. To execute it, we need to get what we typed in the the R chunk (the grey R code) down into the console. How do we do it? There are several ways (let’s do each of them): copy-paste this line into the console (generally not recommended as a primary method) select the line (or simply put the cursor there), and click ‘Run’. This is available from the bar above the file (green arrow) the menu bar: Code &gt; Run Selected Line(s) keyboard shortcut: command-return click the green arrow at the right of the code chunk Challenge Add a few more commands to your code chunk. Execute them by trying the three ways above. Question: What is the difference between running code using the green arrow in the chunk and the command-return keyboard shortcut? 1.3.5 Literate analysis practice Now that we have gone over the basics, let’s go a little deeper by building a simple, small RMarkdown document that represents a literate analysis using real data. Setup Navigate to the following dataset: https://doi.org/10.18739/A25T3FZ8X Download the file “BGchem2008data.csv” Click the “Upload” button in your RStudio server file browser. In the dialog box, make sure the destination directory is the data directory in your R project, click “choose file,” and locate the BGchem2008data.csv file. Press “ok” to upload the file. 1.3.5.1 Developing code in RMarkdown Experienced R users who have never used RMarkdown often struggle a bit in the transition to developing analysis in RMarkdown - which makes sense! It is switching the code paradigm to a new way of thinking. Rather than starting an R chunk and putting all of your code in that single chunk, here I describe what I think is a better way. Open a document and block out the high-level sections you know you’ll need to include using top level headers. Add bullet points for some high level pseudo-code steps you know you’ll need to take. Start filling in under each bullet point the code that accomplishes each step. As you write your code, transform your bullet points into prose, and add new bullet points or sections as needed. For this mini-analysis, we will just have the following sections and code steps: Introduction read in data Analysis calculate summary statistics calculate mean Redfield ratio plot Redfield ratio Challenge Create the ‘outline’ of your document with the information above. Top level bullet points should be top level sections. The second level points should be a list within each section. Next, write a sentence saying where your dataset came from, including a hyperlink, in the introduction section. Hint: Navigate to Help &gt; Markdown Quick Reference to lookup the hyperlink syntax. 1.3.5.2 Read in the data Now that we have outlined our document, we can start writing code! To read the data into our environment, we will use a function from the readr package. R packages are the building blocks of computational reproducibility in R. Each package contains a set of related functions that enable you to more easily do a task or set of tasks in R. There are thousands of community-maintained packages out there for just about every imaginable use of R - including many that you have probably never thought of! To install a package, we use the syntax install.packages('packge_name'). A package only needs to be installed once, so this code can be run directly in the console if needed. To use a package in our analysis, we need to load it into our environment using library(package_name). Even though we have installed it, we haven’t yet told our R session to access it. Because there are so many packages (many with conflicting namespaces) R cannot automatically load every single package you have installed. Instead, you load only the ones you need for a particular analysis. Loading the package is a key part of the reproducible aspect of our Rmarkdown, so we will include it as an R chunk. It is generally good practice to include all of your library calls in a single, dedicated R chunk near the top of your document. This lets collaborators know what packages they might need to install before they start running your code. You should have already installed readr as part of the setup for this course, so add a new R chunk below your setup chunk that calls the readr library, and run it. It should look like this: library(readr) Now, below the introduction that you wrote, add a chunk that uses the read_csv function to read in your data file. About RMarkdown paths In computing, a path specifies the unique location of a file on the filesystem. A path can come in one of two forms: absolute or relative. Absolute paths start at the very top of your file system, and work their way down the directory tree to the file. Relative paths start at an arbitrary point in the file system. In R, this point is set by your working directory. RMarkdown has a special way of handling relative paths that can be very handy. When working in an RMarkdown document, R will set all paths relative to the location of the RMarkdown file. This way, you don’t have to worry about setting a working directory, or changing your colleagues absolute path structure with the correct user name, etc. If your RMarkdown is stored near where the data it analyses are stored (good practice, generally), setting paths becomes much easier! If you saved your “BGchem2008data.csv” data file in the same location as your Rmd, you can just write the following to read it in. The help page (?read_csv, in the console) for this function tells you that the first argument should be a pointer to the file. Rstudio has some nice helpers to help you navigate paths. If you open quotes and press ‘tab’ with your cursor between the quotes, a popup menu will appear showing you some options. bg_chem &lt;- read_csv(&quot;../data/BGchem2008data.csv&quot;) Parsed with column specification: cols( Date = col_date(format = &quot;&quot;), Time = col_datetime(format = &quot;&quot;), Station = col_character(), Latitude = col_double(), Longitude = col_double(), Target_Depth = col_double(), CTD_Depth = col_double(), CTD_Salinity = col_double(), CTD_Temperature = col_double(), Bottle_Salinity = col_double(), d18O = col_double(), Ba = col_double(), Si = col_double(), NO3 = col_double(), NO2 = col_double(), NH4 = col_double(), P = col_double(), TA = col_double(), O2 = col_double() ) Warning messages: 1: In get_engine(options$engine) : Unknown language engine &#39;markdown&#39; (must be registered via knit_engines$set()). 2: Problem with `mutate()` input `Lower`. ℹ NAs introduced by coercion ℹ Input `Lower` is `as.integer(Lower)`. 3: In mask$eval_all_mutate(dots[[i]]) : NAs introduced by coercion If you run this line in your RMarkdown document, you should see the bg_chem object populate in your environment pane. It also spits out lots of text explaining what types the function parsed each column into. This text is important, and should be examined, but we might not want it in our final document. Challenge Use one of two methods to figure out how to suppress warning and message text in your chunk output: The gear icon in the chunk, next to the play button The RMarkdown reference guide (also under Help &gt; Cheatsheets) Aside Why not use read.csv from base R? We chose to show read_csv from the readr package for a few reasons. One is to introduce the concept of packages and showing how to load them, but read_csv has several advantages over read.csv. more reasonable function defaults (no stringsAsFactors!) smarter column type parsing, especially for dates it is much faster than read.csv, which is helpful for large files 1.3.5.3 Calculate Summary Statistics As our “analysis” we are going to calculate some very simple summary statistics and generate a single plot. In this dataset of oceanographic water samples, we will be examining the ratio of nitrogen to phosphate to see how closely the data match the Redfield ratio, which is the consistent 16:1 ratio of nitrogen to phosphorous atoms found in marine phytoplankton. Under the appropriate bullet point in your analysis section, create a new R chunk, and use it to calculate the mean nitrate (NO3), nitrite (NO2), ammonium (NH4), and phosphorous (P) measured. Save these mean values as new variables with easily understandable names, and write a (brief) description of your operation using markdown above the chunk. nitrate &lt;- mean(bg_chem$NO3) nitrite &lt;- mean(bg_chem$NO2) amm &lt;- mean(bg_chem$NH4) phos &lt;- mean(bg_chem$P) In another chunk, use those variables to calculate the nitrogen:phosphate ratio (Redfield ratio). ratio &lt;- (nitrate + nitrite + amm)/phos You can access this variable in your Markdown text by using R in-line in your text. The syntax to call R in-line (as opposed to as a chunk) is a single backtick `, the letter “r”, whatever your simple R command is - here we will use round(ratio) to print the calculated ratio, and a closing backtick `. So: ` 6 `. This allows us to access the value stored in this variable in our explanatory text without resorting to the evaluate-copy-paste method so commonly used for this type of task. The text as it looks in your RMrakdown will look like this: The Redfield ratio for this dataset is approximately `r round(ratio)`. And the rendered text like this: The Redfield ratio for this dataset is approximately 6. Finally, create a simple plot using base R that plots the ratio of the individual measurements, as opposed to looking at mean ratio. plot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4) Challenge Decide whether or not you want the plotting code above to show up in your knitted document along with the plot, and implement your decision as a chunk option. “Knit” your RMarkdown document (by pressing the Knit button) to observe the results. Aside How do I decide when to make a new chunk? Like many of life’s great questions, there is no clear cut answer. My preference is to have one chunk per functional unit of analysis. This functional unit could be 50 lines of code or it could be 1 line, but typically it only does one “thing.” This could be reading in data, making a plot, or defining a function. It could also mean calculating a series of related summary statistics (as above). Ultimately the choice is one related to personal preference and style, but generally you should ensure that code is divided up such that it is easily explainable in a literate analysis as the code is run. 1.3.6 RMarkdown and environments Let’s walk through an exercise with the document you built together to demonstrate how RMarkdown handles environments. We will be deliberately inducing some errors here for demonstration purposes. First, follow these steps: Restart your R session (Session &gt; Restart R) Run the last chunk in your Rmarkdown by pressing the play button on the chunk Perhaps not surprisingly, we get an error: Error in plot(bg_chem$P, bg_chem$NO2 + bg_chem$NO3 + bg_chem$NH4) : object &#39;bg_chem&#39; not found This is because we have not run the chunk of code that reads in the bg_chem data. The R part of Rmarkdown works just like a regular R script. You have to execute the code, and the order that you run it in matters. It is relatively easy to get mixed up in a large RMarkdown document - running chunks out of order, or forgetting to run chunks. To resolve this, follow the next step: Select from the “Run” menu (top right of Rmarkdown editor) “Restart R and run all chunks” Observe the bg_chem variable in your environment. This is one of my favorite ways to reset and re-run my code when things seem to have gone sideways. This is great practice to do periodically since it helps ensure you are writing code that actually runs. For the next demonstration: Restart your R session (Session &gt; Restart R) Press Knit to run all of the code in your document Observe the state of your environment pane Assuming your document knitted and produced an html page, your code ran. Yet the environment pane is empty. What happened? The Knit button is rather special - it doesn’t just run all of the code in your document. It actually spins up a fresh R environment separate from the one you have been working in, runs all of the code in your document, generates the output, and then closes the environment. This is one of the best ways RMarkdown helps ensure you have built a reproducible workflow. If, while you were developing your code, you ran a line in the console as opposed to adding it to your RMarkdown document, the code you develop while working actively in your environment will still work. However, when you knit your document, the environment RStudio spins up doesn’t know anything about that working environment you were in. Thus, your code may error because it doesn’t have that extra piece of information. Commonly, library calls are the source of this kind of frustration when the author runs it in the console, but forgets to add it to the script. To further clarify the point on environments, perform the following steps: Select from the “Run” menu (top right of Rmarkdown editor) “Run All” Observe all of the variables in your environment. Aside What about all my R scripts? Some pieces of R code are better suited for R scripts than RMarkdown. A function you wrote yourself that you use in many different analyses is probably better to define in an R script than repeated across many RMarkdown documents. Some analyses have mundane or repetitive tasks that don’t need to be explained very much. For example, in the document shown in the beginning of this lesson, 15 different excel files needed to be reformatted in slightly different, mundane ways, like renaming columns and removing header text. Instead of including these tasks in the primary markdown, I instead chose to write one R script per file and stored them all in a directory. I took the contents of one script and included it in my literate analysis, using it as an example to explain what the scripts did, and then used the source function to run them all from within my RMarkdown. So, just because you know RMarkdown now, doesn’t mean you won’t be using R scripts anymore. Both .R and .Rmd have their roles to play in analysis. With practice, it will become more clear what works well in RMarkdown, and what belongs in a regular R script. 1.3.7 Go Further Create an RMarkdown document with some of your own data. If you don’t have a good dataset handy, use the example dataset here: Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X. Your document might contain the following sections: Introduction to your dataset Include an external link Simple analysis Presentation of a result A table An in-line R command 1.3.8 Resources RMarkdown Reference Guide RMarkdown Home Page RMarkdown Cheat Sheet 1.3.9 Troubleshooting 1.3.9.1 My RMarkdown won’t knit to PDF If you get an error when trying to knit to PDF that says your computer doesn’t have a LaTeX installation, one of two things is likely happening: Your computer doesn’t have LaTeX installed You have an installation of LaTeX but RStudio cannot find it (it is not on the path) If you already use LaTeX (like to write papers), you fall in the second category. Solving this requires directing RStudio to your installation - and isn’t covered here. If you fall in the first category - you are sure you don’t have LaTeX installed - can use the R package tinytex to easily get an installation recognized by RStudio, as long as you have administrative rights to your computer. To install tinytex run: install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() If you get an error that looks like destination /usr/local/bin not writable, you need to give yourself permission to write to this directory (again, only possible if you have administrative rights). To do this, run this command in the terminal: sudo chown -R `whoami`:admin /usr/local/bin and then try the above install instructions again. More information about tinytex can be found here "],["session-2-reproducible-papers-with-rmarkdown.html", "2 Session 2: Reproducible Papers with RMarkdown 2.1 Reproducibility and Provenance", " 2 Session 2: Reproducible Papers with RMarkdown 2.1 Reproducibility and Provenance 2.1.1 Learning Objectives In this lesson, you will: Learn how to build a reproducible paper in RMarkdown Review the importance of computational reproducibility Review the of provenance metadata Review tools and techniques for reproducibility supported by the NCEAS and DataONE 2.1.1.1 Reproducible Papers A great overview of this approach to reproducible papers comes from: Ben Marwick, Carl Boettiger &amp; Lincoln Mullen (2018) Packaging Data Analytical Work Reproducibly Using R (and Friends), The American Statistician, 72:1, 80-88, doi:10.1080/00031305.2017.1375986 This lesson will draw from existing materials: rrtools Reproducible papers with RMarkdown The key idea in Marwick et al. (2018) is that of the “research compendium”: A single container for not just the journal article associated with your research but also the underlying analysis, data, and even the required software environment required to reproduce your work. Research compendia make it easy for researchers to do their work but also for others to inspect or even reproduce the work because all necessary materials are readily at hand due to being kept in one place. Rather than a constrained set of rules, the research compendium is a scaffold upon which to conduct reproducible research using open science tools such as: R RMarkdown git and GitHub Fortunately for us, Ben Marwick (and others) have written an R package called rrtools that helps us create a research compendium from scratch. To start a reproducible paper with rrtools, run: remotes::install_github(&quot;benmarwick/rrtools&quot;) rrtools::use_compendium(&quot;mypaper&quot;) You should see output similar to the below: &gt; rrtools::use_compendium(&quot;mypaper&quot;) The directory mypaper has been created. ✓ Setting active project to &#39;/Users/bryce/mypaper&#39; ✓ Creating &#39;R/&#39; ✓ Writing &#39;DESCRIPTION&#39; Package: mypaper Title: What the Package Does (One Line, Title Case) Version: 0.0.0.9000 Authors@R (parsed): * First Last &lt;first.last@example.com&gt; [aut, cre] Description: What the package does (one paragraph). License: MIT + file LICENSE ByteCompile: true Encoding: UTF-8 LazyData: true Roxygen: list(markdown = TRUE) RoxygenNote: 7.1.1 ✓ Writing &#39;NAMESPACE&#39; ✓ Writing &#39;mypaper.Rproj&#39; ✓ Adding &#39;.Rproj.user&#39; to &#39;.gitignore&#39; ✓ Adding &#39;^mypaper\\\\.Rproj$&#39;, &#39;^\\\\.Rproj\\\\.user$&#39; to &#39;.Rbuildignore&#39; ✓ Setting active project to &#39;&lt;no active project&gt;&#39; ✓ The package mypaper has been created ✓ Now opening the new compendium... ✓ Done. The working directory is currently /Users/bryce Next, you need to: ↓ ↓ ↓ ● Edit the DESCRIPTION file ● Use other &#39;rrtools&#39; functions to add components to the compendium rrtools has created the beginnings of a research compendium for us. At this point, it looks mostly the same as an R package. That’s because it uses the same underlying folder structure and metadata and therefore it technically is an R package. And this means our research compendium will be easy to install, just like an R package. Before we get to writing our reproducible paper, let’s fill in some more structure. Let’s: Add a license (always a good idea) Set up a README file in the RMarkdown format Create an analysis folder to hold our reproducible paper usethis::use_apache_license() rrtools::use_readme_rmd() rrtools::use_analysis() At this point, we’re ready to start writing the paper. To follow the structure rrtools has put in place for us, here are some pointers: Edit ./analysis/paper/paper.Rmd to begin writing your paper and your analysis in the same document Add any citations to ./analysis/paper/references.bib Add any longer R scripts that don’t fit in your paper in an R folder at the top level Add raw data to ./data/raw_data Write out any derived data (generated in paper.Rmd) to ./data/derived_data Write out any figures in ./analysis/figures It would also be a good idea to initialize this folder as a git repo for maximum reproducibility: usethis::use_git() After that, push a copy up to GitHub. Hopefully, now that you’ve created a research compendium with rrtools, you can imagine how a pre-defined structure like the one rrtools creates might help you organize your reproducible research and also make it easier for others to understand your work. For a more complete example than the one we built above, take a look at benmarwick/teaching-replication-in-archaeology. 2.1.1.2 Reproducibility and Provenance Accelerating synthesis science through reproducible science practices Reproducible Research: Recap Working in a reproducible manner: increases research efficiency, accelerating the pace of your research and collaborations provides transparency by capturing and communicating scientific workflows enables research to stand on the shoulders of giants (build on work that came before) allows credit for secondary usage and supports easy attribution increases trust in science To enable others to fully interpret, reproduce or build upon our research, we need to provide more comprehensive information than is typically included in a figure or publication. What data were used in this study? What methods applied? What were the parameter settings? What documentation or code are available to us to evaluate the results? Can we trust these data and methods? Are the results reproducible? Computational reproducibility is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions. Practically speaking, reproducibility includes: Preserving the data Preserving the software workflow Documenting what you did Describing how to interpret it all Computational Provenance Computational provenance refers to the origin and processing history of data including: Input data Workflow/scripts Output data Figures Methods, dataflow, and dependencies From Provenance to Reproducibility At DataONE we facilitate reproducible science through provenance by: Tracking data derivation history Tracking data inputs and outputs of analyses Tracking analysis and model executions Preserving and documenting software workflows Linking all of these to publications Introducing Prov-ONE, an extension of W3C PROV ProvONE provides the fundamental information required to understand and analyze scientific workflow-based computational experiments. It covers the main aspects that have been identified as relevant in the provenance literature including data structure. This addresses the most relevant aspects of how the data, both used and produced by a computational process, is organized and represented. For scientific workflows this implies the inputs and outputs of the various tasks that form part of the workflow. You will recall the example presented of data from Mark Carls: Mark Carls. Analysis of hydrocarbons following the Exxon Valdez oil spill, Gulf of Alaska, 1989 - 2014. Gulf of Alaska Data Portal. urn:uuid:3249ada0-afe3-4dd6-875e-0f7928a4c171., where we showed the six step workflow comprising four source data files and two output visualizations. This screenshot of the dataset page shows how DataONE renders the Prov-ONE model information as part of our interactive user interface. Data Citation Data citation best practices are focused on providing credit where credit is due and indexing and exposing data citations across international repository networks. In 2014, Force 11 established a Joint Declaration of Data Citation Principles that includes: Importance of data citation Credit and Attribution Evidence Unique Identification Access Persistence Specificity and Verifiability Interoperability and Flexibility Transitive Credit We want to move towards a model such that when a user cites a research publication we will also know: Which data produced it What software produced it What was derived from it Who to credit down the attribution stack This is transitive credit. And it changes the way in which we think about science communication and traditional publications. The 5th Generation Whole Tale simplifies computational reproducibility. It enables researchers to easily package and share ‘tales’. Tales are executable research objects captured in a standards-based tale format complete with metadata. They can contain: data (references) code (computational methods) narrative (traditional science story) compute environment (e.g. RStudio, Jupyter) By combining data, code and the compute environment, tales allow researchers to: re-create the computational results from a scientific study achieve computational reproducibility “set the default to reproducible.” They also empower users to verify and extend results with different data, methods, and environments. You can browse existing tales, run and interact with published tales and create new tales via the Whole Tale Dashboard. By integrating with DataONE and Dataverse, Whole Tale includes over 90 major research repositories from which a user can select datasets to make those datasets the starting point of an interactive data exploration and analysis inside of one of the Whole Tale environments. Within DataONE, we are adding functionality to work with data in the Whole Tale environment directly from the dataset landing page. Full circle reproducibility can be achieved by publishing data, code AND the environment. "],["session-3-version-control-with-git-and-github.html", "3 Session 3: Version Control with git and GitHub 3.1 Learning Objectives 3.2 Introduction to git 3.3 Create a remote repository on GitHub 3.4 Working locally with Git via RStudio 3.5 Setting up git on an existing project 3.6 Go Further", " 3 Session 3: Version Control with git and GitHub 3.1 Learning Objectives In this lesson, you will learn: Why git is useful for reproducible analysis How to use git to track changes to your work over time How to use GitHub to collaborate with others How to structure your commits so your changes are clear to others How to write effective commit messages 3.2 Introduction to git Every file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when problems are discovered. Data files get combined together, then errors are fixed, and then they are split and combined again. In the course of a single analysis, one can expect thousands of changes to files. And yet, all we use to track this are simplistic filenames. You might think there is a better way, and you’d be right: version control. 3.2.1 A Motivating Example Before diving into the details of git and how to use it, let’s start with a motivating example that’s representative of the types of problems git can help us solve. Say, for example, you’re working on an analysis in R and you’ve got it into a state you’re pretty happy with. We’ll call this version 1: You come into the office the following day and you have an email from your boss, “Hey, you know what this model needs?” You’re not entirely sure what she means but you figure there’s only one thing she could be talking about: more cowbell. So you add it to the model in order to really explore the space. But you’re worried about losing track of the old model so, instead of editing the code in place, you comment out the old code and put as serious a warning as you can muster in a comment above it. Commenting out code you don’t want to lose is something probably all of us have done at one point or another but it’s really hard to understand why you did this when you come back years later or you when you send your script to a colleague. Luckily, there’s a better way: Version control. Instead of commenting out the old code, we can change the code in place and tell git to commit our change. So now we have two distinct versions of our analysis and we can always see what the previous version(s) look like. You may have noticed something else in the diagram above: Not only can we save a new version of our analysis, we can also write as much text as we like about the change in the commit message. In addition to the commit message, git also tracks who, when, and where the change was made. Imagine that some time has gone by and you’ve committed a third version of your analysis, version 3, and a colleague emails with an idea: What if you used machine learning instead? Maybe you’re not so sure the idea will work out and this is where a tool like git shines. Without a tool like git, we might copy analysis.R to another file called analysis-ml.R which might end up having mostly the same code except for a few lines. This isn’t particularly problematic until you want to make a change to a bit of shared code and now you have to make changes in two files, if you even remember to. Instead, with git, we can start a branch. Branches allow us to confidently experiment on our code, all while leaving the old code in tact and recoverable. So you’ve been working in a branch and have made a few commits on it and your boss emails again asking you to update the model in some way. If you weren’t using a tool like git, you might panic at this point because you’ve rewritten much of your analysis to use a different method but your boss wants change to the old method. But with git and branches, we can continue developing our main analysis at the same time as we are working on any experimental branches. Branches are great for experiments but also great for organizing your work generally. After all that hard work on the machine learning experiment, you and your colleague could decide to scrap it. It’s perfectly fine to leave branches around and switch back to the main line of development but we can also delete them to tidy up. If, instead, you and your colleague had decided you liked the machine learning experiment, you could also merge the branch with your main development line. Merging branches is analogous to accepting a change in Word’s Track Changes feature but way more powerful and useful. A key takeaway here is that git can drastically increase your confidence and willingness to make changes to your code and help you avoid problems down the road. Analysis rarely follows a linear path and we need a tool that respects this. Finally, imagine that, years later, your colleague asks you to make sure the model you reported in a paper you published together was actually the one you used. Another really powerful feature of git is tags which allow us to record a particular state of our analysis with a meaningful name. In this case, we are lucky because we tagged the version of our code we used to run the analysis. Even if we continued to develop beyond commit 5 (above) after we submitted our manuscript, we can always go back and run the analysis as it was in the past. 3.2.1.1 Summary With git, we can: Avoid using cryptic filenames and comments to keep track of our work Describe our changes with as much information as we like so it’s easier to understand why our code changed (commits) Work on multiple, simultaneous development (branches) of our code at the same time and, optionally, merge them together Go back in time to look at (and even run) older versions of our code Tag specific versions of our code as meaningful (tags) And, as we’ll see below, git has one extra superpower available to us: It’s distributed. Multiple people can work on the same analysis at the same time on their own computer and everyone’s changes can eventually merged together. Version control and Collaboration using Git and GitHub First, just what are git and GitHub? git: version control software used to track files in a folder (a repository) git creates the versioned history of a repository GitHub: web site that allows users to store their git repositories and share them with others Let’s look at a GitHub repository This screen shows the copy of a repository stored on GitHub, with its list of files, when the files and directories were last modified, and some information on who made the most recent changes. If we drill into the “commits” for the repository, we can see the history of changes made to all of the files. Looks like kellijohnson and seananderson were fixing things in June and July: And finally, if we drill into the changes made on June 13, we can see exactly what was changed in each file: Tracking these changes, how they relate to released versions of software and files is exactly what Git and GitHub are good for. And we will show how they can really be effective for tracking versions of scientific code, figures, and manuscripts to accomplish a reproducible workflow. The Git lifecycle As a git user, you’ll need to understand the basic concepts associated with versioned sets of changes, and how they are stored and moved across repositories. Any given git repository can be cloned so that it exist both locally, and remotely. But each of these cloned repositories is simply a copy of all of the files and change history for those files, stored in git’s particular format. For our purposes, we can consider a git repository just a folder with a bunch of additional version-related metadata. In a local git-enabled folder, the folder contains a workspace containing the current version of all files in the repository. These working files are linked to a hidden folder containing the ‘Local repository’, which contains all of the other changes made to the files, along with the version metadata. So, when working with files using git, you can use git commands to indicate specifically which changes to the local working files should be staged for versioning (using the git add command), and when to record those changes as a version in the local repository (using the command git commit). The remaining concepts are involved in synchronizing the changes in your local repository with changes in a remote repository. The git push command is used to send local changes up to a remote repository (possibly on GitHub), and the git pull command is used to fetch changes from a remote repository and merge them into the local repository. git clone: to copy a whole remote repository to local git add (stage): notify git to track particular changes git commit: store those changes as a version git pull: merge changes from a remote repository to our local repository git push: copy changes from our local repository to a remote repository git status: determine the state of all files in the local repository git log: print the history of changes in a repository Those seven commands are the majority of what you need to successfully use git. But this is all super abstract, so let’s explore with some real examples. 3.3 Create a remote repository on GitHub Let’s start by creating a repository on GitHub, then we’ll edit some files. Setup Log into GitHub Click the New repository button Name it training-test Create a README.md Set the LICENSE to Apache 2.0 Add a .gitignore file for R If you were successful, it should look something like this: You’ve now created your first repository! It has a couple of files that GitHub created for you, like the README.md file, and the LICENSE file, and the .gitignore file. For simple changes to text files, you can make edits right in the GitHub web interface. Challenge Navigate to the README.md file in the file listing, and edit it by clicking on the pencil icon. This is a regular Markdown file, so you can just add markdown text. When done, add a commit message, and hit the Commit changes button. Congratulations, you’ve now authored your first versioned commit. If you navigate back to the GitHub page for the repository, you’ll see your commit listed there, as well as the rendered README.md file. Let’s point out a few things about this window. It represents a view of the repository that you created, showing all of the files in the repository so far. For each file, it shows when the file was last modified, and the commit message that was used to last change each file. This is why it is important to write good, descriptive commit messages. In addition, the blue header above the file listing shows the most recent commit, along with its commit message, and its SHA identifier. That SHA identifier is the key to this set of versioned changes. If you click on the SHA identifier (810f314), it will display the set of changes made in that particular commit. In the next section we’ll use the GitHub URL for the GitHub repository you created to clone the repository onto your local machine so that you can edit the files in RStudio. To do so, start by copying the GitHub URL, which represents the repository location: 3.4 Working locally with Git via RStudio For convenience, it would be nice to be able to edit the files locally on our computer using RStudio rather than working on the GitHub website. We can do this by using the clone command to copy the repository from GitHub to our local computer, which then allows us to push changes up to GitHub and pull down any changes that have occurred. We refer to the remote copy of the repository that is on GitHub as the origin repository (the one that we cloned from), and the copy on our local computer as the local copy. RStudio knows how to work with files under version control with Git, but only if you are working within an RStudio project folder. In this next section, we will clone the repository that you created on GitHub into a local repository as an RStudio project. Here’s what we’re going to do: Create the new project Inspect the Git tab and version history Commit a change to the README.md file Commit the changes that RStudio made Inspect the version history Add and commit an Rmd file Push these changes to GitHub View the change history on GitHub Setup In the File menu, select “New Project.” In the dialog that pops up, select the “Version Control” option, and paste the GitHub URL that you copied into the field for the remote repository Repository URL. While you can name the local copy of the repository anything, it’s typical to use the same name as the GitHub repository to maintain the correspondence. Once you hit `Create Project, a new RStudio window will open with all of the files from the remote repository copied locally. Depending on how your version of RStudio is configured, the location and size of the panes may differ, but they should all be present, including a Git tab and the normal Files tab listing the files that had been created in the remote repository. You’ll note that there is one new file training-test.Rproj, and three files that we created earlier on GitHub (.gitignore, LICENSE, and README.md). In the Git tab, you’ll note that two files are listed. This is the status pane that shows the current modification status of all of the files in the repository. In this case, the .gitignore file is listed as M for Modified, and training-test.Rproj is listed with a ? ? to indicate that the file is untracked. This means that git has not stored any versions of this file, and knows nothing about the file. As you make version control decisions in RStudio, these icons will change to reflect the current version status of each of the files. Inspect the history. For now, let’s click on the History button in the Git tab, which will show the log of changes that occurred, and will be identical to what we viewed on GitHub. By clicking on each row of the history, you can see exactly what was added and changed in each of the two commits in this repository. Challenge Commit a README.md change. Next let’s make a change to the README.md file, this time from RStudio. Add a new section to your markdown using a header, and under it include a list three advantages to using git. Once you save, you’ll immediately see the README.md file show up in the Git tab, marked as a modification. You can select the file in the Git tab, and click Diff to see the differences that you saved (but which are not yet committed to your local repository). And here’s what the newly made changes look like compared to the original file. New lines are highlighted in green, while removed lines are in red. Commit the RStudio changes. To commit the changes you made to the README.md file, check the Staged checkbox next to the file (which tells Git which changes you want included in the commit), then provide a descriptive Commit message, and then click Commit. Note that some of the changes in the repository, namely training-test.Rproj are still listed as having not been committed. This means there are still pending changes to the repository. You can also see the note that says: Your branch is ahead of ‘origin/main’ by 1 commit. This means that we have committed 1 change in the local repository, but that commit has not yet been pushed up to the origin repository, where origin is the typical name for our remote repository on GitHub. So, let’s commit the remaining project files by staging them and adding a commit message. When finished, you’ll see that no changes remain in the Git tab, and the repository is clean. Inspect the history. Note that the message now says: Your branch is ahead of ‘origin/main’ by 2 commits. These 2 commits are the two we just made, and have not yet been pushed to GitHub. By clicking on the History button, we can see that there are now a total of four commits in the local repository (while there had only been two on GitHub). Push these changes to GitHub. Now that everything has been changed as desired locally, you can push the changes to GitHub using the Push button. This will prompt you for your GitHub username and password, and upload the changes, leaving your repository in a totally clean and synchronized state. When finished, looking at the history shows all four commits, including the two that were done on GitHub and the two that were done locally on RStudio. And note that the labels indicate that both the local repository (HEAD) and the remote repository (origin/HEAD) are pointing at the same version in the history. So, if we go look at the commit history on GitHub, all the commits will be shown there as well. Aside What should I write in my commit message? Clearly, good documentation of what you’ve done is critical to making the version history of your repository meaningful and helpful. Its tempting to skip the commit message altogether, or to add some stock blurb like ‘Updates’. It’s better to use messages that will be helpful to your future self in deducing not just what you did, but why you did it. Also, commit messaged are best understood if they follow the active verb convention. For example, you can see that my commit messages all started with a past tense verb, and then explained what was changed. While some of the changes we illustrated here were simple and so easily explained in a short phrase, for more complex changes, its best to provide a more complete message. The convention, however, is to always have a short, terse first sentence, followed by a more verbose explanation of the details and rationale for the change. This keeps the high level details readable in the version log. I can’t count the number of times I’ve looked at the commit log from 2, 3, or 10 years prior and been so grateful for diligence of my past self and collaborators. Collaboration and conflict free workflows Up to now, we have been focused on using Git and GitHub for yourself, which is a great use. But equally powerful is to share a GitHub repository with other researchers so that you can work on code, analyses, and models together. When working together, you will need to pay careful attention to the state of the remote repository to avoid and handle merge conflicts. A merge conflict occurs when two collaborators make two separate commits that change the same lines of the same file. When this happens, git can’t merge the changes together automatically, and will give you back an error asking you to resolve the conflict. Don’t be afraid of merge conflicts, they are pretty easy to handle. and there are some great guides. That said, its truly painless if you can avoid merge conflicts in the first place. You can minimize conflicts by: Ensure that you pull down changes just before you commit Ensures that you have the most recent changes But you may have to fix your code if conflict would have occurred Coordinate with your collaborators on who is touching which files You still need to communicate to collaborate 3.5 Setting up git on an existing project Now you have two projects set up in your RStudio environment, training_{USERNAME} and training_test. We set you up with the training_test project since we think it is an easy way to introduce you to git, but more commonly researchers will have an existing directory of code that they then want to make a git repository out of. For the last exercise of this session, we will do this with your training_{USERNAME} project. First, switch to your training_{USERNAME} project using the RStudio project switcher. The project switcher is in the upper right corner of your RStudio pane. Click the dropdown next to your project name (training_test), and then select the training_{USERNAME} project from the “recent projects” list. Next, from the Tools menu, select “Project Options.” In the dialog that pops up, select “Git/SVN” from the menu on the left. In the dropdown at the top of this page, select “Git” and click “Yes” in the confirmation box. Click “Yes” again to restart RStudio. When RStudio restarts, you should have a git tab, with two untracked files (.gitignore and training_{USERNAME}.Rproj). Challenge Add and commit these two files to your git repository Now we have your local repository all set up. You can make as many commits as you want on this repository, and it will likely still be helpful to you, but the power in git and GitHub is really in collaboration. As discussed, GitHub facilitates this, so let’s get this repository on GitHub. Go back to GitHub, and click on the “New Repository” button. In the repository name field, enter the same name as your RProject. So for me, this would be training_jclark. Add a description, keep the repository public, and, most importantly: DO NOT INITIALIZE THE REPOSITORY WITH ANY FILES. We already have the repository set up locally so we don’t need to do this. Initializing the repository will only cause merge issues. Here is what your page should look like: Once your page looks like that, click the “create repository” button. This will open your empty repository with a page that conveniently gives you exactly the instructions you need. In our case, we are “pushing an existing repository from the command line.” Click the clipboard icon to copy the code for the middle option of the three on this page. It should have three lines and look like this: git remote add origin https://github.com/jeanetteclark/training_jclark.git git branch -M main git push -u origin main Back in RStudio, open the terminal by clicking the tab labeled as such next to the console tab. The prompt should look something like this: jclark@included-crab:~/training_jclark$ In the prompt, paste the code that you copied from the GitHub page and press return. You will be prompted to type your GitHub username and password. The code that you copied and pasted did three things: added the GitHub repository as the remote repository renamed the default branch to main pushed the main branch to the remote GitHub repository If you go back to your browser and refresh your GitHub repository page, you should now see your files appear. Challenge On your repository page, GitHub has a button that will help you add a readme file. Click the “Add a README” button and use markdown syntax to create a simple readme. Commit the changes to your repository. Go to your local repository (in RStudio) and pull the changes you made. 3.6 Go Further There’s a lot we haven’t covered in this brief tutorial. There are some great and much longer tutorials that cover advanced topics, such as: Using git on the command line Resolving conflicts Branching and merging Pull requests versus direct contributions for collaboration Using .gitignore to protect sensitive data GitHub Issues and why they are useful and much, much more Try Git is a great interactive tutorial Software Carpentry Version Control with Git Codecademy Learn Git (some paid) "],["session-4-data-modeling-essentials.html", "4 Session 4: Data Modeling Essentials 4.1 Data Modeling &amp; Tidy Data", " 4 Session 4: Data Modeling Essentials 4.1 Data Modeling &amp; Tidy Data 4.1.1 Learning Objectives Understand basics of relational data models aka tidy data Learn how to design and create effective data tables 4.1.2 Introduction In this lesson we are going to learn what relational data models are, and how they can be used to manage and analyze data efficiently. Relational data models are what relational databases use to organize tables. However, you don’t have to be using a relational database (like mySQL, MariaDB, Oracle, or Microsoft Access) to enjoy the benefits of using a relational data model. Additionally, your data don’t have to be large or complex for you to benefit. Here are a few of the benefits of using a relational data model: Powerful search and filtering Handle large, complex data sets Enforce data integrity Decrease errors from redundant updates Simple guidelines for data management A great paper called ‘Some Simple Guidelines for Effective Data Management’ (Borer et al. 2009) lays out exactly that - guidelines that make your data management, and your reproducible research, more effective. The first six guidelines are straightforward, but worth mentioning here: Use a scripted program (like R!) Non-proprietary file formats are preferred (eg: csv, txt) Keep a raw version of data Use descriptive file and variable names (without spaces!) Include a header line in your tabular data files Use plain ASCII text The next three are a little more complex, but all are characteristics of the relational data model: Design tables to add rows, not columns Each column should contain only one type of information Record a single piece of data only once; separate information collected at different scales into different tables. 4.1.3 Recognizing untidy data Before we learn how to create a relational data model, let’s look at how to recognize data that does not conform to the model. Data Organization This is a screenshot of an actual dataset that came across NCEAS. We have all seen spreadsheets that look like this - and it is fairly obvious that whatever this is, it isn’t very tidy. Let’s dive deeper in to exactly why we wouldn’t consider it tidy. Multiple tables Your human brain can see from the way this sheet is laid out that it has three tables within it. Although it is easy for us to see and interpret this, it is extremely difficult to get a computer to see it this way, which will create headaches down the road should you try to read in this information to R or another programming language. Inconsistent observations Rows correspond to observations. If you look across a single row, and you notice that there are clearly multiple observations in one row, the data are likely not tidy. Inconsistent variables Columns correspond to variables. If you look down a column, and see that multiple variables exist in the table, the data are not tidy. A good test for this can be to see if you think the column consists of only one unit type. Marginal sums and statistics Marginal sums and statistics also are not considered tidy, and they are not the same type of observation as the other rows. Instead, they are a combination of observations. 4.1.4 Good enough data modeling Denormalized data When data are “denormalized” it means that observations about different entities are combined. In the above example, each row has measurements about both the site at which observations occurred, as well as observations of two individuals of possibly different species found at that site. This is not normalized data. People often refer to this as wide format, because the observations are spread across a wide number of columns. Note that, should one encounter a new species in the survey, we would have to add new columns to the table. This is difficult to analyze, understand, and maintain. Tabular data Observations. A better way to model data is to organize the observations about each type of entity in its own table. This results in: Separate tables for each type of entity measured Each row represents a single observed entity Observations (rows) are all unique This is normalized data (aka tidy data) Variables. In addition, for normalized data, we expect the variables to be organized such that: All values in a column are of the same type All columns pertain to the same observed entity (e.g., row) Each column represents either an identifying variable or a measured variable Challenge Try to answer the following questions: What are the observed entities in the example above? What are the measured variables associated with those observations? Answer: If we use these questions to tidy our data, we should end up with: one table for each entity observed one column for each measured variable additional columns for identifying variables (such as site ID) Here is what our tidy data look like: Note that this normalized version of the data meets the three guidelines set by (Borer et al. 2009): Design tables to add rows, not columns Each column should contain only one type of information Record a single piece of data only once; separate information collected at different scales into different tables. 4.1.5 Using normalized data Normalizing data by separating it into multiple tables often makes researchers really uncomfortable. This is understandable! The person who designed this study collected all of these measurements for a reason - so that they could analyze the measurements together. Now that our site and species information are in separate tables, how would we use site elevation as a predictor variable for species composition, for example? The answer is keys - and they are the cornerstone of relational data models. When one has normalized data, we often use unique identifiers to reference particular observations, which allows us to link across tables. Two types of identifiers are common within relational data: Primary Key: unique identifier for each observed entity, one per row Foreign Key: reference to a primary key in another table (linkage) Challenge In our normalized tables above, identify the following: the primary key for each table any foreign keys that exist Answer The primary key of the top table is id. The primary key of the bottom table is site. The site column is the primary key of that table because it uniquely identifies each row of the table as a unique observation of a site. In the first table, however, the site column is a foreign key that references the primary key from the second table. This linkage tells us that the first height measurement for the DAPU observation occurred at the site with the name Taku. Entity-Relationship Model (ER) An Entity-Relationship model allows us to compactly draw the structure of the tables in a relational database, including the primary and foreign keys in the tables. In the above model, one can see that each site in the sites observations table must have one or more observations in the species observations table, whereas each species opservation has one and only one site. Merging data Frequently, analysis of data will require merging these separately managed tables back together. There are multiple ways to join the observations in two tables, based on how the rows of one table are merged with the rows of the other. When conceptualizing merges, one can think of two tables, one on the left and one on the right. The most common (and often useful) join is when you merge the subset of rows that have matches in both the left table and the right table: this is called an INNER JOIN. Other types of join are possible as well. A LEFT JOIN takes all of the rows from the left table, and merges on the data from matching rows in the right table. Keys that don’t match from the left table are still provided with a missing value (na) from the right table. A RIGHT JOIN is the same, except that all of the rows from the right table are included with matching data from the left, or a missing value. Finally, a FULL OUTER JOIN includes all data from all rows in both tables, and includes missing values wherever necessary. Sometimes people represent these as Venn diagrams showing which parts of the left and right tables are included in the results for each join. These however, miss part of the story related to where the missing value come from in each result. In the figure above, the blue regions show the set of rows that are included in the result. For the INNER join, the rows returned are all rows in A that have a matching row in B. 4.1.6 Data modeling exercise Break into groups, 1 per table To demonstrate, we’ll be working with a tidied up version of a dataset from ADF&amp;G containing commercial catch data from 1878-1997. The dataset and reference to the original source can be viewed at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2. That site includes metadata describing the full data set, including column definitions. Here’s the first catch table: And here’s the region_defs table: Challenge Using the Invision live session your instructor starts, work through the following tasks: Draw an ER model for the tables Indicate the primary and foreign keys Is the catch table in normal (aka tidy) form? If so, what single type of entity was observed? If not, how might you restructure the data table to make it tidy? Draw a new ER diagram showing this re-designed data structure Optional Extra Challenge If you have time, take on this extra challenge with your group. Navigate to this dataset: Richard Lanctot and Sarah Saalfeld. 2019. Utqiaġvik shorebird breeding ecology study, Utqiaġvik, Alaska, 2003-2018. Arctic Data Center. doi:10.18739/A23R0PT35 Try to determine the primary and foreign keys for the following tables: Utqiagvik_adult_shorebird_banding.csv Utqiagvik_egg_measurements.csv Utqiagvik_nest_data.csv Utqiagvik_shorebird_resightings.csv 4.1.7 Resources Borer et al. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America. White et al. 2013. Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution 6. Software Carpentry SQL tutorial Tidy Data References "],["session-5-cleaning-and-manipulating-data.html", "5 Session 5: Cleaning and Manipulating Data 5.1 Data Cleaning and Manipulation", " 5 Session 5: Cleaning and Manipulating Data 5.1 Data Cleaning and Manipulation 5.1.1 Learning Objectives In this lesson, you will learn: What the Split-Apply-Combine strategy is and how it applies to data The difference between wide vs. tall table formats and how to convert between them How to use dplyr and tidyr to clean and manipulate data for analysis How to join multiple data.frames together using dplyr 5.1.2 Introduction The data we get to work with are rarely, if ever, in the format we need to do our analyses. It’s often the case that one package requires data in one format, while another package requires the data to be in another format. To be efficient analysts, we should have good tools for reformatting data for our needs so we can do our actual work like making plots and fitting models. The dplyr and tidyr R packages provide a fairly complete and extremely powerful set of functions for us to do this reformatting quickly and learning these tools well will greatly increase your efficiency as an analyst. Analyses take many shapes, but they often conform to what is known as the Split-Apply-Combine strategy. This strategy follows a usual set of steps: Split: Split the data into logical groups (e.g., area, stock, year) Apply: Calculate some summary statistic on each group (e.g. mean total length by year) Combine: Combine the groups back together into a single table Figure 1: diagram of the split apply combine strategy As shown above (Figure 1), our original table is split into groups by year, we calculate the mean length for each group, and finally combine the per-year means into a single table. dplyr provides a fast and powerful way to express this. Let’s look at a simple example of how this is done: Assuming our length data is already loaded in a data.frame called length_data: year length_cm 1991 5.673318 1991 3.081224 1991 4.592696 1992 4.381523 1992 5.597777 1992 4.900052 1992 4.139282 1992 5.422823 1992 5.905247 1992 5.098922 We can do this calculation using dplyr like this: length_data %&gt;% group_by(year) %&gt;% summarise(mean_length_cm = mean(length_cm)) Another exceedingly common thing we need to do is “reshape” our data. Let’s look at an example table that is in what we will call “wide” format: site 1990 1991 … 1993 gold 100 118 … 112 lake 100 118 … 112 … … … … … dredge 100 118 … 112 You are probably quite familiar with data in the above format, where values of the variable being observed are spread out across columns (Here: columns for each year). Another way of describing this is that there is more than one measurement per row. This wide format works well for data entry and sometimes works well for analysis but we quickly outgrow it when using R. For example, how would you fit a model with year as a predictor variable? In an ideal world, we’d be able to just run: lm(length ~ year) But this won’t work on our wide data because lm needs length and year to be columns in our table. Or how would we make a separate plot for each year? We could call plot one time for each year but this is tedious if we have many years of data and hard to maintain as we add more years of data to our dataset. The tidyr package allows us to quickly switch between wide format and what is called tall format using the pivot_longer function: site_data %&gt;% pivot_longer(-site, names_to = &quot;year&quot;, values_to = &quot;length&quot;) site year length gold 1990 101 lake 1990 104 dredge 1990 144 … … … dredge 1993 145 In this lesson we’re going to walk through the functions you’ll most commonly use from the dplyr and tidyr packages: dplyr mutate() group_by() summarise() select() filter() arrange() left_join() rename() tidyr pivot_longer() pivot_wider() unite() separate() 5.1.3 Data Cleaning Basics To demonstrate, we’ll be working with a tidied up version of a dataset from ADF&amp;G containing commercial catch data from 1878-1997. The dataset and reference to the original source can be found at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2. First, open a new RMarkdown document. Delete everything below the setup chunk, and add a library chunk that calls dplyr, tidyr, and readr: library(dplyr) library(tidyr) library(readr) Aside A note on loading packages. You may have noticed the following warning messages pop up when you ran your library chunk. Attaching package: ‘dplyr’ The following objects are masked from ‘package:stats’: filter, lag The following objects are masked from ‘package:base’: intersect, setdiff, setequal, union These are important warnings. They are letting you know that certain functions from the stats and base packages (which are loaded by default when you start R) are masked by different functions with the same name in the dplyr package. It turns out, the order that you load the packages in matters. Since we loaded dplyr after stats, R will assume that if you call filter, you mean the dplyr version unless you specify otherwise. Being specific about which version of filter, for example, you call is easy. To explicitly call a function by its unambiguous name, you use the syntax package_name::function_name(...). So, if I wanted to call the stats version of filter in this Rmarkdown document, I would use the syntax stats::filter(...). Challenge The warnings above are important, but we might not want them in our final document. After you have read them, adjust the chunk settings on your library chunk to suppress warnings and messages. Now that we have learned a little mini-lesson on functions, let’s get the data that we are going to use for this lesson. Setup Navigate to the salmon catch dataset: https://knb.ecoinformatics.org/#view/df35b.304.2 Right click the “download” button for the file “byerlySalmonByRegion.csv” Select “copy link address” from the dropdown menu Paste the URL into a read.csv call like below The code chunk you use to read in the data should look something like this: catch_original &lt;- read.csv(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1&quot;) Aside Note that windows users who want to use this method locally also need to use the url function here with the argument method = \"libcurl\" as below: catch_original &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1&quot;, method = &quot;libcurl&quot;)) This dataset is relatively clean and easy to interpret as-is. But while it may be clean, it’s in a shape that makes it hard to use for some types of analyses so we’ll want to fix that first. Challenge Before we get too much further, spend a minute or two outlining your RMarkdown document so that it includes the following sections and steps: Data Sources read in the data Clean and Reshape data remove unnecessary columns check column typing reshape data Join to Regions dataset About the pipe (%&gt;%) operator Before we jump into learning tidyr and dplyr, we first need to explain the %&gt;%. Both the tidyr and the dplyr packages use the pipe operator - %&gt;%, which may look unfamiliar. The pipe is a powerful way to efficiently chain together operations. The pipe will take the output of a previous statement, and use it as the input to the next statement. Say you want to both filter out rows of a dataset, and select certain columns. Instead of writing df_filtered &lt;- filter(df, ...) df_selected &lt;- select(df_filtered, ...) You can write df_cleaned &lt;- df %&gt;% filter(...) %&gt;% select(...) If you think of the assignment operator (&lt;-) as reading like “gets”, then the pipe operator would read like “then.” So you might think of the above chunk being translated as: The cleaned dataframe gets the original data, and then a filter (of the original data), and then a select (of the filtered data). The benefits to using pipes are that you don’t have to keep track of (or overwrite) intermediate data frames. The drawbacks are that it can be more difficult to explain the reasoning behind each step, especially when many operations are chained together. It is good to strike a balance between writing efficient code (chaining operations), while ensuring that you are still clearly explaining, both to your future self and others, what you are doing and why you are doing it. RStudio has a keyboard shortcut for %&gt;% : Ctrl + Shift + M (Windows), Cmd + Shift + M (Mac). Selecting/removing columns: select() The first issue is the extra columns All and notesRegCode. Let’s select only the columns we want, and assign this to a variable called catch_data. catch_data &lt;- catch_original %&gt;% select(Region, Year, Chinook, Sockeye, Coho, Pink, Chum) head(catch_data) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 Much better! select also allows you to say which columns you don’t want, by passing unquoted column names preceded by minus (-) signs: catch_data &lt;- catch_original %&gt;% select(-All, -notesRegCode) head(catch_data) Quality Check Now that we have the data we are interested in using, we should do a little quality check to see that it seems as expected. One nice way of doing this is the glimpse function. glimpse(catch_data) ## Rows: 1,708 ## Columns: 7 ## $ Region &lt;chr&gt; &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;, &quot;SSE&quot;,… ## $ Year &lt;int&gt; 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 18… ## $ Chinook &lt;chr&gt; &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;9… ## $ Sockeye &lt;int&gt; 5, 155, 224, 182, 251, 274, 207, 189, 253, 408, 989, 791, 708,… ## $ Coho &lt;int&gt; 0, 0, 16, 11, 42, 24, 11, 1, 5, 8, 192, 161, 132, 139, 84, 107… ## $ Pink &lt;int&gt; 0, 0, 0, 92, 0, 0, 8, 187, 529, 606, 996, 2218, 673, 1545, 204… ## $ Chum &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 2, 0, 0, 0, 102, 343… Challenge Notice the output of the glimpse function call. Does anything seem amiss with this dataset that might warrant fixing? Answer: The Chinook catch data are character class. Let’s fix it using the function mutate before moving on. Changing column content: mutate() We can use the mutate function to change a column, or to create a new column. First Let’s try to just convert the Chinook catch values to numeric type using the as.numeric() function, and overwrite the old Chinook column. catch_clean &lt;- catch_data %&gt;% mutate(Chinook = as.numeric(Chinook)) ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion head(catch_clean) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 We get a warning “NAs introduced by coercion” which is R telling us that it couldn’t convert every value to an integer and, for those values it couldn’t convert, it put an NA in its place. This is behavior we commonly experience when cleaning datasets and it’s important to have the skills to deal with it when it crops up. To investigate, let’s isolate the issue. We can find out which values are NAs with a combination of is.na() and which(), and save that to a variable called i. i &lt;- which(is.na(catch_clean$Chinook)) i ## [1] 401 It looks like there is only one problem row, lets have a look at it in the original data. catch_data[i,] ## Region Year Chinook Sockeye Coho Pink Chum ## 401 GSE 1955 I 66 0 0 1 Well that’s odd: The value in catch_thousands is I. It turns out that this dataset is from a PDF which was automatically converted into a CSV and this value of I is actually a 1. Let’s fix it by incorporating the ifelse function to our mutate call, which will change the value of the Chinook column to 1 if the value is equal to I, otherwise it will use as.numeric to turn the character representations of numbers into numeric typed values. catch_clean &lt;- catch_data %&gt;% mutate(Chinook = if_else(Chinook == &quot;I&quot;, 1, as.numeric(Chinook))) ## Warning in replace_with(out, !condition, false, fmt_args(~false), glue(&quot;length ## of {fmt_args(~condition)}&quot;)): NAs introduced by coercion head(catch_clean) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 Changing shape: pivot_longer() and pivot_wider() The next issue is that the data are in a wide format and, we want the data in a tall format instead. pivot_longer() from the tidyr package helps us do just this conversion: catch_long &lt;- catch_clean %&gt;% pivot_longer(cols = -c(Region, Year), names_to = &quot;species&quot;, values_to = &quot;catch&quot;) head(catch_long) ## # A tibble: 6 × 4 ## Region Year species catch ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 The syntax we used above for pivot_longer() might be a bit confusing so let’s walk though it. The first argument to pivot_longer is the columns over which we are pivoting. You can select these by listing either the names of the columns you do want to pivot, or the names of the columns you are not pivoting over. The names_to argument takes the name of the column that you are creating from the column names you are pivoting over. The values_to argument takes the name of the column that you are creating from the values in the columns you are pivoting over. The opposite of pivot_longer(), pivot_wider(), works in a similar declarative fashion: catch_wide &lt;- catch_long %&gt;% pivot_wider(names_from = species, values_from = catch) head(catch_wide) ## # A tibble: 6 × 7 ## Region Year Chinook Sockeye Coho Pink Chum ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 Renaming columns with rename() If you scan through the data, you may notice the values in the catch column are very small (these are supposed to be annual catches). If we look at the metadata we can see that the catch column is in thousands of fish so let’s convert it before moving on. Let’s first rename the catch column to be called catch_thousands: catch_long &lt;- catch_long %&gt;% rename(catch_thousands = catch) head(catch_long) ## # A tibble: 6 × 4 ## Region Year species catch_thousands ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 Aside Many people use the base R function names to rename columns, often in combination with column indexing that relies on columns being in a particular order. Column indexing is often also used to select columns instead of the select function from dplyr. Although these methods both work just fine, they do have one major drawback: in most implementations they rely on you knowing exactly the column order your data is in. To illustrate why your knowledge of column order isn’t reliable enough for these operations, considering the following scenario: Your colleague emails you letting you know that she has an updated version of the conductivity-temperature-depth data from this year’s research cruise, and sends it along. Excited, you re-run your scripts that use this data for your phytoplankton research. You run the script and suddenly all of your numbers seem off. You spend hours trying to figure out what is going on. Unbeknownst to you, your colleagues bought a new sensor this year that measures dissolved oxygen. Because of the new variables in the dataset, the column order is different. Your script which previously renamed the 4th column, SAL_PSU to salinity now renames the 4th column, O2_MGpL to salinity. No wonder your results looked so weird, good thing you caught it! If you had written your code so that it doesn’t rely on column order, but instead renames columns using the rename function, the code would have run just fine (assuming the name of the original salinity column didn’t change, in which case the code would have errored in an obvious way). This is an example of a defensive coding strategy, where you try to anticipate issues before they arise, and write your code in such a way as to keep the issues from happening. Adding columns: mutate() Now let’s use mutate again to create a new column called catch with units of fish (instead of thousands of fish). catch_long &lt;- catch_long %&gt;% mutate(catch = catch_thousands * 1000) head(catch_long) Now let’s remove the catch_thousands column for now since we don’t need it. Note that here we have added to the expression we wrote above by adding another function call (mutate) to our expression. This takes advantage of the pipe operator by grouping together a similar set of statements, which all aim to clean up the catch_long data.frame. catch_long &lt;- catch_long %&gt;% mutate(catch = catch_thousands * 1000) %&gt;% select(-catch_thousands) head(catch_long) ## # A tibble: 6 × 4 ## Region Year species catch ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5000 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 We’re now ready to start analyzing the data. group_by and summarise As I outlined in the Introduction, dplyr lets us employ the Split-Apply-Combine strategy and this is exemplified through the use of the group_by() and summarise() functions: mean_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarise(catch_mean = mean(catch)) head(mean_region) ## # A tibble: 6 × 2 ## Region catch_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 ALU 40384. ## 2 BER 16373. ## 3 BRB 2709796. ## 4 CHG 315487. ## 5 CKI 683571. ## 6 COP 179223. Another common use of group_by() followed by summarize() is to count the number of rows in each group. We have to use a special function from dplyr, n(). n_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarize(n = n()) head(n_region) ## # A tibble: 6 × 2 ## Region n ## &lt;chr&gt; &lt;int&gt; ## 1 ALU 435 ## 2 BER 510 ## 3 BRB 570 ## 4 CHG 550 ## 5 CKI 525 ## 6 COP 470 Aside If you are finding that you are reaching for this combination of group_by(), summarise() and n() a lot, there is a helpful dplyr function count() that accomplishes this in one function! Challenge Find another grouping and statistic to calculate for each group. Find out if you can group by multiple variables. Filtering rows: filter() filter() is the verb we use to filter our data.frame to rows matching some condition. It’s similar to subset() from base R. Let’s go back to our original data.frame and do some filter()ing: SSE_catch &lt;- catch_long %&gt;% filter(Region == &quot;SSE&quot;) head(SSE_catch) ## # A tibble: 6 × 4 ## Region Year species catch ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5000 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 Challenge Filter to just catches of over one million fish. Filter to just Chinook from the SSE region Sorting your data: arrange() arrange() is how we sort the rows of a data.frame. In my experience, I use arrange() in two common cases: When I want to calculate a cumulative sum (with cumsum()) so row order matters When I want to display a table (like in an .Rmd document) in sorted order Let’s re-calculate mean catch by region, and then arrange() the output by mean catch: mean_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarise(mean_catch = mean(catch)) %&gt;% arrange(mean_catch) head(mean_region) ## # A tibble: 6 × 2 ## Region mean_catch ## &lt;chr&gt; &lt;dbl&gt; ## 1 BER 16373. ## 2 KTZ 18836. ## 3 ALU 40384. ## 4 NRS 51503. ## 5 KSK 67642. ## 6 YUK 68646. The default sorting order of arrange() is to sort in ascending order. To reverse the sort order, wrap the column name inside the desc() function: mean_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarise(mean_catch = mean(catch)) %&gt;% arrange(desc(mean_catch)) head(mean_region) ## # A tibble: 6 × 2 ## Region mean_catch ## &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 3184661. ## 2 BRB 2709796. ## 3 NSE 1825021. ## 4 KOD 1528350 ## 5 PWS 1419237. ## 6 SOP 1110942. 5.1.4 Joins in dplyr So now that we’re awesome at manipulating a single data.frame, where do we go from here? Manipulating more than one data.frame. If you’ve ever used a database, you may have heard of or used what’s called a “join”, which allows us to to intelligently merge two tables together into a single table based upon a shared column between the two. We’ve already covered joins in Data Modeling &amp; Tidy Data so let’s see how it’s done with dplyr. The dataset we’re working with, https://knb.ecoinformatics.org/#view/df35b.304.2, contains a second CSV which has the definition of each Region code. This is a really common way of storing auxiliary information about our dataset of interest (catch) but, for analylitcal purposes, we often want them in the same data.frame. Joins let us do that easily. Let’s look at a preview of what our join will do by looking at a simplified version of our data: Visualisation of our left_join First, let’s read in the region definitions data table and select only the columns we want. Note that I have piped my read.csv result into a select call, creating a tidy chunk that reads and selects the data that we need. region_defs &lt;- read.csv(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1&quot;) %&gt;% select(code, mgmtArea) head(region_defs) ## code mgmtArea ## 1 GSE Unallocated Southeast Alaska ## 2 NSE Northern Southeast Alaska ## 3 SSE Southern Southeast Alaska ## 4 YAK Yakutat ## 5 PWSmgmt Prince William Sound Management Area ## 6 BER Bering River Subarea Copper River Subarea If you examine the region_defs data.frame, you’ll see that the column names don’t exactly match the image above. If the names of the key columns are not the same, you can explicitly specify which are the key columns in the left and right side as shown below: catch_joined &lt;- left_join(catch_long, region_defs, by = c(&quot;Region&quot; = &quot;code&quot;)) head(catch_joined) ## # A tibble: 6 × 5 ## Region Year species catch mgmtArea ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 SSE 1886 Chinook 0 Southern Southeast Alaska ## 2 SSE 1886 Sockeye 5000 Southern Southeast Alaska ## 3 SSE 1886 Coho 0 Southern Southeast Alaska ## 4 SSE 1886 Pink 0 Southern Southeast Alaska ## 5 SSE 1886 Chum 0 Southern Southeast Alaska ## 6 SSE 1887 Chinook 0 Southern Southeast Alaska Notice that I have deviated from our usual pipe syntax (although it does work here!) because I prefer to see the data.frames that I am joining side by side in the syntax. Another way you can do this join is to use rename to change the column name code to Region in the region_defs data.frame, and run the left_join this way: region_defs &lt;- region_defs %&gt;% rename(Region = code, Region_Name = mgmtArea) catch_joined &lt;- left_join(catch_long, region_defs, by = c(&quot;Region&quot;)) head(catch_joined) Now our catches have the auxiliary information from the region definitions file alongside them. Note: dplyr provides a complete set of joins: inner, left, right, full, semi, anti, not just left_join. separate() and unite() separate() and its complement, unite() allow us to easily split a single column into numerous (or numerous into a single). This can come in really handle when we need to split a column into two pieces by a consistent separator (like a dash). Let’s make a new data.frame with fake data to illustrate this. Here we have a set of site identification codes. with information about the island where the site is (the first 3 letters) and a site number (the 3 numbers). If we want to group and summarize by island, we need a column with just the island information. sites_df &lt;- data.frame(site = c(&quot;HAW-101&quot;, &quot;HAW-103&quot;, &quot;OAH-320&quot;, &quot;OAH-219&quot;, &quot;MAI-039&quot;)) sites_df %&gt;% separate(site, c(&quot;island&quot;, &quot;site_number&quot;), &quot;-&quot;) ## island site_number ## 1 HAW 101 ## 2 HAW 103 ## 3 OAH 320 ## 4 OAH 219 ## 5 MAI 039 Exercise: Split the city column in the following data.frame into city and state_code columns: cities_df &lt;- data.frame(city = c(&quot;Juneau AK&quot;, &quot;Sitka AK&quot;, &quot;Anchorage AK&quot;)) # Write your solution here unite() does just the reverse of separate(). If we have a data.frame that contains columns for year, month, and day, we might want to unite these into a single date column. dates_df &lt;- data.frame(year = c(&quot;1930&quot;, &quot;1930&quot;, &quot;1930&quot;), month = c(&quot;12&quot;, &quot;12&quot;, &quot;12&quot;), day = c(&quot;14&quot;, &quot;15&quot;, &quot;16&quot;)) dates_df %&gt;% unite(date, year, month, day, sep = &quot;-&quot;) ## date ## 1 1930-12-14 ## 2 1930-12-15 ## 3 1930-12-16 Exercise: Use unite() on your solution above to combine the cities_df back to its original form with just one column, city: # Write your solution here Summary We just ran through the various things we can do with dplyr and tidyr but if you’re wondering how this might look in a real analysis. Let’s look at that now: catch_original &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1&quot;, method = &quot;libcurl&quot;)) region_defs &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1&quot;, method = &quot;libcurl&quot;)) %&gt;% select(code, mgmtArea) mean_region &lt;- catch_original %&gt;% select(-All, -notesRegCode) %&gt;% mutate(Chinook = ifelse(Chinook == &quot;I&quot;, 1, Chinook)) %&gt;% mutate(Chinook = as.numeric(Chinook)) %&gt;% pivot_longer(-c(Region, Year), names_to = &quot;species&quot;, values_to = &quot;catch&quot;) %&gt;% mutate(catch = catch*1000) %&gt;% group_by(Region) %&gt;% summarize(mean_catch = mean(catch)) %&gt;% left_join(region_defs, by = c(&quot;Region&quot; = &quot;code&quot;)) head(mean_region) ## # A tibble: 6 × 3 ## Region mean_catch mgmtArea ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ALU 40384. Aleutian Islands Subarea ## 2 BER 16373. Bering River Subarea Copper River Subarea ## 3 BRB 2709796. Bristol Bay Management Area ## 4 CHG 315487. Chignik Management Area ## 5 CKI 683571. Cook Inlet Management Area ## 6 COP 179223. Copper River Subarea "],["session-6-collaboration-practices.html", "6 Session 6: Collaboration Practices 6.1 Thinking preferences 6.2 Collaboration, authorship and data policies", " 6 Session 6: Collaboration Practices 6.1 Thinking preferences 6.1.1 Learning Objectives An activity and discussion that will provide: Opportunity to get to know fellow participants and trainers An introduction to variation in thinking preferences 6.1.2 Thinking Preferences Activity Step 1: Don’t jump ahead in this document. (Did I just jinx it?) Step 2: Review the list of statements here and reflect on your traits. Do you learn through structured activities? Are you conscious of time and are punctual? Are you imaginative? Do you like to take risks? Determine the three statements that resonate most with you and record them. Note the symbol next to each of them. Step 3: Review the symbol key here and assign a color to each of your three remaining statements. Which is your dominant color or are you a mix of three? Step 4: Using the zoom breakout room feature, move between the five breakout rooms and talk to other participants about their dominant color statements. Keep moving until you cluster into a group of ‘like’ dominant colors. If you are a mix of three colors, find other participants that are also a mix. Step 5: When the breakout rooms have reached stasis, each group should note the name and dominant color of your breakout room in slack. Step 6: Take a moment to reflect on one of the statements you selected and share with others in your group. Why do you identify strongly with this trait? Can you provide an example that illustrates this in your life? 6.1.3 About the Whole Brain Thinking System Everyone thinks differently. The way individuals think guides the way they work, and the way groups of individuals think guides how teams work. Understanding thinking preferences facilitates effective collaboration and team work. The Whole Brain Model, developed by Ned Herrmann, builds upon our understanding of brain functioning. For example, the left and right hemispheres are associated with different types of information processing and our neocortex and limbic system regulate different functions and behaviours. The Herrmann Brain Dominance Instrument (HBDI) provides insight into dominant characteristics based on thinking preferences. There are four major thinking styles that reflect the left cerebral, left limbic, right cerebral and right limbic. Analytical (Blue) Practical (Green) Relational (Red) Experimental (Yellow) These four thinking styles are characterized by different traits. Those in the BLUE quadrant have a strong logical and rational side. They analyze information and may be technical in their approach to problems. They are interested in the ‘what’ of a situation. Those in the GREEN quadrant have a strong organizational and sequential side. They like to plan details and are methodical in their approach. They are interested in the ‘when’ of a situation. The RED quadrant includes those that are feelings-based in their apporach. They have strong interpersonal skills and are good communicators. They are interested in the ‘who’ of a situation. Those in the YELLOW quadrant are ideas people. They are imaginative, conceptual thinkers that explore outside the box. Yellows are interested in the ‘why’ of a situation. Most of us identify with thinking styles in more than one quadrant and these different thinking preferences reflect a complex self made up of our rational, theoretical self; our ordered, safekeeping self; our emotional, interpersonal self; and our imaginitive, experimental self. Undertsanding the complexity of how people think and process information helps us understand not only our own approach to problem solving, but also how individuals within a team can contribute. There is great value in diversity of thinking styles within collaborative teams, each type bringing stengths to different aspects of project development. 6.1.4 Bonus Activity: Your Complex Self Using the statements contrained within this document, plot the quadrilateral representing your complex self. 6.2 Collaboration, authorship and data policies 6.2.1 Developing a Code of Conduct Whether you are joining a lab group or establishing a new collaboration, articulating a set of shared agreements about how people in the group will treat each other will help create the conditions for successful collaboration. If agreements or a code of conduct do not yet exist, invite a conversation among all members to create them. Co-creation of a code of conduct will foster collaboration and engagement as a process in and of itself, and is important to ensure all voices heard such that your code of conduct represents the perspectives of your community. If a code of conduct already exists, and your community will be a long-acting collaboration, you might consider revising the code of conduct. Having your group ‘sign off’ on the code of conduct, whether revised or not, supports adoption of the principles. When creating a code of conduct, consider both the behaviors you want to encourage and those that will not be tolerated. For example, the Openscapes code of conduct includes Be respectful, honest, inclusive, accommodating, appreciative, and open to learning from everyone else. Do not attack, demean, disrupt, harass, or threaten others or encourage such behavior. Below are other example codes of conduct: NCEAS Code of Conduct Carpentries Code of Conduct Arctic Data Center Code of Conduct Mozilla Community Participation Guidelines Ecological Society of America Code of Conduct 6.2.2 Authorship and Credit Navigating issues of intellectual property and credit can be a challenge, particularly for early career researchers. Open communication is critical to avoiding misunderstandings and conflicts. Talk to your coauthors and collaborators about authorship, credit, and data sharing early and often. This is particularly important when working with new collaborators and across lab groups or disciplines which may have divergent views on authorship and data sharing. If you feel uncomfortable talking about issues surrounding credit or intellectual property, seek the advice or assistance of a mentor to support you in having these important conversations. The “Publication” section of the Ecological Society of America’s Code of Ethics is a useful starting point for discussions about co-authorship, as are the International Committee of Medical Journal Editors guidelines for authorship and contribution. You should also check guidelines published by the journal(s) to which you anticipate submitting your work. For collaborative research projects, develop an authorship agreement for your group early in the project and refer to it for each product. This example authorship agreement from the Arctic Data Center provides a useful template. It builds from information contained within Weltzin et al (2006) and provides a rubric for inclusion of individuals as authors. Your collaborative team may not choose to adopt the agreement in the current form, however it will prompt thought and discussion in advance of developing a consensus. Some key questions to consider as you are working with your team to develop the agreement: What roles do we anticipate contributors will play? e.g., the CASRAI CRediT project identifies 14 distinct roles: Conceptualization Data curation Formal Analysis Funding acquisition Investigation Methodology Project administration Resources Software Supervision Validation Visualization Writing – original draft Writing – review &amp; editing What are our criteria for authorship? (See the ICMJE guidelines for potential criteria) Will we extend the opportunity for authorship to all group members on every paper or product? Do we want to have an opt in or opt out policy? (In an opt out policy, all group members are considered authors from the outset and must request removal from the paper if they don’t want think they meet the criteria for authorship) Who has the authority to make decisions about authorship? Lead author? PI? Group? How will we decide authorship order? In what other ways will we acknowledge contributions and extend credit to collaborators? How will we resolve conflicts if they arise? 6.2.3 Data Policies As with authorship agreements, it is valuable to establish a shared agreement around handling of data when embarking on collaborative projects. Data collected as part of a funded research activity will typically have been managed as part of the Data Management Plan (DMP) associated with that project. However, collaborative research brings together data from across research projects with different data management plans and can include publicly accessible data from repositories where no management plan is available. For these reasons, a discussion and agreement around the handling of data brought into and resulting from the collaboration is warranted and management of this new data may benefit from going through a data management planning process. Below we discuss example data agreements. The example data policy template provided by the Arctic Data Center addresses three categories of data. Individual data not in the public domain Individual data with public access Derived data resulting from the project For the first category, the agreement considers conditions under which those data may be used and permissions associated with use. It also addresses access and sharing. In the case of individual, publicly accessible data, the agreement stipulates that the team will abide by the attribution and usage policies that the data were published under, noting how those requirements we met. In the case of derived data, the agreement reads similar to a DMP with consideration of making the data public; management, documentation and archiving; pre-publication sharing; and public sharing and attribution. As research data objects receive a persistent identifier (PID), often a DOI, there are citable objects and consideration should be given to authorship of data, as with articles. The following example lab policy from the Wolkovich Lab combines data management practices with authorship guidelines and data sharing agreements. It provides a lot of detail about how this lab approaches data use, attribution and authorship. For example: Section 6: Co-authorship &amp; data If you agree to take on existing data you cannot offer co-authorship for use of the data unless four criteria are met: The co-author agrees to (and does) make substantial intellectual contribution to the work, which includes the reading and editing of all manuscripts on which you are a co-author through the submission-for-publication stage. This includes helping with interpretation of the data, system, study questions. Agreement of co-authorship is made at the start of the project. Agreement is approved of by Lizzie. All data-sharers are given an equal opportunity at authorship. It is not allowed to offer or give authorship to one data-sharer unless all other data-sharers are offered an equal opportunity at authorship—this includes data that are publicly-available, meaning if you offer authorship to one data-sharer and were planning to use publicly-available data you must reach out to the owner of the publicly-available data and strongly offer equivalent authorship as offered to the other data-sharer. As an example, if five people share data freely with you for a meta-analysis and and a sixth wants authorship you either must strongly offer equivalent authorship to all five or deny authorship to the sixth person. Note that the above requirements must also be met in this situation. If one or more datasets are more central or critical to a paper to warrant selective authorship this must be discussed and approved by Lizzie (and has not, to date, occurred within the lab). 6.2.3.1 Policy Preview This policy is communicated with all incoming lab members, from undergraduate to postdocs and visiting scholars, and is shared here with permission from Dr Elizabeth Wolkovich. 6.2.4 Extra Reading Cheruvelil, K. S., Soranno, P. A., Weathers, K. C., Hanson, P. C., Goring, S. J., Filstrup, C. T., &amp; Read, E. K. (2014). Creating and maintaining high-performing collaborative research teams: The importance of diversity and interpersonal skills. Frontiers in Ecology and the Environment, 12(1), 31-38. DOI: 10.1890/130001 "],["session-7-functions-and-packages.html", "7 Session 7: Functions and Packages 7.1 Creating R Functions 7.2 Creating R Packages", " 7 Session 7: Functions and Packages 7.1 Creating R Functions Many people write R code as a single, continuous stream of commands, often drawn from the R Console itself and simply pasted into a script. While any script brings benefits over non-scripted solutions, there are advantages to breaking code into small, reusable modules. This is the role of a function in R. In this lesson, we will review the advantages of coding with functions, practice by creating some functions and show how to call them, and then do some exercises to build other simple functions. 7.1.0.1 Learning outcomes Learn why we should write code in small functions Write code for one or more functions Document functions to improve understanding and code communication 7.1.1 Why functions? In a word: DRY: Don’t Repeat Yourself By creating small functions that only one logical task and do it well, we quickly gain: Improved understanding Reuse via decomposing tasks into bite-sized chunks Improved error testing Temperature conversion Imagine you have a bunch of data measured in Fahrenheit and you want to convert that for analytical purposes to Celsius. You might have an R script that does this for you. airtemps &lt;- c(212, 30.3, 78, 32) celsius1 &lt;- (airtemps[1]-32)*5/9 celsius2 &lt;- (airtemps[2]-32)*5/9 celsius3 &lt;- (airtemps[3]-32)*5/9 Note the duplicated code, where the same formula is repeated three times. This code would be both more compact and more reliable if we didn’t repeat ourselves. Creating a function Functions in R are a mechanism to process some input and return a value. Similarly to other variables, functions can be assigned to a variable so that they can be used throughout code by reference. To create a function in R, you use the function function (so meta!) and assign its result to a variable. Let’s create a function that calculates celsius temperature outputs from fahrenheit temperature inputs. fahr_to_celsius &lt;- function(fahr) { celsius &lt;- (fahr-32)*5/9 return(celsius) } By running this code, we have created a function and stored it in R’s global environment. The fahr argument to the function function indicates that the function we are creating takes a single parameter (the temperature in fahrenheit), and the return statement indicates that the function should return the value in the celsius variable that was calculated inside the function. Let’s use it, and check if we got the same value as before: celsius4 &lt;- fahr_to_celsius(airtemps[1]) celsius4 ## [1] 100 celsius1 == celsius4 ## [1] TRUE Excellent. So now we have a conversion function we can use. Note that, because most operations in R can take multiple types as inputs, we can also pass the original vector of airtemps, and calculate all of the results at once: celsius &lt;- fahr_to_celsius(airtemps) celsius ## [1] 100.0000000 -0.9444444 25.5555556 0.0000000 This takes a vector of temperatures in fahrenheit, and returns a vector of temperatures in celsius. Challenge Now, create a function named celsius_to_fahr that does the reverse, it takes temperature data in celsius as input, and returns the data converted to fahrenheit. Then use that formula to convert the celsius vector back into a vector of fahrenheit values, and compare it to the original airtemps vector to ensure that your answers are correct. Hint: the formula for C to F conversions is celsius*9/5 + 32. # Your code goes here Did you encounter any issues with rounding or precision? 7.1.2 Documenting R functions Functions need documentation so that we can communicate what they do, and why. The roxygen2 package provides a simple means to document your functions so that you can explain what the function does, the assumptions about the input values, a description of the value that is returned, and the rationale for decisions made about implementation. Documentation in ROxygen is placed immediately before the function definition, and is indicated by a special comment line that always starts with the characters #'. Here’s a documented version of a function: #&#39; Convert temperature data from Fahrenheit to Celsius #&#39; #&#39; @param fahr Temperature data in degrees Fahrenheit to be converted #&#39; @return temperature value in degrees Celsius #&#39; @keywords conversion #&#39; @export #&#39; @examples #&#39; fahr_to_celsius(32) #&#39; fahr_to_celsius(c(32, 212, 72)) fahr_to_celsius &lt;- function(fahr) { celsius &lt;- (fahr-32)*5/9 return(celsius) } Note the use of the @param keyword to define the expectations of input data, and the @return keyword for defining the value that is returned from the function. The @examples function is useful as a reminder as to how to use the function. Finally, the @export keyword indicates that, if this function were added to a package, then the function should be available to other code and packages to utilize. 7.1.3 Summary Functions are useful to reduce redundancy, reuse code, and reduce errors Build functions with the function function Document functions with roxygen2 comments Spoiler – the exercise answered Don’t peek until you write your own… # Your code goes here celsius_to_fahr &lt;- function(celsius) { fahr &lt;- celsius*9/5 + 32 return(fahr) } result &lt;- celsius_to_fahr(celsius) airtemps == result ## [1] TRUE TRUE TRUE TRUE 7.1.4 Examples: Minimizing work with functions Functions can of course be as simple or complex as needed. They can be be very effective in repeatedly performing calculations, or for bundling a group of commands that are used on many different input data sources. For example, we might create a simple function that takes fahrenheit temperatures as input, and calculates both celsius and Kelvin temperatures. All three values are then returned in a list, making it very easy to create a comparison table among the three scales. convert_temps &lt;- function(fahr) { celsius &lt;- (fahr-32)*5/9 kelvin &lt;- celsius + 273.15 return(list(fahr=fahr, celsius=celsius, kelvin=kelvin)) } temps_df &lt;- data.frame(convert_temps(seq(-100,100,10))) Once we have a dataset like that, we might want to plot it. One thing that we do repeatedly is set a consistent set of display elements for creating graphs and plots. By using a function to create a custom ggplot theme, we can enable to keep key parts of the formatting flexible. FOr example, in the custom_theme function, we provide a base_size argument that defaults to using a font size of 9 points. Because it has a default set, it can safely be omitted. But if it is provided, then that value is used to set the base font size for the plot. custom_theme &lt;- function(base_size = 9) { ggplot2::theme( axis.ticks = ggplot2::element_blank(), text = ggplot2::element_text(family = &#39;Helvetica&#39;, color = &#39;gray30&#39;, size = base_size), plot.title = ggplot2::element_text(size = ggplot2::rel(1.25), hjust = 0.5, face = &#39;bold&#39;), panel.background = ggplot2::element_blank(), legend.position = &#39;right&#39;, panel.border = ggplot2::element_blank(), panel.grid.minor = ggplot2::element_blank(), panel.grid.major = ggplot2::element_line(colour = &#39;grey90&#39;, size = .25), legend.key = ggplot2::element_rect(colour = NA, fill = NA), axis.line = ggplot2::element_blank() ) } library(ggplot2) ggplot(temps_df, mapping=aes(x=fahr, y=celsius, color=kelvin)) + geom_point() + custom_theme(10) In this case, we set the font size to 10, and plotted the air temperatures. The custom_theme function can be used anywhere that one needs to consistently format a plot. But we can go further. One can wrap the entire call to ggplot in a function, enabling one to create many plots of the same type with a consistent structure. For example, we can create a scatterplot function that takes a data frame as input, along with a point_size for the points on the plot, and a font_size for the text. scatterplot &lt;- function(df, point_size = 2, font_size=9) { ggplot(df, mapping=aes(x=fahr, y=celsius, color=kelvin)) + geom_point(size=point_size) + custom_theme(font_size) } Calling that let’s us, in a single line of code, create a highly customized plot but maintain flexibiity via the arguments passed in to the function. Let’s set the point size to 3 and font to 16 to make the plot more legible. scatterplot(temps_df, point_size=3, font_size = 16) Once these functions are set up, all of the plots built with them can be reformatted by changing the settings in just the functions, whether they were used to create 1, 10, or 100 plots. 7.2 Creating R Packages 7.2.1 Learning Objectives In this lesson, you will learn: The advantages of using R packages for organizing code Simple techniques for creating R packages Approaches to documenting code in packages 7.2.2 Why packages? Most R users are familiar with loading and utilizing packages in their work. And they know how rich CRAN is in providing for many conceivable needs. Most people have never created a package for their own work, and most think the process is too complicated. Really it’s pretty straighforward and super useful in your personal work. Creating packages serves two main use cases: Mechanism to redistribute reusable code (even if just for yourself) Mechanism to reproducibly document analysis and models and their results Even if you don’t plan on writing a package with such broad appeal such as, say, ggplot2 or dplyr, you still might consider creating a package to contain: Useful utility functions you write i.e. a Personal Package. Having a place to put these functions makes it much easier to find and use them later. A set of shared routines for your lab or research group, making it easier to remain consistent within your team and also to save time. The analysis accompanying a thesis or manuscript, making it all that much easier for others to reproduce your results. The usethis, devtools and roxygen2 packages make creating and maintining a package to be a straightforward experience. 7.2.3 Install and load packages library(devtools) library(usethis) library(roxygen2) 7.2.4 Create a basic package Thanks to the great usethis package, it only takes one function call to create the skeleton of an R package using create_package(). Which eliminates pretty much all reasons for procrastination. To create a package called mytools, all you do is: setwd(&#39;..&#39;) create_package(&quot;mytools&quot;) ✔ Setting active project to &#39;/Users/jones/development/mytools&#39; ✔ Creating &#39;R/&#39; ✔ Creating &#39;man/&#39; ✔ Writing &#39;DESCRIPTION&#39; ✔ Writing &#39;NAMESPACE&#39; ✔ Writing &#39;mytools.Rproj&#39; ✔ Adding &#39;.Rproj.user&#39; to &#39;.gitignore&#39; ✔ Adding &#39;^mytools\\\\.Rproj$&#39;, &#39;^\\\\.Rproj\\\\.user$&#39; to &#39;.Rbuildignore&#39; ✔ Opening new project &#39;mytools&#39; in RStudio Note that this will open a new project (mytools) and a new session in RStudio server. The create_package function created a top-level directory structure, including a number of critical files under the standard R package structure. The most important of which is the DESCRIPTION file, which provides metadata about your package. Edit the DESCRIPTION file to provide reasonable values for each of the fields, including your own contact information. Information about choosing a LICENSE is provided in the Extending R documentation. The DESCRIPTION file expects the license to be chose from a predefined list, but you can use it’s various utility methods for setting a specific license file, such as the Apacxhe 2 license: usethis::use_apl2_license(name=&quot;Matthew Jones&quot;) ✔ Setting License field in DESCRIPTION to &#39;Apache License (&gt;= 2.0)&#39; ✔ Writing &#39;LICENSE.md&#39; ✔ Adding &#39;^LICENSE\\\\.md$&#39; to &#39;.Rbuildignore&#39; Once your license has been chosen, and you’ve edited your DESCRIPTION file with your contact information, a title, and a description, it will look like this: Package: mytools Title: Utility Functions Created by Matt Jones Version: 0.1 Authors@R: &quot;Matthew Jones &lt;jones@nceas.ucsb.edu&gt; [aut, cre]&quot; Description: Package mytools contains a suite of utility functions useful whenever I need stuff to get done. Depends: R (&gt;= 3.5.0) License: Apache License (&gt;= 2.0) LazyData: true 7.2.5 Add your code The skeleton package created contains a directory R which should contain your source files. Add your functions and classes in files to this directory, attempting to choose names that don’t conflict with existing packages. For example, you might add a file environemnt_info.R that contains a function environment_info() that you might want to reuse. This one might leave something to be desired…, but you get the point… The usethis::use_r() function will help set up you files in the right places. For example, running: usethis::use_r(&quot;environment_info&quot;) ● Modify &#39;R/environment_info.R&#39; creates the file R/environment_info.R, which you can then modify to add the implementation fo the following function: environment_info &lt;- function(msg) { print(devtools::session_info()) print(paste(&quot;Also print the incoming message: &quot;, msg)) } If your R code depends on functions from another package, then you must declare so in the Imports list in the DESCRIPTION file for your package. In our example above, we depend on the devtools package, and so we need to list it as a dependency. Once again, usethis provides a handy helper method: usethis::use_package(&quot;devtools&quot;) ✔ Adding &#39;devtools&#39; to Imports field in DESCRIPTION ● Refer to functions with `devtools::fun()` 7.2.6 Add documentation You should provide documentation for each of your functions and classes. This is done in the roxygen2 approach of providing embedded comments in the source code files, which are in turn converted into manual pages and other R documentation artifacts. Be sure to define the overall purpose of the function, and each of its parameters. #&#39; A function to print information about the current environment. #&#39; #&#39; This function prints current environment information, and a message. #&#39; #&#39; @param msg The message that should be printed #&#39; #&#39; @keywords debugging #&#39; #&#39; @export #&#39; #&#39; @examples #&#39; environment_info(&quot;This is an important message from your sponsor.&quot;) environment_info &lt;- function(msg) { print(devtools::session_info()) print(paste(&quot;Also print the incoming message: &quot;, msg)) } Once your files are documented, you can then process the documentation using the document() function to generate the appropriate .Rd files that your package needs. devtools::document() Updating mytools documentation Updating roxygen version in /Users/jones/development/mytools/DESCRIPTION Writing NAMESPACE Loading mytools Writing NAMESPACE Writing environment_info.Rd That’s really it. You now have a package that you can check() and install() and release(). See below for these helper utilities. 7.2.7 Test your package You can tests your code using the tetsthat testing framework. The ussethis::use_testthat() function will set up your package for testing, and then you can use the use_test() function to setup individual test files. For example, if you want to create tests of our environment_info functions, set it up like this: usethis::use_testthat() ✔ Adding &#39;testthat&#39; to Suggests field in DESCRIPTION ✔ Creating &#39;tests/testthat/&#39; ✔ Writing &#39;tests/testthat.R&#39; usethis::use_test(&quot;environment_info&quot;) ✔ Writing &#39;tests/testthat/test-environment_info.R&#39; ● Modify &#39;tests/testthat/test-environment_info.R&#39; You can now add tests to the test-environment_info.R, and you can run all of the tests using devtools::test(). For example, if you add a test to the test-environment_info.R file: test_that(&quot;A message is present&quot;, { capture.output(result &lt;- environment_info(&quot;A unique message&quot;)) expect_match(result, &quot;A unique message&quot;) }) Then you can run the tests to be sure all of your functions are working using devtools::test(): devtools::test() Loading mytools Testing mytools ✔ | OK F W S | Context ✔ | 2 | test-environment_info [0.1 s] ══ Results ════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════ Duration: 0.1 s OK: 2 Failed: 0 Warnings: 0 Skipped: 0 Yay, all tests passed! 7.2.8 Checking and installing your package Now that your package is built, you can check it for consistency and completeness using check(), and then you can install it locally using install(), which needs to be run from the parent directory of your module. devtools::check() devtools::install() Your package is now available for use in your local environment. 7.2.9 Sharing and releasing your package The simplest way to share your package with others is to upload it to a GitHub repository, which allows others to install your package using the install_github('mytools','github_username') function from devtools. If your package might be broadly useful, also consider releasing it to CRAN, using the release() method from `devtools(). Releasing a package to CRAN requires a significant amoutn of work to ensure it follows the standards set by the R community, but it is entirely tractable and a valuable contribution to the science community. If you are considering releasing a package more broadly, you may find that the supportive community at ROpenSci provides incredible help and valuable feeback through their onboarding process. Challenge Add temperature conversion functions with full documentation to your package, write tests to ensure the functions work properly, and then document(), check(), and install() the new version of the package. Don’t forget to update the version number before you install! 7.2.10 More reading Hadley Wickham’s awesome book: R Packages Thomas Westlake’s blog Writing an R package from scratch "],["session-8-hands-on-logic-models-and-synthesis-development.html", "8 Session 8: Hands On: Logic Models and Synthesis Development", " 8 Session 8: Hands On: Logic Models and Synthesis Development "],["session-9-hands-on-exploration-of-data-resources-and-synthesis-development.html", "9 Session 9: Hands On: Exploration of Data Resources and Synthesis Development 9.1 Datasets of Interest 9.2 Finding more data", " 9 Session 9: Hands On: Exploration of Data Resources and Synthesis Development 9.1 Datasets of Interest There is an excel spreadsheet of datasets of interest located in the sharepoint here. The vast majority of these datasets are already available online, in some form, which makes synthesis much easier. There are several other datasets that are not yet published, but will be published by week two of the workshop. This chart shows a summary of the excel spreadsheet, with links to published datasets on the left side, and hover text with more information over each bar. The datasets have been categorized (very loosely) into groups, though many of the fish datasets also contain water quality or plankton data. 9.2 Finding more data If, in exploring and discussing the information above, groups decide they are missing pieces of data, searching through DataONE is a good start. We created a trial data portal on DataONE that shows just datasets in our geographic area of interest. You can view the portal here. A portal is a collection of data packages from repositories within the DataONE federated network, displayed on a unique webpage. This Open Science Synthesis for the Delta Science Program portal was constructed by defining geographic boundaries for the region of interest and it displays 9,189 data packages from within the DataONE collection of over 838K. This is a dynamic subset that will continue to grow as more data are added to repositories within the DataONE network. Typically, a research project’s website won’t be maintained beyond the life of the project and all the information on the website that provides context for the data collection is lost. Portals can provide a means to preserve information regarding the projects’ objectives, scopes, and organization and couple this with the data files so it’s clear how to use and interpret the data for years to come. Plus, when datasets are scattered across the repositories in the DataONE network, portals can help see them all in one convenient webpage. Portals also leverage DataONE’s metric features, which create statistics describing the project’s data packages. Information such as total size of data, proportion of data file types, and data collection periods are immediately available from the portal webpage. Here is an example of a fully fledged data portal that contains not only searchable raw and derived data, but also pages describing the process, people, and key results of the working groups. Step-by-step instructoin on how to create a data portal is provided within Section 16, ‘Additional Resources’. "],["tools.html", "10 Session 10: Quantitative synthesis tools Learning outcomes Context Challenge Products Resources Reflection questions Learning outcomes Context Challenge Products Resources Reflection questions", " 10 Session 10: Quantitative synthesis tools Learning outcomes Hone your capacity to review scientific literature and scope critical ideas. Transform your research expertise into formal scientific evidence. Synthesize peer-reviewed science papers. Context To best explore conceptual and mental models for synthesis, a summary of the simple landscape of opportunities and more comprehensive mapping of synthesis and critical big-picture thinking in science is illuminating. Scientific synthesis is big picture science that describe a set of studies on a pinpointed topic (Lortie 2014). There are at least three simple and direct options that are amenable to a capture of the research associated with a scientific challenge that you have identified. Narrative review as a highlight, short commentary, or new idea paper that is a snapshot of the key findings from a field research summarizing the main discoveries and/or listing the most critical research gaps. Papers like these are often called Insights, Forum, or Ignite. Systematic reviews are similar to narrative reviews, but clear criteria are listed explaining how you selected papers, i.e. these search terms were used in the Web of Science and only studies that had these key inclusion attributes were used. Systematic are more replicable because others can follow your steps and get the same set of papers and hopefully reach similar conclusions about the corpus of evidence. These reviews also typically provide some simple quantitative data about the set of studies such as number of countries where the research was done, total sample sizes, number of variables examined, or any attributes that describe what the research was for a specific detail. The narrative component is similar to the first option because it can state what we know and what do not but these reviews do so much more precisely. Even a few numbers go a long way to convincing people about the extent that we know or have studied a subject in science. Papers like these are often termed Short Commentary or Mini-Review. Meta-analyses are systematic reviews plus for each primary study you summarize, you capture the relative efficacy of the treatment tested. Papers like this are often termed Reviews or Meta-analyses but other terms can be used too. Note: in some fields of research the terms ‘systematic review’ and ‘meta-analysis’ are used interchangeably, but in most environmental sciences, meta-analyses always have a measure of the strength of evidence from each studied included in the synthesis whilst systematic reviews typically do not. So for instance, narrative review might provide insights into vaccine research and report that we have tested three vaccine types but need to test more alternatives. A systematic review would state this too but mention how they checked the science, i.e. we checked 100 papers using these terms x,y, &amp; z in The Web of Science, and it might also state how many people were tested in total across all studies. This is a more powerful synthesis. However, the gold standard would be the meta-analysis that summarizes all of the above but also reports how well each vaccine type tested actually works on average across all the studies. Summary of options Evidence framing A more complex and comprehensive summary of the contemporary landscape of synthesis has been evolving. Framing scientific synthesis activities as an open, comprehensive, and diverse aggregation or summary of many forms of evidence expands our capacity to inform decision making. Contemporary synthesis science has innovated on evidence maps as a tool to show the evidence. This can be done through mapping evidence onto a geographic map to show where we know what we know (McKinnon et al. 2015). Evidence maps can also be less literal and map out knowledge as tabular or bubble plots that focus more on the relative frequencies of what we know about different concepts within a synthesis (Miake-Ley et al. 2016). This approach has also been framed as ‘evidence maps for evidence gaps’ because it can become abundantly clear where evidence is missing in terms of research or ideas (O’Leary et al. 2017). Importantly, decision making has also evolved to more substantially rely on synthesis science to inform strategy (Thomas-Walters et al. 2021). There has also been a drive to thus enable and support more synthesis to build capacity and connect disparate fields of research for more holistic solution sets (Ladouceur and Shackelford 2021). Challenge Review the literature from your field of research from the perspective of synthesis. Explore whether a simple three-paper typology is relevant or if the field has begun to incorporate and frame knowledge via other inference tools including these or even others. Methods, concepts, semantics, ontologies, etc. any forms of evidence aggregation that advances insight, innovation, and knowledge production. Familiarize yourself with the scientific synthesis options that can describe and capture the state-of-the-art research for your specific challenge. Select and refine your topic by populating a table with key terms. Check Google Scholar and The Web of Science trying these different terms. Document the relevant frequency of synthesis studies using these terms. Filter to most cited or the last three years of research only if the evidence is too extensive or if you suspect there is a key temporal bias or change within the field of inquiry. Repeat process for primary studies. Contrast the primary study focus with a synthesis focus. Do a cursory read of the abstracts of the synthesis studies. Identify other terms including synonyms and antonyms that you may have not included. Reflect on this challenge and ensure the terms you used and the papers you have are studying the dimension of the challenge you want to summarize. Decide if it makes sense to do a short narrative, systematic review, or meta-analysis (at least at this point in time). Products A clear vision of the challenge you want to tackle. A set of ideas, papers, and the outcome that you ultimately are likely to provide. The landscape of evidence you need to examine a process or challenge across many studies. Resources Slide deck Template to contrast relative frequencies of synthesis versus primary studies Reflection questions What synthesis studies have most informed or inspired your understanding in your field? Is there evidence that is most needed by stakeholders right now that needs to be compiled or derived? Was there any indication that the focus of the synthesis studies you retrieved differed from the primary studies? Do synthesis studies advance theory or depth of knowledge more rapidly than well-executed primary studies or experiments? Learning outcomes Develop a checklist for reading systematic reviews and meta-analyses. Document a quantitative synthesis process. Consolidate understanding of key synthesis elements from the literature. Context Evidence implementation and reuse is a non-trivial process in most disciplines. It is crucial that experts are able to use, reuse, and implement synthesis findings. This process of critical appraisal furthers a novel, big-picture view of scientific findings associated with single studies. This can take the form of a perspective that routinely weights relative evidence by purpose, reason, and other findings to improve decision making by stakeholders, the public, and other scientists. Application of this process to published peer-review evidence can be limited by transparency, level of reporting, missing data, meta-data articulation, and limited moderator reporting. Hence, ten simple rules for evidence reuse were proposed were broadly to highlight these and other challenges in synthesis science (Lortie and Owen 2020). Critical reading of meta-analyses is also a powerful skill for all experts (Lortie et al. 2013). This thinking and evaluation process has also been developed into a more prescriptive set of ten questions to apply to any published meta-analysis or systematic review (Nakagawa et al. 2017). These questions strengthen the reuse of current meta-analyses and provide a checklist for reporting in future systematic reviews and meta-analyses. Checklist reporting List from (Nakagawa et al. 2017): Is the search systematic and transparently documented? What question and what effect size? Is non-independence taken into account? Which meta-analytic model? Is the level of consistency among studies reported? Are the causes of variation among studies investigated? Are effects interpreted in terms of biological importance? Has publication bias been considered? Are results really robust and unbiased? Is the current state (and lack) of knowledge summarized? Purpose Good reporting is supported by good thinking. Purpose prevails. A primer for systematic reviews and meta-analysis in the sports science clearly articulates a clear and direct purpose delineation process for syntheses (Impellizzeri and Bizzini 2021). Goals checklist List from (Impellizzeri and Bizzini 2021): Identifying treatments that are not effective. Summarizing the likely magnitude of benefits of effective treatments. Identifying unanticipated risks of apparently effective treatments. Identifying gaps of knowledge. Auditing the quality of existing trials. Collectively, these how-to papers suggest that it would be ideal if synthesis reporting exceeded the norms and standards associated with primary research reporting to enable next-level synthesis and reproducibility. Nonetheless, it is easy to get lost in the technical details. Consequently, purpose, audience, and reuse should be kept at the forefront of reviewing, reporting, and doing scientific syntheses. These ideas can be mobilized for knowledge mining even in the early steps of evidence retrieval and reviewing for inclusion in a synthesis project. Challenge Apply the checklist to a systematic review and meta-analysis in your discipline published a number of years ago and again to a more recent synthesis study. Check whether the derived data were also published for each synthesis paper. Check main text of each and supplements and note whether a Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) checklist or flow diagram with provided. Do a primary publication research query using Google Scholar and/or The Web Of Science for your specific process. Document the relative frequency of reporting by search terms (complete template provided below). Review a small subset of the papers (screen only) and test out the PRISMAstatement R package for this scoping review. Products A checklist for review and reporting of these specific syntheses. A pilot literature dataset of the literature for a search(es) for a synthesis. 3. A PRISMA statement flow chart for use in retrospective pilot reporting. Resources Slide deck A synthesis review literature template PRISMA statement arguments for R package Reflection questions Was there evidence for a change in better synthesis reporting practices in your discipline? Were the derived data reported in syntheses or would it be possible to update/repeat a published synthesis in your discipline? Does the flow chart to reviewing and reusing research align with your cognitive modality and critical thinking approach to evidence? "],["session-12-synthesis-group-presentations-and-feedback.html", "11 Session 12: Synthesis Group Presentations and Feedback", " 11 Session 12: Synthesis Group Presentations and Feedback "],["meta.html", "12 Session 13: Meta-analysis in R Learning outcomes Context The 5 primary steps for meta-analyses in R. Challenge Products Resources Reflection questions", " 12 Session 13: Meta-analysis in R Learning outcomes Format data for meta-analyses in R. Explore the capacity of the R package metafor. Pilot statistics for two datasets. Context Try out these ideas in a meta-analysis. Securing derived data to replicate an existing synthesis was not common historically, but it is now becoming increasingly viable with open science influences on these contributions. How to do a meta-analysis is well described in the peer-reviewed literature (Field and Gillett 2010) and numerous books (Koricheva et al 2013) to name a few. Doing meta-analyses using the R programming language is similarly well articulated - particularly from the documentation associated with the package metafor. There are no bad choices in R with well over 100 packages associated with and supporting meta-analyses (conservatively listed at 151 packages. The output of Stata (an application specific to many medical meta-analyses), the R package meta, and metafor were virtually identical in several test cases (Lortie and Filazzola 2020). Ten criteria are proposed in contrasting R packages for this task specifically, but at the current time, metafor is the most commonly used and extensively documented. Hence, consider tackling the challenges here with this package as a robust starting point and entry in meta-statistics. The 5 primary steps for meta-analyses in R. Secure primary data. Build conceputal model for factors, reponses, and moderators. Calculate effect size(s). Fit appropriate meta-model. Explore significance levels, heterogeneity, and bias. Challenge Start simple and go with a classic. These wind turbine data and its meta-analysis changed the world. Download these data and appled the 5 steps from above with help from the metafor documentation. In the spirit of reuse and conceputal replication, this process was reported in 2017 with a more comprehensive dataset. Try out one of these datasets. Preventing exercise-induced bronchoconstriction meta-analysis A meta-analysis of restoration outcomes and species richness A synthesis of common garden studies testing for local adaptation to climate A synthesis shrub facilitation studies testing for increases in community diversity estimates Products Two R scripts for meta-analysis. A sense of data structures needed for meta-analyses in the R package metafor. Resources metafor documentation Bookdown free books Reflection questions Did the analytical process differ significantly from primary-data workflows? Do meta-models in R sufficiently report outcomes to assess strength of evidence? What other steps would be an appropriate addendum to this process? What relational or qualitative elements might be worthwhile to consider adding to meta-analyses and their interpretation for stronger evidence framing and reuse? "],["reporting.html", "13 Session 14: Quantitative synthesis workflow reporting Learning outcomes Context Challenge Products Resources Reflection questions", " 13 Session 14: Quantitative synthesis workflow reporting Learning outcomes Interpret meta-analyses. Leverage meta-analyses to inform decision making. Appreciate extent of synthesis tools capacities to direct inference. Context Interpreting meta-analyses and systematic reviews effectively and fairly is critical for the advancement of scientific theory. Conceptual and methodological developments in many fields currently rely on syntheses to evaluate the relative merit of contrasting options (Halpern et al 2020). Importantly however, meta-analyses can fail (Kotiaho and Tomkins 2002). The representativeness of the evidence is crucial because the intent and purpose of the primary studies should in principle align with the purpose of the synthesis (as discussed previously). The selection process for evidence and the statistics can introduce biases and lead to potential spurious interpretations of the relevance of a hypothesis for instance. Failure to find support for a hypothesis does not necessarily mean that this is not a conceptually valid endeavor or that it does not explain the functioning of a specific system. Sparse evidence or publication biases can skew negative results in meta-analyses and lead to erroneous interpretations because of the evidence. What we know about the world (or the functioning of the world) may not match what we know about the science about the world. However, support for hypotheses in spite of biases and evidence limitations is likely to be representative of the underlying processes and patterns in a system. Consequently, meta-analyses can be evaluated, at times based on evidence volume, as more one-sided versus two-sided tests of concepts or methods. Heterogeneity and moderator analyses will also temper the assessment capacity for a meta-analysis to succeed in functioning as a knowledge engineering tool. Evaluation and use of meta-analyses is relevant to society at large. Controversy can be resolved or obfuscated by these syntheses (De Vrieze 2018). Two key factors play into the public reuse arena. The interpretation process and the relative strength of the derived evidence between syntheses that diverge in their conclusions. The collision of ideas can be exacerbated when synthesist scientists do not transparently and clearly report findings. This can be further magnified when the media and others directly reuse a published meta-analysis. The other factor, strength of evidence, must be transparently and accurately handled within each meta-analysis by better reporting. Lack of supporting derived data and inadequate reporting of the study populations (of papers) that comprise contrasting meta-analyses impede quantitative contrasts of syntheses. Description of the differences between studies within a synthesis is foundational to a better mapping of science onto ‘truth’ (Ioannidis 2005). We must set a higher standard for synthesis reporting and adopt more open science components into these projects. Finally, education and discussion within a synthesis of how well that specific process functioned in summarizing an evidence hierarchy must be developed. Challenge Select a meta-analysis or systematic review with extensive reporting. Assess whether the interpretations are well supported by the evidence that was incorporated in the synthesis. Explore one of the case studies provided in this short course and examine the sensitivity of the conclusions to reductions in the volume of evidence. Test full versus reduced models in one of the examples, statistically, to explore moderator influences on net outcomes. Products A set of evidence from a meta-analysis with a sense of study quality. A simple script to explore the statistical interpretations of meta-analyses done in R. Resources Slide deck ROSES RepOrting standards for Systematic Evidence Syntheses: pro forma template Reflection questions Do primary studies need to be scored for quality? Do single large primary studies inform meta-analyses more than many smaller trials or experiments? Is there a mechanism qualitatively or quantitatively to demonstrate representativeness and address matching truths to a system? "],["session-15-hands-on-collaborative-data-review-and-prep-collaborative-synthesis-planning.html", "14 Session 15: Hands On: Collaborative Data Review and Prep, Collaborative Synthesis Planning", " 14 Session 15: Hands On: Collaborative Data Review and Prep, Collaborative Synthesis Planning "],["session-16-additional-resources.html", "15 Session 16: Additional Resources 15.1 Creating a data portal", " 15 Session 16: Additional Resources 15.1 Creating a data portal Data portals are a new feature available through DataONE. Researchers can easily view project information and datasets all in one place across the federated network of repositories. 15.1.1 What is a Portal? A portal is a collection of DataONE federated repository data packages on a unique webpage. Typically, a research project’s website won’t be maintained beyond the life of the project and all the information on the website that provides context for the data collection is lost. Portals can provide a means to preserve information regarding the projects’ objectives, scopes, and organization and couple this with the data files so it’s clear how to use and interpret the data for years to come. Plus, when datasets are scattered across the repositories in the DataONE network, portals can help see them all in one convenient webpage. Portals also leverage DataONE’s metric features, which create statistics describing the project’s data packages. Information such as total size of data, proportion of data file types, and data collection periods are immediately available from the portal webpage. 15.1.2 Portal Uses Portals allow users to bundle supplementary information about their group, data, or project along with the data packages. Data contributors can organize their project specific data packages into a unique portal and customize the portal’s theme and structure according to the needs of that project. Researchers can also use portals to compare their public data packages and highlight and share them with other teams, as well as the broader research audience. To see some example of what portals could look like, view the Toolik Field Station, Ecoblender Lab and SASAP portals. 15.1.3 Portal Main Components Portals have five components: an about page, a settings page, a data page, a metrics page, and customizable free-form pages. 15.1.3.1 About Page This page is the first page users will see after initially creating a new portal, and it is highly recommended that users use this feature to create a description for their portal. Add a picture of your logo, a title and description of your portal, and freeform text using the markdown editor. 15.1.3.2 Settings Tab On the settings tab, users can give the portal a title and assign it a unique url; also referred to as a portal identifier. Users can add a general description of the portal, upload an icon photo for their data, and upload icon photos from any partner organizations that have contributed to the data. These partner icons will appear in the footer banner on every page in a portal, likewise your portal icon will appear in the header banner. Every DataONE portal URL will follow this format: https://search.dataone.org/portals/portal_identifier 15.1.3.3 Data Page The data page is the most important component of the DataONE portal system. This is where users will display the data packages of their choice. It looks and performs just like the main DataONE user interface. 15.1.3.4 Metrics Page Unlike the first two pages, the metrics page cannot be edited or customized. It is a default feature that provides the following information about the data packages within a portal: The total number of publicly-available metadata records The volume (in bytes) of all publicly-available metadata records and data files The most recent date the datasets were last updated (metadata and data are treated separately) The file types of all publicly-available data The years in which data was collected, regardless of upload date Please contact DataONE’s support team at support@dataone.org about any questions or concerns about the metrics page. 15.1.3.5 Freeform Pages Freeform pages are an optional function provided by DataONE portals. Here, users can add as much supplementary information as needed using markdown. 15.1.3.5.1 Example Freeform Pages Below are two examples of ways users can take advantage of portal freeform pages to tie unique content together with their data packages. Users can add as many tabs as needed. The examples shown on this page are from the Toolik Field Station’s portal; visit this portal to explore its contents further. 15.1.4 Creating Portals A step-by-step guide on how to navigate DataONE and create a new portal. Video tutorials on how to create your first portal from the Arctic Data Center, a repository within the DataONE network. 15.1.4.1 Getting Started with Portals If you are on DataONE’s primary website, click on your name in the upper right hand corner when you are signed in to DataONE with your ORCID. A dropdown will appear, and you would select “My Portals”. On your profile settings page, select ‘My Portals’. After the page loads select the yellow button ‘+ New Portal’ to add a new portal, you’ll automatically be directed to a fresh edit session. 15.1.4.2 Portal About and Settings Page In a new edit session, the first thing you’ll see is the about page where you’ll be able to add details about your project page. On the settings tab, you’ll set the basic elements of your portal: Portal title Unique portal identifier This identifier will be used to create the portal URL. If the name is available, a label will indicate it’s available and if the name is taken already, it will note that the name is already taken. This feature ensures the portals are unique. Portal description Partner organization logos Adding collaborators to help you create your portal is as straightforward as copying and pasting in their ORCID into the box below the permissions section. You can choose whether the collaborator can view, edit, or is an owner of the portal. You can have multiples of each role. 15.1.5 Adding Data to Portals Video tutorials on using portal search filters from the Arctic Data Center Navigate to the ‘Data’ tab. In order to populate a portal with the data packages applicable to your needs, you will need to define the search results using the rule options on the top of the page. You can create a rule with any of the metadata fields that DataONE supports, and customize that rule by selecting an operator (like equals, does not equal, or is greater than) the value you select. You can create more than one rule, and use the selctor at the top to stipulate whether you want your portal to follow ALL of the rules you’ve specified or ANY of the rules. When you’ve selected the data you want, hit the green save button in the bottom right. If you need assistance assembling portal data using a complex query, please contact the DataONE Support Team. 15.1.5.1 Data Package Metrics As stated in Portal Main Components, the metrics page is a default function provided by DataONE. This page cannot be edited and cannot be viewed while editing. Users do have the option to delete the page if they’d like. To delete the page, select the arrow next to the word “Metrics” in the tab and choose “Delete” from the dropdown list. To see metric summaries, navigate to your portal in view mode. See Saving and Editing Portals for more information on how to view portals. Please contact the DataONE Support Team about any questions or concerns about the metrics page. 15.1.6 Creating Unique Freeform Pages Video tutorials on creating freeform pages from the Arctic Data Center To add a freeform page to a portal, select the ‘+’ tab next to the data and metric tabs and then choose the freeform option that appears on screen. A freeform page will then populate. Easily customize your banner with a unique image, title, and page description. To change the name of the tab, click on the arrow in the ‘Untitled’ tab and select ‘Rename’ from the dropdown list. Below the banner, there is a markdown text box with some examples on how to use the markdown formatting directives to customize the text display. There is also a formatting header at the top to assist if you’re unfamiliar with markdown. As you write, toggle through the Edit and Preview modes in the markdown text box to make sure your information is displaying as intended. Portals are flexible and can accommodate as many additional freeform pages as needed. The markdown header structure helps to generate the table of contents for the page. Please see these additional resources for help with markdown: Markdown reference Ten minute tutorial For a longer example where you can also preview the results, checkout the Showdown Live Editor 15.1.7 Saving and Editing Portals Be sure to save your portal when you complete a page to ensure your progress is retained. Whenever a portal is saved, a dialogue box will pop up at the top of the page prompting users to view their private portal in view mode. You can choose to ignore this and continue editing. To delete a page from your portal, select the arrow in the tab and choose ‘Delete’ from the dropdown. Users can view and edit their portal from their ‘My Portals’ tab. First, click the arrow your name in the top-right corner to drop down your menu options. Then, select ‘My Portals’ from the dropdown underneath your name. See the section on Getting Started with Portals for more details. Click on the portal title to view it or select the edit button to make changes. 15.1.8 How to Publish Portals New portals are automatically set to private and only visible to the portal creator. The portal will remain private until the owner decides to make it public. To make your portal public, go into the settings of your portal. Under the description, you’ll see a new section called ‘Sharing Options’. You can toggle between your portal being private and your portal being public there. 15.1.9 Sharing Portals In order to share a published portal, users can simply direct recipients to their unique identifier. Portal identifiers are embedded into DataONE’s portal URL: https://search.dataone.org/portals/portal-identifier To view or edit your portal identifier, go into edit mode in your portal. In the settings page, your portal URL will be the first item on the page. Challenge Develop a portal within DataONE, pulling together data related to your research, a topic of interest, or products from a research group or organization. Build a data query to create a meaningful subset of the DataONE collection Use your markdown skills to create an informative About page Consider adding links and embedding images or figures from publicly accessible datasets. 15.1.10 Tutorial Videos For video tutorials on how to create your first portal visit resources on the Arctic Data Center video tutorial page. 15.1.11 Acknowledgements Much of this documentation was composed by ESS-DIVE, which can be found here. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
