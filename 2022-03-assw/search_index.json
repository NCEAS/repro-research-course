[["index.html", "Open Science: Best Practices, Data Sovereignty and Co-production Open Science: Best Practices, Data Sovereignty and Co-production 0.1 About", " Open Science: Best Practices, Data Sovereignty and Co-production Mar 29 2022; 1600-2000 CEST Open Science: Best Practices, Data Sovereignty and Co-production 0.1 About These materials reflect a collaboration between the Arctic Data Center, the Navigating the New Arctic Community Office, and ELOKA as part of our organizational commitments to raising awareness and promoting best practices in data management, increasing data literacy, and engaging the community in conversations about data governance. These materials are associated with a workshop presented as part of the Arctic Science Summit Week 2022. 0.1.1 Schedule This workshop is being held (online) on Tuesday March 29th at 1600 CEST, within the full ASSW programme. Agenda CEST Topic 1600 Welcome and Introductions What is Open Science - Motivation for this workshop 1630 Research Reproducibility Tidy Data best practices 1700 Data Ethics 1730 BREAK 1745 Community Data 1815 Tidy Data 1845 BREAK 1900 Tidy Data Exercise 1930 Discussion 2000 Adjourn 0.1.2 Code of Conduct Please note that by participating in this activity you agree to abide by the NCEAS Code of Conduct and NNA-CO Guiding Principles. 0.1.3 About this book Citation .. "],["session-1-open-science-best-practices.html", "1 Session 1: Open Science Best Practices 1.1 Introduction to Reproducible Research 1.2 Best Practices: Data and Metadata 1.3 Tidy Data: Part 1 1.4 Open Data Ethics 1.5 Community Data", " 1 Session 1: Open Science Best Practices 1.1 Introduction to Reproducible Research 1.1.0.1 What is research reproducibility and how does it relate to open science? Reproducibility is a hallmark of scientific research, which is based on empirical observations coupled with explanatory models. Whether integrating data from across multiple studies and sources, or working with your own data, the data life cycle typically involves some degree of data collection/integration, quality assurance practices, analysis and synthesis. Operating in a reproducible fashion means that each of these steps can be easily re-executed to achieve the same result, ideally as part of a single workflow. Reproducibility means different things to different researchers. For our purposes, practical reproducibility looks like: - Preserving the data - Preserving the software workflow - Documenting what you did - Describing how to interpret it all Reproducibility does not, by definition, require openness. Reproducibility can be achieved within a single research activity or across a research program with a closed group of collaborators. However, when working in an OPEN and REPRODUCIBLE manner, we are better able to transform knowledge into benefits for society. In this section we will expand on the benefits of reproducible research and open science before highlighting some best practices. Why is reproducible research important? Working in a reproducible manner builds efficiencies into your own research practices. The ability to automate processes and rerun analyses as you collect more data, or share your full workflow (including data, code and products) with colleagues, will accelerate the pace of your research and collaborations. However, beyond these direct benefits, reproducible research builds trust in science with the public, policy makers and others. What data were used in this study? What methods applied? What were the parameter settings? What documentation or code are available to us to evaluate the results? Can we trust these data and methods? Are the results reproducible? Ionnidis (2005) contends that “Most research findings are false for most research designs and for most fields”, and a study of replicability in psychology experiments found that “Most replication effects were smaller than the original results” (Open Science Collaboration, 2015). In the case of ‘climategate’, it took three years, and over 300 personnel, to gather the necessary provenance information in order to document how results, figures and other outputs were derived from input sources. Time and effort that could have been significantly reduced with appropriate documentation and reproducible practices. Moving forward, through reproducible research training, practices, and infrastructure, the need to manually chase this information will be reduced enabling replication studies and great trust in science. 1.1.0.2 Open Science To enable full reproducibility by the broader community; researchers, practitioners, policy makers etc, all products of the research activity need to be accessible - open data, open code, and open publications. Further, full research transparency also requires open peer review. There are, of course, data sensitivities and ethical considerations regarding open everything and these will be discussed later. At its core, the aims of Open Science are to: 1. Increase transparency of the research process 1. Enable reproducibility of results and conclusions 1. Accelerate Discovery 1. Enhance and facilitate collaboration 1. Increase diversity, equity and inclusion 1. Transform knowledge into benefits for society Computational reproducibility Computational reproducibility is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions. To be able to evaluate the data, analyses, and models on which conclusions are drawn, computational reproducibility requires open science approaches, including straightforward steps for archiving data and code openly along with the scientific workflows describing the provenance of scientific results (e.g., Hampton et al. (2015), Munafò et al. (2017)). Scientific workflows encapsulate all of the steps from data acquisition, cleaning, transformation, integration, analysis, and visualization. Workflows can range in detail from simple flowcharts to fully executable scripts. R scripts and python scripts are a textual form of a workflow, and when researchers publish specific versions of the scripts and data used in an analysis, it becomes far easier to repeat their computations and understand the provenance of their conclusions. Computational reproducibility provides: transparency by capturing and communicating scientific workflows research to stand on the shoulders of giants (build on work that came before) credit for secondary usage and supports easy attribution increased trust in science Preserving computational workflows enables understanding, evaluation, and reuse for the benefit of future you and your collaborators and colleagues across disciplines. 1.2 Best Practices: Data and Metadata 1.2.1 Learning Objectives In this lesson, you will learn: How to acheive practical reproducibility Some best practices for data and metadata management 1.2.2 Best Practices: Overview The data life cycle has 8 stages: Plan, Collect, Assure, Describe, Preserve, Discover, Integrate, and Analyze. In this section we will cover the following best practices that can help across all stages of the data life cycle: Organizing Data File Formats Large Data Packages Metadata Data Identifiers Provenance Licensing and Distribution Our goal, generally, is to operate within this lifecycle in ways that are FAIR The FAIR principles are principles that help guide research so that it is more reusable. 1.2.2.1 Organizing Data We’ll spend an entire lesson later on that’s dedicated to organizing your data in a tidy and effective manner, but first, let’s focus on the benefits on having “clean” data and complete metadata. Decreases errors from redundant updates Enforces data integrity Helps you and future researchers to handle large, complex datasets Enables powerful search filtering Much has been written on effective data management to enable reuse. The following two papers offer words of wisdom: Some simple guidelines for effective data management. Borer et al. 2009. Bulletin of the Ecological Society of America. Nine simple ways to make it easier to (re)use your data. White et al. 2013. Ideas in Ecology and Evolution 6. In brief, some of the best practices to follow are: Have scripts for all data manipulation that start with the uncorrected raw data file and clean the data programmatically before analysis. Design your tables to add rows, not columns. A column should be only one variable and a row should be only one observation. Include header lines in your tables Use non-proprietary file formats (ie, open source) with descriptive file names without spaces. Non-proprietary file formats are essential to ensure that your data can still be machine readable long into the future. Open formats include text files and binary formats such as NetCDF. Common switches: Microsoft Excel (.xlsx) files - export to text (.txt) or comma separated values (.csv) GIS files - export to ESRI shapefiles (.shp) MATLAB/IDL - export to NetCDF When you have or are going to generate large data packages (in the terabytes or larger), it’s important to establish a relationship with the data center early on. The data center can help come up with a strategy to tile data structures by subset, such as by spatial region, by temporal window, or by measured variable. They can also help with choosing an efficient tool to store the data (ie NetCDF or HDF), which is a compact data format that helps parallel read and write libraries of data. 1.2.2.2 Metadata Guidelines Metadata (data about data) is an important part of the data life cycle because it enables data reuse long after the original collection. Imagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files? The goal is to have enough information for the researcher to understand the data, interpret the data, and then re-use the data in another study. Another way to think about it is to answer the following questions with the documentation: What was measured? Who measured it? When was it measured? Where was it measured? How was it measured? How is the data structured? Why was the data collected? Who should get credit for this data (researcher AND funding agency)? How can this data be reused (licensing)? Bibliographic Details The details that will help your data be cited correctly are: a global identifier like a digital object identifier (DOI); a descriptive title that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data a descriptive abstract that serves as a brief overview off the specific contents and purpose of the data package funding information like the award number and the sponsor; the people and organizations like the creator of the dataset (ie who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset Discovery Details The details that will help your data be discovered correctly are: the geospatial coverage of the data, including the field and laboratory sampling locations, place names and precise coordinates; the temporal coverage of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to; the taxonomic coverage of the data, including what species were measured and what taxonomy standards and procedures were followed; as well as any other contextual information as needed. Interpretation Details The details that will help your data be interpreted correctly are: the collection methods for both field and laboratory data; the full experimental and project design as well as how the data in the dataset fits into the overall project; the processing methods for both field and laboratory samples IN FULL; all sample quality control procedures; the provenance information to support your analysis and modelling methods; information about the hardware and software used to process your data, including the make, model, and version; and the computing quality control procedures like any testing or code review. Data Structure and Contents Well constructed metadata also includes information about the data structure and contents. Everything needs a description: the data model, the data objects (like tables, images, matricies, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation. Variable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (such as 0 = not collected), and any missing values (such as 999 = NA). Not only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable). For example, if you were to search for the character string carbon dioxide flux in the general search box at the Arctic Data Center, not all relevant results will be shown due to varying vocabulary conventions (ie, CO2 flux instead of carbon dioxide flux) across disciplines — only datasets containing the exact words carbon dioxide flux are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search. Demonstrates a typical search for “carbon dioxide flux”, yielding 20 datasets. (right) Illustrates an annotated search for “carbon dioxide flux”, yielding 29 datasets. Note that if you were to interact with the site and explore the results of the figure on the right, the dataset in red of Figure 3 will not appear in the typical search for “carbon dioxide flux.” Rights and Attribution Correctly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like: the citation format to be used when giving credit for the data; the attribution expectations for the dataset; the reuse rights, which describe who may use the data and for what purpose; the redistribution rights, which describe who may copy and redistribute the metadata and the data; and the legal terms and conditions like how the data are licensed for reuse. So, how do you organize all this information? There are a number of metadata standards (think, templates) that you could use, including the Ecological Metadata Language (EML), Geospatial Metadata Standards like ISO 19115 and ISO 19139, the Biological Data Profile (BDP), Dublin Core, Darwin Core, PREMIS, the Metadata Encoding and Transmission Standard (METS), and the list goes on and on. The Arctic Data Center runs on EML. 1.2.3 Data Identifiers Many journals require a DOI - a digital object identifier - be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to. At the Arctic Data Center, we assign a DOI to each published dataset. But, sometimes datasets need to be updated. Each version of a dataset published with the Arctic Data Center has a unique identifier associated with it. Researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available. When there is a newer version available, that will be clearly marked on the original dataset page with a yellow banner indicating as such. Having the data identified in this manner allows us to accurately track the dataset usage metrics. The Arctic Data Center tracks the number of citations, the number of downloads, and the number of views of each dataset in the catalog. We filter out most views by internet bots and repeat views within a small time window in order to make these metrics COUNTER compliant. COUNTER is a standard that libraries and repositories use to provide users with consistent, credible, and comparable usage data. 1.2.4 Data Citation Researchers should get in the habit of citing the data that they use - even if it’s their own data! - in each publication that uses that data. The Arctic Data Center has taken multiple steps towards providing data citation information for all datasets we hold in our catalog, including a feature enabling dataset owners to directly register citations to their datasets. We recently implemented this “Register Citation” feature to allow researchers to register known citations to their datasets. Researchers may register a citation for any occasions where they know a certain publication uses or refers to a certain dataset, and the citation will be viewable on the dataset profile within 24 hours. To register a citation, navigate to the dataset using the DOI and click on the citations tab. Once there, this dialog box will pop up and you’ll be able to register the citation with us. Click that button and you’ll see a very simple form asking for the DOI of the paper and if the paper CITES the dataset (that is, the dataset is explicitly identified or linked to somewhere in the text or references) or USES the dataset (that is, uses the dataset but doesn’t formally cite it). We encourage you to make this part of your workflow, and for you to let your colleagues know about it too! 1.2.5 Provanance &amp; Preserving Computational Workflows While the Arctic Data Center, Knowledge Network for Biocomplexity, and similar repositories do focus on preserving data, we really set our sights much more broadly on preserving entire computational workflows that are instrumental to advances in science. A computational workflow represents the sequence of computational tasks that are performed from raw data acquisition through data quality control, integration, analysis, modeling, and visualization. In addition, these workflows are often not executed all at once, but rather are divided into multiple workflows, earch with its own purpose. For example, a data acquistion and cleaning workflow often creates a derived and integrated data product that is then picked up and used by multiple downstream analytical workflows that produce specific scientific findings. These workflows can each be archived as distinct data packages, with the output of the first workflow becoming the input of the second and subsequent workflows. In an effort to make data more reproducible, datasets also support provenance tracking. With provenance tracking, users of the Arctic Data Center can see exactly what datasets led to what product, using the particular script or workflow that the researcher used. This is a useful tool to make data more compliant with the FAIR principles. In addition to making data more reproducible, it is also useful for building on the work of others; you can produce similar visualizations for another location, for example, using the same code. RMarkdown itself can be used as a provenance tool, as well - by starting with the raw data and cleaning it programmatically, rather than manually, you preserve the steps that you went through and your workflow is reproducible. 1.3 Tidy Data: Part 1 1.3.1 Learning Objectives Understand basics of relational data models aka tidy data Learn how to design and create effective data tables 1.3.2 Introduction In this lesson we are going to learn what relational data models are, and how they can be used to manage and analyze data efficiently. Relational data models are what relational databases use to organize tables. However, you don’t have to be using a relational database (like mySQL, MariaDB, Oracle, or Microsoft Access) to enjoy the benefits of using a relational data model. Additionally, your data don’t have to be large or complex for you to benefit. Here are a few of the benefits of using a relational data model: Powerful search and filtering Handle large, complex data sets Enforce data integrity Decrease errors from redundant updates Simple guidelines for data management A great paper called ‘Some Simple Guidelines for Effective Data Management’ (Borer et al. 2009) lays out exactly that - guidelines that make your data management, and your reproducible research, more effective. Use a scripted program (like R!) A scripted program helps to make sure your work is reproducible. Typically, point-and-click actions, such as clicking on a cell in a spreadsheet program and modifying the value, are not reproducible or easily explained. Programming allows you to both reproduce what you did, and explain it if you use a tool like Rmarkdown. Non-proprietary file formats are preferred (eg: csv, txt) Using a file that can be opened using free and open software greatly increases the longevity and accessibility of your data, since your data do not rely on having any particular software license to open the data file. Keep a raw version of data In conjunction with using a scripted language, keeping a raw version of your data is definitely a requirement to generate a reproducible workflow. When you keep your raw data, your scripts can read from that raw data and create as many derived data products as you need, and you will always be able to re-run your scripts and know that you will get the same output. Use descriptive file and variable names (without spaces!) When you use a scripted language, you will be using file and variable names as arguments to various functions. Programming languages are quite sensitive with what they are able to interpret as values, and they are particularly sensitive to spaces. So, if you are building reproducible workflows around scripting, or plan to in the future, saving your files without spaces or special characters will help you read those files and variables more easily. Additionally, making file and variables descriptive will help your future self and others more quickly understand what type of data they contain. Include a header line in your tabular data files Using a single header line of column names as the first row of your data table is the most common and easiest way to achieve consistency among files. Use plain ASCII text ASCII (sometimes just called plain text) is a very commonly used standard for character encoding, and is far more likely to persist very far into the future than proprietary binary formats such as Excel. The next three are a little more complex, but all are characteristics of the relational data model: Design tables to add rows, not columns Each column should contain only one type of information Record a single piece of data only once; separate information collected at different scales into different tables. 1.3.3 Recognizing untidy data Before we learn how to create a relational data model, let’s look at how to recognize data that does not conform to the model. Data Organization This is a screenshot of an actual dataset that came across NCEAS. We have all seen spreadsheets that look like this - and it is fairly obvious that whatever this is, it isn’t very tidy. Let’s dive deeper in to exactly why we wouldn’t consider it tidy. Multiple tables Your human brain can see from the way this sheet is laid out that it has three tables within it. Although it is easy for us to see and interpret this, it is extremely difficult to get a computer to see it this way, which will create headaches down the road should you try to read in this information to R or another programming language. Inconsistent observations Rows correspond to observations. If you look across a single row, and you notice that there are clearly multiple observations in one row, the data are likely not tidy. Inconsistent variables Columns correspond to variables. If you look down a column, and see that multiple variables exist in the table, the data are not tidy. A good test for this can be to see if you think the column consists of only one unit type. Marginal sums and statistics Marginal sums and statistics also are not considered tidy, and they are not the same type of observation as the other rows. Instead, they are a combination of observations. 1.4 Open Data Ethics Developed by the Exchange for Local Observations and Knowledge of the Arctic (ELOKA) and Navigating the New Arctic Community Office (NNA-CO). 1.4.0.1 Introduction This part of the training offers an introduction to ethics in the context of open science. It was developed with input from ELOKA and the NNA-CO, and is a work-in-progress – this is the first time we are offering this training. The training introduces ethics issues in a broad way and includes discussion of social science data and open science, but the majority of the section focuses on issues related to research with, by, and for Indigenous communities. We recognize that there is a need for more in-depth training and focus on open science for social scientists and others who are not engaging with Indigenous Knowledge holders and Indigenous communities, and hope to develop further resources in this area in the future. Many of the data stewardship practices that have been identified as good practices through Indigenous Data Sovereignty framework development are also relevant for those working with Arctic communities that are not Indigenous, although the rights frameworks and collective ownership is specific to the Indigenous context. The examples we include in this training are primarily drawn from the North American research context. In future trainings, we plan to expand and include examples from other Indigenous Arctic contexts. We welcome suggestions and resources that would strengthen this training for audiences outside of North America. We also recognize the importance of trainings on Indigenous data sovereignty and ethics that are being developed and facilitated by Indigenous organizations and facilitators. In this training we offer some introductory material but there is much more depth offered in IDS specific trainings. We include some suggestions of organizations offering further training in the “resources” section and encourage participants to seek out these opportunities to deepen their understanding. 1.4.0.2 Equity issues in open science Arctic communities (defined based on geographic location in the Arctic/sub-Arctic) are involved in research in diverse ways - as hosts to visiting or non-local researchers, as well as “home” to community researchers who are leading or collaborating on research projects. Over the past decades, community voices of discontent with standard research practices that are often exclusive and perpetuate inequities have grown stronger. The Arctic research community (defined more broadly as the range of institutions, organizations, researchers and local communities involved in research) is in the midst of a complex conversation about equity in research aimed at transforming research practice to make it more equitable and inclusive. One of the drivers of community concerns is the colonial practice of extracting knowledge from a place or group of people without respect for local norms of relationship with people and place and without an ethical commitment to sharing and making benefits of knowledge accessible and accountable to that place. Such approaches to knowledge and data extraction follow hundreds of years of exploration and research that viewed science as a tool of “Enlightenment” yet focused exclusively on benefits to White, European (or “southern” from an Arctic community perspective) researchers and scientists. This prioritization of non-local perspectives and needs (to Arctic communities) continues in Arctic research. Open science is ostensibly about making access to data and knowledge more equitable and open and enabling reproducible research. Because it is being implemented within a social and institutional context that continues to perpetuate inequities in research, however, there are significant challenges remaining to ethically and equitably implementing open science practices. Here are some examples: Many Open Science tools and practices (e.g. GitHub, non-proprietary software) have been adopted from software development, and are not familiar to many, even physical, scientists. This makes barriers to implementation of open science and participation in open science practices higher for some than for others. Some of the main focus areas of open science (making sure data is archived in a clean and reusable format, open access publication) are still not accessible for Arctic residents and others who are not already situated within an academic environment. Open science assumes access to tools and infrastructure, such as computers that have reliable and low-cost or subsidized internet access. These structural inequalities extend even within the academy. Researchers and academics based in lower-income countries have fewer provided resources and sources of support to pay for open access fees and face greater challenges in adopting open science practices. The emphasis of open science on stakeholder and rights holder engagement and knowledge co-production also creates unintended challenges for Arctic communities. For example, when the National Science Foundation’s Navigating the New Arctic initiative, which brought millions of additional funds to Arctic research, suggested that research projects incorporate co-production and collaboration with Arctic communities, community representatives reported being inundated with requests from researchers. Perhaps because NNA was designed, in part, to bring new researchers and disciplines into Arctic research, many of these requests were made without adequate regard for recognized good practices for community engagement (such as the importance of building relationships, reaching out very early for input; communicating “early and often;” and incorporating significant resources for community participants’ time and knowledge into project budgets, among other things). As a letter to NSF written by Kawerak, the Association of Village Council Presidents, the Aleut Community of St. Paul Island, and the Bering Sea Elders Group emphasized, without adequate attention to these practices as well as an emphasis on topics that communities have identified as important, research will not serve community partners and will fail to achieve its broader impact goals. (See also the 2021 update letter from the same organizations). 1.4.0.3 Social science and open data In addition to these equity related challenges in promoting open access, there are also different disciplinary norms and requirements that can create challenges for the adoption of open science practices. For example, traditional social science methods training and IRB processes emphasize confidentiality and privacy. Social scientists generally lack access to training about benefits of sharing data. On the other hand, social scientists are often trained to be attuned to issues of equity and access, including issues related to information equity. Those who lack this background may not give adequate attention or time to the process of partnering with communities. The Arctic Horizons Report reviewed challenges around social science data management. In 2020, a workshop organized by the Arctic Data Center reviewed some of the challenges for sharing and reusing social science data, including: Data heterogeneity, including “unstructured” data that is not always well supported by data repositories; A dearth of metadata support for social sciences, with repositories not always offering relevant metadata fields; A lack of “formal vocabularies” that limits findability of social science data in searches; Limited training and support in data management practices for social scientists; limited examples using social science data in interdisciplinary trainings. In addition to these challenges, a number of broader concerns about sharing data were identified, including: Concerns about proper handling of sensitive data; importance of upholding IRB and ethics requirements, data sharing/use agreements; Importance of context - researchers may feel that data reuse is too difficult given lack of contextual knowledge; Concerns about upholding Indigenous data sovereignty. 1.4.0.4 Open (and other) data challenges for Indigenous Peoples The introduction of open data requirements has raised concerns about how open data objectives such as making data freely available for use and reuse may conflict with Indigenous Data Sovereignty and the right of Indigenous peoples to govern their data (Rainie et al. 2019: 301). Stephanie Carroll (formerly Rainie) and colleagues (2019) have summarized some of the data challenges for Indigenous Peoples, including: Data collection invisibility and bias Insufficient involvement of Indigenous Peoples in broader decisions/discussions about data governance, including decisions about what data is collected, when, how, and by whom, as well as involvement in global data governance bodies. Even if Indigenous peoples are not part of data collection, if the data is collected on Indigenous lands and could be used to support decisions that impact these lands, animals, or Indigenous peoples, they should be part of decision-making about that data. Data access, use, and interpretation, including: Insufficient availability of data about Indigenous peoples - gaps in national data collection Challenges in data access (infrastructure and training) Data collected from a “deficit” lens - reinforces perception of dysfunction, leading to bias and misrepresentation. Open data movement prioritizes making data available for reuse. However, there are concerns about secondary analysis of Indigenous data given the importance of context to understanding/interpreting Indigenous data and emphasis on building and maintaining relationships to Indigeous research (Burrage 2021). Important for researchers generating data that they hope will be useful to Indigenous communities to be aware of ethical frameworks for working with/sharing data and partnering with communities. Data ownership and appropriation Digitization of data/“big data” - the context surrounding data is often very important to Indigenous Knowledge and data. The digitization of data raises concerns about how and to what extent this context can be preserved. Individual vs. collective rights. Data ownership is often focused on individual rights (both in protection of sensitive data as well as in intellectual property rights), while Indigenous peoples have both individual and collective rights in relation to data. All requests for data contributions, clarification, or informational resources are requests for collaboration and should be treated with the same level of importance as larger projects (Murphy 2019). 1.4.0.5 The role of IRBs and Indigenous data In the United States, Institutional Review Boards (IRBs; in Canada they are referred to as Research Ethics Boards or REBs) focus on ensuring ethical treatment and protection of research subjects with a particular focus on vulnerable populations and ethical management of data. For projects proposing research that involves Indigenous peoples in the United States, IRBs will often refer the review to Tribal IRBs, which are research review boards established and implemented directly by Tribal Nations. Tribal IRBs reflect and respond to community needs, changes in research, and revisions to research policy (Around Him et al. 2019). Oversight mechanisms range from federally registered review bodies and policy development to community-specific frameworks and approaches (see Around Him et al. 2019 for differentiation, justification and authority). In addition to IRB review, universities require ethics training for researchers who are doing research with human subjects, including Indigenous Peoples. Many universities use the web-based, third-party CITI training program, which offers different short courses. A basic training course for social and behavioral science researchers covers the history of ethical misconduct in research, how human subjects research is defined, federal regulations that govern research practice, assessing risk, informed consent, privacy and confidentiality, and ethics requirements for different categories of vulnerable populations including prisoners and children. While the CITI human subjects trainings touch on topics related to Indigenous peoples, they are not at all comprehensive. A CITI webinar, “Research with Native American Communities: Important Considerations when Applying Federal Regulations” introduces more topics. The r-ETHICS training (Ethics Training for Health in Indigenous Communities Study) is starting to become an acceptable, recognizable CITI addition for IRB training by tribal entities. Specific universities have adopted tribal consultation policies (such as the Arizona Board of Regents’ (ABOR) Tribal Consultation policy (1-118) adopted in 2016; University of Wisconsin System Board of Regents tribal consultation policy adopted in 2021; Washington State University EP 41 adopted in 2021). These policies highlight where consultation is expected/required and what the process should be (Marley 2019). In Canada, funding agencies have established the Tri-Council Policy Statement on Ethical Conduct for Research Involving Humans; Chapter 9 focuses on research involving First Nations, Inuit and Metis. The chapter points to the importance of addressing issues regarding access to data and use of data with involved communities and suggests using a research agreement to establish any limits on data use and sharing, provisions to offer the opportunity for research participants and collaborators to review reports and publications prior to dissemination/publication, and to include provisions for any anticipated secondary use of information. The Tri-Council Statement has resulted in direct modifications to institutional-level REB (Canada) processes and procedures, such as incorporation of dedicated sections focusing on Indigenous research ethics within University protocols. For example, the “Indigenous Peoples and Community Engagement” section of an REB application at a Canadian university asks researchers to “Describe arrangements for the participating community’s/ies’ ownership and/or sharing of project data and findings, including the OCAP principles.” 1.4.0.6 Indigenous data governance and sovereignty All governing entities, whether national, state, local, or tribal, need access to good, current, relevant data in order to make policy, planning, and programmatic decisions. Indigenous nations and organizations have had to push for data about their peoples and communities to be collected and shared in ethical and culturally appropriate ways, and they have also had to fight for resources and capacity to develop and lead their own research programs. 1.4.0.6.1 Indigenous data definitions: Indigenous data sovereignty “…refers to the right of Indigenous peoples to govern the collection, ownership, and application of data about Indigenous communities, peoples, lands, and resources (Rainie et al. 2019). These governance rights apply “regardless of where/by whom data is held (Rainie et al. 2019). Some Indigenous individuals and communities have expressed dissatisfaction with the term “data” as being too narrowly focused and abstract to represent the embedded and holistic nature of knowledge in Indigenous communities. Knowledge sovereignty is a related term that has a similar meaning but is framed more broadly, and has been defined as: “Tribal communities having control over the documentation and production of knowledge (such as through research activities) which relate to Alaska Native people and the resources they steward and depend on” (Kawerak 2021). Indigenous data is “data in a wide variety of formats inclusive of digital data and data as knowledge and information. It encompasses data, information, and knowledge about Indigenous individuals, collectives, entities, lifeways, cultures, lands, and resources.” (Rainie et al. 2019) Indigenous data governance is “The entitlement to determine how Indigenous data is governed and stewarded” (Rainie et al. 2019) 1.4.0.7 Frameworks in support of IDS 1.4.0.8 Resources 1.5 Community Data References "],["session-2-data-tidying.html", "2 Session 2: Data Tidying 2.1 Tidy Data: Part 2", " 2 Session 2: Data Tidying 2.1 Tidy Data: Part 2 2.1.1 Good enough data modeling Denormalized data When data are “denormalized” it means that observations about different entities are combined. In the above example, each row has measurements about both the community in which observations occurred, as well as observations of two individuals surveyed in that community. This is not normalized data. People often refer to this as wide format, because the observations are spread across a wide number of columns. Note that, should one survey another individual in either community, we would have to add new columns to the table. This is difficult to analyze, understand, and maintain. Tabular data Observations. A better way to model data is to organize the observations about each type of entity in its own table. This results in: Separate tables for each type of entity measured Each row represents a single observation within that entity Observations (rows) are all unique This is normalized data (aka tidy data) Variables. In addition, for normalized data, we expect the variables to be organized such that: All values in a column are of the same type All columns pertain to the same observed entity (e.g., row) Each column represents either an identifying variable or a measured variable Challenge Try to answer the following questions: What are the observed entities in the example above? What are the measured variables associated with those observations? Answer: If we use these questions to tidy our data, we should end up with: one table for each entity observed one column for each measured variable additional columns for identifying variables (such as community) Here is what our tidy data look like: Note that this normalized version of the data meets the three guidelines set by (Borer et al. 2009): Design tables to add rows, not columns Each column should contain only one type of information Record a single piece of data only once; separate information collected at different scales into different tables. 2.1.2 Using normalized data Normalizing data by separating it into multiple tables often makes researchers really uncomfortable. This is understandable! The person who designed this study collected all of this information for a reason - so that they could analyze it together. Now that our community and survey information are in separate tables, how would we use population as a predictor variable for language spoken, for example? The answer is keys - and they are the cornerstone of relational data models. When one has normalized data, we often use unique identifiers to reference particular observations, which allows us to link across tables. Two types of identifiers are common within relational data: Primary Key: unique identifier for each observed entity, one per row Foreign Key: reference to a primary key in another table (linkage) Challenge In our normalized tables above, identify the following: the primary key for each table any foreign keys that exist Answer The primary key of the top table is community. The primary key of the bottom table is id. The community column is the primary key of that table because it uniquely identifies each row of the table as a unique observation of a community. In the second table, however, the community column is a foreign key that references the primary key from the first table. Entity-Relationship Model (ER) An Entity-Relationship model allows us to compactly draw the structure of the tables in a relational database, including the primary and foreign keys in the tables. In the above model, one can see that each community in the community observations table must have one or more survey participants in the survey table, whereas each survey response has one and only one community. Merging data Frequently, analysis of data will require merging these separately managed tables back together. There are multiple ways to join the observations in two tables, based on how the rows of one table are merged with the rows of the other. When conceptualizing merges, one can think of two tables, one on the left and one on the right. The most common (and often useful) join is when you merge the subset of rows that have matches in both the left table and the right table: this is called an INNER JOIN. Other types of join are possible as well. A LEFT JOIN takes all of the rows from the left table, and merges on the data from matching rows in the right table. Keys that don’t match from the left table are still provided with a missing value (na) from the right table. A RIGHT JOIN is the same, except that all of the rows from the right table are included with matching data from the left, or a missing value. Finally, a FULL OUTER JOIN includes all data from all rows in both tables, and includes missing values wherever necessary. Sometimes people represent these as Venn diagrams showing which parts of the left and right tables are included in the results for each join. These however, miss part of the story related to where the missing value come from in each result. In the figure above, the blue regions show the set of rows that are included in the result. For the INNER join, the rows returned are all rows in A that have a matching row in B. 2.1.3 Data modeling exercise Break into groups, 1 per table Our funding agency requires that we take surveys of individuals who complete our training courses so that we can report on the demographics of our trainees and how effective they find our courses to be. In your small groups, design a set of tables that will capture information collected in a participant survey that would apply to many courses. Don’t focus on designing a comprehensive set of questions for the survey, one or two simple stand ins (eg: “Did the course meet your expectations?”, “What could be improved?”, “To what degree did your knowledge increase?”) would be sufficient. Include as variables (columns) a basic set of demographics and identifying information, such as career stage, date of birth, gender, name, and contact information. Draw your entity-relationship model in the invision session for your group that the instructor links to. Note: you don’t have to create an account or log-in. Click “continue as guest” in the upper right hand corner to access the drawing tool. 2.1.4 Resources Borer et al. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America. White et al. 2013. Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution 6. Software Carpentry SQL tutorial Tidy Data "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
