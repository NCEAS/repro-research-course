[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "",
    "text": "About the course\nHosted by the National Center for Ecological Anlysis and Synthesis (NCEAS) and in partnership with UCSB’s Office of Research, this seminar series is a training program where participants engage in synthesis research in a small cohort. It will equip participants with data science and team science tools, and provide them with a platform to conduct collaborative synthesis research. The overall aim is to grow cross-departmental relationships and interdisciplinary research outputs.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#nceas-expertise",
    "href": "index.html#nceas-expertise",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "NCEAS Expertise",
    "text": "NCEAS Expertise\nNCEAS, a research affiliate of UCSB, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science. NCEAS has over 25 years of success with this model among working groups and environmental professionals.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#week-three-communicating-your-science-and-reproducible-workflows",
    "href": "index.html#week-three-communicating-your-science-and-reproducible-workflows",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "Week Three: Communicating your Science and Reproducible Workflows",
    "text": "Week Three: Communicating your Science and Reproducible Workflows\nMarch 25 - 27, 2024\n\nLearning Objectives\n\nExplore visualization tools such as ggplot2, Quarto Dashboards and different packages to plot spatial data.\nBroaden the tool kit of data science analytical tools by exploring text analysis methods and reproducible workflows to access survey data.\nRecap and wrap up the concept of reproducibility and how to apply it into the everyday workflow.\n\n\n\nWeek’s Schedule",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Camila Vargas Poulsen, Rachel King, Casey O’Hara (2024), UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills, March 25-27, NCEAS Learning Hub. URL https://learning.nceas.ucsb.edu/2024-03-ucsb-faculty.\nAdditional contributors: Ben Bolker, Amber E. Budden, Julien Brun, Samantha Csik, Halina Do-Linh, Natasha Haycock-Chavez, S. Jeanette Clark, Julie Lowndes, Stephanie Hampton, Matt Jone, Samanta Katz, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "session_01.html",
    "href": "session_01.html",
    "title": "1  Seminar Series Recap and Reproducibility",
    "section": "",
    "text": "1.1 Reproducibility activity using LEGO®",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Seminar Series Recap and Reproducibility</span>"
    ]
  },
  {
    "objectID": "session_01.html#reproducibility-activity-using-lego",
    "href": "session_01.html#reproducibility-activity-using-lego",
    "title": "1  Seminar Series Recap and Reproducibility",
    "section": "",
    "text": "Learning Objectives\n\nIllustrate elements of good reproducibility through the medium of LEGO®\nDiscuss what is needed and what is not needed for good reproducibility\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThis activity is largely based on the LEGO® Metadata for Reproducibility game pack, which was developed by Mary Donaldson and Matt Mahon.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Seminar Series Recap and Reproducibility</span>"
    ]
  },
  {
    "objectID": "session_01.html#getting-started",
    "href": "session_01.html#getting-started",
    "title": "1  Seminar Series Recap and Reproducibility",
    "section": "1.2 Getting started",
    "text": "1.2 Getting started\n\n\n\n\n\n\nSetup\n\n\n\n\nGather into small groups\nGet LEGO® blocks and worksheets (instructions + metadata documentation)\nFollow directions on worksheets\n\n\n\nAt the end, we will discuss as a group.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Seminar Series Recap and Reproducibility</span>"
    ]
  },
  {
    "objectID": "session_01.html#discussion",
    "href": "session_01.html#discussion",
    "title": "1  Seminar Series Recap and Reproducibility",
    "section": "1.3 Discussion",
    "text": "1.3 Discussion\n\n\nDiscussion Questions\n\n\nDid you find this a simple way to document your process?\nWas there anything you found difficult to capture?\nDid those replicating the builds find it straightforward to follow?\nDid you encounter any ambiguity in the instructions?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Seminar Series Recap and Reproducibility</span>"
    ]
  },
  {
    "objectID": "session_02.html",
    "href": "session_02.html",
    "title": "2  Data Visualization",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_02.html#learning-objectives",
    "href": "session_02.html#learning-objectives",
    "title": "2  Data Visualization",
    "section": "",
    "text": "Understand the fundamentals of how the ggplot2 package works\nUse ggplot2’s theme and other customization functions create publication-grade graphics\nIntroduce the leaflet and DT package to create interactive maps and tables respectively",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_02.html#overview",
    "href": "session_02.html#overview",
    "title": "2  Data Visualization",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nggplot2 is a popular package for visualizing data in R. From the home page:\n\nggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.\n\nIt’s been around for years and has pretty good documentation and tons of example code around the web (like on StackOverflow). The goal of this lesson is to introduce you to the basic components of working with ggplot2 and inspire you to go and explore this awesome resource for visualizing your data.\n\n\n\n\n\n\nggplot2 vs base graphics in R vs others\n\n\n\nThere are many different ways to plot your data in R. All of them work! However, ggplot2 excels at making complicated plots easy and easy plots simple enough\nBase R graphics (plot(), hist(), etc) can be helpful for simple, quick and dirty plots. ggplot2 can be used for almost everything else.\n\n\nLet’s dive into creating and customizing plots with ggplot2.\n\n\n\n\n\n\nSetup\n\n\n\n\nMake sure you’re in the right project (training_{USERNAME}) and use the Git workflow by Pulling to check for any changes. Then, create a new Quarto document, delete the default text, and save this document.\nLoad the packages we’ll need:\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(janitor) # expedite cleaning and exploring data\nlibrary(scales) # scale functions for visualization\nlibrary(leaflet) # interactive maps\nlibrary(DT) # interactive tables\n\n\nLoad the data table directly from the KNB Data Repository: Daily salmon escapement counts from the OceanAK database, Alaska, 1921-2017. Navigate to the link above, hover over the “Download” button for the ADFG_fisrtAttempt_reformatted.csv, right click, and select “Copy Link”.\n\n\nescape_raw &lt;- read_csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Af119a05b-bbe7-4aea-93c6-85434dcb1c5e\")\n\n\nLearn about the data. For this session we are going to be working with data on daily salmon escapement counts in Alaska. Check out the documentation.\nFinally, let’s explore the data we just read into our working environment.\n\n\n## Check out column names\ncolnames(escape_raw)\n\n## Peak at each column and class\nglimpse(escape_raw)\n\n## From when to when\nrange(escape_raw$sampleDate)\n\n## Which species?\nunique(escape$Species)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_02.html#getting-the-data-ready",
    "href": "session_02.html#getting-the-data-ready",
    "title": "2  Data Visualization",
    "section": "2.2 Getting the data ready",
    "text": "2.2 Getting the data ready\nIt is more frequent than not, that we need to do some wrangling before we can plot our data the way we want to. Now that we have read out data and have done some exploration, we’ll put our data wrangling skills to practice to get our data in the desired format.\n\n\n\n\n\n\nSide note on clean column names\n\n\n\njanitor::clean_names() is an awesome function to transform all column names into the same format. The default format for this function is snake_case_format. We highly recommend having clear well formatted column names. It makes your life easier down the line.\nHow it works?\n\nescape &lt;- escape_raw %&gt;% \n    janitor::clean_names()\n\nAnd that’s it! If we look to the colomn names of the object escape we can see all the columns are in a lowercase, snake format.\n\ncolnames(escape)\n\n[1] \"location\"     \"sasap_region\" \"sample_date\"  \"species\"      \"daily_count\" \n[6] \"method\"       \"latitude\"     \"longitude\"    \"source\"      \n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nCalculate the annual escapement by species and sasap_region,\nFilter the main 5 salmon species (Chinook, Sockeye, Chum, Coho and Pink)\n\n\n\n\nannual_esc &lt;- escape %&gt;%\n    separate(sample_date, c(\"year\", \"month\", \"day\"), sep = \"-\") %&gt;%\n    mutate(year = as.numeric(year)) %&gt;%\n    group_by(species, sasap_region, year) %&gt;%\n    summarize(escapement = sum(daily_count)) %&gt;%\n    filter(species %in% c(\"Chinook\", \"Sockeye\", \"Chum\", \"Coho\", \"Pink\"))\n\nhead(annual_esc)\n\n# A tibble: 6 × 4\n# Groups:   species, sasap_region [1]\n  species sasap_region                           year escapement\n  &lt;chr&gt;   &lt;chr&gt;                                 &lt;dbl&gt;      &lt;dbl&gt;\n1 Chinook Alaska Peninsula and Aleutian Islands  1974       1092\n2 Chinook Alaska Peninsula and Aleutian Islands  1975       1917\n3 Chinook Alaska Peninsula and Aleutian Islands  1976       3045\n4 Chinook Alaska Peninsula and Aleutian Islands  1977       4844\n5 Chinook Alaska Peninsula and Aleutian Islands  1978       3901\n6 Chinook Alaska Peninsula and Aleutian Islands  1979      10463\n\n\nThe chunk above used a lot of the dplyr commands that we’ve used, and some that are new. The separate() function is used to divide the sample_date column up into year, month, and day columns, and then we use group_by() to indicate that we want to calculate our results for the unique combinations of species, region, and year. We next use summarize() to calculate an escapement value for each of these groups. Finally, we use a filter and the %in% operator to select only the salmon species.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_02.html#plotting-with-ggplot2",
    "href": "session_02.html#plotting-with-ggplot2",
    "title": "2  Data Visualization",
    "section": "2.3 Plotting with ggplot2",
    "text": "2.3 Plotting with ggplot2\n\n2.3.1 Essentials components\nFirst, we’ll cover some ggplot2 basics to create the foundation of our plot. Then, we’ll add on to make our great customized data visualization.\n\n\n\n\n\n\nThe basics\n\n\n\n\nIndicate we are using ggplot() (call the ggplot2::ggplot() function)\nWhat data do we want to plot? (data = my_data)\nWhat is my mapping aesthetics? What variables do we want to plot? (define usingaes() function)\nDefine the geometry of our plot. This specifies the type of plot we’re making (use geom_*() to indicate the type of plot e.g: point, bar, etc.)\n\nNote To add layers to our plot, for example, additional geometries/aesthetics and theme elements or any ggplot object we use +.\n\n\nFor example, let’s plot total escapement by species. We will show this by creating the same plot in 3 slightly different ways. Each of the options below have the essential pieces of a ggplot.\n\n## Option 1 - data and mapping called in the ggplot() function\nggplot(data = annual_esc,\n       aes(x = species, y = escapement)) +\n    geom_col()\n\n## Option 2 - data called in ggplot function; mapping called in geom\nggplot(data = annual_esc) +\n    geom_col(aes(x = species, y = escapement))\n\n\n## Option 3 - data and mapping called in geom\nggplot() +\n    geom_col(data = annual_esc,\n             aes(x = species, y = escapement))\n\nThey all will create the same plot:\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Looking at different geoms\nHaving the basic structure with the essential components in mind, we can easily change the type of graph by updating the geom_*().\n\n\n\n\n\n\nggplot2 and the pipe operator\n\n\n\nJust like in dplyr and tidyr, we can also pipe a data.frame directly into the first argument of the ggplot function using the %&gt;% operator.\nThis can certainly be convenient, but use it carefully! Combining too many data-tidying or subsetting operations with your ggplot call can make your code more difficult to debug and understand.\n\n\nNext, we will use the pipe operator to pass into ggplot() a filtered version of annual_esc, and make a plot with different geometries.\nBoxplot\n\nannual_esc %&gt;%\n    filter(year == 1974,\n          species %in% c(\"Chum\", \"Pink\")) %&gt;%\n    ggplot(aes(x = species, y = escapement)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nViolin plot\n\nannual_esc %&gt;%\n    filter(year == 1974,\n           species %in% c(\"Chum\", \"Pink\")) %&gt;%\n    ggplot(aes(x = species, y = escapement)) +\n    geom_violin()\n\n\n\n\n\n\n\n\nLine and point\n\nannual_esc %&gt;%\n    filter(species  == \"Sockeye\",\n           sasap_region == \"Bristol Bay\") %&gt;%\n    ggplot(aes(x = year, y = escapement)) +\n    geom_line() +\n    geom_point()\n\n\n\n\n\n\n\n\n\n\n2.3.3 Customizing our plot\nLet’s go back to our base bar graph. What if we want our bars to be blue instead of gray? You might think we could run this:\n\nggplot(annual_esc,\n       aes(x = species, y = escapement,\n           fill = \"blue\")) +\n    geom_col()\n\n\n\n\n\n\n\n\nWhy did that happen?\nNotice that we tried to set the fill color of the plot inside the mapping aesthetic call. What we have done, behind the scenes, is create a column filled with the word “blue” in our data frame, and then mapped it to the fill aesthetic, which then chose the default fill color of red.\nWhat we really wanted to do was just change the color of the bars. If we want do do that, we can call the color option in the geom_col() function, outside of the mapping aesthetics function call.\n\nggplot(annual_esc,\n       aes(x = species, y = escapement)) +\n    geom_col(fill = \"blue\")\n\n\n\n\n\n\n\n\nWhat if we did want to map the color of the bars to a variable, such as region. ggplot() is really powerful because we can easily get this plot to visualize more aspects of our data.\n\nggplot(annual_esc,\n       aes(x = species, y = escapement,\n           fill = sasap_region)) +\n    geom_col()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeep in mind\n\n\n\n\nIf you want to map a variable onto a graph aesthetic (e.g., point color should be based on a specific region), put it within aes().\nIf you want to update your plot base on a constant (e.g. “Make ALL the points BLUE”), you can add the information directly to the relevant geom_ layer.\n\n\n\n\n2.3.3.1 Creating multiple plots\nWe know that in the graph we just plotted, each bar includes escapements for multiple years. Let’s leverage the power of ggplot to plot more aspects of our data in one plot.\nWe are going to plot escapement by species over time, from 2000 to 2016, for each region.\nAn easy way to plot another aspect of your data is using the function facet_wrap(). This function takes a mapping to a variable using the syntax ~{variable_name}. The ~ (tilde) is a model operator which tells facet_wrap() to model each unique value within variable_name to a facet in the plot.\nThe default behavior of facet wrap is to put all facets on the same x and y scale. You can use the scales argument to specify whether to allow different scales between facet plots (e.g scales = \"free_y\" to free the y axis scale). You can also specify the number of columns using the ncol = argument or number of rows using nrow =.\n\n## Subset with data from years 2000 to 2016\n\nannual_esc_2000s &lt;- annual_esc %&gt;%\n    filter(year %in% c(2000:2016))\n\n## Quick check\nunique(annual_esc_2000s$year)\n\n [1] 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014\n[16] 2015 2016\n\n## Plot with facets\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\n2.3.3.2 Setting ggplot themes\nNow let’s work on making this plot look a bit nicer. We are going to”\n\nAdd a title using labs()\nAdjust labels using labs()\nInclude a built in theme using theme_bw()\n\nThere are a wide variety of built in themes in ggplot that help quickly set the look of the plot. Use the RStudio autocomplete theme_ &lt;TAB&gt; to view a list of theme functions.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nYou can see that the theme_bw() function changed a lot of the aspects of our plot! The background is white, the grid is a different color, etc. There are lots of other built in themes like this that come with the ggplot2 package.\n\n\n\n\n\n\nExercise\n\n\n\nUse the RStudio auto complete, the ggplot2 documentation, a cheat sheet, or good old Google to find other built in themes. Pick out your favorite one and add it to your plot.\n\n\n\n\nThemes\n## Useful baseline themes are\ntheme_minimal()\ntheme_light()\ntheme_classic()\n\n\nThe built in theme functions (theme_*()) change the default settings for many elements that can also be changed individually using thetheme() function. The theme() function is a way to further fine-tune the look of your plot. This function takes MANY arguments (just have a look at ?theme). Luckily there are many great ggplot resources online so we don’t have to remember all of these, just Google “ggplot cheat sheet” and find one you like.\nLet’s look at an example of a theme() call, where we change the position of the legend from the right side to the bottom, and remove its title.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    theme_light() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank())\n\n\n\n\n\n\n\n\nNote that the theme() call needs to come after any built-in themes like theme_bw() are used. Otherwise, theme_bw() will likely override any theme elements that you changed using theme().\nYou can also save the result of a series of theme() function calls to an object to use on multiple plots. This prevents needing to copy paste the same lines over and over again!\n\nmy_theme &lt;- theme_light() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank())\n\nSo now our code will look like this:\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    my_theme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nUsing whatever method you like, figure out how to rotate the x-axis tick labels to a 45 degree angle.\n\nHint: You can start by looking at the documentation of the function by typing ?theme() in the console. And googling is a great way to figure out how to do the modifications you want to your plot.\n\nWhat changes do you expect to see in your plot by adding the following line of code? Discuss with your neighbor and then try it out!\n\nscale_x_continuous(breaks = seq(2000,2016, 2))\n\n\n\n\nAnswer\n## Useful baseline themes are\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    scale_x_continuous(breaks = seq(2000, 2016, 2)) +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    my_theme +\n    theme(axis.text.x = element_text(angle = 45,\n                                     vjust = 0.5))\n\n\n\n\n2.3.3.3 Smarter tick labels using scales\nFixing tick labels in ggplot can be super annoying. The y-axis labels in the plot above don’t look great. We could manually fix them, but it would likely be tedious and error prone.\nThe scales package provides some nice helper functions to easily rescale and relabel your plots. Here, we use scale_y_continuous() from ggplot2, with the argument labels, which is assigned to the function name comma, from the scales package. This will format all of the labels on the y-axis of our plot with comma-formatted numbers.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    scale_x_continuous(breaks = seq(2000, 2016, 2)) +\n    scale_y_continuous(labels = comma) +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    my_theme +\n    theme(axis.text.x = element_text(angle = 45,\n                                     vjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n2.3.3.4 Saving plots\nSaving plots using ggplot is easy! The ggsave() function will save either the last plot you created, or any plot that you have saved to a variable. You can specify what output format you want, size, resolution, etc. See ?ggsave() for documentation.\n\nggsave(\"figures/annualsalmon_esc_region.jpg\", width = 8, height = 6, units = \"in\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_02.html#interactive-visualization",
    "href": "session_02.html#interactive-visualization",
    "title": "2  Data Visualization",
    "section": "2.4 Interactive visualization",
    "text": "2.4 Interactive visualization\n\n2.4.1 Tables with DT\nNow that we know how to make great static visualizations, let’s introduce two other packages that allow us to display our data in interactive ways. These packages really shine when used with GitHub Pages, so at the end of this lesson we will publish our figures to the website we created earlier.\nFirst let’s show an interactive table of unique sampling locations using DT. Write a data.frame containing unique sampling locations with no missing values using two new functions from dplyr and tidyr: distinct() and drop_na().\n\nlocations &lt;- escape %&gt;%\n    distinct(location, latitude, longitude) %&gt;%\n    drop_na()\n\nAnd display it as an interactive table using datatable() from the DT package.\n\ndatatable(locations)\n\n\n\n\n\n\n\n2.4.2 Maps with leaflet\nSimilar to ggplot2, you can make a basic leaflet map using just a couple lines of code. Note that unlike ggplot2, the leaflet package uses pipe operators (%&gt;%) and not the additive operator (+).\nThe addTiles() function without arguments will add base tiles to your map from OpenStreetMap. addMarkers() will add a marker at each location specified by the latitude and longitude arguments. Note that the ~ symbol is used here to model the coordinates to the map (similar to facet_wrap() in ggplot).\n\nleaflet(locations) %&gt;%\n    addTiles() %&gt;%\n    addMarkers(\n        lng = ~ longitude,\n        lat = ~ latitude,\n        popup = ~ location\n    )\n\n\n\n\n\n\nYou can also use leaflet to import Web Map Service (WMS) tiles. Here is an example that utilizes the General Bathymetric Map of the Oceans (GEBCO) WMS tiles. In this example, we also demonstrate how to create a more simple circle marker, the look of which is explicitly set using a series of style-related arguments.\n\nleaflet(locations) %&gt;%\n    addWMSTiles(\n        \"https://www.gebco.net/data_and_products/gebco_web_services/web_map_service/mapserv?request=getmap&service=wms&BBOX=-90,-180,90,360&crs=EPSG:4326&format=image/jpeg&layers=gebco_latest&width=1200&height=600&version=1.3.0\",\n        layers = 'GEBCO_LATEST',\n        attribution = \"Imagery reproduced from the GEBCO_2022 Grid, WMS 1.3.0 GetMap, www.gebco.net\"\n    ) %&gt;%\n    addCircleMarkers(\n        lng = ~ longitude,\n        lat = ~ latitude,\n        popup = ~ location,\n        radius = 5,\n        # set fill properties\n        fillColor = \"salmon\",\n        fillOpacity = 1,\n        # set stroke properties\n        stroke = T,\n        weight = 0.5,\n        color = \"white\",\n        opacity = 1\n    )\n\n\n\n\n\n\nLeaflet has a ton of functionality that can enable you to create some beautiful, functional maps with relative ease. Here is an example of some we created as part of the State of Alaskan Salmon and People (SASAP) project, created using the same tools we showed you here. This map hopefully gives you an idea of how powerful the combination of R Markdown and GitHub Pages can be.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_02.html#ggplot2-resources",
    "href": "session_02.html#ggplot2-resources",
    "title": "2  Data Visualization",
    "section": "2.5 ggplot2 Resources",
    "text": "2.5 ggplot2 Resources\n\nWhy not to use two axes, and what to use instead: The case against dual axis charts by Lisa Charlotte Rost.\nCustomized Data Visualization in ggplot2 by Allison Horst.\nA ggplot2 tutorial for beautiful plotting in R by Cedric Scherer.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_03.html",
    "href": "session_03.html",
    "title": "3  Text Analysis in R",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Analysis in R</span>"
    ]
  },
  {
    "objectID": "session_03.html#learning-objectives",
    "href": "session_03.html#learning-objectives",
    "title": "3  Text Analysis in R",
    "section": "",
    "text": "Understand how to work with text data in R using functions from the stringr package\nApply functions from the tidytext package to prepare text and pdf documents for word frequency analysis\nPerform a simple sentiment analysis using stop-word lexicons and sentiment lexicons from the tidytext package",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Analysis in R</span>"
    ]
  },
  {
    "objectID": "session_03.html#overview",
    "href": "session_03.html#overview",
    "title": "3  Text Analysis in R",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nWorking with natural language text data, for example sentences and paragraphs, is quite different from working with tidy data frames of categorical and continuous variables. However, many packages have been developed to facilitate working with text in \\(\\textsf{R}\\). In this tutorial, we will use the stringr package to detect, extract, and modify strings, and the tidytext package to break documents down into dataframes to make them easy to work with in our familiar tidyverse style.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Analysis in R</span>"
    ]
  },
  {
    "objectID": "session_03.html#introduction-to-stringr",
    "href": "session_03.html#introduction-to-stringr",
    "title": "3  Text Analysis in R",
    "section": "3.2 Introduction to stringr",
    "text": "3.2 Introduction to stringr\nThe first part of this tutorial will walk through an exercise in extracting specific information from untidily formatted blocks of text, i.e. sentences and paragraphs rather than a nice data frame or .csv.\nThe example comes from a paper that examined species ranges from different datasets, and found some discrepancies that resulted from systematic errors. Many of the coral species ranges for IUCN rangemaps extended off the continental shelf into very deep waters; but most corals require shallower water and are dependent upon photosynthesis. However, the IUCN also included narrative text about the habitat preferences of each coral species to examine whether these corals, according to the IUCN’s own information, could be found in waters deeper than 200 meters.\n\n3.2.1 Load packages and data\n\n\n\n\n\n\nTip\n\n\n\nThe stringr package is part of the tidyverse meta-package. When you install tidyverse (e.g., install.packages('tidyverse')) the stringr package is installed with our other favorites like dplyr, ggplot2, and readr. When you attach the tidyverse package, stringr is automatically attached as well, so we don’t have to explicitly call library(stringr) - but because it is essential for the purposes of this tutorial, we’ll make it explicit anyway.\n\n\n\nlibrary(dplyr)       ### for general tidy data practices\nlibrary(tidyr)       ### for general tidy data practices\nlibrary(readr)       ### for reading in data\nlibrary(purrr)       ### toolkit for working with functions and vectors\nlibrary(forcats)     ### for working with factors\nlibrary(stringr)     ### for manipulating strings\nlibrary(tidytext)    ### to support text analysis using tidy data methods\nlibrary(pdftools)    ### to extract text data from .pdf documents\nlibrary(ggwordcloud) ### to generate a word cloud\n\nThe data are narratives pulled from the IUCN API (http://apiv3.iucnredlist.org/) for coral species, in order to identify their maximum depth. We’ll also pull up a set of data on species areas, but mostly just because that data includes scientific names for the corals so we can refer to species rather than ID numbers.\n\ncoral_narrs &lt;- read_csv('data/iucn_narratives.csv')\n### interested in species_id, habitat\n\ncoral_info &lt;- read_csv('data/coral_spp_info.csv')\n### info for species mapped in both datasets\n\n### create a dataframe with just ID, scientific name, and habitat\ncoral_habs_raw &lt;- coral_narrs %&gt;%\n  left_join(coral_info, by = 'iucn_sid') %&gt;%\n  select(iucn_sid, sciname, habitat)\n\nHere is an example of the text for the first species of coral. We can see the depth noted in the text; the challenge will be, how do we convert that into a useful format?\n\ncoral_habs_raw$habitat[1]\n\n\nThis species occurs in shallow, tropical reef environments. It is found only in subtidal turbid water, attached to wave washed rock. This species is found on subtidal rock and rocky reefs, the back slope, in lagoons, and on inter-reef rubble substrate. It may be found on the foreslope. This species is found to 10 m. Oulastrea is usually found in muddy areas on the shallow back reef and seldom occurs among dense coral on the fore reef (Wood 1983). It may be more abundant in degraded habitats where other coral species have disappeared, e.g., Jakarta Bay.\n\nIn pseudocode, one possible strategy for extracting depth information from these narrative descriptions might be:\ncoral_habs &lt;- coral_habs_raw %&gt;%\n  split into individual sentences %&gt;%\n  keep the sentences with numbers in them %&gt;%\n  isolate the numbers\n\n\n3.2.2 Intro to stringr functions\nBefore cracking open the coral text data, here we’ll play a little with some basic stringr functions, and pattern vs. vector of strings. Consider especially how we can use str_split, str_detect, str_replace; later we’ll see how to make effective use of str_extract as well.\n\nx &lt;- \"Everybody's got something to hide except for me and my monkey\"\n\n### Manipulate string case (upper, lower, sentence, title)\nstringr::str_to_title(x)\nstr_to_lower(x)\n\n### Split strings into multiple pieces, based on some pattern\nstr_split(x, 'hide'); str_split(x, 't')\n\n### Replace a pattern in a string with a different pattern\nstr_replace(x, 'except for', 'including')\nstr_replace(x, ' ', '_')\nstr_replace_all(x, ' ', '_')\n\n### Detect whether a pattern is found within a string\nstr_detect(x, 't'); str_detect(x, 'monk') ### is pattern in the string? T/F\n\n### Extract instances of patterns within a string.  Note: this is far more\n### powerful and interesting when using wildcards in your pattern!\nstr_extract(x, 't'); str_extract_all(x, 'y')\n\n### Locate the start and endpoint of a pattern within a string\nstr_locate(x, 't'); str_locate_all(x, 'y')\n\n\n\n3.2.3 Break coral data into sentences\nFirst we can use stringr::str_split() to break down the habitat column into manageable chunks, i.e. sentences. What is an easily accessible delimiter we can use to separate a paragraph into sentences?\nTake 1:\n\ncoral_habs &lt;- coral_habs_raw %&gt;%\n  mutate(hab_cut = str_split(habitat, '.'))\n\ncoral_habs$hab_cut[1]\n\nDid that behave as expected? In a moment we’ll see that a period is actually a special character we will later use as a wild card in a “regular expression” or “regex” pattern. Some other characters have special uses as well; so if we want them to be interpreted literally, we need to “escape” them.\nSome languages use a single backslash to “escape” a character (i.e., turn a character into a special function, e.g. \\n indicates a line break, and \\. indicates a literal period instead of a regex wildcard). In R stringr functions, usually you end up having to use a double backslash for R to interpret the escape sequence properly (e.g. \\\\n or \\\\.).\nAlso: why is splitting on just a period on its own probably a bad idea? Recall we are looking for numeric data, which might have a decimal point. But in a sentence, a period is usually followed by a space, while a decimal point is usually followed by another number - so we can slightly modify our pattern for splitting.\nTake 2:\n\n# coral_habs &lt;- coral_habs_raw %&gt;%\n#   mutate(hab_cut = str_split(habitat, '\\. '))\n### Error: '\\.' is an unrecognized escape in character string starting \"'\\.\"\n\ncoral_habs &lt;- coral_habs_raw %&gt;%\n  mutate(hab_cut = str_split(habitat, '\\\\. '))\n### creates a cell with a vector of broken up sentences!\n\n\n\n\n\n\n\n3.2.3.1 Use unnest() to separate out vector into rows\nThe str_split function leaves the chopped string in a difficult format - a vector within a dataframe cell. unnest() will unpack that vector into individual rows for us.\n\ncoral_habs &lt;- coral_habs_raw %&gt;%\n  mutate(hab_cut = str_split(habitat, '\\\\. ')) %&gt;%\n  unnest(hab_cut)\n\nNote the number of observations skyrocketed (562 –&gt; 2210! Each paragraph was a single observation (for one coral species); now each species description is separated out into multiple rows, each containing a sentence.\n\n\n\n3.2.4 Identify sentences with numbers in them\nWe can use str_detect() to identify which observations contain a certain pattern, e.g., a number. We can use this in conjunction with filter to keep only those observations that match the pattern.\nWithout wildcards, we’d have to identify each specific number. This would be annoying. Instead we can use some basic “regular expressions” or “regex” as wild card expressions. We put these in square brackets to create a list of everything we would want to match, e.g. [aeiou] would match any instance of lower case vowels. Ranges of numbers or letters can also be included, e.g., [a-z] is all lower-case letters (without diacritical marks like accents…), [A-Z] is all upper-case letters, [0-9] is all numeric characters. Combinations or subsets work as well: what would [a-f], [A-z], [A-m3-9] match?\nHelpful for testing regex: https://regex101.com/\n\n### Without wildcards\ncoral_habs &lt;- coral_habs_raw %&gt;%\n  mutate(hab_cut = str_split(habitat, '\\\\. ')) %&gt;%\n  unnest(hab_cut) %&gt;%\n  filter(str_detect(hab_cut, '1') | str_detect(hab_cut, '2'))\n\n### With wildcards\ncoral_habs &lt;- coral_habs_raw %&gt;%\n  mutate(hab_cut = str_split(habitat, '\\\\. ')) %&gt;%\n  unnest(hab_cut) %&gt;%\n  filter(str_detect(hab_cut, '[0-9]'))\n\n\n3.2.4.1 But not all numbers are depths\nHow can we differentiate further to get at depth info?\n\nexclude years? Knowing a bit about corals, can probably exclude any four-digit numbers as their depth; problems with that?\nmatch pattern of number followed by ” m” as all depths are given in meters\n\n\ncoral_depth &lt;- coral_habs %&gt;%\n  filter(str_detect(hab_cut, '[0-9] m')) %&gt;%\n  mutate(depth = str_extract(hab_cut, '[0-9] m'))\n\nWhy didn’t that work???? Only matched the single digit next to the “m”!\nWe need to use a regular expressions quantifier in our pattern:\n\n+ means one or more times\n* means zero or more times\n? means zero or one time\n{3} means exactly three times\n{2,4} means two to four times; {2,} means two or more times\n\n\nyears &lt;- coral_habs %&gt;%\n  mutate(year = str_extract(hab_cut, '[0-9]{4}')) \n### looks for four numbers together\n\ncoral_depth &lt;- coral_habs %&gt;%\n  filter(str_detect(hab_cut, '[0-9] m')) %&gt;%\n  mutate(depth = str_extract(hab_cut, '[0-9]+ m')) \n### looks for one or more numbers, followed by ' m'\n### Still misses the ranges e.g. \"3-30 m\" - how to capture?\n\n### let it also capture \"-\" in the brackets\ncoral_depth &lt;- coral_habs %&gt;%\n  filter(str_detect(hab_cut, '[0-9] m')) %&gt;%\n  mutate(depth = str_extract(hab_cut, '[0-9-]+ m'))\n\nAlso can use a “not” operator inside the brackets:\n\n'[^a-z]' matches “anything that is not a lower case letter”\nBUT: '^[a-z]' matches a start of a string, then a lower case letter.\nNOTE: ^ outside brackets means start of a string, inside brackets means “not”\n\n\n### split 'em (using the \"not\" qualifier), convert to numeric, keep the largest\ncoral_depth &lt;- coral_habs %&gt;%\n  filter(str_detect(hab_cut, '[0-9] m')) %&gt;%\n  mutate(depth_char = str_extract(hab_cut, '[0-9-]+ m'),\n         depth_num = str_split(depth_char, '[^0-9]')) %&gt;%\n  unnest(depth_num)\n\ncoral_depth &lt;- coral_depth %&gt;%\n  mutate(depth_num = as.numeric(depth_num)) %&gt;%\n  filter(!is.na(depth_num)) %&gt;%\n  group_by(iucn_sid, sciname) %&gt;%\n  mutate(depth_num = max(depth_num),\n         n = n()) %&gt;%\n  distinct()\n\nNote, still some issues in here: some fields show size e.g. 1 m diameter; other fields have slightly different formatting of depth descriptors; so it’s important to make sure the filters (a) get everything you want and (b) exclude everything you don’t want. We could keep going but we’ll move on for now…\n\n\n\n3.2.5 Other examples of stringr functionality\n\n3.2.5.1 start string, end string, and “or” operator\nCombining multiple tests using “or”, and adding string start and end characters.\n\ncoral_threats &lt;- coral_narrs %&gt;%\n  select(iucn_sid, threats) %&gt;%\n  mutate(threats = tolower(threats),\n         threats_cut = str_split(threats, '\\\\. ')) %&gt;%\n  unnest(threats_cut) %&gt;%\n  filter(str_detect(threats_cut, '^a|s$')) \n    ### NOTE: ^ outside brackets is start of a string, but inside brackets it's a negation\n\n\n\n3.2.5.2 cleaning up column names in a data frame\nSpaces and punctuation in column names can be a hassle, but often when reading in .csvs and Excel files, column names include extra stuff. Use regex and str_replace to get rid of these! (or janitor::clean_names()…)\n\ncrappy_colname &lt;- 'Per-capita income ($US) (2015 dollars)'\ntolower(crappy_colname) %&gt;%\n  str_replace_all('[^a-z0-9]+', '_') %&gt;%\n  str_replace('^_|_$', '') ### in case any crap at the start or end\n\n\n\n3.2.5.3 Lazy vs. greedy evaluation\nWhen using quantifiers in regex patterns, we need to consider lazy vs. greedy evaluation of quantifiers. “Lazy” will find the shortest piece of a string that matches the pattern (gives up at its first opportunity); “greedy” will match the largest piece of a string that matches the pattern (takes as much as it can get). “Greedy” is the default behavior, but if we include a question mark after the quantifier we force it to evaluate in the lazy manner.\n\nx &lt;- \"Everybody's got something to hide except for me and my monkey\"\nx %&gt;% str_replace('b.+e', '...')\nx %&gt;% str_replace('b.+?e', '...')\n\n\n\n3.2.5.4 Lookaround (Lookahead and lookbehind) assertions\nA little more advanced - Lookahead and lookbehind assertions are useful to match a pattern led by or followed by another pattern. The lookaround pattern is not included in the match, but helps to find the right neighborhood for the proper match.\n\ny &lt;- 'one fish two fish red fish blue fish'\ny %&gt;% str_locate('(?&lt;=two) fish')   ### match \" fish\" immediately preceded by \"two\"\ny %&gt;% str_locate('fish (?=blue)')   ### match \"fish \" immediately followed by \"blue\"\ny %&gt;% str_replace_all('(?&lt;=two|blue) fish', '...')\n\n\n\n3.2.5.5 Using regex in list.files() to automate file finding\nlist.files() is a ridiculously handy function when working with tons of data sets. At its most basic, it simply lists all the non-hidden files in a given location. But if you have a folder with more folders with more folders with data you want to pull in, you can get fancy with it:\n\nuse recursive = TRUE to find files in subdirectories\nuse full.names = TRUE to catch the entire path to the file (otherwise just gets the basename of the file)\nuse all.files = TRUE if you need to find hidden files (e.g. .gitignore)\nuse pattern = 'whatever' to only select files whose basename matches the pattern - including regex!\n\n\nlist.files(path = 'sample_files')\nlist.files(path = 'sample_files', pattern = 'jpg$', full.names = TRUE, recursive = TRUE)\nlist.files(path = '~/github/text_workshop/sample_files', pattern = '[0-9]{4}', \n           full.names = TRUE, recursive = TRUE)\n\n\nraster_files &lt;- list.files('sample_files', pattern = '^sample.+[0-9]{4}.tif$') \n  ### note: should technically be '\\\\.tif$' - do you see why?\n\n### then create a raster stack from the files in raster_files, or loop \n### over them, or whatever you need to do!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Analysis in R</span>"
    ]
  },
  {
    "objectID": "session_03.html#reading-text-from-pdfs-using-pdftools",
    "href": "session_03.html#reading-text-from-pdfs-using-pdftools",
    "title": "3  Text Analysis in R",
    "section": "3.3 Reading text from pdfs using pdftools",
    "text": "3.3 Reading text from pdfs using pdftools\n\n3.3.1 Overview\nHow often have you run across a published paper with awesome-looking data, but it is only available in PDF format? ARGHHH! But using the pdftools package and some stringr functions with regex patterns, we can get that data out and into a usable format.\n\n\n3.3.2 The pdftools package\nThe pdftools package basically has five functions:\n\npdf_info(pdf, opw = \"\", upw = \"\") to get metadata about the pdf itself\npdf_text(pdf, opw = \"\", upw = \"\") to get the text out of the pdf.\npdf_fonts(pdf, opw = \"\", upw = \"\") to find out what fonts are used (including embedded fonts)\npdf_attachments(pdf, opw = \"\", upw = \"\") umm attachments I guess?\npdf_toc(pdf, opw = \"\", upw = \"\") and a table of contents.\n\nReally we’ll just focus on pdf_text().\n\npdf_smith &lt;- file.path('pdfs/smith_wilen_2003.pdf')\n\nsmith_text &lt;- pdf_text(pdf_smith)\n\npdf_text() returns a vector of strings, one for each page of the pdf. To enable us to mess with it in tidyverse style, let’s turn it into a dataframe, and keep track of the pages.\nThen we can use stringr::str_split() to break the pages up into individual lines. Each line of the pdf is concluded with a backslash-n, so we can split on this. We will also add a line number in addition to the page number.\n\nsmith_df &lt;- data.frame(text = smith_text) # one row per page\n\nsmith_df &lt;- data.frame(text = smith_text) %&gt;%\n  mutate(page = 1:n()) %&gt;%\n  mutate(text_sep = str_split(text, '\\\\n')) %&gt;% # split by line, text_set=lists of lines \n  unnest(text_sep) # separate lists into rows\n\nsmith_df &lt;- data.frame(text = smith_text) %&gt;%\n  mutate(page = 1:n()) %&gt;%\n  mutate(text_sep = str_split(text, '\\\\n')) %&gt;%\n  unnest(text_sep) %&gt;%\n  group_by(page) %&gt;%\n  mutate(line = 1:n()) %&gt;% # add line #s by page\n  ungroup()\n\n\n\n3.3.3 Getting the table out of the PDF\nLet’s look at the PDF: specifically we want to get the data in the table on page 8 of the document. More specifically, the table data is in lines 8 to 18 on page 8. This is a table comparing the number of active urchin divers to the number of patches in which they dove for urchins, from 1988 to 1999.\n\nsmith_df %&gt;% filter(page == 8 & between(line, 7, 25)) %&gt;% pull(text_sep)\n\n\n                                            Number of active divers, No. of patches       1988    1989       1990    1991   1992    1993       1994   1995   1996   1997   1998   1999, active in, 1                      50       60      146     139     139      99       107     49     37     36     43     43, 2                      44       51       72      72      62      65        67     48     44     45     33     39, 3                      20       19       59      60      52      46        45     30     25     33     27     24, 4                      10        8       36      32      36      32        25     21     21     22     18      9, 5                       4        0       20      23      38      18        13     15      0      7      5      9, 6                       1        0        6      12      13      15        13      3      4      3      3      1, 7                       0        0        2       8      15       9         2      1      0      0      1      0, 8                       0        0        0       4       3       0         0      1      0      0      0      0, 9                       0        0        1       3       0       0         0      0      0      0      0      0, 10                      0        0        0       0       0       0         0      0      0      0      0      0, 11                      0        0        0       0       0       0         0      0      0      0      0      0, , Total divers          129     138       342     353     358     284       272    168    131    146    130    125, , Weighted average     2.05     1.82      2.25    2.53    2.68    2.60      2.33   2.54   2.35   2.51   2.40   2.24, no. patches\n\n\nThe column headings are annoyingly just years, and R doesn’t like numbers as column names, so we’ll rename them as ‘y####’ for year.\nBreak up the columns separated by (probably tabs but we can just use) spaces. We’ll use the tidyr::separate function to separate into columns by spaces. Note, one space and multiple spaces should count the same way - how can we do that in regex? (quantifiers!)\n\n\n3.3.3.1 extract the table (not run in workshop)\n\n### We want to extract data from the table on page 8\npage8_df &lt;- smith_df %&gt;%\n  filter(page == 8)\n\n### Let's just brute force cut out the table\ncol_lbls &lt;- c('n_patches', paste0('y', 1988:1999))\n\ntable1_df &lt;- page8_df %&gt;%\n  filter(line %in% 8:18) %&gt;%\n  separate(col = text_sep, into = col_lbls, sep = ' +') \n\n\n\n3.3.3.2 format the extracted table (not run in workshop)\nNow we can ditch the text, page, and line columns, and pull the result into a tidy format (long format rather than wide) for easier operations.\n\nWhen we pull the ‘y####’ columns into a year column, let’s turn those into integers instead of a character\nSame goes for the number of patches and number of divers - they’re all character instead of integer (because it started out as text).\n\n\ntable1_tidy_df &lt;- table1_df %&gt;%\n  select(-text, -line, -page) %&gt;%\n  gather(year, n_divers, starts_with('y')) %&gt;%\n  mutate(year = str_replace(year, 'y', ''), ### or str_extract(year, '[0-9]{4}')\n         year = as.integer(year),\n         n_patches = as.integer(n_patches),\n         n_divers = as.integer(n_divers))\n\nDT::datatable(table1_tidy_df)\n\n\n\n\n\nWith pdftools, we extracted a data table, but we could also just extract the text itself if that’s what we really wanted…",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Analysis in R</span>"
    ]
  },
  {
    "objectID": "session_03.html#introduction-to-tidytext-and-sentiment-analysis",
    "href": "session_03.html#introduction-to-tidytext-and-sentiment-analysis",
    "title": "3  Text Analysis in R",
    "section": "3.4 Introduction to tidytext and sentiment analysis",
    "text": "3.4 Introduction to tidytext and sentiment analysis\nSentiment analysis is a fairly basic way to get a sense of the mood of a piece of text. In an eco-data-science sense, we can use sentiment analysis to understand perceptions of topics in environmental policy.\nA good example is “Public Perceptions of Aquaculture: Evaluating Spatiotemporal Patterns of Sentiment around the World” by researchers Halley Froehlich, Becca Gentry, and Ben Halpern, in which they examine public perceptions of aquaculture by performing sentiment analyses on newspaper headlines from around the globe and government-solicited public comments on aquaculture policy and development. This paper is included in the ‘pdfs’ folder on Github, or available here.\nAnother excellent example of sentiment analysis (among other text analyses) is an examination of then candidate Donald Trump’s tweets from 2016, which noted that tweets from an iPhone and an Android phone were markedly different in tone; the thought was that the Android account (with generally far more negative tweets) was run by Trump while the iPhone (with generally more positive tweets) was tweets from a staffer. See here for details.\nHere we will apply word frequency analysis and sentiment analysis to a set of recent articles from the New York Times, each article discussing challenges and/or benefits of offshore wind energy development. On 3/21/2024, I searched the New York Times website for the term “offshore wind” and grabbed the text from the first six articles listed.\n\n3.4.1 Read in the newspaper articles\nThese articles are in plain text format. We can use list.files to find the correct files, then use purrr::map to read each file in, then combine the files into a dataframe. To uniquely identify the articles, we will then extract the publication date from the file path using stringr::str_extract and some regex magic, and also assign the title (first full line) across all rows of the dataframe.\n\nnyt_files &lt;- list.files('data', pattern = 'nytimes.+txt', full.names = TRUE)\nnyt_text &lt;- purrr::map_chr(nyt_files, read_file)\nnyt_df &lt;- data.frame(text = nyt_text, file = basename(nyt_files)) %&gt;%\n  ### because the dates are in yyyy-mm-dd format (with dashes), extract with:\n  mutate(date = str_extract(file, '[0-9-]+')) %&gt;%\n  ### Isolate the title: keep everything up to the first carriage return\n  mutate(title = str_extract(text, '^.+(?=(\\r|\\n))'))\n\n\n\n\n\n\n\nTip\n\n\n\nThat last regular expression looks like a cat walked across a keyboard! But it reads, from left to right:\n\n^: start at the beginning of the string\n.: match any character (period as wildcard)\n+: repeat the previous match (in this case, any character) one or more times\n(?=(\\r|\\n)): look ahead to see if there is either a \\r or a \\n:\n\n(?=...) starts a “lookahead assertion”\n(\\r|\\n) matches either the carriage return \\r OR (|) the end of line \\n.\n\n\nSo: start at the beginning, and match all characters until you see the designated pattern (in this case a carriage return or end-of-line) as the next character, and then stop matching (without matching that designated pattern).\n\n\nEach row is a single file, representing a single article. Time for some tidytext! From the vignette:\n\nUsing tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use. Much of the infrastructure needed for text mining with tidy data frames already exists in packages like dplyr, broom, tidyr and ggplot2. In this package, we provide functions and supporting data sets to allow conversion of text to and from tidy formats, and to switch seamlessly between tidy tools and existing text mining packages.\n\nThe package authors, Julia Silge and David Robinson, have also written a fantastic resource for text mining with R, cleverly titled: Text Mining with R.\n\n\n3.4.2 Word count analysis\nWe can use tidytext::unnest_tokens() to take our full text data and break it down into different chunks, called “tokens” by those in the know. Tokens include individual words (default), sentence, paragraph, n-grams (combinations of \\(n\\) words in a row, e.g., bigrams or trigrams are two- and three-word phrases). To perform a word frequency analysis and sentiment analysis, we will focus on words as our token of choice, though more complex analyses might involve bigrams, trigrams, or other methods entirely. Let’s tokenize our text and count up how frequently each word appears in each article.\n\nnyt_words_df &lt;- nyt_df %&gt;% \n  unnest_tokens(output = word, input = text, token = 'words')\n\nnyt_wordcount &lt;- nyt_words_df %&gt;% \n  group_by(date, word) %&gt;%\n  summarize(n = n(), .groups = 'drop')\n\n…OK, but check out which words show up the most. There are a lot of numbers, and a bunch of words that don’t contain much interesting information. How can we limit those?\n\n3.4.2.1 Remove stop words\nThose very common (and often uninteresting) words are called “stop words.” See ?stop_words and View(stop_words) to look at documentation for stop words lexicons (from the tidytext package).\nWe will remove stop words using tidyr::anti_join(), which will omit any words in our nyt_words_df dataframe that appear in stop_words. Let’s also remove numbers. Then let’s rerun our word count.\n\nnyt_words_clean &lt;- nyt_words_df %&gt;% \n  anti_join(stop_words, by = 'word') %&gt;%\n  filter(!str_detect(word, '[0-9]'))\n\nnyt_wordcount &lt;- nyt_words_clean %&gt;% \n  group_by(date, word) %&gt;%\n  summarize(n = n(), .groups = 'drop')\n\n\n\n3.4.2.2 Find the top 5 words from each article (by date)\n\ntop_5_words &lt;- nyt_wordcount %&gt;% \n  group_by(date) %&gt;% \n  slice_max(order_by = n, n = 5) %&gt;%\n  ungroup()\n\nggplot(data = top_5_words, aes(x = n, y = word)) +\n  geom_col(fill = \"blue\") +\n  facet_wrap(~date, scales = \"free\")\n\n\n\n\n\n\n\n\nMany of these words make perfect sense for articles about wind power! In addition to wind, power, project, and offshore, we see place names: Virginia, (New) Jersey, (New) York, and (Martha’s) Vineyard.\nNow let’s generate a wordcloud of the top 25 words in the first article, using ggwordcloud::geom_text_wordcloud() in conjunction with ggplot().\n\ntop25 &lt;- nyt_wordcount %&gt;% \n  filter(date == first(date)) %&gt;%\n  slice_max(order_by = n, n = 25)\n\nword_cloud &lt;- ggplot(data = top25, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n), shape = \"diamond\") +\n  scale_size_area(max_size = 10) +\n  scale_color_gradientn(colors = c(\"darkgreen\",\"blue\",\"purple\")) +\n  theme_minimal()\n\nword_cloud\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLemmatization\nNote in the above plots, we see the word “project” and the word “projects” separately - our code doesn’t differentiate between the singular and plural forms of the word. Similarly, if we dug further into our data, we would see different conjugations and tenses of various verbs (“write” vs “wrote” vs “writing”) and forms of adjectives.\n“Lemmatization” is the process of converting all these various forms to a single root word prior to the analysis. This would convert instances of “projects” to “project”, “writing” to “write”, etc. Then, the word count analysis would be able to sum across the lemmatized root word for a more accurate picture.\nLemmatization is an important step in natural language processing, but it is beyond the scope for this introductory workshop.\n\n\n\n\n\n3.4.3 Sentiment analysis\nFirst, check out the sentiment lexicons, included in the tidytext package through the get_sentiments() function. From Julia Silge and David Robinson (https://www.tidytextmining.com/sentiment.html):\n\nThe three general-purpose lexicons are\n\nAFINN from Finn Årup Nielsen,\nbing from Bing Liu and collaborators, and\nnrc (National Research Council Canada) from Saif Mohammad and Peter Turney\n\nAll three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the sentiments dataset, and tidytext provides a function get_sentiments() to get specific sentiment lexicons without the columns that are not used in that lexicon.\n\nLet’s explore the sentiment lexicons. bing is included in tidytext, other lexicons (afinn, nrc, loughran) you’ll be prompted to to download.\n\n\n\n\n\n\nImportant\n\n\n\nWarning: These collections include some of the most offensive words you can think of.\n\n\n“afinn”: Words ranked from -5 (very negative) to +5 (very positive)\n\nafinn_lex &lt;- get_sentiments(lexicon = \"afinn\")\n### you may be prompted to download an updated lexicon - say yes!\n\n# Let's look at the pretty positive words:\nafinn_pos &lt;- get_sentiments(\"afinn\") %&gt;% \n  filter(value %in% c(3,4,5))\n\nFor comparison, check out the bing lexicon:\n\nbing_lex &lt;- get_sentiments(lexicon = \"bing\")\n\nAnd the nrc lexicon:https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm Includes bins for 8 emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and positive / negative.\nCitation for NRC lexicon: Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.\nNow nrc:\n\nnrc_lex &lt;- get_sentiments(lexicon = \"nrc\")\n\nFor this tutorial, let’s use the bing lexicon which ranks each word simply as positive or negative.\nFirst, bind words in nyt_words_clean to bing lexicon:\n\nnyt_bing &lt;- nyt_words_clean %&gt;% \n  inner_join(bing_lex, by = 'word')\n\nLet’s find some counts of positive vs negative:\n\nbing_counts &lt;- nyt_bing %&gt;% \n  group_by(date, sentiment) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n\n# Plot them: \nggplot(data = bing_counts, aes(x = sentiment, y = n)) +\n  geom_col() +\n  facet_wrap(~date)\n\n\n\n\n\n\n\n\nTaking the ratio of positive to negative, rather than the total counts per chapter, adjusts for some articles just being longer than others. Highly negative articles would have a value between 0 and 1, highly positive could go from 1 to infinity, so that’s a problem. But plotting as log ratio, i.e., \\(\\ln\\left(\\frac{positive}{negative}\\right)\\), balances that so a chapter with 10:1 positive:negative ratio would have the same absolute value (\\(\\log(10) = 2.3\\)) as a chapter with 1:10 positive:negative ratio (\\(\\log(0.1) = -2.3\\)).\nSince all articles come from the same source (though not necessarily same author), we might also need to consider that the overall tone of the New York Times’s prose is darker or lighter, so let’s find the overall log ratio for the entire set of articles, and subtract that out. Similar adjustments might be made if comparing the sentiment across multiple chapters or books written by the same author, to account for the author’s overall tone.\n\n### find log ratio score overall:\nbing_log_ratio_all &lt;- nyt_bing %&gt;% \n  summarize(n_pos = sum(sentiment == 'positive'),\n            n_neg = sum(sentiment == 'negative'),\n            log_ratio = log(n_pos / n_neg))\n\n### Find the log ratio score by article (date): \nbing_log_ratio_article &lt;- nyt_bing %&gt;% \n  group_by(date, title) %&gt;% \n  summarize(n_pos = sum(sentiment == 'positive'),\n            n_neg = sum(sentiment == 'negative'),\n            log_ratio = log(n_pos / n_neg),\n            .groups = 'drop') %&gt;%\n  mutate(log_ratio_adjust = log_ratio - bing_log_ratio_all$log_ratio) %&gt;%\n  mutate(pos_neg = ifelse(log_ratio_adjust &gt; 0, 'pos', 'neg'))\n\nFinally, let’s plot the log ratios, and also include the title of each article as a geom_text().\n\nggplot(data = bing_log_ratio_article, \n       aes(x = log_ratio_adjust,\n           y = fct_rev(factor(date)),\n           fill = pos_neg)) +\n  geom_col() +\n  geom_text(x = 0, aes(label = title), hjust = .5, vjust = .5, size = 4) +\n  labs(x = 'Adjusted log(positive/negative)',\n       y = 'Article') +\n  scale_fill_manual(values = c('pos' = 'slateblue', 'neg' = 'darkred')) +\n  theme_minimal() +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nBased on those titles, the resulting balance of positive vs. negative seems pretty reasonable!\n\n\n\n\n\n\nTip\n\n\n\nThis has been a very simple sentiment analysis. The sentimentr package (https://cran.r-project.org/web/packages/sentimentr/index.html) seems to be able to parse things at the sentence level, accounting for negations etc. (e.g. “I am not having a good day.”)\n\n\n\n3.4.3.1 Sentiment analysis with afinn (not run in workshop):\nFirst, bind words in nyt_words_clean to afinn lexicon: ::: {.cell}\nnyt_afinn &lt;- nyt_words_clean %&gt;% \n  inner_join(afinn_lex, by = 'word')\n:::\nLet’s find some counts (by sentiment ranking): ::: {.cell}\nafinn_counts &lt;- nyt_afinn %&gt;% \n  group_by(date, value) %&gt;%\n  summarize(n = n())\n\n### Plot them: \nggplot(data = afinn_counts, aes(x = value, y = n)) +\n  geom_col() +\n  facet_wrap(~date)\n\n# Find the mean afinn score by article date: \nafinn_means &lt;- nyt_afinn %&gt;% \n  group_by(date) %&gt;% \n  summarize(mean_afinn = mean(value))\n\nggplot(data = afinn_means, \n       aes(y = fct_rev(factor(date)),\n           x = mean_afinn)) +\n  geom_col() +\n  labs(y = 'Article date')\n:::\n\n\n3.4.3.2 Sentiment analysis with NRC lexicon (not run in workshop)\nRecall, this assigns words to sentiment bins. Let’s bind our article data to the NRC lexicon:\n\nnyt_nrc &lt;- nyt_words_clean %&gt;% \n  inner_join(get_sentiments(\"nrc\"))\n\nLet’s find the count of words by article and sentiment bin:\n\nnyt_nrc_counts &lt;- nyt_nrc %&gt;% \n  group_by(date, sentiment) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup()\n\nggplot(data = nyt_nrc_counts, aes(x = n, y = sentiment)) +\n  geom_col() +\n  facet_wrap(~date)\n### perhaps order or color the sentiments by positive/negative\n\nggplot(data = nyt_nrc_counts, aes(x = n, \n                                     y = factor(date) %&gt;%\n                                       fct_rev())) +\n  geom_col() +\n  facet_wrap(~sentiment) +\n  labs(y = 'Date of article')",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Analysis in R</span>"
    ]
  },
  {
    "objectID": "session_04.html",
    "href": "session_04.html",
    "title": "4  Accesing Survey Data Using a Reproducible Workflow",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Accesing Survey Data Using a Reproducible Workflow</span>"
    ]
  },
  {
    "objectID": "session_04.html#learning-objectives",
    "href": "session_04.html#learning-objectives",
    "title": "4  Accesing Survey Data Using a Reproducible Workflow",
    "section": "",
    "text": "Overview of survey tools\nGenerating a reproducible survey report with Qualtrics",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Accesing Survey Data Using a Reproducible Workflow</span>"
    ]
  },
  {
    "objectID": "session_04.html#introduction",
    "href": "session_04.html#introduction",
    "title": "4  Accesing Survey Data Using a Reproducible Workflow",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nSurveys and questionnaires are commonly used research methods within social science and other fields. For example, understanding regional and national population demographics, income, and education as part of the National Census activity, assessing audience perspectives on specific topics of research interest (e.g. the work by Tenopir and colleagues on Data Sharing by Scientists), evaluation of learning deliverable and outcomes, and consumer feedback on new and upcoming products. These are distinct from the use of the term survey within natural sciences, which might include geographical surveys (“the making of measurement in the field from which maps are drawn”), ecological surveys (“the process whereby a proposed development site is assess to establish any environmental impact the development may have”) or biodiversity surveys (“provide detailed information about biodiversity and community structure”) among others.\nAlthough surveys can be conducted on paper or verbally, here we focus on surveys done via software tools. Needs will vary according to the nature of the research being undertaken. However, there is fundamental functionality that survey software should provide including:\n\nThe ability to create and customize questions\nThe ability to include different types of questions\nThe ability to distribute the survey and manage response collection\nThe ability to collect, summarize, and (securely) store response data\n\nMore advanced features can include:\n\nVisual design and templates - custom design might include institutional branding or aesthetic elements. Templates allow you to save these designs and apply to other surveys\nQuestion piping - piping inserts answers from previous questions into upcoming questions and can personalize the survey experience for users\nSurvey logic - with question logic and skip logic you can control the inclusion / exclusion of questions based on previous responses\nRandomization - the ability to randomize the presentation of questions within (blocks of) the survey\nBranching - this allows for different users to take different paths through the survey. Similar to question logic but at a bigger scale\nLanguage support - automated translation or multi-language presentation support\nShared administration - enables collaboration on the survey and response analysis\nSurvey export - ability to download (export) the survey instrument\nReports - survey response visualization and reporting tools\nInstitutional IRB approved - institutional IRB policy may require certain software be used for research purposes\n\nCommonly used survey software within academic (vs market) research include Qualtrics, Survey Monkey and Google Forms. Both Qualtrics and Survey Monkey are licensed (with limited functionality available at no cost) and Google forms is free.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Accesing Survey Data Using a Reproducible Workflow</span>"
    ]
  },
  {
    "objectID": "session_04.html#building-workflows-using-qualtrics",
    "href": "session_04.html#building-workflows-using-qualtrics",
    "title": "4  Accesing Survey Data Using a Reproducible Workflow",
    "section": "4.2 Building workflows using Qualtrics",
    "text": "4.2 Building workflows using Qualtrics\nIn this lesson we will use the qualtRics package to reproducible access some survey results set up for this course.\n\n4.2.1 Survey Instrument\nThe survey is very short, only four questions. The first question is on it’s own page and is a consent question, after a couple of short paragraphs describing what the survey is, it’s purpose, how long it will take to complete, and who is conducting it. This type of information is required if the survey is governed by an IRB, and the content will depend on the type of research being conducted. In this case, this survey is not for research purposes, and thus is not governed by IRB, but we still include this information as it conforms to the Belmont Principles. The Belmont Principles identify the basic ethical principles that should underlie research involving human subjects.\n\nThe three main questions of the survey have three types of responses: a multiple choice answer, a multiple choice answer which also includes an “other” write in option, and a free text answer. We’ll use the results of this survey, which was sent out to NCEAS staff to fill out, to learn about how to create a reproducible survey report.\n\n\n\n4.2.2 Working with qualtiRcs\n\n\n\n\n\n\nSet up\n\n\n\nFirst, open a new Quarto document and add a chunk to load the libraries we’ll need for this lesson:\n\nlibrary(qualtRics)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(kableExtra)\nlibrary(dplyr)\n\nNext, we need to set the API credentials. The qualtrics_api_credentials function creates environment variables to hold your Qualtrics account information. The function can either temporarily store this information for just this session, or it can modify the .Renviron file to set your API key and base URL so that you can access Qualtrics programmatically from any session.\nThe API key is as good as a password, so care should be taken to not share it publicly. For example, you would never want to save it in a script. The function below is the rare exception of code that should be run in the console and not saved. It works in a way that you only need to run it once, unless you are working on a new computer or your credentials changed. Note that in this book, we have not shared the actual API key, for the reasons outlined above. For the course, we will share the key via a file or by e-mail. Provide the key as a string to the api_key argument in the function below:\n\nkey_file &lt;- read_lines(\"/tmp/qualtrics-key.txt\")\nqualtrics_api_credentials(api_key = key_file[1], base_url = \"ucsb.co1.qualtrics.com\", install = FALSE, overwrite = FALSE)\n\n\n\n\n\n\n\n\n\nAside note\n\n\n\nThe .Renviron file is a special user controlled file that can create environment variables. Every time you open Rstudio, the variables in your environment file are loaded as…environment variables! Environment variables are named values that are accessible by your R process. They will not show up in your environment pane, but you can get a list of all of them using Sys.getenv(). Many are system defaults.\nTo view or edit your .Renviron file, you can use usethis::edit_r_environ().\n\n\nTo get a list of all the surveys in your Qualtrics instance, use the all_surveys function.\n\nsurveys &lt;- all_surveys()\nkable(surveys) %&gt;%\n    kable_styling()\n\nThis function returns a list of surveys, in this case only one, and information about each, including an identifier and it’s name. We’ll need that identifier later, so let’s go ahead and extract it using base R from the data frame.\n\ni &lt;- which(surveys$name == \"Survey for Data Science Training\")\nid &lt;- surveys$id[i]\n\nYou can retrieve a list of the questions the survey asked using the survey_questions function and the survey id.\n\nquestions &lt;- survey_questions(id)\nkable(questions) %&gt;%\n    kable_styling()\n\nThis returns a data.frame with one row per question with columns for question id, question name, question text, and whether the question was required. This is helpful to have as a reference for when you are looking at the full survey results.\nTo get the full survey results, run fetch_survey with the survey id.\n\nsurvey_results &lt;- fetch_survey(id)\nglimpse(survey_results)\n\nThe survey results table has tons of information in it, not all of which will be relevant depending on your survey. The table has identifying information for the respondents (eg: ResponseID, IPaddress, RecipientEmail, RecipientFirstName, etc), much of which will be empty for this survey since it is anonymous. It also has information about the process of taking the survey, such as the StartDate, EndDate, Progress, and Duration. Finally, there are the answers to the questions asked, with columns labeled according to the qname column in the questions table (eg: Q1, Q2, Q3). Depending on the type of question, some questions might have multiple columns associated with them. We’ll have a look at this more closely in a later example.\n\n4.2.2.1 Question 2\nLet’s look at the responses to the second question in the survey, “How long have you been programming?” Remember, the first question was the consent question.\nWe’ll use the dplyr and tidyr tools we learned earlier to extract the information. Here are the steps:\n\nselect the column we want (Q1)\ngroup_by and summarize the values\n\n\nq2 &lt;- survey_results %&gt;% \n    select(Q2) %&gt;% \n    group_by(Q2) %&gt;% \n    summarise(n = n())\n\nWe can show these results in a table using the kable function from the knitr package:\n\nkable(q2, col.names = c(\"How long have you been programming?\",\n                        \"Number of responses\")) %&gt;%\n    kable_styling()\n\n\n\n4.2.2.2 Question 3\nFor question 3, we’ll use a similar workflow. For this question, however there are two columns containing survey answers. One contains the answers from the controlled vocabulary, the other contains any free text answers users entered.\nTo present this information, we’ll first show the results of the controlled answers as a plot. Below the plot, we’ll include a table showing all of the free text answers for the “other” option.\n\nq3 &lt;- survey_results %&gt;% \n    select(Q3) %&gt;% \n    group_by(Q3) %&gt;% \n    summarise(n = n())\n\n\nggplot(data = q3, \n       mapping = aes(x = Q3, y = n)) +\n    geom_col() +\n    labs(x = \"What language do you currently use most frequently?\", y = \"Number of reponses\") +\n    theme_minimal()\n\nNow we’ll extract the free text responses:\n\nq3_text &lt;- survey_results %&gt;% \n    select(Q3_7_TEXT) %&gt;% \n    drop_na()\n\nkable(q3_text, col.names = c(\"Other responses to 'What language do you currently use mose frequently?'\")) %&gt;% \n    kable_styling()\n\n\n\n4.2.2.3 Question 4\nThe last question is just a free text question, so we can just display the results as is.\n\nq4 &lt;- survey_results %&gt;% \n    select(Q4) %&gt;% \n    rename(`What data science tool or language are you most excited to learn next?` = Q4) %&gt;% \n    drop_na()\n\nkable(q4, col.names = \"What data science tool or language are you most excited to learn next?\") %&gt;% \n    kable_styling()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Accesing Survey Data Using a Reproducible Workflow</span>"
    ]
  },
  {
    "objectID": "session_04.html#other-survey-tools",
    "href": "session_04.html#other-survey-tools",
    "title": "4  Accesing Survey Data Using a Reproducible Workflow",
    "section": "4.3 Other survey tools",
    "text": "4.3 Other survey tools\n\n4.3.1 Google forms\nGoogle forms can be a great way to set up surveys, and it is very easy to interact with the results using R. The benefits of using google forms are a simple interface and easy sharing between collaborators, especially when writing the survey instrument.\nThe downside is that google forms has far fewer features than Qualtrics in terms of survey flow and appearance.\nTo show how we can link R into our survey workflows, I’ve set up a simple example survey here.\nI’ve set up the results so that they are in a new spreadsheet here:. To access them, we will use the googlesheets4 package.\nFirst, open up a new Quarto doc and load the googlesheets4 library:\n\nlibrary(googlesheets4)\n\nNext, we can read the sheet in using the same URL that you would use to share the sheet with someone else. Right now, this sheet is public\n\nresponses &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1CSG__ejXQNZdwXc1QK8dKouxphP520bjUOnZ5SzOVP8/edit?usp=sharing\")\n\n✔ Reading from \"Example Survey Form (Responses)\".\n\n\n✔ Range 'Form Responses 1'.\n\n\nThe first time you run this, you should get a popup window in your web browser asking you to confirm that you want to provide access to your google sheets via the tidyverse (googlesheets) package.\nMy dialog box looked like this:\n\nMake sure you click the third check box enabling the Tidyverse API to see, edit, create, and delete your sheets. Note that you will have to tell it to do any of these actions via the R code you write.\nWhen you come back to your R environment, you should have a data frame containing the data in your sheet! Let’s take a quick look at the structure of that sheet.\n\ndplyr::glimpse(responses)\n\nRows: 10\nColumns: 5\n$ Timestamp                                              &lt;dttm&gt; 2022-04-15 13:…\n$ `To what degree did the event meet your expectations?` &lt;chr&gt; \"Met expectatio…\n$ `To what degree did your knowledge improve?`           &lt;chr&gt; \"Increase\", \"Si…\n$ `What did you like most about the event?`              &lt;chr&gt; \"the cool instr…\n$ `What might you change about the event?`               &lt;chr&gt; \"more snacks\", …\n\n\nSo, now that we have the data in a standard R data.frame, we can easily summarize it and plot results. By default, the column names in the sheet are the long fully descriptive questions that were asked, which can be hard to type. We can save those questions into a vector for later reference, like when we want to use the question text for plot titles.\n\nquestions &lt;- colnames(responses)[2:5]\ndplyr::glimpse(questions)\n\n chr [1:4] \"To what degree did the event meet your expectations?\" ...\n\n\nWe can make the responses data frame more compact by renaming the columns of the vector with short numbered names of the form Q1. Note that, by using a sequence, this should work for sheets from just a few columns to many hundreds of columns, and provides a consistent question naming convention.\n\nnames(questions) &lt;- paste0(\"Q\", seq(1,4))\n\nquestions\n\n                                                    Q1 \n\"To what degree did the event meet your expectations?\" \n                                                    Q2 \n          \"To what degree did your knowledge improve?\" \n                                                    Q3 \n             \"What did you like most about the event?\" \n                                                    Q4 \n              \"What might you change about the event?\" \n\ncolnames(responses) &lt;- c(\"Timestamp\", names(questions))\ndplyr::glimpse(responses)\n\nRows: 10\nColumns: 5\n$ Timestamp &lt;dttm&gt; 2022-04-15 13:48:58, 2022-04-15 13:49:43, 2022-04-15 13:50:…\n$ Q1        &lt;chr&gt; \"Met expectations\", \"Above expectations\", \"Above expectation…\n$ Q2        &lt;chr&gt; \"Increase\", \"Significant increase\", \"Significant increase\", …\n$ Q3        &lt;chr&gt; \"the cool instructors\", \"R is rad!\", \"everything\", \"the pizz…\n$ Q4        &lt;chr&gt; \"more snacks\", \"no pineapple pizza!\", \"nothing\", \"needs more…\n\n\nNow that we’ve renamed our columns, let’s summarize the responses for the first question. We can use the same pattern that we usually do to split the data from Q1 into groups, then summarize it by counting the number of records in each group, and then merge the count of each group back together into a summarized data frame. We can then plot the Q1 results using ggplot:\n\nq1 &lt;- responses %&gt;% \n    dplyr::select(Q1) %&gt;% \n    dplyr::group_by(Q1) %&gt;% \n    dplyr::summarise(n = dplyr::n())\n\nggplot2::ggplot(data = q1, mapping = aes(x = Q1, y = n)) +\n    geom_col() +\n    labs(x = questions[1], \n         y = \"Number of reponses\",\n         title = \"To what degree did the course meet expectations?\") +\n    theme_minimal()\n\n\nBypassing authentication for public sheets\nIf you don’t want to go through a little interactive dialog every time you read in a sheet, and your sheet is public, you can run the function gs4_deauth() to access the sheet as a public user. This is helpful for cases when you want to run your code non-interactively. This is actually how I set it up for this book to build!\n\n\n\n\n4.3.2 Survey Monkey\nSimilar to Qualtrics and qualtRics, there is an open source R package for working with data in Survey Monkey called svmkR. This package provides a suite of tools to work with Survey Monkey surveys. Note that this package is only available to install through GitHub. This is great, it makes is open source and easy to use. However, it is important tot make sure that the package is being maintain. In this case if we look at the GitHub repository with this package we can see that last updates were done somewhat recent. We are not going to cover this package here but fell free to explore and try it out if you use Survey Monkey as a platform to collect data. We hope that the principles describe in this lesson help you navigate how to access the data (in a reproducible way!).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Accesing Survey Data Using a Reproducible Workflow</span>"
    ]
  },
  {
    "objectID": "session_04.html#resourcese",
    "href": "session_04.html#resourcese",
    "title": "4  Accesing Survey Data Using a Reproducible Workflow",
    "section": "4.4 Resourcese",
    "text": "4.4 Resourcese\n\nHow to get your Qualtrics API key",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Accesing Survey Data Using a Reproducible Workflow</span>"
    ]
  },
  {
    "objectID": "session_05.html",
    "href": "session_05.html",
    "title": "5  Visualizing Spatial Data",
    "section": "",
    "text": "6 Helpful Background\nThis workshop will be most helpful if you have spent some time working with data in R, either for data analysis or visualization. Specifically, the workshop will be most helpful if you are:\nThe primary purpose is to demonstrate how to use R to visualize spatial data, rather than give a full tutorial on the basics of geospatial data though we will review some basic concepts quickly. Therefore we may not cover some important concepts that are important for spatial data analysis, For more in-depth learning I recommend reading “Geocomputation with R” by Lovelace et al. (2024), a free ebook available here: .",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_05.html#loading-packages",
    "href": "session_05.html#loading-packages",
    "title": "5  Visualizing Spatial Data",
    "section": "8.1 Loading Packages",
    "text": "8.1 Loading Packages\nFirst we need to load packages and data for our mapping. Note that some of these packages will only work if you have already installed GDAL (Geospatial Data Abstraction Library) - a “computer software library for reading and writing raster and vector geospatial data formats”.\n\n# General Packages\nlibrary(tidyr)    # data wrangling\nlibrary(dplyr)    # data wrangling\nlibrary(readr)   # read in files\nlibrary(ggplot2)   # plotting and mapping\nlibrary(patchwork) # combining maps/plots\n\n# Spatial Packages\nlibrary(sf)        # manipulating spatial data\nlibrary(ggspatial) # retrieving basemap, adding scale and arrowbar\nlibrary(maptiles)  # retrieving basemap\nlibrary(terra)     # working with raster data\nlibrary(tidyterra) # functions for working with raster\n\n# Data \nlibrary(tigris)  # shapefiles with various administrative boundaries and roads\n\nAnother good option for plotting spatial data in R is the tmap package with companion package tmaptools. However, the syntax is slightly different so I won’t cover that today. It does allow an interactive view feature via Leaflet which is nice, and maybe the subject for a future EcoDataScience workshop.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_05.html#load-data",
    "href": "session_05.html#load-data",
    "title": "5  Visualizing Spatial Data",
    "section": "9.1 Load Data",
    "text": "9.1 Load Data\n\n9.1.1 About the Data\nThe data used here are open source data available either on the KNB data repository or from the US Government. The specific data sources and citations are listed below:\n\n\n\nData\nOriginal Dataset\n\n\n\n\nStudy Sites\ncj lortie, M Zuliani, Nargol Ghazian, Jenna Braun, S Haas, & Rachel King. 2022. A list of ecological study sites within Central California Drylands. Knowledge Network for Biocomplexity. doi:10.5063/F1F76B1R.\n\n\nCalifornia Ecoregions\nU.S. Environmental Protection Agency. 2012. Level IV Ecoregions of California. U.S. EPA Office of Research and Development (ORD) - National Health and Environmental Effects Research Laboratory (NHEERL), Corvallis, OR. California Level IV Ecoregions\n\n\nCalifornia State Boundary\nU.S. Census Bureau TIGER/Line Shapefiles. (2023). \n\n\n\n\n\n9.1.2 Method 1: From a .csv file\nWe will download the site data from KNB to show how to read in data from a .csv file and prepare it for mapping. When you use a .csv you will have to convert it to an sf object to use the sf package functions for easier plotting, which I will show in a moment.\nDownload site data from: California Dryland Sites\nhttps://doi.org/10.5063/F1F76B1R\n\n# read in data from .csv file\nsite_df &lt;- read_csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A5d23a3f4-6ed8-47f0-b34e-000f6cfb8313\")\n\nRows: 51 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): state, desert, region, experiment, site_acronym, site_code, founda...\ndbl  (9): elevation, area_m2, area_block_m2, MAT, MAP, long, lat, aridity, n...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# alternatively download into your working directory and run \n# site_df &lt;- read_csv(\"tidy_regional_2023.csv\")\n\n\n\n9.1.3 Method 2: From a shapefile\nThe second data layer we will read in is a shapefile with ecoregions for California that is provided by the US EPA: California Level IV Ecoregions\nYou’ll need to download and unzip the data file into your working directory for the following code to work.\nWe will use the read_sf function from the sf package, which loads the shapefile as a tidyverse tibble. You can also use st_read to load shapefiles, but it contains more messages and loads the data as a base R data.frame instead of a tibble.\n\n\n\n\n\n\nImportant\n\n\n\nWhen reading in a shapefile, you use the filename with the .shp extension, but the associated files from the zipped folder are required for the data to read in properly. So, if you only unzip and then only copy the .shp file into your working directory or data folder for a project you will get an error message.\n\n\n\n# read in data from shapefile\necoreg_sf &lt;- sf::read_sf(\"data/ca_ecoregion_shape/ca_eco_l4.shp\")\n\n\n\nhere() starts at /home/runner/work/repro-research-course/repro-research-course/nceas-training/materials\n\n\n\n\n9.1.4 Method 3: From a package\nThe tigris package allows you to easily download TIGER/Line shapefiles from the US Census Bureau for things like state boundaries, roads, county lines, etc. The tidycensus package that was in a previous EcoDataScience Workshop uses this package when getting spatial data.\nWe’ll use it to download some boundaries for the state of California.\n\n# read in data using package \nstates_sf &lt;- tigris::states(progress_bar = FALSE)\nCA_counties_sf &lt;- tigris::counties(state = \"CA\", progress_bar = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_05.html#working-with-sf-data",
    "href": "session_05.html#working-with-sf-data",
    "title": "5  Visualizing Spatial Data",
    "section": "9.2 Working with sf data",
    "text": "9.2 Working with sf data\nNow we’ll see some useful features of the sf package and how to access and set important spatial information from your data.\nMost functions in sf start with st_[function_name], which stands for spatial transformation. These functions perform various spatial data operations so we can manipulate our data for better visualizations.\n\n9.2.1 Identify and set the CRS\nThe Coordinate Reference System (CRS) is an important piece of information that contains the instructions for how to display the spatial data. It tells R where the points are located and how to project the points from a spherical (or ellipsoidal) 3D object to a 2D surface. See this data carpentry lesson, Section 2.4 in Geocomputation in R, or this post from ESRI for more information on CRS’s.\nWhen you read in data from a shapefile or a package the data should already have the CRS information associated with the data. If it is an unusual CRS you should check the metadata for additional details about the CRS.\nWhen you are reading in data from a .csv file, you need to set the CRS yourself when you convert your data.frame into an sf object. Thus, you’ll need to check the metadata to find the appropriate CRS.\nYou’ll often see lat/long data use the WGS84 CRS, which is a geographic (unprojected) coordinate reference system. This is the CRS used by the site_df.\nAn easy way to specify this is to use the EPSG code (European Petroleum Survey Group), which references a database of the associated geodetic parameters for common CRS. The code for the WGS84 is EPSG:4326.\n\ncolnames(site_df) # check column names\n\n [1] \"state\"         \"desert\"        \"region\"        \"experiment\"   \n [5] \"site_acronym\"  \"site_code\"     \"foundation\"    \"elevation\"    \n [9] \"aspect\"        \"area_m2\"       \"area_calc\"     \"polygon_block\"\n[13] \"area_block_m2\" \"MAT\"           \"MAP\"           \"long\"         \n[17] \"lat\"           \"aridity\"       \"ndvi_2023\"    \n\n# The coordinate information is in the \"long\" and \"lat column\nsite_sf &lt;- site_df %&gt;% \n  st_as_sf(\n    coords = c(\"long\", \"lat\"), # specify where spatial data is; \"longitude\" is first\n    crs    = \"EPSG:4326\"      # need to tell it what the CRS is\n  )\n\nNow you’ll see that our site data not just a data frame but is also now an sf object, which allows us to use all of the sf functions with this data now.\n\nclass(site_sf)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nhead(site_sf)\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -120.1018 ymin: 34.87623 xmax: -114.9 ymax: 36.06219\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 18\n  state     desert region experiment site_acronym site_code foundation elevation\n  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Californ… Mojave Mojave invasion   Antelope_va… Antelope… Ephedra c…       NA \n2 Californ… San J… San J… BNLL diet  Avenal - hw… Avenal    Ephedra c…       NA \n3 Californ… Mojave Mojave ERG        Barstow      Barstow_1 Ephedra c…      496.\n4 Californ… Mojave Mojave invasion   Barstow_2    Barstow_2 Ephedra c…       NA \n5 Nevada    Mojave Mojave invasion   Cal-Nev-Ari  Cal_nev_… Larrea tr…       NA \n6 Californ… San J… Carri… BNLL       plant-animal Carrizo_1 Ephedra c…       NA \n# ℹ 10 more variables: aspect &lt;chr&gt;, area_m2 &lt;dbl&gt;, area_calc &lt;chr&gt;,\n#   polygon_block &lt;chr&gt;, area_block_m2 &lt;dbl&gt;, MAT &lt;dbl&gt;, MAP &lt;dbl&gt;,\n#   aridity &lt;dbl&gt;, ndvi_2023 &lt;dbl&gt;, geometry &lt;POINT [°]&gt;\n\n\nYou’ll also see that we have some new attribute information associated with this object:\n\nGeometry Type: Point\nDimension\nBounding Box\nCRS\n\nWe can also access this info with some common sf functions:\n\nst_crs to check the CRS of the object\nst_bbox retrieves the bounding box around the entire object\nst_geometry_type identifies the geometry of each observation/row in the dataset\n\n\n# get the crs\nst_crs(site_sf)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n# retrieve the bounding box around all points in the object\nst_bbox(site_sf)\n\n      xmin       ymin       xmax       ymax \n-120.81229   34.20568 -114.07000   36.76000 \n\nst_geometry_type(site_sf)\n\n [1] POINT POINT POINT POINT POINT POINT POINT POINT POINT POINT POINT POINT\n[13] POINT POINT POINT POINT POINT POINT POINT POINT POINT POINT POINT POINT\n[25] POINT POINT POINT POINT POINT POINT POINT POINT POINT POINT POINT POINT\n[37] POINT POINT POINT POINT POINT POINT POINT POINT POINT POINT POINT POINT\n[49] POINT POINT POINT\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\n\nIn addition to the sf operations, objects of class sf usually function very similarly to other tibbles or data.frames and you can pretty much perform operations on these objects as usual. However, the geometry column can complicate some things, and if you want to remove it you need to call st_drop_geometry.\nWhen working with multiple data sources, it is important to have them in the same CRS for visualizing or they won’t map properly. So, we need to check the CRS for the other datasets to see if they match:\n\n# crs for ecoregions\nst_crs(ecoreg_sf)\n\nCoordinate Reference System:\n  User input: USA_Contiguous_Albers_Equal_Area_Conic_USGS_version \n  wkt:\nPROJCRS[\"USA_Contiguous_Albers_Equal_Area_Conic_USGS_version\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"USA_Contiguous_Albers_Equal_Area_Conic_USGS_version\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",23,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-96,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",29.5,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",45.5,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Not known.\"],\n        AREA[\"United States (USA) - CONUS onshore - Alabama; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming.\"],\n        BBOX[24.41,-124.79,49.38,-66.91]],\n    ID[\"ESRI\",102039]]\n\nst_crs(states_sf)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nSo, all of our spatial data sets currently have different CRS. We will need to reproject the data from two of the three so that we will be able to map all our data at the same time.\n\n\n9.2.2 Basic Spatial Data Operations\nWe can use the st_transform function to project our data into the same CRS.\nBut which CRS should we choose for our map making? It depends on what you are doing… there are projections that preserve area, distance, angles, so you will need to decide based on what is important in your situation. For a static map it may make sense to use a projection more specific to your area or goal (e.g. equal area). For simplicity, today I’ll transform the state and site data to the CRS of the ecoregion data.\n\n# project site and state lines to ecoreg CRS\nstates_proj_sf &lt;- st_transform(states_sf, st_crs(ecoreg_sf))\nsite_proj_sf   &lt;- st_transform(site_sf, st_crs(ecoreg_sf))\n\nThe states dataset also contains data for ALL US states. We can filter this data just like any other dataset so we just have the outline for California:\n\nca_proj_sf &lt;- states_proj_sf %&gt;%\n  filter(NAME == \"California\")\n\nThere are some sites that are outside the state of California, and I only want to map the California locations. While we could filter based on the state column in the dataset, sometimes you won’t have that. Another way to filter is to use the state outline to filter our site data to just include the sites within our area of interest:\n\nunique(site_proj_sf$state) \n\n[1] \"California\" \"Nevada\"    \n\nsite_ca_proj_sf &lt;- site_proj_sf %&gt;% \n  st_filter(ca_proj_sf, .predicate = st_covered_by)\n\nunique(site_ca_proj_sf$state)\n\n[1] \"California\"\n\n\nHere we used the st_filter function which can take a user-specified predicate function to “keep the geometries in x where .predicate(x,y) returns any match in y for x”. So, st_covered_by will return all values in x (site_proj_sf) that are covered by y (ca_proj_sf).\nNow, if I wanted to know the ecoregion of each of my sites I could also find that out using a spatial join. Using st_join we will add in the attributes of the ecoreg_sf object at any location that intersects the coordinates from the site_ca_proj_sf object.\n\nsite_ecoreg_sf &lt;- st_join(site_ca_proj_sf, ecoreg_sf, join = st_intersects)\n# st_intersects is the default\n\ncolnames(site_ecoreg_sf)\n\n [1] \"state\"         \"desert\"        \"region\"        \"experiment\"   \n [5] \"site_acronym\"  \"site_code\"     \"foundation\"    \"elevation\"    \n [9] \"aspect\"        \"area_m2\"       \"area_calc\"     \"polygon_block\"\n[13] \"area_block_m2\" \"MAT\"           \"MAP\"           \"aridity\"      \n[17] \"ndvi_2023\"     \"geometry\"      \"OBJECTID\"      \"US_L4CODE\"    \n[21] \"US_L4NAME\"     \"US_L3CODE\"     \"US_L3NAME\"     \"NA_L3CODE\"    \n[25] \"NA_L3NAME\"     \"NA_L2CODE\"     \"NA_L2NAME\"     \"NA_L1CODE\"    \n[29] \"NA_L1NAME\"     \"STATE_NAME\"    \"EPA_REGION\"    \"L4_KEY\"       \n[33] \"L3_KEY\"        \"L2_KEY\"        \"L1_KEY\"        \"Shape_Leng\"   \n[37] \"Shape_Area\"   \n\n\nThere are many other spatial operations you can perform with the sf package. For additional details you can see Section 4.2 of Geocomputation in R. Geometry operations are covered in Ch. 5.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_05.html#making-the-map",
    "href": "session_05.html#making-the-map",
    "title": "5  Visualizing Spatial Data",
    "section": "9.3 Making the map",
    "text": "9.3 Making the map\nFinally it’s time to make our map! One of the simplest ways to do this is to use ggplot2 which works well with sf objects. To plot sf objects, we use the geom_sf instead of geom_point or geom_line that you might use with graphs or figures in ggplot2.\n\nggplot(data = ca_proj_sf) +\n  geom_sf() \n\n\n\n\n\n\n\n\nWhen adding multiple layers, I usually specify the data in the geom_sf call so it is a bit more clear what each call is plotting. However, this means you must add data = before the data type or it will throw an error.\n\nggplot() + # don't specify data here since we have multiple data sets to plot\n  geom_sf(data = ca_proj_sf) + # must specify \"data = \" or it will throw an error\n  geom_sf(data = ecoreg_sf) + \n  geom_sf(data = site_ca_proj_sf)\n\n\n\n\n\n\n\n\nOne of the things I like about mapping with ggplot is that if you are familiar with it already, modifying the plot appearance is very similar so you don’t have to learn many new things to visualize your data. For example, you can set fill and color in the same way via the aesthetics or in the main call to geom_sf.\n\nggplot() + # don't specify data here since we have multiple data sets to plot\n  geom_sf(data = ca_proj_sf, fill = NA) + # specify no fill\n  geom_sf(data = ecoreg_sf, aes(fill = US_L3NAME), color = \"white\") + # like other data you can set fill \n  geom_sf(data = site_ca_proj_sf) +\n  scale_fill_discrete(name = \"Ecoregion\") +\n  theme_bw() \n\n\n\n\n\n\n\n\nWhen visualizing the data, you can see that it will scale to automatically include all of the data plotted. However, here we may want to focus in on the sites since there are no sites in norther California. Normally to modify the extent of the plot with ggplot you can use coord_cartesian, but with geom_sf you have to use coord_sf. You can also use scale_x_continuous or scale_y_continuous to set limits for each axis as well.\n\n# find extent of sites to get x and y limits\nst_bbox(site_ca_proj_sf)\n\n    xmin     ymin     xmax     ymax \n-2171214  1424401 -1733924  1802929 \n\n# with coord_sf\nggplot() + # don't specify data here since we have multiple data sets to plot\n  geom_sf(data = ca_proj_sf, fill = NA) + # specify no fill\n  geom_sf(data = ecoreg_sf, aes(fill = US_L3NAME), color = \"grey40\") + # like other data you can set fill \n  geom_sf(data = site_ca_proj_sf) +\n  scale_fill_discrete(name = \"Ecoregion\") +\n  theme_bw() + \n  coord_sf(xlim = c(-2171214, -1733924), ylim = c(1424401, 1802929))\n\n\n\n\n\n\n\n# with scale_*_continuous\nggplot() + # don't specify data here since we have multiple data sets to plot\n  geom_sf(data = ca_proj_sf, fill = NA) + # specify no fill\n  geom_sf(data = ecoreg_sf, aes(fill = US_L3NAME), color = \"grey40\") + # like other data you can set fill \n  geom_sf(data = site_ca_proj_sf) +\n  scale_fill_discrete(name = \"Ecoregion\") +\n  theme_bw() + \n  scale_x_continuous(limits = c(-2171214, -1733924)) +\n  scale_y_continuous(limits = c(1424401, 1802929))\n\n\n\n\n\n\n\n\nZooming in is nice, but it does mean you lose some geographic context for the broader region. In Part III I will show you how to create an inset map so you can zoom in on a focal region as well as include a larger region for context.\nYou can also use facet_wrap() to split your map into smaller maps with any of your variables.\n\nggplot() + # don't specify data here since we have multiple data sets to plot\n  geom_sf(data = ca_proj_sf, fill = NA) + # must specify \"data = \" or it will throw an error\n  geom_sf(data = ecoreg_sf, aes(fill = US_L3NAME)) + # like other data you can set fill \n  geom_sf(data = site_ca_proj_sf) +\n  scale_fill_discrete(name = \"Ecoregion\") +\n  theme_bw() +\n  facet_wrap(~desert) +\n  coord_sf(xlim = c(-2171214, -1733924), ylim = c(1424401, 1802929))\n\n\n\n\n\n\n\n\nUnfortunately, when using geom_sf the scales = \"free\" option doesn’t work with faceting, so if you want to create a map with something that looks like facets it is probably easier to create two separate maps and combine them using either the patchwork or cowplot package.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_05.html#my-favorite-basemap-providers-packages-and-functions",
    "href": "session_05.html#my-favorite-basemap-providers-packages-and-functions",
    "title": "5  Visualizing Spatial Data",
    "section": "10.1 My Favorite Basemap Providers, Packages and Functions",
    "text": "10.1 My Favorite Basemap Providers, Packages and Functions\nThere are many options for basemaps for your mapping projects in R, as well as several different packages for working with those map layers. Basemaps are provided as a series of tiles from various basemap providers, some are freely available and others require registration and/or a subscription to access them via an API key. Below I list the providers of various basemaps\nBasemap Data Providers:\n*: requires registration but has a free tier\n**: paid service\n\nOpenStreetMap\nStamen via Stadia*\nThunderforest*\nCarto**\nMapbox*\nGoogle\n\n\n\n\nImage: maptiles package documentation\n\n\nBasemap R Packages\nMake it “easy” to download and bring in the basemap tiles to R. In reality, I have found that some packages are much easier than others to work with.\nMy favorites:\n\nggspatial\n\nuses rosm::osm.image() to display/fetch map tiles\neasy integration with ggplot (returns a ggplot layer)\ncan’t directly specify spatial extent, uses your data\n\nmaptiles\n\npretty easy to download\ndoesn’t\n\n\nOther Packages to Examine\n\nceramic: another package for webmap tiles, defaults to Mapbox map with other options; visualize with terra\nggmap: requires google maps API key to use, even if tile provider doesn’t require one; visualizes with ggplot; good for getting Google Maps\nmapboxapi: requires Mapbox account (there is a free tier)\nOpenStreetMap: requires Java installation\nrosm: not designed to work with ggplot, uses prettymapr\nbasemapR: works with ggplot but not on CRAN\nbasemaps: many formats to download basemaps, but removed from CRAN and currently down’t work for me\nrgooglemaps: another way to access google maps\nmapsapi: another interface to google maps API\nggOceanMaps: designed for ocean sciences, visualizes with ggplot2\n\nsome additional ocean map packages: marmap, oceanmap, oce",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_05.html#retrieving-your-basemap",
    "href": "session_05.html#retrieving-your-basemap",
    "title": "5  Visualizing Spatial Data",
    "section": "10.2 Retrieving your basemap",
    "text": "10.2 Retrieving your basemap\nThis usually requires you to set an extent or bounding box for your download, as well as specify the zoom level you want.\nThe zoom is important as if you specify one that is too low (coarse, low resolution), your basemap may appear fuzzy/blurred but if it is too high (fine, high resolution) it may take a very long time to retrieve the necessary tiles.\nThis table lists some appropriate zoom levels, obtained from the OpenStreetMap wiki:\n\n\n\nZoom\n# Tiles\nExample Area to Represent\n\n\n\n\n0\n1\nWhole World\n\n\n3\n64\nLargest Country\n\n\n5\n1024\nLarge African Country\n\n\n6\n4096\nLarge European Country\n\n\n7\n16384\nSmall country, US State\n\n\n10\n1048576\nMetropolitan Area\n\n\n\nThey go higher, but this gives you a sense of how many more tiles there are to download at different zoom levels.\n\n10.2.1 Using ggspatial\nWhat are the built-in basemap options?\n\nrosm::osm.types()\n\n [1] \"osm\"                    \"opencycle\"              \"hotstyle\"              \n [4] \"loviniahike\"            \"loviniacycle\"           \"stamenbw\"              \n [7] \"stamenwatercolor\"       \"osmtransport\"           \"thunderforestlandscape\"\n[10] \"thunderforestoutdoors\"  \"cartodark\"              \"cartolight\"            \n\n\n\nmap_nobase &lt;- ggplot() + # don't specify data here since we have multiple data sets to plot\n  geom_sf(data = ca_proj_sf, fill = NA) + # must specify \"data = \" or it will throw an error\n  geom_sf(data = site_ecoreg_sf, aes(color = US_L3NAME)) +\n  scale_color_discrete(name = \"Level 3 Ecoregion\") +\n  guides(fill = guide_legend(ncol = 1, title.position = \"top\")) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\"\n  )\n\nmap_nobase\n\n\n\n\n\n\n\n\n\nmap_wbase &lt;- ggplot() + # don't specify data here since we have multiple data sets to plot\n  ggspatial::annotation_map_tile(type = \"osm\", zoom = 6) + \n  geom_sf(data = ca_proj_sf, fill = NA) + # must specify \"data = \" or it will throw an error\n  geom_sf(data = site_ecoreg_sf, aes(color = US_L3NAME)) +\n  scale_color_discrete(name = \"Level 3 Ecoregion\") +\n  guides(fill = guide_legend(ncol = 1, title.position = \"top\")) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\"\n  )\n\nmap_wbase\n\nZoom: 6\n\n\nFetching 9 missing tiles\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |======================================================================| 100%\n\n\n...complete!\n\n\n\n\n\nBasemap imagery: (C) OpenStreetMap contributors.\n\n\n\n\nCompare the visual with and without the basemap. Aside from the basemap, can you notice any differences?\n\nlibrary(patchwork)\n\nmap_nobase + map_wbase\n\nZoom: 6\n\n\n\n\n\nBasemap imagery: (C) OpenStreetMap contributors.\n\n\n\n\nYes, the projection is different! That is one of the annoying things about using ggspatial - it downloads the basemaps in a specific CRS - the Web Mercator projection (EPSG:3857). If it is the first layer added, then other layers are reprojected to Web Mercator, which is why the two maps look different.\nIf you want to have the map in the CRS of the data, specify a data argument in the call to ggplot().\n\nggplot(data = ca_proj_sf) + # specify data here to set CRS \n  ggspatial::annotation_map_tile(type = \"osm\", zoom = 6) + \n  geom_sf(fill = NA) + \n  geom_sf(\n    data = site_ecoreg_sf, \n    aes(color = US_L3NAME)\n    ) +\n  scale_color_discrete(name = \"Level 3 Ecoregion\") +\n  guides(fill = guide_legend(ncol = 1, title.position = \"top\")) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\"\n  )\n\nLoading required namespace: raster\n\n\nZoom: 6\n\n\nFetching 3 missing tiles\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\n\n\n...complete!\n\n\n\n\n\nBasemap imagery: (C) OpenStreetMap contributors.\n\n\n\n\nNow the map is back in the projection of the data, which is helpful if you don’t want to be stuck with only one option for the CRS of your map.\n\n\n\n\n\n\nCaution\n\n\n\nI am still figuring out how R does this under the hood. If the above code involves reprojecting the basemap tiles, what I suspect, as opposed to downloading the tiles in a different CRS I’d be careful as reprojecting involves resampling the raster into a new grid, which may slightly alter the appearance of the basemap.\n\n\n\n\n10.2.2 Using maptiles\nmaptiles has different basemap options than ggspatial. See the help for get_tiles to see all of the options, some of which do require an API key. The default is OpenStreetMap.\n\nca3857 &lt;- st_transform(ca_proj_sf, \"epsg:3857\")\nca_osm &lt;- get_tiles(ca3857, crop = TRUE)\n\nca_osm\n\nclass       : SpatRaster \ndimensions  : 273, 236, 3  (nrow, ncol, nlyr)\nresolution  : 4891.97, 4891.97  (x, y)\nextent      : -13858950, -12704446, 3830412, 5165920  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / Pseudo-Mercator (EPSG:3857) \nsource(s)   : memory\ncolors RGB  : 1, 2, 3 \nnames       : lyr.1, lyr.2, lyr.3 \nmin values  :    35,    35,    35 \nmax values  :   252,   252,   250 \n\nggplot(data = ca3857) + \n  geom_spatraster_rgb(data = ca_osm, r = 1, g = 2, b = 3) +\n  geom_sf(fill = NA) +\n  geom_sf(\n    data = st_transform(site_ecoreg_sf, crs = \"epsg:3857\"), \n    aes(color = US_L3NAME) \n    #inherit.aes = FALSE # I've found I sometimes need to add this if I am having issues\n    ) +\n  scale_color_discrete(name = \"Level 3 Ecoregion\") +\n  guides(fill = guide_legend(ncol = 1, title.position = \"top\")) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\"\n  )\n\n\n\n\nBasemap imagery: (C) OpenStreetMap contributors.\n\n\n\n\n\n\n10.2.3 Additional basemap options\nIn addition to the built-in tile servers, you can also source a basemap from using a URL for an xyz raster source with ggspatial. Make sure you include attributions for the basemap service you use. The OpenStreetMap wiki page has a list of Tile providers and the urls for you to use to access them. Some of these servers require you to register, but many have a free tier for low volume users.\nFor this to work with ggspatial, you just put the URL in the “type” argument. You also need to make sure that the URL you use has a “$” in front of the {z}, {y}, and {x} otherwise the call returns an error. Additionally, make sure that there is an extension on the URL (.png, .jpg, or .jpeg) - this is also required for the function call to run properly.\n\n# the OpenTopo map tile provider\nopen_topo &lt;- \"https://a.tile.opentopomap.org/${z}/${x}/${y}.png\"\n\nggplot(data = ca_proj_sf) + # specify data here to set CRS \n  ggspatial::annotation_map_tile(\n    type = open_topo,\n    zoom = 7\n    ) + \n  geom_sf(fill = NA, linewidth = 2, color = \"black\") + # don't need to specify data here since we did in ggplot call\n  geom_sf(\n    data = site_ecoreg_sf, \n    aes(color = US_L3NAME) \n    ) +\n  scale_color_discrete(name = \"Level 3 Ecoregion\") +\n  guides(fill = guide_legend(ncol = 1, title.position = \"top\")) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\"\n  )\n\nZoom: 7\n\n\nFetching 35 missing tiles\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\n\n...complete!\n\n\n\n\n\nBasemap imagery: © OpenStreetMap-Mitwirkende, SRTM | Kartendarstellung: © OpenTopoMap (CC-BY-SA)\n\n\n\n\nYou can also do this with the maptiles package using the create_provider function:\n\n# code not run, shown as example: \nopentopomap &lt;- create_provider(\n  name = \"otm\",\n  url = \"https://{s}.tile.opentopomap.org/{z}/{x}/{y}.png\",\n  sub = c(\"a\", \"b\", \"c\"),\n  citation = \"map data: © OpenStreetMap contributors, SRTM | map style: © OpenTopoMap (CC-BY-SA)\"\n)\n\nPlaces to Retrieve Basemaps\n\nmaptiler: XYZ rasters, free tier, requires API\nxyzservices: list of xyz sources, some free some require subscriptions\nOpenStreetMap wiki\nhttps://www.xyht.com/spatial-itgis/using-openstreetmap-basemaps-qgis-3-0/\nhttps://github.com/roblabs/xyz-raster-sources\n\n\n\n10.2.4 Basemap from File\nYou can also add a basemap from a file in a similar way to the maptiles code, if your basemap layer is a GeoTiff file (.tif). Simply read in your data layer with terra and add it to your plot with tidyterra::geom_spatraster_rgb() or tidyterra::geom_spatraster() if it is a single layer.\nExample:\n\n# This code not run, shown as a demo\n\n# load aerial imagery from National Agricultural Imagery Program\nnaip_tif &lt;- rast(\"data/naip_imagery.tif\") \nsamp_pts_sf &lt;- st_read(\"data/shapefiles/sample_points.shp\")\n\nggplot(data = samp_pts_sf) +\n  geom_spatraster_rgb(data = naip_tif) + \n  geom_sf()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_05.html#add-a-scalebar-and-north-arrow",
    "href": "session_05.html#add-a-scalebar-and-north-arrow",
    "title": "5  Visualizing Spatial Data",
    "section": "10.3 Add a scalebar and north arrow",
    "text": "10.3 Add a scalebar and north arrow\nIt is usually helpful to have a scalebar and north arrow for reference and orientation. There are built-in functions in ggspatial to make this easy:\n\nca_osm &lt;- get_tiles(ca_proj_sf, crop = TRUE, zoom = 6, provider = \"Esri.WorldImagery\")\n\nggplot(data = ca_proj_sf) + # specify data here to set CRS \n  geom_spatraster_rgb(data = ca_osm) +\n  geom_sf(fill = \"transparent\", color = \"black\", linewidth = 1) + # don't need to specify data here since we did in ggplot call\n  geom_sf(\n    data = site_ecoreg_sf, \n    aes(color = US_L3NAME) \n    #inherit.aes = FALSE # I've found I sometimes need to add this if I am having issues\n    ) +\n  scale_color_discrete(name = \"Level 3 Ecoregion\") +\n  guides(fill = guide_legend(ncol = 1, title.position = \"top\")) +\n  theme_void() +\n  ggspatial::annotation_scale() +\n  ggspatial::annotation_north_arrow()\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n\n\n10.3.1 Manipulating scale and north arrow\nYou’ll notice that with the basic options, the scale bar and north arrow overlap, and the scale or arrow may not display at an appropriate size with the default options. Some helpful arguments for manipulating the appearance of the scale bar and north arrows are shown below.\nArguments for both annotation_scale and annotation_north_arrow\n\npad_y and pad_x: change the positioning of the arrows from the corner\nheight and width: change the size of the arrow\n\n\n\n\n\n\n\nCaution\n\n\n\nYou must use the unit([value], \"unit\") function to specify position and size options for the padding, height, and width options or the call will return an error message. For example:\nannotation_scale(pad_x = unit(0.5, \"cm\"))\n\n\n\nstyle:\n\nfor north arrow, specify style with a call to one of the built-in arrow functions, e.g. north_arrow_fancy_orienteering. This also allows you to change line and text color of the arrow.\nfor scale bar, one of “bar” or “ticks”\n\nlocation: general location to put the scalebar or north arrow, e.g. “tl” for “top left”\n\nNorth arrow only\n\nwhich_north: “grid” - north arrow points up; “true” - north arrow points to north pole (may need to modify depending on projection)\n\nScale bar only\n\nwidth_hint: roundabout way of controlling how wide the scale_bar is, which is the only way to change the number of breaks displayed\nunit_category: “metric” or “imperial” units\n\n\nggplot(data = ca_proj_sf) + # specify data here to set CRS \n  geom_spatraster_rgb(data = ca_osm) +\n  geom_sf(fill = \"transparent\", color = \"black\", linewidth = 1) + \n  geom_sf(\n    data = site_ecoreg_sf, \n    aes(color = US_L3NAME) \n    ) +\n  scale_color_discrete(name = \"Level 3 Ecoregion\") +\n  guides(fill = guide_legend(ncol = 1, title.position = \"top\")) +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\"\n  ) +\n  ggspatial::annotation_scale(\n    pad_y = unit(0.5, \"cm\"), \n    width_hint = 0.3,\n    unit_category = \"imperial\"\n    ) +\n  ggspatial::annotation_north_arrow(\n    pad_y = unit(1, \"cm\"),\n    pad_x = unit(.75, \"cm\"),\n    height = unit(1, \"cm\"),\n    width = unit(1, \"cm\"),\n    which_north = \"true\",\n    style = ggspatial::north_arrow_fancy_orienteering(\n      line_col = \"white\",\n      text_col = \"white\",\n      fill = c(\"white\", \"black\")\n    ))\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_05.html#exercises",
    "href": "session_05.html#exercises",
    "title": "5  Visualizing Spatial Data",
    "section": "10.4 Exercises",
    "text": "10.4 Exercises\n\nUse the tigris package and either ggspatial or maptiles package to download a basemap for a different state and then visualize it with ggplot\n\n\n# Template code\n\n\nToo easy? Try downloading either another data type from the tigris package (e.g., roads or county lines) or another data repository of your choice and plotting that on top of your basemap.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_05.html#making-an-inset-map",
    "href": "session_05.html#making-an-inset-map",
    "title": "5  Visualizing Spatial Data",
    "section": "11.1 Making an inset map",
    "text": "11.1 Making an inset map\nAs mentioned above, there may be times when you want to focus on a small region but provide context for the larger region. An inset map can help with this, and one way to do this in R is to create two separate maps and then combine them. I use the cowplot package to layer them.\nHere I am modifying the California ecoregion shapefile to highlight the central drylands of California.\n\n# select central california dryland ecoregions\necoreg_sjv &lt;- c(\"6ac\", \"6an\", \"6ak\", \"6ao\", \"6am\", \"6ad\") \necoreg_pan &lt;- c(\"6aa\", \"6ab\", \"7v\", \"7u\", \"6al\")\necoreg_CV &lt;- c(\"7o\", \"7m\", \"7r\", \"7t\", \"7s\",\"7q\", \"7d\", \"7p\", \"7n\")\n\n# create dataframe for grouping\nall_groups &lt;-  data.frame(\n  group = c(\n    rep(\"SJV\", length(ecoreg_sjv)),\n    rep(\"PAN\", length(ecoreg_pan)),\n    rep(\"CV\", length(ecoreg_CV))\n  ),\n  l4_code = c(ecoreg_sjv, ecoreg_pan, ecoreg_CV)\n)\n\n# filter ecoregion shapefile to just the l4 codes specified fro the central drylands\nall_eco &lt;- ecoreg_sf %&gt;%\n  inner_join(all_groups, by = c(\"US_L4CODE\" = \"l4_code\"))\n\n# remove the inner boundaries of the L4 with st_union to create a shapefile with \n# only L3 boundaries \nall_eco &lt;- all_eco %&gt;% \n  group_by(group) %&gt;%\n  summarize(geometry = st_union(geometry)) \n\n# transform to proper CRS for plotting\ndryland_eco_3857 &lt;- st_transform(all_eco, crs = \"epsg:3857\")\n\nThis code creates two separate maps, then combines them with cowplot. ::: {.cell}\nlibrary(cowplot) # for combining maps\n\n# transform outline to get tiles\nca3857 &lt;- st_transform(ca_proj_sf, \"epsg:3857\")\nca_osm &lt;- get_tiles(\n  st_buffer(dryland_eco_3857, 50000), \n  crop = TRUE, \n  zoom = 7, \n  provider = \"Esri.WorldImagery\"\n  )\n\n# create plot boundary with a buffer\n# this creates a buffer of 50 km (50000 m, the CRS units)\nplot_bbox &lt;- sf::st_bbox(st_buffer(dryland_eco_3857, 50000))\n\n# join the boundaries to remove the L3 ecoregion layers\ndryland_eco_join_3857 &lt;- st_union(dryland_eco_3857)\n\n# make inset map\ninset_map &lt;- ggplot() + \n  geom_sf(data = dryland_eco_3857, fill = \"grey80\", color = \"black\", linewidth = 0.5) + \n  geom_sf(data = ca3857, fill = NA, color = \"black\", linewidth = 0.4) + \n  theme_bw() +\n  theme(axis.text        = element_blank(),\n        panel.grid       = element_blank(),\n        axis.ticks       = element_blank(),\n        panel.background = element_blank(),\n        panel.border     = element_blank(),\n        plot.background  = element_rect(colour = \"black\", fill = \"white\"))\n\n# make main map \nmain_map &lt;- ggplot() + \n  tidyterra::geom_spatraster_rgb(data = ca_osm) + \n  geom_sf(data = dryland_eco_join_3857, fill = NA, color = \"black\", linewidth = 0.7) + \n  geom_sf(data = ca3857, fill = NA, color = \"black\", linewidth = 0.4) + \n  coord_sf(\n    xlim = plot_bbox[c(1,3)],\n    ylim = plot_bbox[c(2,4)],\n    expand = FALSE\n  ) +\n  theme_void() +\n  ggspatial::annotation_scale(\n    text_col = \"white\", \n    line_col = \"black\", \n    pad_y    = unit(0.5, \"cm\")) +\n  ggspatial::annotation_north_arrow(\n    pad_y = unit(1, \"cm\"), \n    style =  ggspatial::north_arrow_fancy_orienteering(text_col = 'white', line_col = \"white\")) \n\n# combine into one plot\nggdraw() +\n  draw_plot(main_map) +\n  draw_plot(inset_map,\n            height = 0.28,\n            x = 0.16,\n            y = 0.68)\n\n\n\n\nMap of central California drylands including the San Joaquin Desert. Basemap imagery from ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n:::",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_05.html#creating-a-theme-and-setting-defaults",
    "href": "session_05.html#creating-a-theme-and-setting-defaults",
    "title": "5  Visualizing Spatial Data",
    "section": "11.2 Creating a Theme and Setting Defaults",
    "text": "11.2 Creating a Theme and Setting Defaults\n\n11.2.1 Create your own theme\nSpecifying the theme parameters each time takes up a lot of code, so I like to find base theme, modify it, and then save it as a new theme so I can add it to the map with one line of code. This also makes it easier to modify multiple plots since you will only have to change the code in one place to modify your theme as opposed to all of your code to make each map.\nFor mapping, I like to start with either theme_bw() if you want lat/long included or theme_void() if you don’t want any of the normal plot features.\nThere are a large number of different elements you can control with themes (see https://ggplot2-book.org/themes for an overview), but to manipulate the plot appearance you will usually follow this general template:\nplot + theme(element.name = element_function(argument = \"option\"))\nMost theme elements are also hierarchical, so they inherit from the parent element. This means that if you want to change all of the font for your plot, you only need to change the text element. Or, you could just change the axis.text.\n\ntheme_bw_ag &lt;- theme_bw(base_size = 14) +\n  theme(\n    text             = element_text(family = \"AvantGarde\"),\n    panel.background = element_blank(), # use element_blank() to remove elements\n    plot.background  = element_blank(),\n    panel.border     = element_rect(colour = \"grey92\"),\n    axis.ticks       = element_line(colour = \"grey92\"),\n    axis.text        = element_text(colour = \"grey40\"),\n    axis.text.x      = element_text(angle = 35, hjust = 1, vjust = 1)\n  )\n\nYou can also specify default color and fill options by creating a layer with the theme and scale_*_manual options as shown below\n\n# fill colors\necoreg_fill &lt;- c(\n  \"#fed725\",\n  \"#cde11d\",\n  \"#98d83e\",\n  \"#67cc5c\",\n  \"#40bd72\",\n  \"#25ac82\",\n  \"#1f998e\",\n  \"#24878e\",\n  \"#2b748e\",\n  \"#34618d\",\n  \"#3d4d8a\",\n  \"#453581\",\n  \"#481c6e\",\n  \"#440154\"\n)\n\necoreg_theme &lt;- list(\n  theme_bw(base_size = 14),\n  scale_fill_manual(values = ecoreg_fill, name = \"US L3 Ecoregion\"),\n  theme(\n    text             = element_text(family = \"Times\"),\n    panel.background = element_blank(), # use element_blank() to remove elements\n    plot.background  = element_blank(),\n    panel.border     = element_rect(colour = \"grey92\"),\n    axis.ticks       = element_line(colour = \"grey92\"),\n    axis.text        = element_text(colour = \"grey40\"),\n    axis.text.x      = element_text(angle = 35, hjust = 1, vjust = 1)\n  )\n)\n\n\n\n\n\n\n\nTip\n\n\n\nFor better font support and options see the extrafont package\n\n\n\nggplot() + # don't specify data here since we have multiple data sets to plot\n  geom_sf(data = ca_proj_sf, fill = NA) + # must specify \"data = \" or it will throw an error\n  geom_sf(data = ecoreg_sf, aes(fill = US_L3NAME), color = NA) + # like other data you can set fill \n  geom_sf(data = site_ca_proj_sf) +\n  guides(fill = guide_legend(ncol = 1, title.position = \"top\")) +\n  ecoreg_theme\n\n\n\n\n\n\n\n\n\n\n11.2.2 Set Geometry Defaults\nYou are also able to set some defaults for parameters that you can’t easily modify with ggplot themes. For example, specifying shapes and sizes for points or linewidth and colors for polygons.\n\n# set defaults for point geometry\nupdate_geom_defaults(\"point\", list(shape = 21, color = \"black\", fill = \"grey20\", size = 2))\n\n\nggplot() + # don't specify data here since we have multiple data sets to plot\n  geom_sf(data = ecoreg_sf, aes(fill = stringr::str_wrap(US_L3NAME, 35)), color = NA) + \n  geom_sf(data = site_ca_proj_sf, alpha = 0.5) +\n  geom_sf(data = ca_proj_sf, fill = NA, linewidth = 1, color = \"grey40\") + \n  guides(fill = guide_legend(ncol = 1, title.position = \"top\")) +\n  ecoreg_theme",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_05.html#functional-programming",
    "href": "session_05.html#functional-programming",
    "title": "5  Visualizing Spatial Data",
    "section": "11.3 Functional Programming",
    "text": "11.3 Functional Programming\nWhat if you want to make a lot of maps? Copying and pasting code is annoying and also leads to issues with reproducibility. If you are going to be making multiple maps that are very similar you can turn your call to ggplot into a function so that all you need to do is provide the data or other parameters you want to manipulate and save yourself many lines of code. This will also allow you to more easily modify multiple plots by only having to change your code in one place.\nAs an example, let’s play around with the California ecoregions some more.\nGoal: Create a function to plot each ecoregion with a basemap\nSteps:\n\nCreate a template map\nIdentify the components that will vary for each ecoregion - these become the arguments\nReplace the varying components with a variable\nPut into function\nCreate a list of arguments to run the function over\nPass the list to the function using purrr::map\n\n\nca_l3_ecoreg &lt;- unique(ecoreg_sf$US_L3NAME)\n\necoreg_sf_tmp &lt;- filter(ecoreg_sf, US_L3NAME == ca_l3_ecoreg[1])\n\nggplot(ecoreg_sf_tmp) +\n  geom_sf(aes(fill = US_L4NAME))\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n# now get a basemap\ntmp_basemap &lt;- get_tiles(ecoreg_sf_tmp, zoom = 7, provider = \"Esri.WorldImagery\")\n\n# add to plot\nggplot(ecoreg_sf_tmp) +\n  geom_spatraster_rgb(data = tmp_basemap) +\n  geom_sf(aes(fill = US_L4NAME), alpha = 0.5, color = \"black\")\n\n&lt;SpatRaster&gt; resampled to 501239 cells for plotting\n\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n\nNow we need to turn this into a generic function\n\nplot_ecoreg &lt;- function(ecoreg_name){\n  ecoreg_sf_tmp &lt;- filter(ecoreg_sf, US_L3NAME == ecoreg_name)\n  tmp_basemap &lt;- get_tiles(ecoreg_sf_tmp, zoom = 7, provider = \"Esri.WorldImagery\")\n  \n  ggplot(ecoreg_sf_tmp) +\n    geom_spatraster_rgb(data = tmp_basemap) +\n    geom_sf(aes(fill = US_L4NAME), alpha = 0.5, color = \"black\") \n}\n\nLet’s test it out with our list\n\nplot_ecoreg(ca_l3_ecoreg[1])\n\n&lt;SpatRaster&gt; resampled to 501239 cells for plotting\n\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\npurrr::map(ca_l3_ecoreg[1:5], plot_ecoreg)\n\n&lt;SpatRaster&gt; resampled to 501239 cells for plotting\n&lt;SpatRaster&gt; resampled to 500472 cells for plotting\n&lt;SpatRaster&gt; resampled to 500516 cells for plotting\n\n\n[[1]]\n\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n\n\n[[2]]\n\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n\n\n[[3]]\n\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n\n\n[[4]]\n\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n\n\n[[5]]\n\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n\nI’d like to add a title for each figure so we know which ecoregion it represents.\n\nplot_ecoreg &lt;- function(ecoreg_name){\n  ecoreg_sf_tmp &lt;- filter(ecoreg_sf, US_L3NAME == ecoreg_name)\n  tmp_basemap   &lt;- get_tiles(ecoreg_sf_tmp, zoom = 7, provider = \"Esri.WorldImagery\")\n  title         &lt;- ecoreg_sf_tmp$US_L3NAME\n  \n  ggplot(ecoreg_sf_tmp) +\n    geom_spatraster_rgb(data = tmp_basemap) +\n    geom_sf(aes(fill = US_L4NAME), alpha = 0.5, color = \"black\") +\n    ggtitle(title) +\n    theme_void() +\n    theme(legend.position = \"none\")\n}\n\nNow let’s try it again:\n\npurrr::map(ca_l3_ecoreg[1:5], plot_ecoreg)\n\n&lt;SpatRaster&gt; resampled to 501239 cells for plotting\n&lt;SpatRaster&gt; resampled to 500472 cells for plotting\n&lt;SpatRaster&gt; resampled to 500516 cells for plotting\n\n\n[[1]]\n\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n\n\n[[2]]\n\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n\n\n[[3]]\n\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n\n\n[[4]]\n\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n\n\n[[5]]\n\n\n\n\n\nBasemap ESRI World Imagery, Sources: Esri, DigitalGlobe, GeoEye, i-cubed, USDA FSA, USGS, AEX, Getmapping, Aerogrid, IGN, IGP, swisstopo, and the GIS User Community.\n\n\n\n\nThis is pretty quick and dirty illustration of how to turn a simple map into a function for faster and reproducible programming - but it’s one of the big perks of mapping in R. With just a couple lines of code we could make a map for all of the ecoregions of California - which would have taken way more code if we had to make each map individually.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Spatial Data</span>"
    ]
  },
  {
    "objectID": "session_08.html",
    "href": "session_08.html",
    "title": "8  Science Communication: Message Box",
    "section": "",
    "text": "Learning Objective",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#learning-objective",
    "href": "session_08.html#learning-objective",
    "title": "8  Science Communication: Message Box",
    "section": "",
    "text": "Discuss about the importance of science communication.\nDistinguish between how scientist communicate sciences vs how the rest of the world communicates.\nIntroduce and practice using the Message Box as a tool to communicate science to a specific audience.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#communicating-science",
    "href": "session_08.html#communicating-science",
    "title": "8  Science Communication: Message Box",
    "section": "8.1 Communicating Science",
    "text": "8.1 Communicating Science\nHow we communicate our science with the rest of the world matters! There are many different reasons and ways to engage and practice science communication. From writing a blog post as a creative outlet to posting in social media to make science accessible to a broader audience. Science communication is more and more becoming a formal responsibility within researchers as funding organizations encourage or require scientist to do outreach of their work (Borowiec (2023)).\nCommunication can come easy, but doing it well is hard. As scientist, it is important to spread the word of our discoveries, engage with non-scientist audiences, and build empathy and trust on what we do (PLOS SciCom). This way make sure the world understand how important, necessary and meaningful science is.\n\n\n\nJarreau, Paige B (2015): #MySciBlog Interviewee Motivations to Blog about Science\n\n\n\n\n8.1.1 Scholarly publications\n\n“The ingredients of good science are obvious—novelty of research topic, comprehensive coverage of the relevant literature, good data, good analysis including strong statistical support, and a thought-provoking discussion. The ingredients of good science reporting are obvious—good organization, the appropriate use of tables and figures, the right length, writing to the intended audience— do not ignore the obvious” - Bourne 2005\n\nPeer-reviewed publication is the primary mechanism for direct and efficient communication of research findings. Other scholarly communications include abstracts, technical reports, books and book chapters. These communications are largely directed towards students and peers; individuals learning about or engaged in the process of scientific research whether in a university, non-profit, agency, commercial or other setting.\nThe following tabling is a good summary of ‘10 Simple Rules’ for writing research papers (adapted from Zhang 2014, published in Budden and Michener, 2017)\n\n\n\n\n\n\n\n\nMake it a Driving Force\n\n“Design a project with an ultimate paper firmly in mind”\n\n\n\nLess Is More\n\n“Fewer but more significant papers serve both the research community and one’s career better than more papers of less significance”\n\n\n\nPick the Right Audience\n\n“This is critical for determining the organization of the paper and the level of detail of the story, so as to write the paper with the audience in mind.”\n\n\n\nBe Logical\n\n“The foundation of ‘’lively’’ writing for smooth reading is a sound and clear logic underlying the story of the paper.” “An effective tactic to help develop a sound logical flow is to imaginatively create a set of figures and tables, which will ultimately be developed from experimental results, and order them in a logical way based on the information flow through the experiments.”\n\n\n\nBe Thorough and Make It Complete\n\nPresent the central underlying hypotheses; interpret the insights gleaned from figures and tables and discuss their implications; provide sufficient context so the paper is self-contained; provide explicit results so readers do not need to perform their own calculations; and include self-contained figures and tables that are described in clear legends\n\n\n\nBe Concise\n\n“the delivery of a message is more rigorous if the writing is precise and concise”\n\n\n\nBe Artistic\n\n“concentrate on spelling, grammar, usage, and a ‘’lively’’ writing style that avoids successions of simple, boring, declarative sentences”\n\n\n\nBe Your Own Judge\n\nReview, revise and reiterate. “…put yourself completely in the shoes of a referee and scrutinize all the pieces—the significance of the work, the logic of the story, the correctness of the results and conclusions, the organization of the paper, and the presentation of the materials.”\n\n\n\nTest the Water in Your Own Backyard\n\n“…collect feedback and critiques from others, e.g., colleagues and collaborators.”\n\n\n\nBuild a Virtual Team of Collaborators\n\nTreat reviewers as collaborators and respond objectively to their criticisms and recommendations. This may entail redoing research and thoroughly re-writing a paper.\n\n\n\nEven though reporting our results in a clear and efficient way through scholarly publications is important, the ways scientist communicate is not how the rest of the rest of the world typically communicates. In a scientific paper, we establish credibility in the introduction and methods, provide detailed data and results, and then share the significance of our work in the discussion and conclusions. But the rest of the world leads with the impact, the take home message. An example of this are newspaper headlines.\n“But the rest of the world leads with the conclusions, because that’s what people want to know. What are your findings and why is this relevant to them? In other words, what’s your bottom line?”\n\n\n\n8.1.2 Other communications\nCommunicating your research outside of peer-reviewed journal articles is increasingly common, and important. These non academic communications can reach a more broad and diverse audience than traditional publications (and should be tailored for specific audience groups accordingly), are not subject to the same pay-walls as journal articles, and can augment and amplify scholarly publications. For example, this twitter announcement from Dr Heather Kropp, providing a synopsis of their synthesis publication in Environmental Research (note the reference to a repository where the data are located, though a DOI would be better).\n\nWhether this communication occurs through blogs, social media, or via interviews with others, developing practices to refine your messaging is critical for successful communication. One tool to support your communication practice is ‘The Message Box’ developed by COMPASS, an organization that helps scientists develop communications skills in order to share their knowledge and research across broad audiences without compromising the accuracy of their research.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#the-message-box",
    "href": "session_08.html#the-message-box",
    "title": "8  Science Communication: Message Box",
    "section": "8.2 The Message Box",
    "text": "8.2 The Message Box\n\n\nThe Message Box is a tool that helps researchers take the information they hold about their research and communicate it in a way that resonates with the chosen audience. It can be used to help prepare for interviews with journalists or employers, plan for a presentation, outline a paper or lecture, prepare a grant proposal, or clearly, and with relevancy, communicate your work to others. While the message box can be used in all these ways, you must first identify the audience for your communication.\n“Whether a journalist, a policymaker, a room of colleagues at a professional meeting, or a class of second-graders—doesn’t have deep knowledge of your subject matter. But that doesn’t mean you should explain everything you know in a fire hose of information. In fact, cognitive research tells us that the human brain can only absorb three to five pieces of information at a time. So prioritize what you share based on what your audience needs to know. Your goal as you fill out your Message Box is to identify the information that is critical to your audience—what really matters to them—and share that. Effective science communication is less about expounding on all the exciting details that you might want to convey, and more about knowing your audience and providing them with what they need or want to know from you.”\nThe Message Box comprises five sections to help you sort and distill your knowledge in a way that will resonate with your (chosen) audience.\nThe five sections of the Message Box are provided below. For a detailed explanation of the sections and guidance on how to use the Message Box, work through the Message Box Workbook\n\n8.2.1 Message Box Sections\nThe Issue\nThe Issue section in the center of the box identifies and describes the overarching issue or topic that you’re addressing in broad terms. It’s the big-picture context of your work. This should be very concise and clear; no more than a short phrase. You might find you revisit the Issue after you’ve filled out your Message Box, to see if your thinking on the overarching topic has changed since you started.\n\nDescribes the overarching issue or topic: Big Picture\nBroad enough to cover key points\nSpecific enough to set up what’s to come\nConcise and clear\n‘Frames’ the rest of the message box\n\nThe Problem\nThe Problem is the chunk of the broader issue that you’re addressing in your area of expertise. It’s your piece of the pie, reflecting your work and expert knowledge. Think about your research questions and what aspect of the specific problem you’re addressing would matter to your audience. The Problem is also where you set up the So What and describe the situation you see and want to address.\n\nThe part of the broader issue that your work is addressing\nBuilds upon your work and expert knowledge\nTry to focus on one problem per audience\nOften the Problem is your research question\nThis section sets you up for So What\n\nThe So What\nThe crux of the Message Box, and the critical question the COMPASS team seeks to help scientists answer, is “So what?” Why should your audience care? What about your research or work is important for them to know? Why are you talking to them about it? The answer to this question may change from audience to audience, and you’ll want to be able to adjust based on their interests and needs. We like to use the analogy of putting a message through a prism that clarifies the importance to different audiences. Each audience will be interested in different facets of your work, and you want your message to reflect their interests and accommodate their needs. The prism below includes a spectrum of audiences you might want to reach, and some of the questions they might have about your work.\n\nThis is the crux of the message box\nWhy should you audience care?\nWhat about your research is important for them to know?\nWhy are you talking to them about it?\n\n\nThe Solution\nThe Solution section outlines the options for solving the problem you identified. When presenting possible solutions, consider whether they are something your audience can influence or act upon. And remind yourself of your communication goals: Why are you communicating with this audience? What do you want to accomplish?\n\nOutlines the options for solving the Problem\nCan your audience influence or act upon this?\nThere may be multiple solutions\nMake sure your Solution relates back to the Problem. Edit one or both as needed\n\nThe Benefit\nIn the Benefit section, you list the benefits of addressing the Problem — all the good things that could happen if your Solution section is implemented. This ties into the So What of why your audience cares, but focuses on the positive results of taking action (the So What may be a negative thing — for example, inaction could lead to consequences that your audience cares about). If possible, it can be helpful to be specific here — concrete examples are more compelling than abstract. Who is likely to benefit, and where, and when?\n\nWhat are the benefits of addressing the Problem?\nWhat good things come from implementing your Solution?\nMake sure it connects with your So What\nBenefits and So What may be similar\n\nFinally, to make you message more memorable you should:\n\nSupport your message with data\nLimit the use of numbers and statistics\nUse specific examples\nCompare numbers to concepts, help people relate\nAvoid jargon\nLead with what you know\n\n\nIn addition to the Message Box Workbook, COMPASS have resources on how to increase the impact of your message (include important statistics, draw comparisons, reduce jargon, use examples), exercises for practicing and refining your message and published examples.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#resources",
    "href": "session_08.html#resources",
    "title": "8  Science Communication: Message Box",
    "section": "8.3 Resources",
    "text": "8.3 Resources\n\n\n\nDataONE Webinar: Communication Strategies to Increase Your Impact from DataONE on Vimeo.\n\n\nBudden, AE and Michener, W (2017) Communicating and Disseminating Research Findings. In: Ecological Informatics, 3rd Edition. Recknagel, F, Michener, W (eds.) Springer-Verlag\nCOMPASS Core Principles of Science Communication\nExample Message Boxes",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#your-turn",
    "href": "session_08.html#your-turn",
    "title": "8  Science Communication: Message Box",
    "section": "8.4 Your Turn",
    "text": "8.4 Your Turn\nLet’s take a look on how the Message Box looks in practice.\n\n\n\n\n\n\nExercise\n\n\n\n\nLook into real examples of scienctist using the message box here.\nThink about your synthesis project and a potential audience you would like to communicate the results. Define your audience and start filling in the different components of the Message Box.\n\nNote: This is just the first iteration to help your think about your work in a different way.\n\n\n\n\n\n\nBorowiec, Brittney G. 2023. “Ten Simple Rules for Scientists Engaging in Science Communication.” PLOS Computational Biology 19 (7): 1–8. https://doi.org/10.1371/journal.pcbi.1011251.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: Message Box</span>"
    ]
  },
  {
    "objectID": "session_09.html",
    "href": "session_09.html",
    "title": "9  Publishing to the Web",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Publishing to the Web</span>"
    ]
  },
  {
    "objectID": "session_09.html#learning-objectives",
    "href": "session_09.html#learning-objectives",
    "title": "9  Publishing to the Web",
    "section": "",
    "text": "How to use Git, GitHub (+Pages), and Quarto to publish an analysis to the web",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Publishing to the Web</span>"
    ]
  },
  {
    "objectID": "session_09.html#introduction",
    "href": "session_09.html#introduction",
    "title": "9  Publishing to the Web",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nSharing your work with others in engaging ways is an important part of the scientific process.\nSo far in this course, we’ve introduced a small set of powerful tools for doing open science:\n\nR and its many packages\nRStudio\nGit\nGitHub\nQuarto\n\nQuarto, in particular, is amazingly powerful for creating scientific reports but, so far, we haven’t tapped its full potential for sharing our work with others.\nIn this lesson, we’re going to take our training_{USERNAME} GitHub repository and turn it into a beautiful and easy to read web page using the tools listed above.\n\n\n\n\n\n\nSet up\n\n\n\n\nMake sure you are in training_{USERNAME} project\nAdd a new Quarto file at the top level called index.qmd\n\nGo to the RStudio menu File -&gt; New File -&gt; Quarto Document\nThis will bring up a dialog box. Add the title “GitHub Pages Example”, keep the Default Output Format as “HTML”, and then click “OK”\n\nSave the Quarto Document you just created. Use index.qmd as the file name\n\nBe sure to use the exact case (lower case “index”) as different operating systems handle case differently and it can interfere with loading your web page later\n\nPress “Render” and observe the rendered output\n\nNotice the new file in the same directory index.html\nThis is our Quarto file rendered as HTML (a web page)\n\nCommit your changes (for both index.qmd and index.html) with a commit message, and push to GitHub\nOpen your web browser to the github.com and navigate to the page for your training_{USERNAME} repository\nActivate GitHub Pages for the main branch\n\nGo to Settings -&gt; Pages (underneath the Code and Automation section)\nKeep the “Source” as “Deploy from a branch”\nUnder “Branch” you’ll see a message that says “GitHub Pages is currently disabled”. To change this, change the branch from “None” to main. Keep the folder as the root and then click “Save”\nYou should see the message change to “Your GitHub Pages site is currently being built from the main branch”\n\n\nNote: index.qmd represents the default file for a web site, and is returned whenever you visit the web site but doesn’t specify an explicit file to be returned.\n\n\nNow, the rendered website version of your repo will show up at a special URL.\nGitHub Pages follows a convention like this:\n\nNote that it changes from github.com to github.io\n\nGo to https://{username}.github.io/{repo_name}/ (Note the trailing /)\nObserve the awesome rendered output\n\nNow that we’ve successfully published a web page from an Quarto Document, let’s make a change to our Quarto Document and follow the steps to publish the change on the web:\n\n\n\n\n\n\nUpdate content in your published page\n\n\n\n\nGo back to your index.qmd\nDelete all the content, except the YAML frontmatter\nType “Hello world”\nUse Git workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\nGo back to https://{username}.github.io/{repo_name}/\n\n\n\nNext, we will show how you can link different qmd’s rendered into html so you can easily share different parts of your work.\n\n\n\n\n\n\nExercise\n\n\n\nIn this exercise, you’ll create a table of contents with the lessons of this course on the main page, and link some of the files we have work on so far.\n\nGo back to the RStudio server and to your index.qmd file\nCreate a table of contents with the names of the main technical lessons of this course, like so:\n\n## Course Lessons\n\n- Introduction to Literate Analysis Using Quarto \n- Cleaning and Wrangling data\n- Data Visualization\n- Functions and packages\n\n## Course Practice Session\n\n- Practice I: Cleaning and Wrnagling and Data Viz\n- Practice II: Writing functions\n\n\nMake sure you have the html versions of your intro-to-qmd.qmd and data-cleaning.qmd files. If you only see the qmd version, you need to “Render” your files first\nIn your index.qmd let’s add the links to the html files we want to show on our webpage. Do you remember the Markdown syntax to create a link?\n\n\n\nMarkdown syntax to create a link:\n\n\n[Text you want to hyperlink](link)\n\nExample: [Data wrangling and cleaning](data-wrangling-cleaning.html)\n\n\n\n\nUse Git workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\n\nNow when you visit your web site, you’ll see the table of contents, and can navigate to the others file you linked.\n\n\nQuarto web pages are a great way to share work in progress with your colleagues. Here we showed an example with the materials we have created in this course. However, you can use these same steps to share the different files and progress of a project you’ve been working on. To do so simply requires thinking through your presentation so that it highlights the workflow to be reviewed. You can include multiple pages and build a simple web site and make your work accessible to people who aren’t set up to open your project in R. Your site could look something like this:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Publishing to the Web</span>"
    ]
  },
  {
    "objectID": "session_12.html",
    "href": "session_12.html",
    "title": "12  Reproducibility and Provenance",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reproducibility and Provenance</span>"
    ]
  },
  {
    "objectID": "session_12.html#learning-objectives",
    "href": "session_12.html#learning-objectives",
    "title": "12  Reproducibility and Provenance",
    "section": "",
    "text": "Discuss the concept of reproducible workflows including computational reproducibility and provenance metadata\nLearn how to use R to package your work by building a reproducible paper in RMarkdown/Quarto\nIntroduce tools and techniques for reproducibility supported by the NCEAS and DataONE\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn this lesson, we will be leveraging RMarkdown instead of Quarto so that we can use a very cool R package called rticles. Quarto has the same functionality as RMarkdown with rticles - making journal formatted articles from a code notebook - but it is done from the command line without additional R packages. See the Quarto documentation for details\n\n\n\n12.0.1 Reproducible Research: Recap\nWorking in a reproducible manner:\n\nIncreases research efficiency, accelerating the pace of your research and collaborations.\nProvides transparency by capturing and communicating scientific workflows.\nEnables research to stand on the shoulders of giants (build on work that came before).\nAllows credit for secondary usage and supports easy attribution.\nIncreases trust in science.\n\nTo enable others to fully interpret, reproduce or build upon our research, we need to provide more comprehensive information than is typically included in a figure or publication. The methods sections of papers are typically inadequate to fully reproduce the work described in the paper.\n\nFor example, if we look at the figure above convey multiple messages. But, by looking at the figure we don’t get the full story how did scientist got to make this plot. What data were used in this study? What methods applied? What were the parameter settings? What documentation or code are available to us to evaluate the results? Can we trust these data and methods? Are the results reproducible?\nComputational reproducibility is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions.\nPractically speaking, reproducibility includes:\n\nPreserving the data\nPreserving the software workflow\nDocumenting what you did\nDescribing how to interpret it all\n\nA recent study of publicly-available datasets in the Harvard Database repository containing R files found that only 26% of R files ran without error in the initial execution. 44% were able to be run after code cleaning, showing the importance of good programming practice (Trisovic et al. 2022). The figure below from Trisovic et al. shows a sankey diagram of how code cleaning was able to fix common errors.\n\n\n\n12.0.2 Computational Provenance and Workflows\nComputational provenance refers to the origin and processing history of data including:\n\nInput data\nWorkflow/scripts\nOutput data\nFigures\nMethods, dataflow, and dependencies\n\nWhen we put these all together with formal documentation, we create a computational workflow that captures all of the steps from initial data cleaning and integration, through analysis, modeling, and visualization. In other words, computational provenance is a formalized description of a workflow from the origin of the data to it’s final outcome.\nHere’s an example of a computational workflow from Mark Carls: Mark Carls. Analysis of hydrocarbons following the Exxon Valdez oil spill, Gulf of Alaska, 1989 - 2014. Gulf of Alaska Data Portal. urn:uuid:3249ada0-afe3-4dd6-875e-0f7928a4c171., that represents a three step workflow comprising four source data files and two output visualizations.\n\n\nThis image is a screenshot of an interactive user interface of a workflow built by DataONE. You can clearly see which data files were inputs to the process, the scripts that are used to process and visualize the data, and the final output objects that are produced, in this case two graphical maps of Prince William Sound in Alaska.\n\n\n12.0.3 From Provenance to Reproducibility\n\nDataONE provides a tool to track and visualize provenance. It facilitates reproducible science through provenance by:\n\nTracking data derivation history\nTracking data inputs and outputs of analyses\nPreserving and documenting software workflows\nTracking analysis and model executions\nLinking all of these to publications\n\n\nOne way to illustrate this is to look into the structure of a data package. A data package is the unit of publication of your data, including datasets, metadata, software and provenance. The image below represents a data package and all it’s components and how these components relate to each other.\n\n\n\n\n12.0.4 Data Citation and Transitive Credit\nWe want to move towards a model such that when a user cites a research publication we will also know:\n\nWhich data produced it\nWhat software produced it\nWhat was derived from it\nWho to credit down the attribution stack\n\n\nThis is transitive credit. And it changes the way in which we think about science communication and traditional publications.\n\n\n12.0.5 Reproducible Papers with rrtools\nA great overview of this approach to reproducible papers comes from:\n\nBen Marwick, Carl Boettiger & Lincoln Mullen (2018) Packaging Data Analytical Work Reproducibly Using R (and Friends), The American Statistician, 72:1, 80-88, doi:10.1080/00031305.2017.1375986\n\nThe key idea in Marwick et al. (2018) is that of the research compendium: A single container for not just the journal article associated with your research but also the underlying analysis, data, and even the required software environment required to reproduce your work.\nResearch compendium makes it easy for researchers to do their work but also for others to inspect or even reproduce the work because all necessary materials are readily at hand due to being kept in one place. Rather than a constrained set of rules, the research compendium is a scaffold upon which to conduct reproducible research using open science tools such as:\n\nR\nRMarkdown\nQuarto\ngit and GitHub\n\nFortunately for us, Ben Marwick (and others) have written an R package called rrtools that helps us create a research compendium from scratch.\n\n\n\n\n\n\nSet up\n\n\n\nTo start a reproducible paper with rrtools:\n\nClose your username-training project. Go to the project switcher dropdown, just click “close project.” This will set your working directory back to your home directory.\nIn console run the following line of code\n\n\n## \"mypaper\" is the name of the Rproj with my research compendia\nrrtools::use_compendium(\"mypaper\")\n\nrrtools has created the beginnings of a research compendium for us. The structure of this compendium is similar to the one needed to built an R package. That’s because it uses the same underlying folder structure and metadata and therefore it technically is an R package (called mypaper). And this means our research compendium could be easy to install in someone elses’ computer, similar to an R package.\n\nrrtools also helps you set up some key information like:\n\n\nSet up a README file in the RMarkdown format\nCreate an analysis folder to hold our reproducible paper\n\n\nrrtools::use_readme_rmd()\nrrtools::use_analysis()\n\nThis creates a standard, predictable layout for our code and data and outputs that multiple people can understand. At this point, we’re technically ready to start writing the paper. But.. What about GitHub?\n\n\n\n12.0.5.1 Creating a git and GitHub repository with usethis\n\nusethis is a package that facilitates interactive workflows for R project creation and development. It automates repetitive tasks that arise during project setup and development.\n\nWe are going to use two functions to start tracking our work in git, create a remote repository in GitHub and be able to push and pull between the local version and the remote. To learn more about this package checkout the package documentation.\n\n\n\n\n\n\nSet up\n\n\n\n\nMake sure your are in “mypaper” Rproj.\nIn the Console run usethis::use_git() to create a local git repo. Choose yes to both questions when prompted (to commit files, and to restart R).\nThen, in the Console, run usethis::use_github() to create an upstream remote repo (in GitHub).\n\nAnd that’s it! Now your have your research compendium in your local computer and your changes are being tracked by git and your can pull and push to GitHub.\n\n\nLet’s explore the structure rrtools has put in place for us. Inside the analysis folder we have 5 folders. Different parts of our project will go into this different folders. Our data into the data folder, when the time comes to save any figure, we should save them into the figures folder, and so on.\n\n\n\nResearch compendia from Marwick et al.\n\n\nYou’ll notice a analysis/templates directory that contains journal citation style language (CSL) files which set the style of citations and reference list for the journal (the Journal of Archaeological Science, in this example). The template.Rmd renders into the template.docx. This document is called in the paper.qmd YAML to style the output of the paper created in paper.qmd.\nWhat if I want a template from another journal, different from the Journal of Archeological Science? We can create other journal’s template with the rticles package. This package will provide the templates and necessary information to render your paper in the journal of your choice (note: not all journal are in the rticles package). With that in mind, we will delete the existing paper directory and create a new one shortly.\n\n\n\n12.0.6 RMarkdown templates with rticles\nThe rticles package provides a lot of other great templates for formatting your paper specifically to the requirements of many journals. In addition to a custom CSL file for reference customization, rticles supports custom LATeX templates that fit the formatting requirements of each journals.\n\n\n\n\n\n\nTinytex and rendering to PDF\n\n\n\nTo be able to render your document to PDF you need to have tinytex installed in your machine.\nIn the console run:\n\ninstall.packages('tinytex') ## this package is already installed in our server\n\ntinytex::install_tinytex() ## this may take several minutes\n\n\n\n\n\n\n\n\n\nSet up\n\n\n\n\nIf you do not have rticle installed, go aherad and inatall calling the following function in the console: install.packages('rticles') Restart your RStudio session\nTo create a new file from rticlescustom templates, got to File | New File | R Markdown... menu, which shows the following dialog:\n\n\n\nGo to “From Template” in the left side menu.\nSelect the “PNAS” template, give the file a name and set the location of the files to be mypaper/analysis, and click “OK”.\nYou can now Knit the Rmd file to see a highly-targeted article format, like this one for PNAS:\n\n\n\n\n\n\n12.0.7 Workflow in a nutshell\n\n\n\n\n\n\nSummary\n\n\n\n\nUse rrtools to generate the core directory layout and approach to data handling.\nThen use rticles to create the structure of the paper itself. The combination is incredibly flexible.\n\n\n\nThings we can do with our research compendium:\n\nEdit ./analysis/paper/paper.Rmd to begin writing your paper and your analysis in the same document\nAdd any citations to ./analysis/paper/pnas-sample.bib\nAdd any longer R scripts that don’t fit in your paper in an R folder at the top level\nAdd raw data to ./data/raw_data\nWrite out any derived data (generated in paper.Rmd) to ./data/derived_data\nWrite out any figures in ./analysis/figures\n\nYou can then write all of your R code in your RMarkdown/Quarto, and generate your manuscript all in the format needed for your journal (using it’s .csl file, stored in the paper directory).\n\n\n12.0.8 Adding renv to conserve your environment\n\nrrtools has a couple more tricks up it’s sleeve to help your compendium be as reproducible and portable as possible.\nTo capture the R packages and versions this project depends on, we can use the renv package.\nRunning renv::init(), will initiate tracking of the R packages in your project.\nThis action will create a new folder called renv in your top directory.\nrenv::init() automatically detects dependencies in your code (by looking for library calls, at the DESCRIPTION file, etc.) and installs them to a private project specific library. This means that your project mypaper can use a different version of dplyr than another project which may need an older version without any hassle.\nrenv also write the package dependencies to a special file in the repository called renv.lock.\nIf any of your packages you are using is updated, while your are working on your project, you can run renv::snapshot() to update the renv.lock file and your project-installed packages.\nYou can read the renv.lock file using renv::restore(), when needed. This will install the versions of the packages needed.\n\n\n\n12.0.9 Conserve your computational environement with Docker\n\nThe rrtools package then uses this renv.lock file to build what is called a Dockerfile.\nDocker allows you to build containers, a standard unit of software that packages up code and all its dependencies so an application runs quickly and reliably from one computing environment to another.\nA container is an “image” of all the software specified, and this image can be run on other computers such that the software stack looks exactly as you specify.\nThis is important when it comes to reproducibility, because when running someone else code, you may get different results or errors if you are using different versions of software (like an old version of dplyr).\nA Dockerfile contains the instructions for how to recreate the computational environment where your analysis was run.\n\nIn practice\n\nOnce you have your research compendium, you can called rrtools::use_dockerfile(). If needed, re-install rrtools directly from GitHub remotes::install_github(\"benmarwick/rrtools\")\nThis, first creates a Dockerfile that loads a standard image for using R with the tidyverse,\nAnd then has more instructions for how to create the environment so that it has the very specific R packages and versions you need.\nIf we look at the Dockerfile (example below), it calls to renv::restore(), as described above.\nThe last line of the docker file renders our Quarto/RMarkdown reproducible paper!\n\n# get the base image, the rocker/verse has R, RStudio and pandoc\nFROM rocker/verse:4.2.2\n\n# required\nMAINTAINER Your Name &lt;your_email@somewhere.com&gt;\n\nCOPY . /&lt;REPO&gt;\n\n# go into the repo directory\nRUN . /etc/environment \\\n  # Install linux depedendencies here\n  # e.g. need this for ggforce::geom_sina\n  && sudo apt-get update \\\n  && sudo apt-get install libudunits2-dev -y \\\n  # build this compendium package\n  && R -e \"install.packages('remotes', repos = c(CRAN = 'https://cloud.r-project.org'))\" \\\n  && R -e \"remotes::install_github(c('rstudio/renv', 'quarto-dev/quarto-r'))\" \\\n  # install pkgs we need\n  && R -e \"renv::restore()\" \\\n  # render the manuscript into a docx, you'll need to edit this if you've\n  # customised the location and name of your main qmd file\n  && R -e \"quarto::quarto_render('/&lt;REPO&gt;/analysis/paper/paper.qmd')\"\n\nAfter running rrtools::use_dockerfile(), the package also sets up GitHub Actions for you.\nActions are processes that are triggered in GitHub events (like a push) and run automatically.\nIn this case, the Action that is set up will build your Docker image on GitHub.\nThis means that the code that knits your paper is run, and an updated version of your paper is knit.\nThis is called continuous integration, and is extremely convenient for developing products like this, since the build step can be taken care of automatically as you push to your repository.\n\n\n\n\n12.0.10 The 5th Generation of Reproducible Papers\n\nWhole Tale is a project that aims to simplify computational reproducibility. It enables researchers to easily package and share ‘tales’. Tales are executable research objects captured in a standards-based tale format complete with metadata. They can contain:\n\nData (references)\nCode (computational methods)\nNarrative (traditional science story)\nCompute environment (e.g. RStudio, Jupyter)\n\n\nBy combining data, code and the compute environment, tales allow researchers to:\n\nRe-create the computational results from a scientific study\nAchieve computational reproducibility\n“Set the default to reproducible.”\n\nFull circle reproducibility can be achieved by publishing data, code AND the computational environment.\n\n\n12.0.11 Resources\n\nrrtools documentation\nThe rticles\nusethis documentation\n\n\n\n\n\nTrisovic, Ana, Matthew K. Lau, Thomas Pasquier, and Mercè Crosas. 2022. “A Large-Scale Study on Research Code Quality and Execution.” Scientific Data 9 (1). https://doi.org/10.1038/s41597-022-01143-6.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reproducibility and Provenance</span>"
    ]
  },
  {
    "objectID": "session_13.html",
    "href": "session_13.html",
    "title": "13  Check and Set your PAT",
    "section": "",
    "text": "13.1 Steps to check if your Personal Access Token is valid",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Check and Set your PAT</span>"
    ]
  },
  {
    "objectID": "session_13.html#steps-to-check-if-your-personal-access-token-is-valid",
    "href": "session_13.html#steps-to-check-if-your-personal-access-token-is-valid",
    "title": "13  Check and Set your PAT",
    "section": "",
    "text": "Login to included-crab\nOpen training_LASTNAME Rproj\nIn the console run: usethis::git_sitrep()\n\n\n\n\n\n\n\nIf your Personal Access Token is , you have to go ahead and reset it following the instructions on how to Set (or reset) your PAT.\n\n\n\n\n\n\n\nSet (or reset) your PAT\n\n\n\n\n\n\n\nSetting up your PAT\n\n\n\n\nRun usethis::create_github_token() in the Console.\nA new browser window should open up to GitHub, showing all the scopes options. You can review the scopes, but you don’t need to worry about which ones to select this time. Using create_github_token() automatically pre-selects some recommended scopes. Go ahead and scroll to the bottom and click “Generate Token”.\nCopy the generated token.\nBack in RStudio, run gitcreds::gitcreds_set() in the Console.\nPaste your PAT when the prompt asks for it.\nLast thing, run usethis::git_sitrep() in the Console to check your Git configuration and that you’ve successful stored your PAT.\n\n\n\n\nCheck your PAT is .\n\n\nIn the console run usethis::git_sitrep()\nExpect to see this result:",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Check and Set your PAT</span>"
    ]
  }
]