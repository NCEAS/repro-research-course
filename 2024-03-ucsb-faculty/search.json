[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "",
    "text": "About the course\nHosted by the National Center for Ecological Anlysis and Synthesis (NCEAS) and in partnership with UCSB’s Office of Research, this seminar series is a training program where participants engage in synthesis research in a small cohort. It will equip participants with data science and team science tools, and provide them with a platform to conduct collaborative synthesis research. The overall aim is to grow cross-departmental relationships and interdisciplinary research outputs.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#nceas-expertise",
    "href": "index.html#nceas-expertise",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "NCEAS Expertise",
    "text": "NCEAS Expertise\nNCEAS, a research affiliate of UCSB, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science. NCEAS has over 25 years of success with this model among working groups and environmental professionals.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#week-three-communicating-your-science-and-reproducible-workflows",
    "href": "index.html#week-three-communicating-your-science-and-reproducible-workflows",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "Week Three: Communicating your Science and Reproducible Workflows",
    "text": "Week Three: Communicating your Science and Reproducible Workflows\nMarch 25 - 27, 2024\n\nLearning Objectives\n\nADD\n\n\n\nWeek’s Schedule",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Camila Vargas Poulsen, Rachel King, Casey O’Hara (2024), UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills, March 25-27, NCEAS Learning Hub. URL https://learning.nceas.ucsb.edu/2024-03-ucsb-faculty.\nAdditional contributors: Ben Bolker, Amber E. Budden, Julien Brun, Samantha Csik, Halina Do-Linh, Natasha Haycock-Chavez, S. Jeanette Clark, Julie Lowndes, Stephanie Hampton, Matt Jone, Samanta Katz, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "session_01.html",
    "href": "session_01.html",
    "title": "1  Seminar Series Recap and Reproducibility",
    "section": "",
    "text": "1.1 Reproducibility activity using LEGO®",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Seminar Series Recap and Reproducibility</span>"
    ]
  },
  {
    "objectID": "session_01.html#reproducibility-activity-using-lego",
    "href": "session_01.html#reproducibility-activity-using-lego",
    "title": "1  Seminar Series Recap and Reproducibility",
    "section": "",
    "text": "Learning Objectives\n\nIllustrate elements of good reproducibility through the medium of LEGO®\nDiscuss what is needed and what is not needed for good reproducibility\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThis activity is largely based on the LEGO® Metadata for Reproducibility game pack, which was developed by Mary Donaldson and Matt Mahon.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Seminar Series Recap and Reproducibility</span>"
    ]
  },
  {
    "objectID": "session_01.html#getting-started",
    "href": "session_01.html#getting-started",
    "title": "1  Seminar Series Recap and Reproducibility",
    "section": "1.2 Getting started",
    "text": "1.2 Getting started\n\n\n\n\n\n\nSetup\n\n\n\n\nGather into small groups\nGet LEGO® blocks and worksheets (instructions + metadata documentation)\nFollow directions on worksheets\n\n\n\nAt the end, we will discuss as a group.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Seminar Series Recap and Reproducibility</span>"
    ]
  },
  {
    "objectID": "session_01.html#discussion",
    "href": "session_01.html#discussion",
    "title": "1  Seminar Series Recap and Reproducibility",
    "section": "1.3 Discussion",
    "text": "1.3 Discussion\n\n\nDiscussion Questions\n\n\nDid you find this a simple way to document your process?\nWas there anything you found difficult to capture?\nDid those replicating the builds find it straightforward to follow?\nDid you encounter any ambiguity in the instructions?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Seminar Series Recap and Reproducibility</span>"
    ]
  },
  {
    "objectID": "session_02.html",
    "href": "session_02.html",
    "title": "2  Data Visualization",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_02.html#learning-objectives",
    "href": "session_02.html#learning-objectives",
    "title": "2  Data Visualization",
    "section": "",
    "text": "Understand the fundamentals of how the ggplot2 package works\nUse ggplot2’s theme and other customization functions create publication-grade graphics\nIntroduce the leaflet and DT package to create interactive maps and tables respectively",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_02.html#overview",
    "href": "session_02.html#overview",
    "title": "2  Data Visualization",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nggplot2 is a popular package for visualizing data in R. From the home page:\n\nggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.\n\nIt’s been around for years and has pretty good documentation and tons of example code around the web (like on StackOverflow). The goal of this lesson is to introduce you to the basic components of working with ggplot2 and inspire you to go and explore this awesome resource for visualizing your data.\n\n\n\n\n\n\nggplot2 vs base graphics in R vs others\n\n\n\nThere are many different ways to plot your data in R. All of them work! However, ggplot2 excels at making complicated plots easy and easy plots simple enough\nBase R graphics (plot(), hist(), etc) can be helpful for simple, quick and dirty plots. ggplot2 can be used for almost everything else.\n\n\nLet’s dive into creating and customizing plots with ggplot2.\n\n\n\n\n\n\nSetup\n\n\n\n\nMake sure you’re in the right project (training_{USERNAME}) and use the Git workflow by Pulling to check for any changes. Then, create a new Quarto document, delete the default text, and save this document.\nLoad the packages we’ll need:\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(janitor) # expedite cleaning and exploring data\nlibrary(scales) # scale functions for visualization\nlibrary(leaflet) # interactive maps\nlibrary(DT) # interactive tables\n\n\nLoad the data table directly from the KNB Data Repository: Daily salmon escapement counts from the OceanAK database, Alaska, 1921-2017. Navigate to the link above, hover over the “Download” button for the ADFG_fisrtAttempt_reformatted.csv, right click, and select “Copy Link”.\n\n\nescape_raw &lt;- read_csv(\"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Af119a05b-bbe7-4aea-93c6-85434dcb1c5e\")\n\n\nLearn about the data. For this session we are going to be working with data on daily salmon escapement counts in Alaska. Check out the documentation.\nFinally, let’s explore the data we just read into our working environment.\n\n\n## Check out column names\ncolnames(escape_raw)\n\n## Peak at each column and class\nglimpse(escape_raw)\n\n## From when to when\nrange(escape_raw$sampleDate)\n\n## Which species?\nunique(escape$Species)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_02.html#getting-the-data-ready",
    "href": "session_02.html#getting-the-data-ready",
    "title": "2  Data Visualization",
    "section": "2.2 Getting the data ready",
    "text": "2.2 Getting the data ready\nIt is more frequent than not, that we need to do some wrangling before we can plot our data the way we want to. Now that we have read out data and have done some exploration, we’ll put our data wrangling skills to practice to get our data in the desired format.\n\n\n\n\n\n\nSide note on clean column names\n\n\n\njanitor::clean_names() is an awesome function to transform all column names into the same format. The default format for this function is snake_case_format. We highly recommend having clear well formatted column names. It makes your life easier down the line.\nHow it works?\n\nescape &lt;- escape_raw %&gt;% \n    janitor::clean_names()\n\nAnd that’s it! If we look to the colomn names of the object escape we can see all the columns are in a lowercase, snake format.\n\ncolnames(escape)\n\n[1] \"location\"     \"sasap_region\" \"sample_date\"  \"species\"      \"daily_count\" \n[6] \"method\"       \"latitude\"     \"longitude\"    \"source\"      \n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nCalculate the annual escapement by species and sasap_region,\nFilter the main 5 salmon species (Chinook, Sockeye, Chum, Coho and Pink)\n\n\n\n\nannual_esc &lt;- escape %&gt;%\n    separate(sample_date, c(\"year\", \"month\", \"day\"), sep = \"-\") %&gt;%\n    mutate(year = as.numeric(year)) %&gt;%\n    group_by(species, sasap_region, year) %&gt;%\n    summarize(escapement = sum(daily_count)) %&gt;%\n    filter(species %in% c(\"Chinook\", \"Sockeye\", \"Chum\", \"Coho\", \"Pink\"))\n\nhead(annual_esc)\n\n# A tibble: 6 × 4\n# Groups:   species, sasap_region [1]\n  species sasap_region                           year escapement\n  &lt;chr&gt;   &lt;chr&gt;                                 &lt;dbl&gt;      &lt;dbl&gt;\n1 Chinook Alaska Peninsula and Aleutian Islands  1974       1092\n2 Chinook Alaska Peninsula and Aleutian Islands  1975       1917\n3 Chinook Alaska Peninsula and Aleutian Islands  1976       3045\n4 Chinook Alaska Peninsula and Aleutian Islands  1977       4844\n5 Chinook Alaska Peninsula and Aleutian Islands  1978       3901\n6 Chinook Alaska Peninsula and Aleutian Islands  1979      10463\n\n\nThe chunk above used a lot of the dplyr commands that we’ve used, and some that are new. The separate() function is used to divide the sample_date column up into year, month, and day columns, and then we use group_by() to indicate that we want to calculate our results for the unique combinations of species, region, and year. We next use summarize() to calculate an escapement value for each of these groups. Finally, we use a filter and the %in% operator to select only the salmon species.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_02.html#plotting-with-ggplot2",
    "href": "session_02.html#plotting-with-ggplot2",
    "title": "2  Data Visualization",
    "section": "2.3 Plotting with ggplot2",
    "text": "2.3 Plotting with ggplot2\n\n2.3.1 Essentials components\nFirst, we’ll cover some ggplot2 basics to create the foundation of our plot. Then, we’ll add on to make our great customized data visualization.\n\n\n\n\n\n\nThe basics\n\n\n\n\nIndicate we are using ggplot() (call the ggplot2::ggplot() function)\nWhat data do we want to plot? (data = my_data)\nWhat is my mapping aesthetics? What variables do we want to plot? (define usingaes() function)\nDefine the geometry of our plot. This specifies the type of plot we’re making (use geom_*() to indicate the type of plot e.g: point, bar, etc.)\n\nNote To add layers to our plot, for example, additional geometries/aesthetics and theme elements or any ggplot object we use +.\n\n\nFor example, let’s plot total escapement by species. We will show this by creating the same plot in 3 slightly different ways. Each of the options below have the essential pieces of a ggplot.\n\n## Option 1 - data and mapping called in the ggplot() function\nggplot(data = annual_esc,\n       aes(x = species, y = escapement)) +\n    geom_col()\n\n## Option 2 - data called in ggplot function; mapping called in geom\nggplot(data = annual_esc) +\n    geom_col(aes(x = species, y = escapement))\n\n\n## Option 3 - data and mapping called in geom\nggplot() +\n    geom_col(data = annual_esc,\n             aes(x = species, y = escapement))\n\nThey all will create the same plot:\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Looking at different geoms\nHaving the basic structure with the essential components in mind, we can easily change the type of graph by updating the geom_*().\n\n\n\n\n\n\nggplot2 and the pipe operator\n\n\n\nJust like in dplyr and tidyr, we can also pipe a data.frame directly into the first argument of the ggplot function using the %&gt;% operator.\nThis can certainly be convenient, but use it carefully! Combining too many data-tidying or subsetting operations with your ggplot call can make your code more difficult to debug and understand.\n\n\nNext, we will use the pipe operator to pass into ggplot() a filtered version of annual_esc, and make a plot with different geometries.\nBoxplot\n\nannual_esc %&gt;%\n    filter(year == 1974,\n          species %in% c(\"Chum\", \"Pink\")) %&gt;%\n    ggplot(aes(x = species, y = escapement)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nViolin plot\n\nannual_esc %&gt;%\n    filter(year == 1974,\n           species %in% c(\"Chum\", \"Pink\")) %&gt;%\n    ggplot(aes(x = species, y = escapement)) +\n    geom_violin()\n\n\n\n\n\n\n\n\nLine and point\n\nannual_esc %&gt;%\n    filter(species  == \"Sockeye\",\n           sasap_region == \"Bristol Bay\") %&gt;%\n    ggplot(aes(x = year, y = escapement)) +\n    geom_line() +\n    geom_point()\n\n\n\n\n\n\n\n\n\n\n2.3.3 Customizing our plot\nLet’s go back to our base bar graph. What if we want our bars to be blue instead of gray? You might think we could run this:\n\nggplot(annual_esc,\n       aes(x = species, y = escapement,\n           fill = \"blue\")) +\n    geom_col()\n\n\n\n\n\n\n\n\nWhy did that happen?\nNotice that we tried to set the fill color of the plot inside the mapping aesthetic call. What we have done, behind the scenes, is create a column filled with the word “blue” in our data frame, and then mapped it to the fill aesthetic, which then chose the default fill color of red.\nWhat we really wanted to do was just change the color of the bars. If we want do do that, we can call the color option in the geom_col() function, outside of the mapping aesthetics function call.\n\nggplot(annual_esc,\n       aes(x = species, y = escapement)) +\n    geom_col(fill = \"blue\")\n\n\n\n\n\n\n\n\nWhat if we did want to map the color of the bars to a variable, such as region. ggplot() is really powerful because we can easily get this plot to visualize more aspects of our data.\n\nggplot(annual_esc,\n       aes(x = species, y = escapement,\n           fill = sasap_region)) +\n    geom_col()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeep in mind\n\n\n\n\nIf you want to map a variable onto a graph aesthetic (e.g., point color should be based on a specific region), put it within aes().\nIf you want to update your plot base on a constant (e.g. “Make ALL the points BLUE”), you can add the information directly to the relevant geom_ layer.\n\n\n\n\n2.3.3.1 Creating multiple plots\nWe know that in the graph we just plotted, each bar includes escapements for multiple years. Let’s leverage the power of ggplot to plot more aspects of our data in one plot.\nWe are going to plot escapement by species over time, from 2000 to 2016, for each region.\nAn easy way to plot another aspect of your data is using the function facet_wrap(). This function takes a mapping to a variable using the syntax ~{variable_name}. The ~ (tilde) is a model operator which tells facet_wrap() to model each unique value within variable_name to a facet in the plot.\nThe default behavior of facet wrap is to put all facets on the same x and y scale. You can use the scales argument to specify whether to allow different scales between facet plots (e.g scales = \"free_y\" to free the y axis scale). You can also specify the number of columns using the ncol = argument or number of rows using nrow =.\n\n## Subset with data from years 2000 to 2016\n\nannual_esc_2000s &lt;- annual_esc %&gt;%\n    filter(year %in% c(2000:2016))\n\n## Quick check\nunique(annual_esc_2000s$year)\n\n [1] 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014\n[16] 2015 2016\n\n## Plot with facets\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\n2.3.3.2 Setting ggplot themes\nNow let’s work on making this plot look a bit nicer. We are going to”\n\nAdd a title using labs()\nAdjust labels using labs()\nInclude a built in theme using theme_bw()\n\nThere are a wide variety of built in themes in ggplot that help quickly set the look of the plot. Use the RStudio autocomplete theme_ &lt;TAB&gt; to view a list of theme functions.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nYou can see that the theme_bw() function changed a lot of the aspects of our plot! The background is white, the grid is a different color, etc. There are lots of other built in themes like this that come with the ggplot2 package.\n\n\n\n\n\n\nExercise\n\n\n\nUse the RStudio auto complete, the ggplot2 documentation, a cheat sheet, or good old Google to find other built in themes. Pick out your favorite one and add it to your plot.\n\n\n\n\nThemes\n## Useful baseline themes are\ntheme_minimal()\ntheme_light()\ntheme_classic()\n\n\nThe built in theme functions (theme_*()) change the default settings for many elements that can also be changed individually using thetheme() function. The theme() function is a way to further fine-tune the look of your plot. This function takes MANY arguments (just have a look at ?theme). Luckily there are many great ggplot resources online so we don’t have to remember all of these, just Google “ggplot cheat sheet” and find one you like.\nLet’s look at an example of a theme() call, where we change the position of the legend from the right side to the bottom, and remove its title.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    theme_light() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank())\n\n\n\n\n\n\n\n\nNote that the theme() call needs to come after any built-in themes like theme_bw() are used. Otherwise, theme_bw() will likely override any theme elements that you changed using theme().\nYou can also save the result of a series of theme() function calls to an object to use on multiple plots. This prevents needing to copy paste the same lines over and over again!\n\nmy_theme &lt;- theme_light() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank())\n\nSo now our code will look like this:\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    my_theme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nUsing whatever method you like, figure out how to rotate the x-axis tick labels to a 45 degree angle.\n\nHint: You can start by looking at the documentation of the function by typing ?theme() in the console. And googling is a great way to figure out how to do the modifications you want to your plot.\n\nWhat changes do you expect to see in your plot by adding the following line of code? Discuss with your neighbor and then try it out!\n\nscale_x_continuous(breaks = seq(2000,2016, 2))\n\n\n\n\nAnswer\n## Useful baseline themes are\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    scale_x_continuous(breaks = seq(2000, 2016, 2)) +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    my_theme +\n    theme(axis.text.x = element_text(angle = 45,\n                                     vjust = 0.5))\n\n\n\n\n2.3.3.3 Smarter tick labels using scales\nFixing tick labels in ggplot can be super annoying. The y-axis labels in the plot above don’t look great. We could manually fix them, but it would likely be tedious and error prone.\nThe scales package provides some nice helper functions to easily rescale and relabel your plots. Here, we use scale_y_continuous() from ggplot2, with the argument labels, which is assigned to the function name comma, from the scales package. This will format all of the labels on the y-axis of our plot with comma-formatted numbers.\n\nggplot(annual_esc_2000s,\n       aes(x = year,\n           y = escapement,\n           color = species)) +\n    geom_line() +\n    geom_point() +\n    scale_x_continuous(breaks = seq(2000, 2016, 2)) +\n    scale_y_continuous(labels = comma) +\n    facet_wrap( ~ sasap_region,\n                scales = \"free_y\") +\n    labs(title = \"Annual Salmon Escapement by Region\",\n         y = \"Escapement\") +\n    my_theme +\n    theme(axis.text.x = element_text(angle = 45,\n                                     vjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n2.3.3.4 Saving plots\nSaving plots using ggplot is easy! The ggsave() function will save either the last plot you created, or any plot that you have saved to a variable. You can specify what output format you want, size, resolution, etc. See ?ggsave() for documentation.\n\nggsave(\"figures/annualsalmon_esc_region.jpg\", width = 8, height = 6, units = \"in\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_02.html#interactive-visualization",
    "href": "session_02.html#interactive-visualization",
    "title": "2  Data Visualization",
    "section": "2.4 Interactive visualization",
    "text": "2.4 Interactive visualization\n\n2.4.1 Tables with DT\nNow that we know how to make great static visualizations, let’s introduce two other packages that allow us to display our data in interactive ways. These packages really shine when used with GitHub Pages, so at the end of this lesson we will publish our figures to the website we created earlier.\nFirst let’s show an interactive table of unique sampling locations using DT. Write a data.frame containing unique sampling locations with no missing values using two new functions from dplyr and tidyr: distinct() and drop_na().\n\nlocations &lt;- escape %&gt;%\n    distinct(location, latitude, longitude) %&gt;%\n    drop_na()\n\nAnd display it as an interactive table using datatable() from the DT package.\n\ndatatable(locations)\n\n\n\n\n\n\n\n2.4.2 Maps with leaflet\nSimilar to ggplot2, you can make a basic leaflet map using just a couple lines of code. Note that unlike ggplot2, the leaflet package uses pipe operators (%&gt;%) and not the additive operator (+).\nThe addTiles() function without arguments will add base tiles to your map from OpenStreetMap. addMarkers() will add a marker at each location specified by the latitude and longitude arguments. Note that the ~ symbol is used here to model the coordinates to the map (similar to facet_wrap() in ggplot).\n\nleaflet(locations) %&gt;%\n    addTiles() %&gt;%\n    addMarkers(\n        lng = ~ longitude,\n        lat = ~ latitude,\n        popup = ~ location\n    )\n\n\n\n\n\n\nYou can also use leaflet to import Web Map Service (WMS) tiles. Here is an example that utilizes the General Bathymetric Map of the Oceans (GEBCO) WMS tiles. In this example, we also demonstrate how to create a more simple circle marker, the look of which is explicitly set using a series of style-related arguments.\n\nleaflet(locations) %&gt;%\n    addWMSTiles(\n        \"https://www.gebco.net/data_and_products/gebco_web_services/web_map_service/mapserv?request=getmap&service=wms&BBOX=-90,-180,90,360&crs=EPSG:4326&format=image/jpeg&layers=gebco_latest&width=1200&height=600&version=1.3.0\",\n        layers = 'GEBCO_LATEST',\n        attribution = \"Imagery reproduced from the GEBCO_2022 Grid, WMS 1.3.0 GetMap, www.gebco.net\"\n    ) %&gt;%\n    addCircleMarkers(\n        lng = ~ longitude,\n        lat = ~ latitude,\n        popup = ~ location,\n        radius = 5,\n        # set fill properties\n        fillColor = \"salmon\",\n        fillOpacity = 1,\n        # set stroke properties\n        stroke = T,\n        weight = 0.5,\n        color = \"white\",\n        opacity = 1\n    )\n\n\n\n\n\n\nLeaflet has a ton of functionality that can enable you to create some beautiful, functional maps with relative ease. Here is an example of some we created as part of the State of Alaskan Salmon and People (SASAP) project, created using the same tools we showed you here. This map hopefully gives you an idea of how powerful the combination of R Markdown and GitHub Pages can be.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_02.html#ggplot2-resources",
    "href": "session_02.html#ggplot2-resources",
    "title": "2  Data Visualization",
    "section": "2.5 ggplot2 Resources",
    "text": "2.5 ggplot2 Resources\n\nWhy not to use two axes, and what to use instead: The case against dual axis charts by Lisa Charlotte Rost.\nCustomized Data Visualization in ggplot2 by Allison Horst.\nA ggplot2 tutorial for beautiful plotting in R by Cedric Scherer.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "session_03.html",
    "href": "session_03.html",
    "title": "3  Text Analysis in R",
    "section": "",
    "text": "LINK TO CASEY’S MATERIALS",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Analysis in R</span>"
    ]
  },
  {
    "objectID": "session_04.html",
    "href": "session_04.html",
    "title": "4  Reproducible Survey Workflow",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Survey Workflow</span>"
    ]
  },
  {
    "objectID": "session_04.html#learning-objectives",
    "href": "session_04.html#learning-objectives",
    "title": "4  Reproducible Survey Workflow",
    "section": "",
    "text": "Overview of survey tools\nGenerating a reproducible survey report with Qualtrics",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Survey Workflow</span>"
    ]
  },
  {
    "objectID": "session_04.html#introduction",
    "href": "session_04.html#introduction",
    "title": "4  Reproducible Survey Workflow",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nSurveys and questionnaires are commonly used research methods within social science and other fields. For example, understanding regional and national population demographics, income, and education as part of the National Census activity, assessing audience perspectives on specific topics of research interest (e.g. the work by Tenopir and colleagues on Data Sharing by Scientists), evaluation of learning deliverable and outcomes, and consumer feedback on new and upcoming products. These are distinct from the use of the term survey within natural sciences, which might include geographical surveys (“the making of measurement in the field from which maps are drawn”), ecological surveys (“the process whereby a proposed development site is assess to establish any environmental impact the development may have”) or biodiversity surveys (“provide detailed information about biodiversity and community structure”) among others.\nAlthough surveys can be conducted on paper or verbally, here we focus on surveys done via software tools. Needs will vary according to the nature of the research being undertaken. However, there is fundamental functionality that survey software should provide including:\n\nThe ability to create and customize questions\nThe ability to include different types of questions\nThe ability to distribute the survey and manage response collection\nThe ability to collect, summarize, and (securely) store response data\n\nMore advanced features can include:\n\nVisual design and templates - custom design might include institutional branding or aesthetic elements. Templates allow you to save these designs and apply to other surveys\nQuestion piping - piping inserts answers from previous questions into upcoming questions and can personalize the survey experience for users\nSurvey logic - with question logic and skip logic you can control the inclusion / exclusion of questions based on previous responses\nRandomization - the ability to randomize the presentation of questions within (blocks of) the survey\nBranching - this allows for different users to take different paths through the survey. Similar to question logic but at a bigger scale\nLanguage support - automated translation or multi-language presentation support\nShared administration - enables collaboration on the survey and response analysis\nSurvey export - ability to download (export) the survey instrument\nReports - survey response visualization and reporting tools\nInstitutional IRB approved - institutional IRB policy may require certain software be used for research purposes\n\nCommonly used survey software within academic (vs market) research include Qualtrics, Survey Monkey and Google Forms. Both Qualtrics and Survey Monkey are licensed (with limited functionality available at no cost) and Google forms is free.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Survey Workflow</span>"
    ]
  },
  {
    "objectID": "session_04.html#building-workflows-using-qualtrics",
    "href": "session_04.html#building-workflows-using-qualtrics",
    "title": "4  Reproducible Survey Workflow",
    "section": "4.2 Building workflows using Qualtrics",
    "text": "4.2 Building workflows using Qualtrics\nIn this lesson we will use the qualtRics package to reproducible access some survey results set up for this course.\n\n4.2.1 Survey Instrument\nThe survey is very short, only four questions. The first question is on it’s own page and is a consent question, after a couple of short paragraphs describing what the survey is, it’s purpose, how long it will take to complete, and who is conducting it. This type of information is required if the survey is governed by an IRB, and the content will depend on the type of research being conducted. In this case, this survey is not for research purposes, and thus is not governed by IRB, but we still include this information as it conforms to the Belmont Principles. The Belmont Principles identify the basic ethical principles that should underlie research involving human subjects.\n\nThe three main questions of the survey have three types of responses: a multiple choice answer, a multiple choice answer which also includes an “other” write in option, and a free text answer. We’ll use the results of this survey, which was sent out to NCEAS staff to fill out, to learn about how to create a reproducible survey report.\n\n\n\n4.2.2 Working with qualtiRcs\n\n\n\n\n\n\nSet up\n\n\n\nFirst, open a new Quarto document and add a chunk to load the libraries we’ll need for this lesson:\n\nlibrary(qualtRics)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(kableExtra)\nlibrary(dplyr)\n\nNext, we need to set the API credentials. The qualtrics_api_credentials function creates environment variables to hold your Qualtrics account information. The function can either temporarily store this information for just this session, or it can modify the .Renviron file to set your API key and base URL so that you can access Qualtrics programmatically from any session.\nThe API key is as good as a password, so care should be taken to not share it publicly. For example, you would never want to save it in a script. The function below is the rare exception of code that should be run in the console and not saved. It works in a way that you only need to run it once, unless you are working on a new computer or your credentials changed. Note that in this book, we have not shared the actual API key, for the reasons outlined above. For the course, we will share the key via a file or by e-mail. Provide the key as a string to the api_key argument in the function below:\n\nkey_file &lt;- read_lines(\"/tmp/qualtrics-key.txt\")\nqualtrics_api_credentials(api_key = key_file[1], base_url = \"ucsb.co1.qualtrics.com\", install = FALSE, overwrite = FALSE)\n\n\n\n\n\n\n\n\n\nAside note\n\n\n\nThe .Renviron file is a special user controlled file that can create environment variables. Every time you open Rstudio, the variables in your environment file are loaded as…environment variables! Environment variables are named values that are accessible by your R process. They will not show up in your environment pane, but you can get a list of all of them using Sys.getenv(). Many are system defaults.\nTo view or edit your .Renviron file, you can use usethis::edit_r_environ().\n\n\nTo get a list of all the surveys in your Qualtrics instance, use the all_surveys function.\n\nsurveys &lt;- all_surveys()\nkable(surveys) %&gt;%\n    kable_styling()\n\nThis function returns a list of surveys, in this case only one, and information about each, including an identifier and it’s name. We’ll need that identifier later, so let’s go ahead and extract it using base R from the data frame.\n\ni &lt;- which(surveys$name == \"Survey for Data Science Training\")\nid &lt;- surveys$id[i]\n\nYou can retrieve a list of the questions the survey asked using the survey_questions function and the survey id.\n\nquestions &lt;- survey_questions(id)\nkable(questions) %&gt;%\n    kable_styling()\n\nThis returns a data.frame with one row per question with columns for question id, question name, question text, and whether the question was required. This is helpful to have as a reference for when you are looking at the full survey results.\nTo get the full survey results, run fetch_survey with the survey id.\n\nsurvey_results &lt;- fetch_survey(id)\nglimpse(survey_results)\n\nThe survey results table has tons of information in it, not all of which will be relevant depending on your survey. The table has identifying information for the respondents (eg: ResponseID, IPaddress, RecipientEmail, RecipientFirstName, etc), much of which will be empty for this survey since it is anonymous. It also has information about the process of taking the survey, such as the StartDate, EndDate, Progress, and Duration. Finally, there are the answers to the questions asked, with columns labeled according to the qname column in the questions table (eg: Q1, Q2, Q3). Depending on the type of question, some questions might have multiple columns associated with them. We’ll have a look at this more closely in a later example.\n\n4.2.2.1 Question 2\nLet’s look at the responses to the second question in the survey, “How long have you been programming?” Remember, the first question was the consent question.\nWe’ll use the dplyr and tidyr tools we learned earlier to extract the information. Here are the steps:\n\nselect the column we want (Q1)\ngroup_by and summarize the values\n\n\nq2 &lt;- survey_results %&gt;% \n    select(Q2) %&gt;% \n    group_by(Q2) %&gt;% \n    summarise(n = n())\n\nWe can show these results in a table using the kable function from the knitr package:\n\nkable(q2, col.names = c(\"How long have you been programming?\",\n                        \"Number of responses\")) %&gt;%\n    kable_styling()\n\n\n\n4.2.2.2 Question 3\nFor question 3, we’ll use a similar workflow. For this question, however there are two columns containing survey answers. One contains the answers from the controlled vocabulary, the other contains any free text answers users entered.\nTo present this information, we’ll first show the results of the controlled answers as a plot. Below the plot, we’ll include a table showing all of the free text answers for the “other” option.\n\nq3 &lt;- survey_results %&gt;% \n    select(Q3) %&gt;% \n    group_by(Q3) %&gt;% \n    summarise(n = n())\n\n\nggplot(data = q3, \n       mapping = aes(x = Q3, y = n)) +\n    geom_col() +\n    labs(x = \"What language do you currently use most frequently?\", y = \"Number of reponses\") +\n    theme_minimal()\n\nNow we’ll extract the free text responses:\n\nq3_text &lt;- survey_results %&gt;% \n    select(Q3_7_TEXT) %&gt;% \n    drop_na()\n\nkable(q3_text, col.names = c(\"Other responses to 'What language do you currently use mose frequently?'\")) %&gt;% \n    kable_styling()\n\n\n\n4.2.2.3 Question 4\nThe last question is just a free text question, so we can just display the results as is.\n\nq4 &lt;- survey_results %&gt;% \n    select(Q4) %&gt;% \n    rename(`What data science tool or language are you most excited to learn next?` = Q4) %&gt;% \n    drop_na()\n\nkable(q4, col.names = \"What data science tool or language are you most excited to learn next?\") %&gt;% \n    kable_styling()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Survey Workflow</span>"
    ]
  },
  {
    "objectID": "session_04.html#other-survey-tools",
    "href": "session_04.html#other-survey-tools",
    "title": "4  Reproducible Survey Workflow",
    "section": "4.3 Other survey tools",
    "text": "4.3 Other survey tools\n\n4.3.1 Google forms\nGoogle forms can be a great way to set up surveys, and it is very easy to interact with the results using R. The benefits of using google forms are a simple interface and easy sharing between collaborators, especially when writing the survey instrument.\nThe downside is that google forms has far fewer features than Qualtrics in terms of survey flow and appearance.\nTo show how we can link R into our survey workflows, I’ve set up a simple example survey here.\nI’ve set up the results so that they are in a new spreadsheet here:. To access them, we will use the googlesheets4 package.\nFirst, open up a new Quarto doc and load the googlesheets4 library:\n\nlibrary(googlesheets4)\n\nNext, we can read the sheet in using the same URL that you would use to share the sheet with someone else. Right now, this sheet is public\n\nresponses &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1CSG__ejXQNZdwXc1QK8dKouxphP520bjUOnZ5SzOVP8/edit?usp=sharing\")\n\n✔ Reading from \"Example Survey Form (Responses)\".\n\n\n✔ Range 'Form Responses 1'.\n\n\nThe first time you run this, you should get a popup window in your web browser asking you to confirm that you want to provide access to your google sheets via the tidyverse (googlesheets) package.\nMy dialog box looked like this:\n\nMake sure you click the third check box enabling the Tidyverse API to see, edit, create, and delete your sheets. Note that you will have to tell it to do any of these actions via the R code you write.\nWhen you come back to your R environment, you should have a data frame containing the data in your sheet! Let’s take a quick look at the structure of that sheet.\n\ndplyr::glimpse(responses)\n\nRows: 10\nColumns: 5\n$ Timestamp                                              &lt;dttm&gt; 2022-04-15 13:…\n$ `To what degree did the event meet your expectations?` &lt;chr&gt; \"Met expectatio…\n$ `To what degree did your knowledge improve?`           &lt;chr&gt; \"Increase\", \"Si…\n$ `What did you like most about the event?`              &lt;chr&gt; \"the cool instr…\n$ `What might you change about the event?`               &lt;chr&gt; \"more snacks\", …\n\n\nSo, now that we have the data in a standard R data.frame, we can easily summarize it and plot results. By default, the column names in the sheet are the long fully descriptive questions that were asked, which can be hard to type. We can save those questions into a vector for later reference, like when we want to use the question text for plot titles.\n\nquestions &lt;- colnames(responses)[2:5]\ndplyr::glimpse(questions)\n\n chr [1:4] \"To what degree did the event meet your expectations?\" ...\n\n\nWe can make the responses data frame more compact by renaming the columns of the vector with short numbered names of the form Q1. Note that, by using a sequence, this should work for sheets from just a few columns to many hundreds of columns, and provides a consistent question naming convention.\n\nnames(questions) &lt;- paste0(\"Q\", seq(1,4))\n\nquestions\n\n                                                    Q1 \n\"To what degree did the event meet your expectations?\" \n                                                    Q2 \n          \"To what degree did your knowledge improve?\" \n                                                    Q3 \n             \"What did you like most about the event?\" \n                                                    Q4 \n              \"What might you change about the event?\" \n\ncolnames(responses) &lt;- c(\"Timestamp\", names(questions))\ndplyr::glimpse(responses)\n\nRows: 10\nColumns: 5\n$ Timestamp &lt;dttm&gt; 2022-04-15 13:48:58, 2022-04-15 13:49:43, 2022-04-15 13:50:…\n$ Q1        &lt;chr&gt; \"Met expectations\", \"Above expectations\", \"Above expectation…\n$ Q2        &lt;chr&gt; \"Increase\", \"Significant increase\", \"Significant increase\", …\n$ Q3        &lt;chr&gt; \"the cool instructors\", \"R is rad!\", \"everything\", \"the pizz…\n$ Q4        &lt;chr&gt; \"more snacks\", \"no pineapple pizza!\", \"nothing\", \"needs more…\n\n\nNow that we’ve renamed our columns, let’s summarize the responses for the first question. We can use the same pattern that we usually do to split the data from Q1 into groups, then summarize it by counting the number of records in each group, and then merge the count of each group back together into a summarized data frame. We can then plot the Q1 results using ggplot:\n\nq1 &lt;- responses %&gt;% \n    dplyr::select(Q1) %&gt;% \n    dplyr::group_by(Q1) %&gt;% \n    dplyr::summarise(n = dplyr::n())\n\nggplot2::ggplot(data = q1, mapping = aes(x = Q1, y = n)) +\n    geom_col() +\n    labs(x = questions[1], \n         y = \"Number of reponses\",\n         title = \"To what degree did the course meet expectations?\") +\n    theme_minimal()\n\n\nBypassing authentication for public sheets\nIf you don’t want to go through a little interactive dialog every time you read in a sheet, and your sheet is public, you can run the function gs4_deauth() to access the sheet as a public user. This is helpful for cases when you want to run your code non-interactively. This is actually how I set it up for this book to build!\n\n\n\n\n4.3.2 Survey Monkey\nSimilar to Qualtrics and qualtRics, there is an open source R package for working with data in Survey Monkey: Rmonkey. However, the last updates were made 5 years ago, an eternity in the software world, so it may or may not still function as intended.\nThere are also commercial options available. For example, cdata have a driver and R package that enable access to an analysis of Survey Monkey data through R.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Survey Workflow</span>"
    ]
  },
  {
    "objectID": "session_04.html#resourcese",
    "href": "session_04.html#resourcese",
    "title": "4  Reproducible Survey Workflow",
    "section": "4.4 Resourcese",
    "text": "4.4 Resourcese\n\nHow to get your Qualtrics API key",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Survey Workflow</span>"
    ]
  },
  {
    "objectID": "session_08.html",
    "href": "session_08.html",
    "title": "8  Science Communication: Message Box",
    "section": "",
    "text": "Learning Objective",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#learning-objective",
    "href": "session_08.html#learning-objective",
    "title": "8  Science Communication: Message Box",
    "section": "",
    "text": "Discuss about the importance of science communication.\nDistinguish between how scientist communicate sciences vs how the rest of the world communicates.\nIntroduce and practice using the Message Box as a tool to communicate science to a specific audience.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#communicating-science",
    "href": "session_08.html#communicating-science",
    "title": "8  Science Communication: Message Box",
    "section": "8.1 Communicating Science",
    "text": "8.1 Communicating Science\nHow we communicate our science with the rest of the world matters! There are many different reasons and ways to engage and practice science communication. From writing a blog post as a creative outlet to posting in social media to make science accessible to a broader audience. Science communication is more and more becoming a formal responsibility within researchers as funding organizations encourage or require scientist to do outreach of their work (Borowiec (2023)).\nCommunication can come easy, but doing it well is hard. As scientist, it is important to spread the word of our discoveries, engage with non-scientist audiences, and build empathy and trust on what we do (PLOS SciCom). This way make sure the world understand how important, necessary and meaningful science is.\n\n\n\nJarreau, Paige B (2015): #MySciBlog Interviewee Motivations to Blog about Science\n\n\n\n\n8.1.1 Scholarly publications\n\n“The ingredients of good science are obvious—novelty of research topic, comprehensive coverage of the relevant literature, good data, good analysis including strong statistical support, and a thought-provoking discussion. The ingredients of good science reporting are obvious—good organization, the appropriate use of tables and figures, the right length, writing to the intended audience— do not ignore the obvious” - Bourne 2005\n\nPeer-reviewed publication is the primary mechanism for direct and efficient communication of research findings. Other scholarly communications include abstracts, technical reports, books and book chapters. These communications are largely directed towards students and peers; individuals learning about or engaged in the process of scientific research whether in a university, non-profit, agency, commercial or other setting.\nThe following tabling is a good summary of ‘10 Simple Rules’ for writing research papers (adapted from Zhang 2014, published in Budden and Michener, 2017)\n\n\n\n\n\n\n\n\nMake it a Driving Force\n\n“Design a project with an ultimate paper firmly in mind”\n\n\n\nLess Is More\n\n“Fewer but more significant papers serve both the research community and one’s career better than more papers of less significance”\n\n\n\nPick the Right Audience\n\n“This is critical for determining the organization of the paper and the level of detail of the story, so as to write the paper with the audience in mind.”\n\n\n\nBe Logical\n\n“The foundation of ‘’lively’’ writing for smooth reading is a sound and clear logic underlying the story of the paper.” “An effective tactic to help develop a sound logical flow is to imaginatively create a set of figures and tables, which will ultimately be developed from experimental results, and order them in a logical way based on the information flow through the experiments.”\n\n\n\nBe Thorough and Make It Complete\n\nPresent the central underlying hypotheses; interpret the insights gleaned from figures and tables and discuss their implications; provide sufficient context so the paper is self-contained; provide explicit results so readers do not need to perform their own calculations; and include self-contained figures and tables that are described in clear legends\n\n\n\nBe Concise\n\n“the delivery of a message is more rigorous if the writing is precise and concise”\n\n\n\nBe Artistic\n\n“concentrate on spelling, grammar, usage, and a ‘’lively’’ writing style that avoids successions of simple, boring, declarative sentences”\n\n\n\nBe Your Own Judge\n\nReview, revise and reiterate. “…put yourself completely in the shoes of a referee and scrutinize all the pieces—the significance of the work, the logic of the story, the correctness of the results and conclusions, the organization of the paper, and the presentation of the materials.”\n\n\n\nTest the Water in Your Own Backyard\n\n“…collect feedback and critiques from others, e.g., colleagues and collaborators.”\n\n\n\nBuild a Virtual Team of Collaborators\n\nTreat reviewers as collaborators and respond objectively to their criticisms and recommendations. This may entail redoing research and thoroughly re-writing a paper.\n\n\n\nEven though reporting our results in a clear and efficient way through scholarly publications is important, the ways scientist communicate is not how the rest of the rest of the world typically communicates. In a scientific paper, we establish credibility in the introduction and methods, provide detailed data and results, and then share the significance of our work in the discussion and conclusions. But the rest of the world leads with the impact, the take home message. An example of this are newspaper headlines.\n“But the rest of the world leads with the conclusions, because that’s what people want to know. What are your findings and why is this relevant to them? In other words, what’s your bottom line?”\n\n\n\n8.1.2 Other communications\nCommunicating your research outside of peer-reviewed journal articles is increasingly common, and important. These non academic communications can reach a more broad and diverse audience than traditional publications (and should be tailored for specific audience groups accordingly), are not subject to the same pay-walls as journal articles, and can augment and amplify scholarly publications. For example, this twitter announcement from Dr Heather Kropp, providing a synopsis of their synthesis publication in Environmental Research (note the reference to a repository where the data are located, though a DOI would be better).\n\nWhether this communication occurs through blogs, social media, or via interviews with others, developing practices to refine your messaging is critical for successful communication. One tool to support your communication practice is ‘The Message Box’ developed by COMPASS, an organization that helps scientists develop communications skills in order to share their knowledge and research across broad audiences without compromising the accuracy of their research.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#the-message-box",
    "href": "session_08.html#the-message-box",
    "title": "8  Science Communication: Message Box",
    "section": "8.2 The Message Box",
    "text": "8.2 The Message Box\n\n\nThe Message Box is a tool that helps researchers take the information they hold about their research and communicate it in a way that resonates with the chosen audience. It can be used to help prepare for interviews with journalists or employers, plan for a presentation, outline a paper or lecture, prepare a grant proposal, or clearly, and with relevancy, communicate your work to others. While the message box can be used in all these ways, you must first identify the audience for your communication.\n“Whether a journalist, a policymaker, a room of colleagues at a professional meeting, or a class of second-graders—doesn’t have deep knowledge of your subject matter. But that doesn’t mean you should explain everything you know in a fire hose of information. In fact, cognitive research tells us that the human brain can only absorb three to five pieces of information at a time. So prioritize what you share based on what your audience needs to know. Your goal as you fill out your Message Box is to identify the information that is critical to your audience—what really matters to them—and share that. Effective science communication is less about expounding on all the exciting details that you might want to convey, and more about knowing your audience and providing them with what they need or want to know from you.”\nThe Message Box comprises five sections to help you sort and distill your knowledge in a way that will resonate with your (chosen) audience.\nThe five sections of the Message Box are provided below. For a detailed explanation of the sections and guidance on how to use the Message Box, work through the Message Box Workbook\n\n8.2.1 Message Box Sections\nThe Issue\nThe Issue section in the center of the box identifies and describes the overarching issue or topic that you’re addressing in broad terms. It’s the big-picture context of your work. This should be very concise and clear; no more than a short phrase. You might find you revisit the Issue after you’ve filled out your Message Box, to see if your thinking on the overarching topic has changed since you started.\n\nDescribes the overarching issue or topic: Big Picture\nBroad enough to cover key points\nSpecific enough to set up what’s to come\nConcise and clear\n‘Frames’ the rest of the message box\n\nThe Problem\nThe Problem is the chunk of the broader issue that you’re addressing in your area of expertise. It’s your piece of the pie, reflecting your work and expert knowledge. Think about your research questions and what aspect of the specific problem you’re addressing would matter to your audience. The Problem is also where you set up the So What and describe the situation you see and want to address.\n\nThe part of the broader issue that your work is addressing\nBuilds upon your work and expert knowledge\nTry to focus on one problem per audience\nOften the Problem is your research question\nThis section sets you up for So What\n\nThe So What\nThe crux of the Message Box, and the critical question the COMPASS team seeks to help scientists answer, is “So what?” Why should your audience care? What about your research or work is important for them to know? Why are you talking to them about it? The answer to this question may change from audience to audience, and you’ll want to be able to adjust based on their interests and needs. We like to use the analogy of putting a message through a prism that clarifies the importance to different audiences. Each audience will be interested in different facets of your work, and you want your message to reflect their interests and accommodate their needs. The prism below includes a spectrum of audiences you might want to reach, and some of the questions they might have about your work.\n\nThis is the crux of the message box\nWhy should you audience care?\nWhat about your research is important for them to know?\nWhy are you talking to them about it?\n\n\nThe Solution\nThe Solution section outlines the options for solving the problem you identified. When presenting possible solutions, consider whether they are something your audience can influence or act upon. And remind yourself of your communication goals: Why are you communicating with this audience? What do you want to accomplish?\n\nOutlines the options for solving the Problem\nCan your audience influence or act upon this?\nThere may be multiple solutions\nMake sure your Solution relates back to the Problem. Edit one or both as needed\n\nThe Benefit\nIn the Benefit section, you list the benefits of addressing the Problem — all the good things that could happen if your Solution section is implemented. This ties into the So What of why your audience cares, but focuses on the positive results of taking action (the So What may be a negative thing — for example, inaction could lead to consequences that your audience cares about). If possible, it can be helpful to be specific here — concrete examples are more compelling than abstract. Who is likely to benefit, and where, and when?\n\nWhat are the benefits of addressing the Problem?\nWhat good things come from implementing your Solution?\nMake sure it connects with your So What\nBenefits and So What may be similar\n\nFinally, to make you message more memorable you should:\n\nSupport your message with data\nLimit the use of numbers and statistics\nUse specific examples\nCompare numbers to concepts, help people relate\nAvoid jargon\nLead with what you know\n\n\nIn addition to the Message Box Workbook, COMPASS have resources on how to increase the impact of your message (include important statistics, draw comparisons, reduce jargon, use examples), exercises for practicing and refining your message and published examples.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#resources",
    "href": "session_08.html#resources",
    "title": "8  Science Communication: Message Box",
    "section": "8.3 Resources",
    "text": "8.3 Resources\n\n\n\nDataONE Webinar: Communication Strategies to Increase Your Impact from DataONE on Vimeo.\n\n\nBudden, AE and Michener, W (2017) Communicating and Disseminating Research Findings. In: Ecological Informatics, 3rd Edition. Recknagel, F, Michener, W (eds.) Springer-Verlag\nCOMPASS Core Principles of Science Communication\nExample Message Boxes",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#your-turn",
    "href": "session_08.html#your-turn",
    "title": "8  Science Communication: Message Box",
    "section": "8.4 Your Turn",
    "text": "8.4 Your Turn\nLet’s take a look on how the Message Box looks in practice.\n\n\n\n\n\n\nExercise\n\n\n\n\nLook into real examples of scienctist using the message box here.\nThink about your synthesis project and a potential audience you would like to communicate the results. Define your audience and start filling in the different components of the Message Box.\n\nNote: This is just the first iteration to help your think about your work in a different way.\n\n\n\n\n\n\nBorowiec, Brittney G. 2023. “Ten Simple Rules for Scientists Engaging in Science Communication.” PLOS Computational Biology 19 (7): 1–8. https://doi.org/10.1371/journal.pcbi.1011251.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: Message Box</span>"
    ]
  },
  {
    "objectID": "session_09.html",
    "href": "session_09.html",
    "title": "9  Publishing to the Web",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Publishing to the Web</span>"
    ]
  },
  {
    "objectID": "session_09.html#learning-objectives",
    "href": "session_09.html#learning-objectives",
    "title": "9  Publishing to the Web",
    "section": "",
    "text": "How to use Git, GitHub (+Pages), and Quarto to publish an analysis to the web",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Publishing to the Web</span>"
    ]
  },
  {
    "objectID": "session_09.html#introduction",
    "href": "session_09.html#introduction",
    "title": "9  Publishing to the Web",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nSharing your work with others in engaging ways is an important part of the scientific process.\nSo far in this course, we’ve introduced a small set of powerful tools for doing open science:\n\nR and its many packages\nRStudio\nGit\nGitHub\nQuarto\n\nQuarto, in particular, is amazingly powerful for creating scientific reports but, so far, we haven’t tapped its full potential for sharing our work with others.\nIn this lesson, we’re going to take our training_{USERNAME} GitHub repository and turn it into a beautiful and easy to read web page using the tools listed above.\n\n\n\n\n\n\nSet up\n\n\n\n\nMake sure you are in training_{USERNAME} project\nAdd a new Quarto file at the top level called index.qmd\n\nGo to the RStudio menu File -&gt; New File -&gt; Quarto Document\nThis will bring up a dialog box. Add the title “GitHub Pages Example”, keep the Default Output Format as “HTML”, and then click “OK”\n\nSave the Quarto Document you just created. Use index.qmd as the file name\n\nBe sure to use the exact case (lower case “index”) as different operating systems handle case differently and it can interfere with loading your web page later\n\nPress “Render” and observe the rendered output\n\nNotice the new file in the same directory index.html\nThis is our Quarto file rendered as HTML (a web page)\n\nCommit your changes (for both index.qmd and index.html) with a commit message, and push to GitHub\nOpen your web browser to the github.com and navigate to the page for your training_{USERNAME} repository\nActivate GitHub Pages for the main branch\n\nGo to Settings -&gt; Pages (underneath the Code and Automation section)\nKeep the “Source” as “Deploy from a branch”\nUnder “Branch” you’ll see a message that says “GitHub Pages is currently disabled”. To change this, change the branch from “None” to main. Keep the folder as the root and then click “Save”\nYou should see the message change to “Your GitHub Pages site is currently being built from the main branch”\n\n\nNote: index.qmd represents the default file for a web site, and is returned whenever you visit the web site but doesn’t specify an explicit file to be returned.\n\n\nNow, the rendered website version of your repo will show up at a special URL.\nGitHub Pages follows a convention like this:\n\nNote that it changes from github.com to github.io\n\nGo to https://{username}.github.io/{repo_name}/ (Note the trailing /)\nObserve the awesome rendered output\n\nNow that we’ve successfully published a web page from an Quarto Document, let’s make a change to our Quarto Document and follow the steps to publish the change on the web:\n\n\n\n\n\n\nUpdate content in your published page\n\n\n\n\nGo back to your index.qmd\nDelete all the content, except the YAML frontmatter\nType “Hello world”\nUse Git workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\nGo back to https://{username}.github.io/{repo_name}/\n\n\n\nNext, we will show how you can link different qmd’s rendered into html so you can easily share different parts of your work.\n\n\n\n\n\n\nExercise\n\n\n\nIn this exercise, you’ll create a table of contents with the lessons of this course on the main page, and link some of the files we have work on so far.\n\nGo back to the RStudio server and to your index.qmd file\nCreate a table of contents with the names of the main technical lessons of this course, like so:\n\n## Course Lessons\n\n- Introduction to Literate Analysis Using Quarto \n- Cleaning and Wrangling data\n- Data Visualization\n- Functions and packages\n\n## Course Practice Session\n\n- Practice I: Cleaning and Wrnagling and Data Viz\n- Practice II: Writing functions\n\n\nMake sure you have the html versions of your intro-to-qmd.qmd and data-cleaning.qmd files. If you only see the qmd version, you need to “Render” your files first\nIn your index.qmd let’s add the links to the html files we want to show on our webpage. Do you remember the Markdown syntax to create a link?\n\n\n\nMarkdown syntax to create a link:\n\n\n[Text you want to hyperlink](link)\n\nExample: [Data wrangling and cleaning](data-wrangling-cleaning.html)\n\n\n\n\nUse Git workflow: Stage (add) -&gt; Commit -&gt; Pull -&gt; Push\n\nNow when you visit your web site, you’ll see the table of contents, and can navigate to the others file you linked.\n\n\nQuarto web pages are a great way to share work in progress with your colleagues. Here we showed an example with the materials we have created in this course. However, you can use these same steps to share the different files and progress of a project you’ve been working on. To do so simply requires thinking through your presentation so that it highlights the workflow to be reviewed. You can include multiple pages and build a simple web site and make your work accessible to people who aren’t set up to open your project in R. Your site could look something like this:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Publishing to the Web</span>"
    ]
  },
  {
    "objectID": "session_11.html",
    "href": "session_11.html",
    "title": "11  GitHub for Project Management",
    "section": "",
    "text": "&lt;!–{{&lt; include /sections/visualization-ggplot-leaflet.qmd &gt;}}–&lt;",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>GitHub for Project Management</span>"
    ]
  },
  {
    "objectID": "session_12.html",
    "href": "session_12.html",
    "title": "12  Reproducibility and Provenance",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reproducibility and Provenance</span>"
    ]
  },
  {
    "objectID": "session_12.html#learning-objectives",
    "href": "session_12.html#learning-objectives",
    "title": "12  Reproducibility and Provenance",
    "section": "",
    "text": "Discuss the concept of reproducible workflows including computational reproducibility and provenance metadata\nLearn how to use R to package your work by building a reproducible paper in RMarkdown/Quarto\nIntroduce tools and techniques for reproducibility supported by the NCEAS and DataONE\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn this lesson, we will be leveraging RMarkdown instead of Quarto so that we can use a very cool R package called rticles. Quarto has the same functionality as RMarkdown with rticles - making journal formatted articles from a code notebook - but it is done from the command line without additional R packages. See the Quarto documentation for details\n\n\n\n12.0.1 Reproducible Research: Recap\nWorking in a reproducible manner:\n\nIncreases research efficiency, accelerating the pace of your research and collaborations.\nProvides transparency by capturing and communicating scientific workflows.\nEnables research to stand on the shoulders of giants (build on work that came before).\nAllows credit for secondary usage and supports easy attribution.\nIncreases trust in science.\n\nTo enable others to fully interpret, reproduce or build upon our research, we need to provide more comprehensive information than is typically included in a figure or publication. The methods sections of papers are typically inadequate to fully reproduce the work described in the paper.\n\nFor example, if we look at the figure above convey multiple messages. But, by looking at the figure we don’t get the full story how did scientist got to make this plot. What data were used in this study? What methods applied? What were the parameter settings? What documentation or code are available to us to evaluate the results? Can we trust these data and methods? Are the results reproducible?\nComputational reproducibility is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions.\nPractically speaking, reproducibility includes:\n\nPreserving the data\nPreserving the software workflow\nDocumenting what you did\nDescribing how to interpret it all\n\nA recent study of publicly-available datasets in the Harvard Database repository containing R files found that only 26% of R files ran without error in the initial execution. 44% were able to be run after code cleaning, showing the importance of good programming practice (Trisovic et al. 2022). The figure below from Trisovic et al. shows a sankey diagram of how code cleaning was able to fix common errors.\n\n\n\n12.0.2 Computational Provenance and Workflows\nComputational provenance refers to the origin and processing history of data including:\n\nInput data\nWorkflow/scripts\nOutput data\nFigures\nMethods, dataflow, and dependencies\n\nWhen we put these all together with formal documentation, we create a computational workflow that captures all of the steps from initial data cleaning and integration, through analysis, modeling, and visualization. In other words, computational provenance is a formalized description of a workflow from the origin of the data to it’s final outcome.\nHere’s an example of a computational workflow from Mark Carls: Mark Carls. Analysis of hydrocarbons following the Exxon Valdez oil spill, Gulf of Alaska, 1989 - 2014. Gulf of Alaska Data Portal. urn:uuid:3249ada0-afe3-4dd6-875e-0f7928a4c171., that represents a three step workflow comprising four source data files and two output visualizations.\n\n\nThis image is a screenshot of an interactive user interface of a workflow built by DataONE. You can clearly see which data files were inputs to the process, the scripts that are used to process and visualize the data, and the final output objects that are produced, in this case two graphical maps of Prince William Sound in Alaska.\n\n\n12.0.3 From Provenance to Reproducibility\n\nDataONE provides a tool to track and visualize provenance. It facilitates reproducible science through provenance by:\n\nTracking data derivation history\nTracking data inputs and outputs of analyses\nPreserving and documenting software workflows\nTracking analysis and model executions\nLinking all of these to publications\n\n\nOne way to illustrate this is to look into the structure of a data package. A data package is the unit of publication of your data, including datasets, metadata, software and provenance. The image below represents a data package and all it’s components and how these components relate to each other.\n\n\n\n\n12.0.4 Data Citation and Transitive Credit\nWe want to move towards a model such that when a user cites a research publication we will also know:\n\nWhich data produced it\nWhat software produced it\nWhat was derived from it\nWho to credit down the attribution stack\n\n\nThis is transitive credit. And it changes the way in which we think about science communication and traditional publications.\n\n\n12.0.5 Reproducible Papers with rrtools\nA great overview of this approach to reproducible papers comes from:\n\nBen Marwick, Carl Boettiger & Lincoln Mullen (2018) Packaging Data Analytical Work Reproducibly Using R (and Friends), The American Statistician, 72:1, 80-88, doi:10.1080/00031305.2017.1375986\n\nThe key idea in Marwick et al. (2018) is that of the research compendium: A single container for not just the journal article associated with your research but also the underlying analysis, data, and even the required software environment required to reproduce your work.\nResearch compendium makes it easy for researchers to do their work but also for others to inspect or even reproduce the work because all necessary materials are readily at hand due to being kept in one place. Rather than a constrained set of rules, the research compendium is a scaffold upon which to conduct reproducible research using open science tools such as:\n\nR\nRMarkdown\nQuarto\ngit and GitHub\n\nFortunately for us, Ben Marwick (and others) have written an R package called rrtools that helps us create a research compendium from scratch.\n\n\n\n\n\n\nSet up\n\n\n\nTo start a reproducible paper with rrtools:\n\nClose your username-training project. Go to the project switcher dropdown, just click “close project.” This will set your working directory back to your home directory.\nIn console run the following line of code\n\n\n## \"mypaper\" is the name of the Rproj with my research compendia\nrrtools::use_compendium(\"mypaper\")\n\nrrtools has created the beginnings of a research compendium for us. The structure of this compendium is similar to the one needed to built an R package. That’s because it uses the same underlying folder structure and metadata and therefore it technically is an R package (called mypaper). And this means our research compendium could be easy to install in someone elses’ computer, similar to an R package.\n\nrrtools also helps you set up some key information like:\n\n\nSet up a README file in the RMarkdown format\nCreate an analysis folder to hold our reproducible paper\n\n\nrrtools::use_readme_rmd()\nrrtools::use_analysis()\n\nThis creates a standard, predictable layout for our code and data and outputs that multiple people can understand. At this point, we’re technically ready to start writing the paper. But.. What about GitHub?\n\n\n\n12.0.5.1 Creating a git and GitHub repository with usethis\n\nusethis is a package that facilitates interactive workflows for R project creation and development. It automates repetitive tasks that arise during project setup and development.\n\nWe are going to use two functions to start tracking our work in git, create a remote repository in GitHub and be able to push and pull between the local version and the remote. To learn more about this package checkout the package documentation.\n\n\n\n\n\n\nSet up\n\n\n\n\nMake sure your are in “mypaper” Rproj.\nIn the Console run usethis::use_git() to create a local git repo. Choose yes to both questions when prompted (to commit files, and to restart R).\nThen, in the Console, run usethis::use_github() to create an upstream remote repo (in GitHub).\n\nAnd that’s it! Now your have your research compendium in your local computer and your changes are being tracked by git and your can pull and push to GitHub.\n\n\nLet’s explore the structure rrtools has put in place for us. Inside the analysis folder we have 5 folders. Different parts of our project will go into this different folders. Our data into the data folder, when the time comes to save any figure, we should save them into the figures folder, and so on.\n\n\n\nResearch compendia from Marwick et al.\n\n\nYou’ll notice a analysis/templates directory that contains journal citation style language (CSL) files which set the style of citations and reference list for the journal (the Journal of Archaeological Science, in this example). The template.Rmd renders into the template.docx. This document is called in the paper.qmd YAML to style the output of the paper created in paper.qmd.\nWhat if I want a template from another journal, different from the Journal of Archeological Science? We can create other journal’s template with the rticles package. This package will provide the templates and necessary information to render your paper in the journal of your choice (note: not all journal are in the rticles package). With that in mind, we will delete the existing paper directory and create a new one shortly.\n\n\n\n12.0.6 RMarkdown templates with rticles\nThe rticles package provides a lot of other great templates for formatting your paper specifically to the requirements of many journals. In addition to a custom CSL file for reference customization, rticles supports custom LATeX templates that fit the formatting requirements of each journals.\n\n\n\n\n\n\nTinytex and rendering to PDF\n\n\n\nTo be able to render your document to PDF you need to have tinytex installed in your machine.\nIn the console run:\n\ninstall.packages('tinytex') ## this package is already installed in our server\n\ntinytex::install_tinytex() ## this may take several minutes\n\n\n\n\n\n\n\n\n\nSet up\n\n\n\n\nIf you do not have rticle installed, go aherad and inatall calling the following function in the console: install.packages('rticles') Restart your RStudio session\nTo create a new file from rticlescustom templates, got to File | New File | R Markdown... menu, which shows the following dialog:\n\n\n\nGo to “From Template” in the left side menu.\nSelect the “PNAS” template, give the file a name and set the location of the files to be mypaper/analysis, and click “OK”.\nYou can now Knit the Rmd file to see a highly-targeted article format, like this one for PNAS:\n\n\n\n\n\n\n12.0.7 Workflow in a nutshell\n\n\n\n\n\n\nSummary\n\n\n\n\nUse rrtools to generate the core directory layout and approach to data handling.\nThen use rticles to create the structure of the paper itself. The combination is incredibly flexible.\n\n\n\nThings we can do with our research compendium:\n\nEdit ./analysis/paper/paper.Rmd to begin writing your paper and your analysis in the same document\nAdd any citations to ./analysis/paper/pnas-sample.bib\nAdd any longer R scripts that don’t fit in your paper in an R folder at the top level\nAdd raw data to ./data/raw_data\nWrite out any derived data (generated in paper.Rmd) to ./data/derived_data\nWrite out any figures in ./analysis/figures\n\nYou can then write all of your R code in your RMarkdown/Quarto, and generate your manuscript all in the format needed for your journal (using it’s .csl file, stored in the paper directory).\n\n\n12.0.8 Adding renv to conserve your environment\n\nrrtools has a couple more tricks up it’s sleeve to help your compendium be as reproducible and portable as possible.\nTo capture the R packages and versions this project depends on, we can use the renv package.\nRunning renv::init(), will initiate tracking of the R packages in your project.\nThis action will create a new folder called renv in your top directory.\nrenv::init() automatically detects dependencies in your code (by looking for library calls, at the DESCRIPTION file, etc.) and installs them to a private project specific library. This means that your project mypaper can use a different version of dplyr than another project which may need an older version without any hassle.\nrenv also write the package dependencies to a special file in the repository called renv.lock.\nIf any of your packages you are using is updated, while your are working on your project, you can run renv::snapshot() to update the renv.lock file and your project-installed packages.\nYou can read the renv.lock file using renv::restore(), when needed. This will install the versions of the packages needed.\n\n\n\n12.0.9 Conserve your computational environement with Docker\n\nThe rrtools package then uses this renv.lock file to build what is called a Dockerfile.\nDocker allows you to build containers, a standard unit of software that packages up code and all its dependencies so an application runs quickly and reliably from one computing environment to another.\nA container is an “image” of all the software specified, and this image can be run on other computers such that the software stack looks exactly as you specify.\nThis is important when it comes to reproducibility, because when running someone else code, you may get different results or errors if you are using different versions of software (like an old version of dplyr).\nA Dockerfile contains the instructions for how to recreate the computational environment where your analysis was run.\n\nIn practice\n\nOnce you have your research compendium, you can called rrtools::use_dockerfile(). If needed, re-install rrtools directly from GitHub remotes::install_github(\"benmarwick/rrtools\")\nThis, first creates a Dockerfile that loads a standard image for using R with the tidyverse,\nAnd then has more instructions for how to create the environment so that it has the very specific R packages and versions you need.\nIf we look at the Dockerfile (example below), it calls to renv::restore(), as described above.\nThe last line of the docker file renders our Quarto/RMarkdown reproducible paper!\n\n# get the base image, the rocker/verse has R, RStudio and pandoc\nFROM rocker/verse:4.2.2\n\n# required\nMAINTAINER Your Name &lt;your_email@somewhere.com&gt;\n\nCOPY . /&lt;REPO&gt;\n\n# go into the repo directory\nRUN . /etc/environment \\\n  # Install linux depedendencies here\n  # e.g. need this for ggforce::geom_sina\n  && sudo apt-get update \\\n  && sudo apt-get install libudunits2-dev -y \\\n  # build this compendium package\n  && R -e \"install.packages('remotes', repos = c(CRAN = 'https://cloud.r-project.org'))\" \\\n  && R -e \"remotes::install_github(c('rstudio/renv', 'quarto-dev/quarto-r'))\" \\\n  # install pkgs we need\n  && R -e \"renv::restore()\" \\\n  # render the manuscript into a docx, you'll need to edit this if you've\n  # customised the location and name of your main qmd file\n  && R -e \"quarto::quarto_render('/&lt;REPO&gt;/analysis/paper/paper.qmd')\"\n\nAfter running rrtools::use_dockerfile(), the package also sets up GitHub Actions for you.\nActions are processes that are triggered in GitHub events (like a push) and run automatically.\nIn this case, the Action that is set up will build your Docker image on GitHub.\nThis means that the code that knits your paper is run, and an updated version of your paper is knit.\nThis is called continuous integration, and is extremely convenient for developing products like this, since the build step can be taken care of automatically as you push to your repository.\n\n\n\n\n12.0.10 The 5th Generation of Reproducible Papers\n\nWhole Tale is a project that aims to simplify computational reproducibility. It enables researchers to easily package and share ‘tales’. Tales are executable research objects captured in a standards-based tale format complete with metadata. They can contain:\n\nData (references)\nCode (computational methods)\nNarrative (traditional science story)\nCompute environment (e.g. RStudio, Jupyter)\n\n\nBy combining data, code and the compute environment, tales allow researchers to:\n\nRe-create the computational results from a scientific study\nAchieve computational reproducibility\n“Set the default to reproducible.”\n\nFull circle reproducibility can be achieved by publishing data, code AND the computational environment.\n\n\n12.0.11 Resources\n\nrrtools documentation\nThe rticles\nusethis documentation\n\n\n\n\n\nTrisovic, Ana, Matthew K. Lau, Thomas Pasquier, and Mercè Crosas. 2022. “A Large-Scale Study on Research Code Quality and Execution.” Scientific Data 9 (1). https://doi.org/10.1038/s41597-022-01143-6.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reproducibility and Provenance</span>"
    ]
  }
]