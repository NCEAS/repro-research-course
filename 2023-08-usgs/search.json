[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "",
    "text": "Overview"
  },
  {
    "objectID": "index.html#about-this-training",
    "href": "index.html#about-this-training",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "About this training",
    "text": "About this training\nThe NCEAS openS Program consists of three 1-week long workshops, geared towards early career researchers. Participants engage in a mix of lectures, exercises, and synthesis research activities to conduct synthesis science and implement best practices in open data science.\nopenS has been adapted to coincide with each USGS CAP training, for a total of four training sessions across 2 years."
  },
  {
    "objectID": "index.html#why-nceas",
    "href": "index.html#why-nceas",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "Why NCEAS",
    "text": "Why NCEAS\nThe National Center for Ecological Analysis and Synthesis (NCEAS), a research affiliate of the University of California, Santa Barbara, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science.\nNCEAS has over 25 years of success with this model among working groups and environmental professionals, and we are excited to pass on this cumulative expertise to you."
  },
  {
    "objectID": "index.html#week-one-successful-synthesis-and-data-management-best-practices",
    "href": "index.html#week-one-successful-synthesis-and-data-management-best-practices",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "Week One: Successful Synthesis and Data Management Best Practices",
    "text": "Week One: Successful Synthesis and Data Management Best Practices\n\nLearning Objectives:\n\nImplement reproducible scientific workflows throughout all aspects of a project\nIncrease your familiarity and confidence with data science tools\nEffectively manage and wrangle data using tidy data practices\nAccessing, interpreting and developing metadata for synthesis research\nOrganize and initiate synthesis projects\n\n\n\nSchedule\n\n\n\n\n\n\n\n\n\nDate\nTime\nSession Title\nLead Facilitator\n\n\n\n\nMonday 08/21\nBlock 3\n1:45PM-3:15PM\nSuccessful Synthesis Science\nHalina Do-Linh, Data Training Program Manager\n\n\nTuesday 08/22\nBlock 2\n10:45AM-12:15PM\nPanel for PIs: Successful Mentorship in Synthesis Science\nHalina Do-Linh, Data Training Program Manager\n\n\nWednesday 08/23\nBlock 1\n9AM-10:30AM\nData Management Essentials\nCamila Vargas Poulsen, Data Training Coordinator"
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS with the support of DataONE, to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors for this version are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Halina Do-Linh, Camila Vargas Poulsen. 2023. openS for USGS Climate Adaptation Postdoctoral (CAP) Fellows Program. Week One. NCEAS Learning Hub & USGS Climate Adaptation Science Centers (CASCs).\nAdditional contributors: Ben Bolker, Julien Brun, Amber E. Budden, Jeanette Clark, Samantha Csik, Carmen Galaz García, Stephanie Hampton, Natasha Haycock-Chavez, Matthew B. Jones, Samanta Katz, Julie Lowndes, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "session_01.html#learning-objectives",
    "href": "session_01.html#learning-objectives",
    "title": "1  Successful Synthesis Science",
    "section": "Learning Objectives",
    "text": "Learning Objectives"
  },
  {
    "objectID": "session_01.html#what-is-synthesis-science",
    "href": "session_01.html#what-is-synthesis-science",
    "title": "1  Successful Synthesis Science",
    "section": "1.1 What is Synthesis Science?",
    "text": "1.1 What is Synthesis Science?\nSynthesis science is the process of distilling existing data, ideas, theories, or methods drawn from many sources, across multiple fields of inquiry, to accelerate the generation of new scientific knowledge, often at a broad scale.\nResearch in ecological and environmental sciences is often inherently complex and well-suited to synthesis approaches. It is a key approach for understanding complexity across scales, leveraging data from various disciplines, facilitating discovery of general patterns in natural systems, and informing policy.\nIn synthesis research, it is also vital to consider and increase diversity of participants and to foster inclusive research environments. When diveristy and inclusion is not prioritized, syntheses risk being biased and less broadly applicable, which may lead to unjust and inequitable outcomes. Although the principles of diversity and inclusion are relevant to any scientific endeavor, they are particularly salient for synthesis given its integrative nature.\n\n1.1.1 Conceptual Synthesis\n\n\n1.1.2 Meta-Analysis Synthesis\n\n\n1.1.3 Data set Synthesis"
  },
  {
    "objectID": "session_01.html#nceas-and-synthesis-science",
    "href": "session_01.html#nceas-and-synthesis-science",
    "title": "1  Successful Synthesis Science",
    "section": "1.2 NCEAS and Synthesis Science",
    "text": "1.2 NCEAS and Synthesis Science"
  },
  {
    "objectID": "session_01.html#nceas-case-studies",
    "href": "session_01.html#nceas-case-studies",
    "title": "1  Successful Synthesis Science",
    "section": "1.3 NCEAS Case Studies",
    "text": "1.3 NCEAS Case Studies\n\n1.3.1 OHI\n\n\n1.3.2 SASAP"
  },
  {
    "objectID": "session_01.html#future-of-synthesis",
    "href": "session_01.html#future-of-synthesis",
    "title": "1  Successful Synthesis Science",
    "section": "1.4 Future of Synthesis",
    "text": "1.4 Future of Synthesis\nIn February 2021, NCEAS hosted a virtual workshop where 127 ecologists, environmental and natural scientists convened to discuss the future priorities of synthesis science. Participants across career stages, institutions, backgrounds, and geographies were in attendance.\nParticipants proposed ideas or questions in pre-workshop brainstorming sessions; added and upvoted questions online; and worked in breakout teams during the workshop to refine upvoted questions into lists of top three questions. These final lists were then grouped into themes by the 12-person steering committee and discussed at length by the workshop participants.\nIn January 2023, over dozens of scientists authored and published Priorities for synthesis research in ecology and environmental science the results of the workshop.\nSeven priority research topics emerged: (1) diversity, equity, inclusion, and justice (DEIJ), (2) human and natural systems, (3) actionable and use-inspired science, (4) scale, (5) generality, (6) complexity and resilience, and (7) predictability. Additionally, two issues regarding the general practice of synthesis emerged: the need for increased participant diversity and inclusive research practices; and increased and improved data flow, access, and skill-building."
  },
  {
    "objectID": "session_01.html#best-practices",
    "href": "session_01.html#best-practices",
    "title": "1  Successful Synthesis Science",
    "section": "1.5 Best Practices?",
    "text": "1.5 Best Practices?"
  },
  {
    "objectID": "session_02.html#learning-objectives",
    "href": "session_02.html#learning-objectives",
    "title": "2  Logic Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nProvide an overview of Logic Models\nApply the principles of Logic Models to synthesis development\nRefine synthesis group challenges"
  },
  {
    "objectID": "session_02.html#logic-models",
    "href": "session_02.html#logic-models",
    "title": "2  Logic Models",
    "section": "2.1 Logic Models",
    "text": "2.1 Logic Models\nLogic models are a planning tool that are designed to support program development by depicting the flow of resources and processes leading to a desired result. They are also used for outcomes-based evaluation of a program and are often requested as part of an evaluation planning process by funders or stakeholders.\nA simplified logic models comprise three main parts: Inputs, Outputs and Outcomes.\n\nInputs reflect what is invested, outputs are what is done and outcomes are the results of the program.\nIn a more detailed logic model, outputs and outcomes are further broken down. Outputs are often represented as ‘Activities’ and ‘Participants’. By including participation (or participants), the logic model is explicitly considering the intended audience, or stakeholders, impacted by the program. Engagement of this audience is an output. In the case of outcomes, these can be split into short, medium and long-term outcomes. Sometimes this last category may be labeled ‘Impact’\n\nDefining the inputs, outputs and outcomes early in a planning process enables teams to visualize the workflow from activity to results and can help mitigate potential challenges. Logic models can be thought of as having an ‘if this then that’ structure where inputs -&gt; outputs -&gt; outcomes.\n\nIn the example below we have constructed a simple logic model for a hypothetical project where training materials are being developed for a group of educators to implement at their respective institutions.\n\nLinkages are not always sequential and can be within categories, bi-directional and/or include feedback loops. Detailing this complexity of relationships, or theory of action, can be time consuming but is a valuable part of the thought process for project planning. In exploring all relationships, logic modeling also allows for assessing program feasibility.\n\nThe above graphics include two sections within Outputs - Activities and Participants - and this is quite common. There is variation in logic model templates, including versions with a third type of output - “Products’. Sometimes description of these products is contained within the Activities section - for example, ‘develop curricula’, ‘produce a report’ - however calling these out explicitly is beneficial for teams focused on product development.\nProgram development (and logic modeling) occurs in response to a given ‘Situation’ or need, and exploring this is the first step in modeling. The situation defines the objective, or problem, that the program is designed to solve hence some logic models may omit the left-hand situation column but be framed with Problem and Solution statements. Finally, comprehensive logic modeling takes into consideration assumptions that are made with respect to the resources available, the people involved, or the way the program will work and also recognizes that there are external factors that can impact the program’s success.\n\nIn summary:\nLogic models support program development and evaluation and comprise three primary steps in the workflow:\n\nInputs: Resources, contributions, and investments required for a program;\nOutputs: Activities conducted, participants reached, and products produced; and\nOutcomes: Results or expected changes arising from the program structured as short-, medium- and long-term."
  },
  {
    "objectID": "session_02.html#logic-models-for-synthesis-development",
    "href": "session_02.html#logic-models-for-synthesis-development",
    "title": "2  Logic Models",
    "section": "2.2 Logic models for synthesis development",
    "text": "2.2 Logic models for synthesis development\nLogic models are one tool for program development and have sufficient flexibility for a variety of situations, including planning for a research collaboration. While some logic model categories may feel less relevant (can we scale up to a long-term outcome from a published synthesis?), the process of articulating the research objective, proposed outcome, associated resources and activities has value. Below are examples of questions that a typical logic model (LM) will ask, and how these might be reframed for a research collaboration (RC).\nObjective/Problem Statement\nLM: What is the problem? Why is this a problem? Who does this impact?\nRC: What is the current state of knowledge? What gaps exists in understanding? Why is more information / synthesis important?\nInputs\nLM: What resources are needed for the program? Personnel, money, time, equipment, partnerships ..\nRC: What is needed to undertake the synthesis research? For personnel, think in terms of the roles that are needed - data manager, statistician, writer, editor etc. Consider the time frame. DATA - what data are needed and what already exists?\nOutputs - Activities\nLM: What will be done? Development, design, workshops, conferences, counseling, outreach..\nRC: What activities are needed to conduct the research? This could be high level or it could be broken down into details such as the types of statistical approaches.\nOutputs - Participants\nLM: Who will we reach? Clients, Participants, Customers..\nRC: Who is the target audience? Who will be impacted by this work? Who is positioned to leverage this work?\nOutputs - Products\nLM: What will you create? Publications, websites, media communications …\nRC: What research products are planned / expected? Consider this in relation to the intended audience. Is a peer-reviewed publication, report or white paper most appropriate? How will derived data be handled? Will documentation, workflows, or code be published?\nShort-term Outcomes\nLM: What short-term outcomes are anticipated among participants. These can include changes in awareness, knowledge, skills, attitudes, opinions and intent.\nRC: Will this work represent a significant contribution to current understanding?\nMedium-term Outcomes\nLM: What medium-term outcomes are predicted among participants? These might include changes in behaviors, decision-making and actions.\nRC: Will this work promote increased research activity or open new avenues of inquiry?\nLong-term Outcomes\nLM: What long-term benefits, or impacts, are expected? Changes in social, economic, civic, and environmental conditions?\nRC: Will this work result in local, regional or national policy change? What will be the long-term impact of increased investment in the ecosystem?\n\n\n\n\n\n\nBreakout: Synthesis planning with logic models\n\n\n\nBreakout groups will focus on refining ideas for synthesis topcis using the logic modeling tools described in this section. The goal for this session is to develop one or more high-level logic models that:\n\nSummarize the synthesis challenge\nDefine the inputs needed to approach the synthesis\nDefine the outputs, including activities and products that would would be needed to address the issue\nDefine the short term outcomes and longer-term impacts of the work\n\nOften it is helpful to start with a brainstorming activity to list activities and products that might be used to address the synthesis challenge, then connect those in terms of outcomes and impacts, and then circle back to the resource and data inputs needed to feed the logic model. Thinking of the whole model as a workflow can help conceptualize the dependencies among steps.\nTools:\n\nPowerpoint logic model template\nMermaid flowcharts embedded in Quarto documents\n\n\n\n\n\nflowchart LR\n    INPUTS --&gt; ACTIVITIES --&gt; OUTPUTS --&gt; OUTCOMES/IMPACTS\n\n    Scenario{{Accelerate synthesis via data science training}}\n\n    R1[Instructor] & R2[Classroom space] & R3[Projector] --&gt; B{Data Science Workshop}\n    B --&gt; C(Workshop Curriculum)\n    B --&gt; D(Presentations and Practice)\n    \n    C & D --&gt; E[/Improved Delta management/] & F[/Increased analytic efficiency/]\n\n\n\n\n\nSource\n```{mermaid}\nflowchart LR\n    INPUTS --&gt; ACTIVITIES --&gt; OUTPUTS --&gt; OUTCOMES/IMPACTS\n\n    Scenario{{Accelerate synthesis via data science training}}\n\n    R1[Instructor] & R2[Classroom space] & R3[Projector] --&gt; B{Data Science Workshop}\n    B --&gt; C(Workshop Curriculum)\n    B --&gt; D(Presentations and Practice)\n    \n    C & D --&gt; E[/Improved Delta management/] & F[/Increased analytic efficiency/]\n```"
  },
  {
    "objectID": "session_02.html#resources",
    "href": "session_02.html#resources",
    "title": "2  Logic Models",
    "section": "2.3 Resources",
    "text": "2.3 Resources\n\nLogic model template (ppt) on Sharepoint\n\nSame Logic model template on Google Drive"
  },
  {
    "objectID": "session_03.html#learning-objectives",
    "href": "session_03.html#learning-objectives",
    "title": "3  Data Management Essentials",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nUPDATE\n\nUnderstand the importance of data management for successfully preserving data\nWrite an outline of a data management plan\nLearn about metadata guidelines and best practices for reproducibility\nBecome familiar with environmental data repositories for accessing and publishing data?? (Maybe??)"
  },
  {
    "objectID": "session_03.html#introduction",
    "href": "session_03.html#introduction",
    "title": "3  Data Management Essentials",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nData management is the process of handling, organizing, documenting, and preserving data used in a research project. This is particularly important in synthesis science given the nature of synthesis, which involves combining data and information from multiple sources to answer broader questions, generate knowledge and provide insights into a particular problem or phenomenon."
  },
  {
    "objectID": "session_03.html#benefits-of-managing-your-data",
    "href": "session_03.html#benefits-of-managing-your-data",
    "title": "3  Data Management Essentials",
    "section": "3.2 Benefits of managing your data",
    "text": "3.2 Benefits of managing your data\nSuccessfully managing your data throughout a research project helps ensures its preservation for future use. It also facilitates collaboration within your team, and it helps advance your scientific outcomes.\nFrom a researcher perspective\n\nKeep yourself organized – be able to find your files (data inputs, analytic scripts, outputs at various stages of the analytic process, etc.)\nTrack your science processes for reproducibility – be able to match up your outputs with exact inputs and transformations that produced them\nBetter control versions of data – easily identify versions that can be periodically purged\nQuality control your data more efficiently\nTo avoid data loss (e.g. making backups)\nFormat your data for re-use (by yourself or others)\nBe prepared to document your data for your own recollection, accountability, and re-use (by yourself or others)\nGain credibility and recognition for your science efforts through data sharing!\n\nAdvancement of science\n\nData is a valuable asset – it is expensive and time consuming to collect\nMaximize the effective use and value of data and information assets\nContinually improve the quality including: data accuracy, integrity, integration, timeliness of data capture and presentation, relevance, and usefulness\nEnsure appropriate use of data and information\nFacilitate data sharing\nEnsure sustainability and accessibility in long term for re-use in science"
  },
  {
    "objectID": "session_03.html#the-data-life-cycle",
    "href": "session_03.html#the-data-life-cycle",
    "title": "3  Data Management Essentials",
    "section": "3.3 The Data Life Cycle",
    "text": "3.3 The Data Life Cycle\nThe Data Life Cycle gives you an overview of meaningful steps in a research project. This step-by-step breakdown facilitates successful management and preservation of data throughout a project. Some research activities might use only part of the life cycle. For example, a meta-analysis might focus on the Discover, Integrate, and Analyze steps, while a project focused on primary data collection and analysis might bypass the Discover and Integrate steps.\n\n\n\nSource: DataOne\n\n\nThe first step to working with data is identifying what is the Data Life Cycle is for my project. Using the data life cycle stages, create your own cycle that best fits your project needs.\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\nPlan\nMap out the processes and resources for the entire data life cycle. Start with the project goals (desired outputs, outcomes, and impacts) and work backwards to build a data management plan, supporting data policies, and sustainability plans.\n\n\nCollect\nObservations are made either by hand or with sensors or other instruments, and the data are placed into digital form. You can structure the process of collecting data upfront to better implement data management.\n\n\nAssure\nEmploy quality assurance and quality control procedures that enhance the quality of data (e.g., training participants, routine instrument calibration) and identify potential errors and techniques to address them.\n\n\nDescribe\nDocument data by describing the why, who, what, when, where, and how of the data. Metadata, or data about data, is key to data sharing and reuse, and many tools, such as standards and software, are available to help describe data.\n\n\nPreserve\nPlan to preserve data in the short term to minimize potential losses (e.g., via accidents), and in the long term so that project stakeholders and others can access, interpret, and use the data in the future. Decide what data to preserve, where to preserve it, and what documentation needs to accompany the data.\n\n\nDiscover\nIdentify complementary data sets that can add value to project data. Strategies to help endure the data have a maximum impact include registering the project on a project directory site, depositing data in an open repository, and adding data descriptions to metadata clearing houses.\n\n\nIntegrate\nData from multiple sources are combined into a form that can be readily analyzed. For example, you could combine citizen science project data with other sources of data to enable new analyses and investigations. Successful data integration depends on documentation of the integration process, clearly citing and making accessible the data you are using, and employing good data management practices throughout the Data Life Cycle.\n\n\nAnalyze\nCreate analyses and visualizations to identify patterns, test hypotheses, and illustrate findings. During this process, record your methods, document data processing steps, and ensure your data are reproducible. Learn about these best practices and more.\n\n\n\nMore details on each of these steps best practices at DataONE’s Data Management Skillbuilding Hub)\n\n\n3.3.1 Planning is a key step\nAs you can see there is a lot happening around the Data Life Cycle. No matter how your data life cycle looks like, Plan will always be at the top of the cycle. It is advisable to initiate your data management planning at the beginning of your research process before any data has been collected or discovered.\nIn order to better plan and keep track of all the moving pieces when working with data, a good place to start is creating a Data Management Plan. However, this is not only the starting point. This is a “living” document that should be consulted and updated throughout the project.\nA Data Management Plan (DMP) is a document that describes how you will use your data during a research project, as well as what you will do with your data long after the project ends. DMPs are living documents and should be updated as research plans change to ensure new data management practices are captured (Environmental Data Initiative).\nA well-thought-out plan means you are more likely to:\n\nstay organized\nwork efficiently\ntruly share data\nengage your team\nmeet funder requirements as DMPs are becoming common in the submission process for proposals\n\nA DMP is both a straightforward blueprint for how you manage your data, and provides guidelines for your and your team on policies, access, roles, and more. While it is important to plan, it is equally important to recognize that no plan is perfect, as change is inevitable. To make your DMP as robust as possible review it periodically with your team and adjust as the needs of the project change.\n\n\n3.3.2 How to Plan\n\nPlan early: research shows that over time, information is lost and this is inevitable so it’s important to think about long-term plans for your research at the beginning before you’re deep in your project. And ultimately, you’ll save more time.\nPlan in collaboration: high engagement of your team and other important contributors is not only a benefit to your project, but it also makes your DMP more resilient. When you include diverse expertise and perspectives to the planning stages, you’re more likely to overcome obstacles in the future.\nUtilize existing resources: don’t reinvent the wheel! There are many great DMP resources out there. Consider the article Ten Simple Rules for Creating a Good Data Management Plan (Michener 2015), which has succinct guidelines on what to include in a DMP. Or use an online tool like DMPTool, which provides official DMP templates from funders like NSF, example answers, and allows for collaboration. Note USGS has several templates available within DMPTool.\nMake revising part of the process: Don’t let your DMP collect dust after your initially write it. Make revising the DMP part of your research project and use it as a guide to ensure you’re keeping on track.\nInclude tidy and ethical lens: It is important to start thinking through these lenses during the planning process of your DMP, it will make it easier to include and maintain tidy and ethical principles throughout the entire project. Find information about these topics in the following links to NCEAS Learning Hub materials for a previous course: - Tidy data, and our content on - FAIR principle - Data ethics lens though the CARE principles later this week.\n\n\n\n3.3.3 What to include in a DMP\nTen simple rules to start planning for your plan 1. Determine what are the USGS requirements 2. Identify the desired/necessary data sets for the project 3. Define how the data will be organized 4. Explain how the data will be documented 5. Describe how data quality will be assured 6. Have a data storage strategy 7. Define the project’s data policies 8. What data products will be made available and how? 9. Assign roles and responsibilities 10. Is there a cost associated to managing your data? 11. Data preservation plan\nAdditional information\nProject details - Project title - Abstract - Research domain - Start and end date - Funder\nCollaborators - Name - Contact information -"
  },
  {
    "objectID": "session_03.html#metadata-best-practices",
    "href": "session_03.html#metadata-best-practices",
    "title": "3  Data Management Essentials",
    "section": "3.4 Metadata Best Practices",
    "text": "3.4 Metadata Best Practices\nWithin the data life cycle you can be collecting data (creating new data) or integrating data that has all ready been collected. Either way, metadata plays plays a major role to successfully spin around the cycle because it enables data reuse long after the original collection.\nImagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files?\nThe goal is to have enough information for the researcher to understand the data, interpret the data, and then reuse the data in another study.\n\n3.4.1 Overall Guidelines\nAnother way to think about metadata is to answer the following questions with the documentation:\n\nWhat was measured?\nWho measured it?\nWhen was it measured?\nWhere was it measured?\nHow was it measured?\nHow is the data structured?\nWhy was the data collected?\nWho should get credit for this data (researcher AND funding agency)?\nHow can this data be reused (licensing)?\n\n\n\n3.4.2 Bibliographic Guidelines\nThe details that will help your data be cited correctly are:\n\nGlobal identifier like a digital object identifier (DOI)\nDescriptive title that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data\nDescriptive abstract that serves as a brief overview off the specific contents and purpose of the data package\nFunding information like the award number and the sponsor\nPeople and organizations like the creator of the dataset (i.e. who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset\n\n\n\n3.4.3 Discovery Guidelines\nThe details that will help your data be discovered correctly are:\n\nGeospatial coverage of the data, including the field and laboratory sampling locations, place names and precise coordinates\nTemporal coverage of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to\nTaxonomic coverage of the data, including what species were measured and what taxonomy standards and procedures were followed\nAny other contextual information as needed\n\n\n\n3.4.4 Interpretation Guidelines\nThe details that will help your data be interpreted correctly are:\n\nCollection methods for both field and laboratory data the full experimental and project design as well as how the data in the dataset fits into the overall project\nProcessing methods for both field and laboratory samples\nAll sample quality control procedures\nProvenance information to support your analysis and modelling methods\nInformation about the hardware and software used to process your data, including the make, model, and version\nComputing quality control procedures like testing or code review\n\n\n\n3.4.5 Data Structure and Contents\n\nEverything needs a description: the data model, the data objects (like tables, images, matrices, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation.\nVariable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (i.e. 0 = not collected), and any missing values (i.e. 999 = NA).\n\nNot only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable).\nFor example, if you were to search for the character string “carbon dioxide flux” in a data repository, not all relevant results will be shown due to varying vocabulary conventions (i.e., “CO2 flux” instead of “carbon dioxide flux”) across disciplines — only datasets containing the exact words “carbon dioxide flux” are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search.\n\n\n3.4.6 Rights and Attribution\nCorrectly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like:\n\nCitation format to be used when giving credit for the data\nAttribution expectations for the dataset\nReuse rights, which describe who may use the data and for what purpose\nRedistribution rights, which describe who may copy and redistribute the metadata and the data\nLegal terms and conditions like how the data are licensed for reuse.\n\n\n\n3.4.7 Metadata Standards\nSo, how does a computer organize all this information? There are a number of metadata standards that make your metadata machine readable and therefore easier for data curators to publish your data.\n\nEcological Metadata Language (EML)\nGeospatial Metadata Standards (ISO 19115 and ISO 19139)\n\nSee NOAA’s ISO Workbook\n\nBiological Data Profile (BDP)\nDublin Core\nDarwin Core\nPREservation Metadata: Implementation Strategies (PREMIS)\nMetadata Encoding Transmission Standard (METS)\n\nNote this is not an exhaustive list.\n\n\n3.4.8 Data Identifiers\nMany journals require a DOI (a digital object identifier) be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to.\nSome data repositories assign a DOI for each dataset you publish on their repository. But, if you need to update the datasets, check the policy of the data repository. Some repositories assign a new DOI after you update the dataset. If this is the case, researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available.\n\n\n3.4.9 Data Citation\nResearchers should get in the habit of citing the data that they use (even if it’s their own data!) in each publication that uses that data."
  },
  {
    "objectID": "session_03.html#data-sharing-preservation",
    "href": "session_03.html#data-sharing-preservation",
    "title": "3  Data Management Essentials",
    "section": "3.5 Data Sharing & Preservation",
    "text": "3.5 Data Sharing & Preservation\n\n\n3.5.1 Data Packages\n\nWe define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve.\n\nSometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\nMany data repositories assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier, often assigned to the metadata and becomes a publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. This allows to identify a digital entity within a data package.\nIn the graphic to the side, the package can be cited with the DOI doi:10.5063/F1Z1899CZ,and each of the individual files have their own identifiers as well.\n\n\n\n3.5.2 Data Repositories: Built for Data (and code)\n\nGitHub is not an archival location\nExamples of dedicated data repositories:\n\nKNB\nArctic Data Center\ntDAR\nEDI\nZenodo\n\nDedicated data repositories are:\n\nRich in metadata\nArchival in their mission\nCertified\n\nData papers, e.g., Scientific Data\nre3data is a global registry of research data repositories\nRepository Finder is a pilot project and tool to help researchers find an appropriate repository for their work\n\n\n3.5.2.1 DataOne Federation\nDataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses.\nDataONE can be searched on the web, which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the (currently 44!) repositories one at a time."
  },
  {
    "objectID": "session_03.html#summary",
    "href": "session_03.html#summary",
    "title": "3  Data Management Essentials",
    "section": "3.6 Summary",
    "text": "3.6 Summary\n\nThe Data Life Cycle help us see the big picture of our data project.\nOnce we identify the necessary steps it is helpful to think through each one and plan accordingly.\nIt is extremely helpful to develop a data management plan is to stay organized.\nDocument everything. Having rich metadata is a key factor to enable data reuse. Describe your data and files and use an appropriate metadata standard.\nPublish your data in a stable long live repository and assign a unique identifier."
  },
  {
    "objectID": "session_03.html#activity-lets-think-about-the-data-life-cycle-for-your-project",
    "href": "session_03.html#activity-lets-think-about-the-data-life-cycle-for-your-project",
    "title": "3  Data Management Essentials",
    "section": "3.7 Activity: Let’s think about the Data Life Cycle for your project",
    "text": "3.7 Activity: Let’s think about the Data Life Cycle for your project\nGOAL: Get them to start thinking about the data needs of their prject and how they envision prpating for each step.\n\nLooking into your logic models: What are the data needs?\nHow can we start planning the way we are goingt o organize these data sets\nLook into DPM examples\nAnswer guided question to outline a DMP plan\n\n\n\n\n\nMichener, William K. 2015. “Ten Simple Rules for Creating a Good Data Management Plan.” PLOS Computational Biology 11 (10): 1–9. https://doi.org/10.1371/journal.pcbi.1004525."
  }
]