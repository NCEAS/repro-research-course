[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "",
    "text": "Overview"
  },
  {
    "objectID": "index.html#about-this-training",
    "href": "index.html#about-this-training",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "About this training",
    "text": "About this training\nThe NCEAS openS Program consists of three 1-week long workshops, geared towards early career researchers. Participants engage in a mix of lectures, exercises, and synthesis research activities to conduct synthesis science and implement best practices in open data science.\nopenS has been adapted to coincide with each USGS CAP training, for a total of four training sessions across 2 years."
  },
  {
    "objectID": "index.html#why-nceas",
    "href": "index.html#why-nceas",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "Why NCEAS",
    "text": "Why NCEAS\nThe National Center for Ecological Analysis and Synthesis (NCEAS), a research affiliate of the University of California, Santa Barbara, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science.\nNCEAS has over 25 years of success with this model among working groups and environmental professionals, and we are excited to pass on this cumulative expertise to you."
  },
  {
    "objectID": "index.html#week-one-successful-synthesis-and-data-management-best-practices",
    "href": "index.html#week-one-successful-synthesis-and-data-management-best-practices",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "Week One: Successful Synthesis and Data Management Best Practices",
    "text": "Week One: Successful Synthesis and Data Management Best Practices\n\nLearning Objectives:\n\nImplement reproducible scientific workflows throughout all aspects of a project\nIncrease your familiarity and confidence with data science tools\nEffectively manage and wrangle data using tidy data practices\nAccessing, interpreting and developing metadata for synthesis research\nOrganize and initiate synthesis projects\n\n\n\nSchedule\n\n\n\n\n\n\n\n\n\nDate\nTime\nSession Title\nLead Facilitator\n\n\n\n\nMonday 08/21\nBlock 3\n1:45PM-3:15PM\nSuccessful Synthesis Science\nHalina Do-Linh, Data Training Program Manager\n\n\nTuesday 08/22\nBlock 2\n10:45AM-12:15PM\nPanel for PIs: Successful Mentorship in Synthesis Science\nHalina Do-Linh, Data Training Program Manager\n\n\nWednesday 08/23\nBlock 1\n9AM-10:30AM\nData Management Essentials\nCamila Vargas Poulsen, Data Training Coordinator"
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS with the support of DataONE, to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors for this version are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Halina Do-Linh, Camila Vargas Poulsen. 2023. openS for USGS Climate Adaptation Postdoctoral (CAP) Fellows Program. Week One. NCEAS Learning Hub & USGS Climate Adaptation Science Centers (CASCs).\nAdditional contributors: Ben Bolker, Julien Brun, Amber E. Budden, Jeanette Clark, Samantha Csik, Carmen Galaz García, Stephanie Hampton, Natasha Haycock-Chavez, Matthew B. Jones, Samanta Katz, Julie Lowndes, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "session_01.html#learning-objectives",
    "href": "session_01.html#learning-objectives",
    "title": "1  Successful Synthesis Science",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify synthesis science research and different synthesis science approaches\nImplement synthesis science approaches to your research"
  },
  {
    "objectID": "session_01.html#what-is-synthesis-science",
    "href": "session_01.html#what-is-synthesis-science",
    "title": "1  Successful Synthesis Science",
    "section": "1.1 What is Synthesis Science?",
    "text": "1.1 What is Synthesis Science?\n\n\n\nGraphic by Judith Sitters\n\n\n\n\n\n\n\n\nSynthesis Science in Ecological and Environmental Sciences\n\n\n\nSynthesis science is the process of distilling existing data, ideas, theories, or methods drawn from many sources, across multiple fields of inquiry, to accelerate the generation of new scientific knowledge, often at a broad scale.\n\n\nTo break that down further, think of it this way: synthesis science utilizes existing data and knowledge to generate new inights. It’s like putting together puzzle pieces from different sources to create a bigger picture.\nSynthesis science is fundamentally different than other types of academic research in that it does not involve collecting new or original data to conduct research or to answer a research question.\nSince the ecological and environmental sciences are complex and often interdisciplinary, synthesis approaches are ideal for “understanding complexity across scales, leveraging data from various disciplines, facilitating discovery of general patterns in natural systems, and informing policy” (Halpern et al. 2023) in the ecological and environmental sciences.\n\n1.1.1 The Value and Impact of Synthesis Research\n\n\n\nGraphic adapted from Engingeering Ideas newsletter on Substack\n\n\nData is everywhere and information is more accessible than ever, but how can we leverage both data and knowledge to further our understanding of a topic and turn those insights into meaningful action? This is where synthesis research shines.\nThere are many studies that value the importance of synthesis for varying reasons including:\n\nElevating scientific research as evidence that is influential in policy-making (Dicks, Walsh, and Sutherland 2014)\nAddressing the challenge of “information overload” (Hampton and Parker 2011)\nDelivering research products that enhance scientific knowledge in decision-making (Hampton and Parker 2011)\nProviding critical knowledge to solve environmental issues (Carpenter et al. 2009)\nInitiating new collaborations and new insights across disciplines and non-academic sectors (Hampton and Parker 2011)\n\nAs synthesis science and research synthesis approaches have been gaining prominenance in the ecological and environmental fields, researchers have sought to understand the impacts of synthesis. In particular, Wyborn et al. (2018) identified the following types of impacts from research synthesis:\n\nKnowledge is used to change understanding of a system or phenomena\nKnowledge changes understanding of the interconnections between actors, sectors or systems\nKnowledge is used to support a particular view of a problem\nKnowledge is used as the prime source of information to inform policy change or reform\nKnowledge use is dependent on the capacity to understand or apply knowledge in a given context"
  },
  {
    "objectID": "session_01.html#research-synthesis-approaches",
    "href": "session_01.html#research-synthesis-approaches",
    "title": "1  Successful Synthesis Science",
    "section": "1.2 Research Synthesis Approaches",
    "text": "1.2 Research Synthesis Approaches\nIn general, research synthesis approaches include utilizing existing research on a particular topic with the intention of integrating all the finding together.\nThere are multiple different types of approaches such as narrative reviews, vote counting (this quantitative method has been discredited, but you may find examples of it in your research), case studies, and more. However, systematic reviews and meta-analysis are considered by some to be the “gold standard” of research synthesis and are often used at NCEAS, which is why we’re highlighting them here.\n\n\n\n\n\n\n\nSystematic Review\nA form of synthesis to study a specific question or problem by following clear steps: finding and choosing studies, checking them carefully, and gathering information from them to create a reliable review.\nNote: Not all systematic reviews contain meta-analysis\n\n\nMeta-Analysis\nA form of synthesis that uses statistical and qunatitative methods to integrate and compare (quantitatively) results from a large number of individual studies.\n\n\n\n\n1.2.1 What about data?\nData is essential to synthesis in ecological and environmental research. Synthesizing data can involve combining and analyzing data from multiple sources to address research questions, to conduct a systematic review or meta-analysis, to identify patterns, and draw meaningful conclusions that don’t appear in siloed approaches.\nAt NCEAS, our approach tends to include combining multiple datasets from various sources, then we leverage specific expertise from diverse disciplines and researchers, and together we use those two elements to unleash new insights to answer large-scale questions.\nWe often see this approach with working groups at NCEAS. Working groups are teams that include researchers, agencies and organizational practitioners, and more who come together (and typically at NCEAS, a neutral ground) for focused and intensive working sessions around their research questions."
  },
  {
    "objectID": "session_01.html#nceas-and-synthesis-science",
    "href": "session_01.html#nceas-and-synthesis-science",
    "title": "1  Successful Synthesis Science",
    "section": "1.3 NCEAS and Synthesis Science",
    "text": "1.3 NCEAS and Synthesis Science\n\n\n\n\n\nIn 1995, the National Science Foundation (NSF) responded to the increasing calls by environmental researchers and professionals for interdisciplinary and transdisciplinary explanations, methods to connect science to applications, and ways to advance research technologies by creating NCEAS, the first synthesis center.\nThere are multiple other synthesis centers that have been established since, but each one has a foundational goal to use synthesis to “advance basic science, organize ecological information for decision makers concerned with pressing national issues, and make cost-effective use of the nation’s extant and accumulating database” (Hampton and Parker 2011).\nSynthesis centers are specifically designed to catalyze collaboration leading to breakthrough ideas, and one way NCEAS (among other synthesis centers) achieve this are through working groups. Since its inception, NCEAS has hosted hundreds of working groups and has supported them in varying ways including: structuring group size or composition, facilitation of working sessions, supplying computing and informatics support, offering training opporunities for data-focused team members, providing a neutral space for meetings and co-working, and more. In short, NCEAS lowers the “activation energy” for working group members so that they can focus on the cross-pollination of ideas, creative thinking, and emergent research (Rodrigo et al. 2013)."
  },
  {
    "objectID": "session_01.html#working-groups-at-nceas",
    "href": "session_01.html#working-groups-at-nceas",
    "title": "1  Successful Synthesis Science",
    "section": "1.4 Working Groups at NCEAS",
    "text": "1.4 Working Groups at NCEAS\n\n1.4.1 Ocean Health Index (OHI)\n\n\n\n\n\nThe Ocean Health Index brought together a diverse group of experts including marine scientists, economists, policy experts, and more. This collaborative effort aimed to comprehensively assess the health of oceans worldwide by integrating data from various sources, including physical, biological, and socioeconomic factors.\nThe success of the Ocean Health Index highlights several key aspects of effective synthesis science in working groups:\n\nInterdisciplinary Collaboration: The working group included experts from different disciplines, fostering a holistic understanding of ocean health that goes beyond individual scientific perspectives.\nData Integration: The Index integrated data from multiple sources and scales, ranging from local to global, to provide a comprehensive view of ocean health.\nStandardized Metrics: The working group developed standardized metrics that allowed for consistent assessment and comparison of ocean health across regions and over time.\nPolicy Relevance: The synthesis findings were designed to inform policy and management decisions, demonstrating the practical applications of synthesis science in addressing real-world challenges.\nCommunication and Outreach: The Index effectively communicated its findings to a wide range of stakeholders, raising awareness about ocean health and fostering engagement in conservation efforts.\n\n\n\n1.4.2 State of Alaska’s Salmon and People (SASAP)\n\n\n\n\n\nThe SASAP project was a partnership between NCEAS and Anchorage-based Nautilus Impact Investing to create an equitable decision-making platform for salmon management through information synthesis, collaboration, and stakeholder engagement.\nThe project sought to address the need for accessible, up-to-date, and complete information about salmon and the factors shaping their future. It convened eight collaborative working groups consisting of leading experts at the University of Alaska and other universities, local indigenous leaders, and specialists across resource sectors – an intentional integration of western scientific perspectives and indigenous knowledge."
  },
  {
    "objectID": "session_01.html#priorities-of-synthesis-here-now-and-beyond",
    "href": "session_01.html#priorities-of-synthesis-here-now-and-beyond",
    "title": "1  Successful Synthesis Science",
    "section": "1.5 Priorities of Synthesis Here, Now, and Beyond",
    "text": "1.5 Priorities of Synthesis Here, Now, and Beyond\nIn February 2021, NCEAS hosted a virtual workshop where 127 ecologists, environmental and natural scientists convened to discuss the future priorities of synthesis science. Participants across career stages, institutions, backgrounds, and geographies were in attendance and together they identified seven research topics, and two pressing needs for conducting research synthesis practices.\nLater in January 2023, dozens of scientists authored and published Priorities for synthesis research in ecology and environmental science the results of the workshop. We’ve summarized these results in the tables below.\n\n\n\n\n\n\n\nResearch Topic\nDescription\n\n\n\n\nDiversity, Equity, Inclusion, and Justice (DEIJ)\nEnvironmental justice shows the link between nature and fairness for people, especially those who are disadvantaged. Unequal distribution of nature due to factors like racism impacts cities and ecosystems. Integrating DEIJ into research can improve knowledge and involve underrepresented groups, while engaging diverse communities can make research more impactful and inclusive.\n\n\nHuman and Natural systems\nEnvironmental synthesis has given us valuable insights into environmental change, but it needs to also look at how human actions and decisions affect the environment. For example, decisions about things like population growth and greenhouse gas policies impact species and nature. Bringing together information from different fields is challenging, especially when considering different scales and practices. Synthesis science can help us better understand how humans and nature are connected, but it takes time and effort to combine ideas from different disciplines.\n\n\nActionable and Use-inspired science\nSynthesis science in ecology and environmental science is great for helping decision makers. Methods like systematic reviews and predictive models can simplify research for environmental policy. To be more effective, scientists need to work closely with practitioners and use different types of knowledge. Communication, teamwork, and using various knowledge sources can improve how decisions are made and carried out based on scientific findings.\n\n\nScale\nUnderstanding ecosystems across different sizes and time periods is hard for social-ecological research. Synthesis helps by combining information and methods from different scales, and connecting these findings to policy decisions. The challenges come from not having enough long-term data, difficulty mixing different information for analysis, and lack of skills and cooperation across scales. To overcome these challenges, more efforts are needed to collect, organize, and combine data over long periods and big areas.\n\n\nGenerality\nScience looks for common rules to explain how things work, but sometimes it’s hard to find them. Synthesis science can help find these general rules and figure out when they apply. Two problems are not having enough similar studies to compare and bias in what gets studied and reported. To fix this, scientists can share their work openly, use the same rules for reporting, and make sure all types of results are published. Working together across different areas can also help find these general rules.\n\n\nComplexity and Resilience\nEcosystems are very complicated, especially when considering things like society and economics. This complexity can make systems strong or weak. Using this complex view helps solve problems like water and food. But putting all this information together (synthesis) is hard because it depends on the situation, and finding general rules is tough. To make it work better, more scientists need to learn to work together across different areas and share their data. The goal is to understand how complexity affects ecosystems and find ways to help both people and the environment.\n\n\nPredictability\nThis time of big changes in the environment makes it important to predict how ecosystems will be in the future and how they’ll affect people. Predicting things in the short term (like months or years) is common in some fields but not in ecology. Some people worry that the predictions might not be perfect, but even imperfect predictions can help us learn and make better decisions. To make predictions better in ecology, scientists can work together, use standardized methods and data, and talk to the people who use the predictions. This will help us manage natural resources more wisely.\n\n\n\n\n\n\n\n\n\n\nPressing Need\nDescription\n\n\n\n\nNeed for increased participant diversity and inclusive research practices\nSynthesis science is most effective when it combines data, perspectives, and insights from different regions and ecosystems. By including diverse participants and creating inclusive research environments, we can better address important questions and find fair solutions to challenges. Ignoring diversity and inclusion can lead to biased syntheses and unfair outcomes. The large scope of synthesis science calls for involving people, ideas, and data from various backgrounds and languages. Efforts should be made to ensure representation, provide support, and promote open dialogue to increase diversity and inclusivity in synthesis research, ultimately fostering better understanding and more equitable outcomes.\n\n\nIncreased and improved data flow, access, and skill-building\nThe amount of data for synthesis science is increasing, and there are efforts to organize and combine data from various sources to answer ecological questions. Funding is needed to manage and store data properly. It’s important to think about ethical issues when using different datasets, especially if some people can’t share their data openly. Some data may be hard to access due to language or quality issues, and including diverse types of data, like traditional knowledge, can be challenging. Tools like machine learning can help make sense of different data sources. To improve data synthesis, global collaboration among diverse groups is essential.\n\n\n\n\n\n\n\nCarpenter, Stephen R., E. Virginia Armbrust, Peter W. Arzberger, F. Stuart Chapin, James J. Elser, Edward J. Hackett, Anthony R. Ives, et al. 2009. “Accelerate Synthesis in Ecology and Environmental Sciences.” BioScience 59 (8): 699–701. https://doi.org/10.1525/bio.2009.59.8.11.\n\n\nDicks, Lynn V., Jessica C. Walsh, and William J. Sutherland. 2014. “Organising Evidence for Environmental Management Decisions: A ‘4S’ Hierarchy.” Trends in Ecology & Evolution 29 (11): 607–13. https://doi.org/10.1016/j.tree.2014.09.004.\n\n\nHalpern, Benjamin S., Carl Boettiger, Michael C. Dietze, Jessica A. Gephart, Patrick Gonzalez, Nancy B. Grimm, Peter M. Groffman, et al. 2023. “Priorities for Synthesis Research in Ecology and Environmental Science.” Ecosphere 14 (1). https://doi.org/10.1002/ecs2.4342.\n\n\nHampton, Stephanie E., and John N. Parker. 2011. “Collaboration and Productivity in Scientific Synthesis.” BioScience 61 (11): 900–910. https://doi.org/10.1525/bio.2011.61.11.9.\n\n\nRodrigo, Allen, Susan Alberts, Karen Cranston, Joel Kingsolver, Hilmar Lapp, Craig McClain, Robin Smith, Todd Vision, Jory Weintraub, and Brian Wiegmann. 2013. “Science Incubators: Synthesis Centers and Their Role in the Research Ecosystem.” PLoS Biology 11 (1): e1001468. https://doi.org/10.1371/journal.pbio.1001468.\n\n\nWyborn, Carina, Elena Louder, Jerry Harrison, Jensen Montambault, Jasper Montana, Melanie Ryan, Angela Bednarek, et al. 2018. “Understanding the Impacts of Research Synthesis.” Environmental Science & Policy 86 (August): 72–84. https://doi.org/10.1016/j.envsci.2018.04.013."
  },
  {
    "objectID": "session_02.html#learning-objectives",
    "href": "session_02.html#learning-objectives",
    "title": "2  Logic Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nBecome familiar with Logic Models\nApply the principles of Logic Models to synthesis development\nRefine synthesis group research question"
  },
  {
    "objectID": "session_02.html#logic-models",
    "href": "session_02.html#logic-models",
    "title": "2  Logic Models",
    "section": "2.1 Logic Models",
    "text": "2.1 Logic Models\nLogic models are a planning tool that are designed to support program development by depicting the flow of resources and processes leading to a desired result. They are also used for outcomes-based evaluation of a program and are often requested as part of an evaluation planning process by funders or stakeholders.\nA simplified logic models comprise three main parts: Inputs, Outputs and Outcomes.\n\nInputs reflect what is invested, outputs are what is done and outcomes are the results of the program.\nIn a more detailed logic model, outputs and outcomes are further broken down. Outputs are often represented as ‘Activities’ and ‘Participants’. By including participation (or participants), the logic model is explicitly considering the intended audience, or stakeholders, impacted by the program. Engagement of this audience is an output. In the case of outcomes, these can be split into short, medium and long-term outcomes. Sometimes this last category may be labeled ‘Impact’\n\nDefining the inputs, outputs and outcomes early in a planning process enables teams to visualize the workflow from activity to results and can help mitigate potential challenges. Logic models can be thought of as having an ‘if this then that’ structure where inputs -&gt; outputs -&gt; outcomes.\n\nIn the example below we have constructed a simple logic model for a hypothetical project where training materials are being developed for a group of educators to implement at their respective institutions.\n\nLinkages are not always sequential and can be within categories, bi-directional and/or include feedback loops. Detailing this complexity of relationships, or theory of action, can be time consuming but is a valuable part of the thought process for project planning. In exploring all relationships, logic modeling also allows for assessing program feasibility.\n\nThe above graphics include two sections within Outputs - Activities and Participants - and this is quite common. There is variation in logic model templates, including versions with a third type of output - “Products’. Sometimes description of these products is contained within the Activities section - for example, ‘develop curricula’, ‘produce a report’ - however calling these out explicitly is beneficial for teams focused on product development.\nProgram development (and logic modeling) occurs in response to a given ‘Situation’ or need, and exploring this is the first step in modeling. The situation defines the objective, or problem, that the program is designed to solve hence some logic models may omit the left-hand situation column but be framed with Problem and Solution statements. Finally, comprehensive logic modeling takes into consideration assumptions that are made with respect to the resources available, the people involved, or the way the program will work and also recognizes that there are external factors that can impact the program’s success.\n\nIn summary:\nLogic models support program development and evaluation and comprise three primary steps in the workflow:\n\nInputs: Resources, contributions, and investments required for a program;\nOutputs: Activities conducted, participants reached, and products produced; and\nOutcomes: Results or expected changes arising from the program structured as short-, medium- and long-term."
  },
  {
    "objectID": "session_02.html#logic-models-for-synthesis-research",
    "href": "session_02.html#logic-models-for-synthesis-research",
    "title": "2  Logic Models",
    "section": "2.2 Logic Models for Synthesis Research",
    "text": "2.2 Logic Models for Synthesis Research\nLogic models are one tool for program development and have sufficient flexibility for a variety of situations, including planning for synthesis research. While some logic model categories may feel less relevant, the process of articulating a research objective, associated resources and activities and proposed outcome(s) has value.\nBelow is a table of logic model elements and example questions for each element framed for both a typical logic model and synthesis research.\n\n\n\n\n\n\n\n\nLogic Model Element\nLogic Model Framing\nSynthesis Research Framing\n\n\n\n\nProblem Statement\nWhat is the problem? Why is this a problem? Who does this impact?\nWhat is the current state of knowledge? What gaps exists in understanding? Why is more information / synthesis important?\n\n\nInputs\nWhat resources are needed for the program (e.g. personnel, money, time, equipment, partnerships, etc.)?\nWhat is needed to undertake the synthesis research? For personnel, think in terms of the roles that are needed (e.g. data manager, statistician, writer, editor etc.) Consider the time frame. Condsider what data are needed and what already exists?\n\n\nOutputs - Activities\nWhat will be done (e.g. development, design, workshops, conferences, counseling, outreach, etc.)?\nWhat activities are needed to conduct the research?\nThis could be high level or it could be broken down into details such as the types of statistical approaches.\n\n\nOutputs - Participants\nWho will we reach (e.g. clients, participants, customers, etc.)?\nWho is the target audience? Who will be impacted by this work? Who is positioned to leverage this work?\n\n\nOutputs - Products\nWhat will you create (e.g. publications, websites, media communications, etc.)?\nWhat research products are planned or expected? Consider this in relation to the intended audience. Is a peer-reviewed publication, report or white paper most appropriate? How will derived data be handled? Will documentation, workflows, or code be published?\n\n\nOutcomes - Short-term\nWhat short-term outcomes are anticipated among participants?\nThese might include changes in awareness, knowledge, skills, attitudes, opinions or intent.\nWill this work represent a significant contribution to current understanding?\n\n\nOutcomes - Medium-term\nWhat medium-term outcomes are predicted among participants?\nThese might include changes in behaviors, decision-making and actions.\nWill this work promote increased research activity or open new avenues of inquiry?\n\n\nOutcomes - Long-term\nWhat long-term benefits, or impacts, are expected?\nThis might include changes in social, economic, civic, or environmental conditions.\nWill this work result in local, regional or national policy change? What will be the long-term impact of increased investment in the ecosystem?"
  },
  {
    "objectID": "session_02.html#activity-synthesis-planning-with-logic-models",
    "href": "session_02.html#activity-synthesis-planning-with-logic-models",
    "title": "2  Logic Models",
    "section": "2.3 Activity: Synthesis Planning with Logic Models",
    "text": "2.3 Activity: Synthesis Planning with Logic Models\n\n\n\n\n\n\nSetup\n\n\n\nBreakout into your research synthesis groups to refine your ideas for potential synthesis topics. Use either the Logic Model Template or a Mermaid Flowchart to create your logic model.\nThe goal for this activity is to develop one or more high-level logic models that:\n\nSummarize the synthesis research question\nDefine the inputs needed to approach the synthesis research question\nDefine the outputs including the activities, participants, and products needed to address the research question\nDefine at least the short-term outcomes and long-term outcomes of the research\n\nOften it is helpful to start by working backwards. Consider what outcomes are you trying to achieve, what outputs would help you get to those outcomes, and finally what inputs do you need to create those outputs.\n\n\n\nLogic Model Template\nUse our Logic Model Template to create your Logic Model. When you access the link, you must create your own copy to start editing.\n\n\n\nLogic Model Template\n\n\n\n\nMermaid Flowchart\nUse a Mermaid Flowchart to create your Logic Model. These is a tool that is embedded in Quarto documents.\n\n\n\n\nflowchart LR\n    INPUTS --&gt; ACTIVITIES --&gt; OUTPUTS --&gt; OUTCOMES/IMPACTS\n\n    Scenario{{Accelerate synthesis via data science training}}\n\n    R1[Instructor] & R2[Classroom space] & R3[Projector] --&gt; B{Data Science Workshop}\n    B --&gt; C(Workshop Curriculum)\n    B --&gt; D(Presentations and Practice)\n    \n    C & D --&gt; E[/Improved Delta management/] & F[/Increased analytic efficiency/]\n\n\n\n\n\nSource Code for Mermaid Flowhart Example (above):\n\n```{mermaid}\n\nflowchart LR\n    INPUTS --&gt; ACTIVITIES --&gt; OUTPUTS --&gt; OUTCOMES/IMPACTS\n\n    Scenario{{Accelerate synthesis via data science training}}\n\n    R1[Instructor] & R2[Classroom space] & R3[Projector] --&gt; B{Data Science Workshop}\n    B --&gt; C(Workshop Curriculum)\n    B --&gt; D(Presentations and Practice)\n    \n    C & D --&gt; E[/Improved Delta management/] & F[/Increased analytic efficiency/]\n```"
  },
  {
    "objectID": "session_03.html#learning-objectives",
    "href": "session_03.html#learning-objectives",
    "title": "3  Data Management Essentials",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the importance of data management for successfully preserving data\nReview the Data Life Cycle and how it can guide the data management in a project\nFamiliarize with data management plans and start drafting one for your project\nLearn about metadata guidelines and best practices for reproducibility\nBecome familiar with environmental data repositories for accessing and publishing data"
  },
  {
    "objectID": "session_03.html#the-big-idea",
    "href": "session_03.html#the-big-idea",
    "title": "3  Data Management Essentials",
    "section": "3.1 The Big Idea",
    "text": "3.1 The Big Idea\nThis lesson aims to get you thinking about how are you going to manage your data. Even though you are in the early stages of defining you research project, we believe that when it comes to data management, the earlier you start thinking about it the better."
  },
  {
    "objectID": "session_03.html#introduction",
    "href": "session_03.html#introduction",
    "title": "3  Data Management Essentials",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nData management is the process of handling, organizing, documenting, and preserving data used in a research project. This is particularly important in synthesis science given the nature of synthesis, which involves combining data and information from multiple sources to answer broader questions, generate knowledge and provide insights into a particular problem or phenomenon.\n\n3.2.1 Benefits of managing your data\nSuccessfully managing your data throughout a research project helps ensures its preservation for future use. It also facilitates collaboration within your team, and it helps advance your scientific outcomes.\nFrom a researcher perspective\n\nKeep yourself organized – be able to find your files (data inputs, analytic scripts, outputs at various stages of the analytic process, etc.)\nTrack your science processes for reproducibility – be able to match up your outputs with exact inputs and transformations that produced them\nBetter control versions of data – easily identify versions that can be periodically purged\nQuality control your data more efficiently\nTo avoid data loss (e.g. making backups)\nFormat your data for re-use (by yourself or others)\nBe prepared to document your data for your own recollection, accountability, and re-use (by yourself or others)\nGain credibility and recognition for your science efforts through data sharing!\n\nAdvancement of science\n\nData is a valuable asset – it is expensive and time consuming to collect\nMaximize the effective use and value of data and information assets\nContinually improve the quality including: data accuracy, integrity, integration, timeliness of data capture and presentation, relevance, and usefulness\nEnsure appropriate use of data and information\nFacilitate data sharing\nEnsure sustainability and accessibility in long term for re-use in science"
  },
  {
    "objectID": "session_03.html#the-data-life-cycle",
    "href": "session_03.html#the-data-life-cycle",
    "title": "3  Data Management Essentials",
    "section": "3.3 The Data Life Cycle",
    "text": "3.3 The Data Life Cycle\nThe Data Life Cycle gives you an overview of meaningful steps data goes through in a research project, from planning to archival. This step-by-step breakdown facilitates overseeing individual actions, operations and processes required at each stage. This is a visual tool that aims o help scientists plan and anticipate what will be the “data needs” for a specific project (ADD CITATION Faundeen et al 2013) .\n\n\n\nSource: Adapted from Faundeen et al 2013, USGS & DataONE\n\n\n\n3.3.1 Primary Elements\n\n\n\n\n\n\n\n\nStep\nDescription\nTool\n\n\n\n\nPlan\nMap out the processes and resources for all activities related to the handling of the project’s data assets. Start with the project goals (desired outputs, outcomes, and impacts) and work backwards to build a data management plan, supporting data policies, and sustainability plans for each step.\nData Management Plan (DPM)\n\n\nAcquire & Discover\nActivities needed to collect new or existing data. You can structure the process of collecting data upfront to better implement data management. Consider data policies and best practices that maintain the provenance and integrity of the data.\nIdentifying data sources and mechanisms to access data\n\n\nProcess\nEvery step needed to prepare new or existing data to be able to use it as an input for synthesis. Consider the structure of the data, unit transformation, extrapolations, etc\nCleaning & Wrangling data skills\n\n\nIntegrate\nData from multiple sources are combined into a form that can be readily analyzed. For example, you could combine citizen science project data with other sources of data to enable new analyses and investigations. Successful data integration depends on documentation of the integration process, clearly citing and making accessible the data you are using, and employing good data management practices throughout the Data Life Cycle.\nModeling & Interpretation\n\n\nAnalyze\nCreate analyses and visualizations to identify patterns, test hypotheses, and illustrate findings. During this process, record your methods, document data processing steps, and ensure your data are reproducible. Learn about these best practices and more.\nModeling, Interpretation & Statistics\n\n\nPreserve\nPlan on how you are going to store your data for long-term use and accessibility so others can access, interpret, and use the data in the future. Decide what data to preserve, where to preserve it, and what documentation needs to accompany the data.\nData packages & repositories\n\n\nPublish and Share\nPublication and distribution of your data through the web or in data catalogs, social media or other venues to increase the chances of your data being discovered. Data is a research product as publications are.\nDOIs and citations\n\n\n\n\n\n3.3.2 Cross-Cutting Elements\nThese elements are involved across all stages describes above. They need to constantly by addressed throughout all the Data Life Cycle, making sure effetive data management is in place.\n\n\n\n\n\n\n\n\nStep\nDescription\nTool\n\n\n\n\nDescribe\nDocument data and each of the data stages by describing the why, who, what, when, where, and how of the data and processes. Metadata, or data about data, is key to data sharing and reuse. Documentation such as software code comments, data models, and workflows facilitate indexing, acquiring, understanding, and future uses of the data\nMetadata and documentation\n\n\nManage Quality\nEmploy quality assurance and quality control procedures that enhance the quality of data, making sure the measurements or outputs within expected values.Identify potential errors and techniques to address them.\nQuality Control and Quality Assurance techniques\n\n\nBackup and Secure\nPlan to preserve data in the short term to minimize potential losses (e.g., via software failure, human error, natural disaster). This avoids risk and ensures data is accessible to collaborators. This applies to raw and process data, original science plan, data management plan, data acquisition strategy, processing procedures, versioning, analysis methods, published products, and associated metadata\nServers, secure data sharing services"
  },
  {
    "objectID": "session_03.html#data-management-plans",
    "href": "session_03.html#data-management-plans",
    "title": "3  Data Management Essentials",
    "section": "3.4 Data Management Plans",
    "text": "3.4 Data Management Plans\nAs you can see there is a lot happening around the Data Life Cycle. This is why PLANNING is a key first step. It is advisable to initiate your data management planning at the beginning of your research process before any data has been collected or discovered.\nIn order to better plan and keep track of all the moving pieces when working with data, a good place to start is creating a Data Management Plan. However, this is not only the starting point. This is a “living” document that should be consulted and updated throughout the project.\nA Data Management Plan (DMP) is a document that describes how you will use your data during a research project, as well as what you will do with your data long after the project ends. DMPs should be updated as research plans change to ensure new data management practices are captured (Environmental Data Initiative).\nA well-thought-out plan means you are more likely to:\n\nstay organized\nwork efficiently\ntruly share data\nengage your team\nmeet funder requirements as DMPs are becoming common in the submission process for proposals\n\nA DMP is both a straightforward blueprint for how you manage your data, and provides guidelines for you and your team on policies, roles and responsibilities. While it is important to plan, it is equally important to recognize that no plan is perfect, as change is inevitable. To make your DMP as robust as possible, review it periodically with your team and adjust as the needs of the project change.\n\n\n\n\n\n\nHow to plan\n\n\n\n\nPlan early - overtime information gets loss. Think about your data needs as you are starting your project.\nPlan in collaboration - engaging all the team makes your plan more resilient, including diverse expertise and perspectives.\nMake revision part of the process - adapt as needed, revising your plan helps you make sure your are on track.\nInclude a tidy data and data ethic lens \n\n\n\n\n\n3.4.1 Creating a Good Data management Plan\nThe article Ten Simple Rules for Creating a Good Data Management Plan {Michener (2015)} outlines the main ideas to keep in mind when you start your “planning stage”. Here we summarize each point and provide useful resources to help you achieve this “rules” and write an awesome DMP.\n\nDetermine what are the organization and/or sponsor requirements\n\n\nGenerally, each organization or funding agency have specific expectations on how to manage and disseminate data. Even though a DMP is a good idea to keep you organize. It will save you significant time and effort by first understanding the specific requirements set by the organization you are working for. Each organization often provide templates on how to structure your DMP.\nResources\n\nThe DMPTool provides templates for different funding agencies plan requirements.\nUSGS has multiple resources on DMPs. Here the Data Policy and Guidance for the Climate Adaptation Science Centers\n\n\n\nIdentify the desired/necessary data sets for the project\n\n\nData is the ultimate reason why we create a DMP. Identifying what data will be use is crusial to planning. Key aspects of the data to consider are:\n\nType (text, spatial, images, tabualar, etc)\nSource (where does the data currently live?, is it propertary data?)\nVolume (10 terabytes, 10 megabytes?)\nFormat (csv, xlsx, shapefiles, etc)\n\nResource\n\nHere is a template spreadsheet to collect all information about the data set you intent to use for your synthesis project. Please adapt as needed. \n\n\n\nDefine how the data will be organized\n\n\nOnce you know the data you will be using (rule #2) it is time to define how are you going to work with your data. Where will the raw data live? How are the different collaborators going to access the data? The needs vary widely from one project to another depending on the data. When drafting your DMP is helpful to focus on identifying what products and software you will be using. When collaborating with a team it is important to identify f there are any limitations to accessing any software or tool.\nResource\n\nHere is an example from the LTER Scientific Computing Support Team on working on NCEAS Server.\n\n\n\nExplain how the data will be documented\n\n\nWe know documenting data is very important. To successfully achieve this we need a plan in place. Three main steps to plan accordingly are:\n\nIdentifying the type of infoomation you want/need to collect to document your data throughly\nDetermine if the is a metadata standard or schema (organized set of elements) you will follow (eg. EML, Dublin Core, ISO 19115, ect). In many cases this relates with what data repository you intend to archive your data.\nEstablish tools that can help you create and manage metadata content.\n\nResource\n\nExcel-to EML by Li Kui is a workflow that provides a spreadsheet template to collect metadata based on the Ecological Metadata Language Schema (EML)\n\n\n\nDescribe how data quality will be assured\n\n\nQuality assurance and quality control (QA/QC) are the procedures taken to ensure data looks how we expect it to be. The ultimate goal is to improve the quality of the data products. Some fields of study, data types or funding organizations have specific set of guidelines for QA/QCing data. However, when writing your DMP it is important to describe what measures you plan to take to QA/QC the data (e.g: instrument calibration, verification tests, visualization approaches for error detection, etc.)\nResources\n\nEnvironmental Data Initiative (EDI) description and examples of Quality Assurance and Quality Control\n\n\n\nHave a data storage strategy (short and long term)\n\n\nPapers get lost, hardware disk crash, URLs break, different media format degrade. It’s inevitable! Plan ahead and think on where your data will live in the long-term to ensure the access and use of this data long after the project. \n\n\nDefine the project’s data policies\n\n\n“Many research sponsors require that DMPs include explicit policy statements about how data will be managed and shared. Such policies include:\n\nlicensing or sharing arrangements that pertain to the use of preexisting materials;\nplans for retaining, licensing, sharing, and embargoing (i.e., limiting use by others for a period of time) data, code, and other materials; and\nlegal and ethical restrictions on access and use of human subject and other sensitive data”. \n\n\n\nWhat data products will be made available and how?\n\n\n“A good dissemination plan includes a few concise statements. State when, how, and what data products will be made available. Generally, making data available to the greatest extent and with the fewest possible restrictions at the time of publication or project completion is encouraged”. \n\n\nAssign roles and responsibilities\n\n\n“A comprehensive DMP clearly articulates the roles and responsibilities of every named individual and organization associated with the project. Roles may include data collection, data entry, QA/QC, metadata creation and management, backup, data preparation and submission to an archive, and systems administration.” \n\n\nIs there a cost associated to managing your data?\n\n\n“Data management takes time and costs money in terms of software, hardware, and personnel. Review your plan and make sure that there are lines in the budget to support the people that manage the data as well as pay for the requisite hardware, software, and services.”"
  },
  {
    "objectID": "session_03.html#metadata-best-practices",
    "href": "session_03.html#metadata-best-practices",
    "title": "3  Data Management Essentials",
    "section": "3.5 Metadata Best Practices",
    "text": "3.5 Metadata Best Practices\nWithin the data life cycle you can be collecting data (creating new data) or integrating data that has all ready been collected. Either way, metadata plays plays a major role to successfully spin around the cycle because it enables data reuse long after the original collection.\nImagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files?\nThe goal is to have enough information for the researcher to understand the data, interpret the data, and then reuse the data in another study.\n\n3.5.1 Overall Guidelines\nAnother way to think about metadata is to answer the following questions with the documentation:\n\nWhat was measured?\nWho measured it?\nWhen was it measured?\nWhere was it measured?\nHow was it measured?\nHow is the data structured?\nWhy was the data collected?\nWho should get credit for this data (researcher AND funding agency)?\nHow can this data be reused (licensing)?\n\n\n\n3.5.2 Bibliographic Guidelines\nThe details that will help your data be cited correctly are:\n\nGlobal identifier like a digital object identifier (DOI)\nDescriptive title that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data\nDescriptive abstract that serves as a brief overview off the specific contents and purpose of the data package\nFunding information like the award number and the sponsor\nPeople and organizations like the creator of the dataset (i.e. who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset\n\n\n\n3.5.3 Discovery Guidelines\nThe details that will help your data be discovered correctly are:\n\nGeospatial coverage of the data, including the field and laboratory sampling locations, place names and precise coordinates\nTemporal coverage of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to\nTaxonomic coverage of the data, including what species were measured and what taxonomy standards and procedures were followed\nAny other contextual information as needed\n\n\n\n3.5.4 Interpretation Guidelines\nThe details that will help your data be interpreted correctly are:\n\nCollection methods for both field and laboratory data the full experimental and project design as well as how the data in the dataset fits into the overall project\nProcessing methods for both field and laboratory samples\nAll sample quality control procedures\nProvenance information to support your analysis and modelling methods\nInformation about the hardware and software used to process your data, including the make, model, and version\nComputing quality control procedures like testing or code review\n\n\n\n3.5.5 Data Structure and Contents\n\nEverything needs a description: the data model, the data objects (like tables, images, matrices, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation.\nVariable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (i.e. 0 = not collected), and any missing values (i.e. 999 = NA).\n\nNot only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable).\nFor example, if you were to search for the character string “carbon dioxide flux” in a data repository, not all relevant results will be shown due to varying vocabulary conventions (i.e., “CO2 flux” instead of “carbon dioxide flux”) across disciplines — only datasets containing the exact words “carbon dioxide flux” are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search.\n\n\n3.5.6 Rights and Attribution\nCorrectly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like:\n\nCitation format to be used when giving credit for the data\nAttribution expectations for the dataset\nReuse rights, which describe who may use the data and for what purpose\nRedistribution rights, which describe who may copy and redistribute the metadata and the data\nLegal terms and conditions like how the data are licensed for reuse.\n\n\n\n3.5.7 Metadata Standards\nSo, how does a computer organize all this information? There are a number of metadata standards that make your metadata machine readable and therefore easier for data curators to publish your data.\n\nEcological Metadata Language (EML)\nGeospatial Metadata Standards (ISO 19115 and ISO 19139)\n\nSee NOAA’s ISO Workbook\n\nBiological Data Profile (BDP)\nDublin Core\nDarwin Core\nPREservation Metadata: Implementation Strategies (PREMIS)\nMetadata Encoding Transmission Standard (METS)\n\nNote this is not an exhaustive list.\n\n\n3.5.8 Data Identifiers\nMany journals require a DOI (a digital object identifier) be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to.\nSome data repositories assign a DOI for each dataset you publish on their repository. But, if you need to update the datasets, check the policy of the data repository. Some repositories assign a new DOI after you update the dataset. If this is the case, researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available.\n\n\n3.5.9 Data Citation\nResearchers should get in the habit of citing the data that they use (even if it’s their own data!) in each publication that uses that data."
  },
  {
    "objectID": "session_03.html#data-preservation-sharing",
    "href": "session_03.html#data-preservation-sharing",
    "title": "3  Data Management Essentials",
    "section": "3.6 Data Preservation & Sharing",
    "text": "3.6 Data Preservation & Sharing\n\n\n3.6.1 Data Packages\n\nWe define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve.\n\nSometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\nMany data repositories assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier, often assigned to the metadata and becomes a publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. This allows to identify a digital entity within a data package.\nIn the graphic to the side, the package can be cited with the DOI doi:10.5063/F1Z1899CZ,and each of the individual files have their own identifiers as well.\n\n\n\n3.6.2 Data Repositories: Built for Data (and code)\n\nGitHub is not an archival location\nExamples of dedicated data repositories:\n\nKNB\nArctic Data Center\ntDAR\nEDI\nZenodo\n\nDedicated data repositories are:\n\nRich in metadata\nArchival in their mission\nCertified\n\nData papers, e.g., Scientific Data\nre3data is a global registry of research data repositories\nRepository Finder is a pilot project and tool to help researchers find an appropriate repository for their work\n\n\n3.6.2.1 DataOne Federation\nDataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses.\nDataONE can be searched on the web, which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the (currently 44!) repositories one at a time."
  },
  {
    "objectID": "session_03.html#summary",
    "href": "session_03.html#summary",
    "title": "3  Data Management Essentials",
    "section": "3.7 Summary",
    "text": "3.7 Summary\n\nThe Data Life Cycle help us see the big picture of our data project.\nIt is extremely helpful to develop a data management plan describing each step of the data life cycle to stay organized.\nDocument everything. Having rich metadata is a key factor to enable data reuse. Describe your data and files and use an appropriate metadata standard.\nIdentify software and tools that will help you and your team organize and document the project’s data life cycle.\nPublish your data in a stable long live repository and assign a unique identifier."
  },
  {
    "objectID": "session_03.html#activity",
    "href": "session_03.html#activity",
    "title": "3  Data Management Essentials",
    "section": "3.8 Activity",
    "text": "3.8 Activity"
  },
  {
    "objectID": "session_03.html#familiarize-with-a-dmp",
    "href": "session_03.html#familiarize-with-a-dmp",
    "title": "3  Data Management Essentials",
    "section": "3.9 Familiarize with a DMP",
    "text": "3.9 Familiarize with a DMP\n\nLook into DPM examples\n\n\n3.9.1 How will Data Life Cycle look for your project?\nA. Look at the Data Life Cycle diagram and discuss with your neighbor ideas that come to mind on how to plan for each of the stages.\nB. Use the logic model you created during the Monday session and answer the following questions:\nNote: Remember this is just a starting point. It is ok to answer to the best of your capacity at the stage where you are with your project\n\nIs there any data going to be created? If yes describe what kind of data and how is it going to be created?\nRoles and Responsibilities\n\nWho will be the main contact person responsible to keeping the DMP up to date?\nWho will be the Data Manages?\nWho will be the person responsible for writing and maintaining metadata for the project?\n\nWhat tools and software are you going to use to access, process and check your data?\nProvide a description of the data that will be used in this project.\n\nUse template \n“Description, source, use restrictions, format, fees, quality checks, data processing and scientific workflows, backup and storage, data volume estimates, and data citation. OR Provide a description about how & where this information will be documented throughout the project.”\n\nAre there any costs associated to data or tools you plan to use?\nWhat models do you plan on using for this project? Provide a brief description.\n\nWhere will this information de documented?\n\nProvide a description/abstract of the data product for your project.\nAny ideas on how to QA/QC your data? Describe the process for managing the quality of the data throughout the project.\nWhich metadata standard ar you planning on using and why?\nHow will the metadata be created? How will this process be documented though the project?\nAccess and use constraints for your project’s data products\n\nWill the re be any requirements or legal restrictions for accessing the data products?\nWill the re be any requirements or legal restrictions for usings the data products?\n\nWhere do you envision archiving the data products for their long-term preservation?\nHow will the data be shared\n\n\n\n\n\nMichener, William K. 2015. “Ten Simple Rules for Creating a Good Data Management Plan.” PLOS Computational Biology 11 (10): 1–9. https://doi.org/10.1371/journal.pcbi.1004525."
  }
]