[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "",
    "text": "Overview"
  },
  {
    "objectID": "index.html#about-this-training",
    "href": "index.html#about-this-training",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "About this training",
    "text": "About this training\nThe NCEAS openS Program consists of three 1-week long workshops, geared towards early career researchers. Participants engage in a mix of lectures, exercises, and synthesis research activities to conduct synthesis science and implement best practices in open data science.\nopenS has been adapted to coincide with each USGS CAP training, for a total of four training sessions across 2 years."
  },
  {
    "objectID": "index.html#why-nceas",
    "href": "index.html#why-nceas",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "Why NCEAS",
    "text": "Why NCEAS\nThe National Center for Ecological Analysis and Synthesis (NCEAS), a research affiliate of UCSB, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science.\nNCEAS has over 25 years of success with this model among working groups and environmental professionals. Together with the Delta Science Program and the Delta Stewardship Council we are excited to pass along skills, workflows, mindsets learn throughout the years.\n\nWeek 3: Scaling up and presenting synthesis\nOctober 23 – 27, 2023\n\nBig data workflows and parallel computing\nPresenting results using Shiny or flexdashboards\nRevisit reproducible and git workflows\nSynthesis presentations and next steps"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS with the support of DataONE, to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors for this version are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Halina Do-Linh, Camila Vargas Poulsen. 2023. openS for USGS Climate Adaptation Postdoctoral (CAP) Fellows Program. Week One. NCEAS Learning Hub & USGS Climate Adaptation Science Centers (CASCs).\nAdditional contributors: Ben Bolker, Julien Brun, Amber E. Budden, Jeanette Clark, Samantha Csik, Stephanie Hampton, Natasha Haycock-Chavez, Samanta Katz, Julie Lowndes, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "session_01.html#learning-objectives",
    "href": "session_01.html#learning-objectives",
    "title": "1  Mapping Census Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nReview how the tidycensus package works\nGet acquaint on how to work with spatial census data\nIntroduce tools to create static and interactive maps to visualize census data\n\n\n\n\n\n\n\nAcknowledgement\n\n\n\nThis lesson is heavily based on Kyle Walker’s talk “Mapping And Spatial Analysis with ACS data in R” As part of the Census Data Workshops given at the University of Michigan in February 2023."
  },
  {
    "objectID": "session_01.html#census-data-with-tidycensus",
    "href": "session_01.html#census-data-with-tidycensus",
    "title": "1  Mapping Census Data",
    "section": "1.1 Census data with tidycensus",
    "text": "1.1 Census data with tidycensus\nThe tidycensus package (Walker and Matt (2021)) was developed to systematize the process of working with U.S Census data using R. It integrates the Census Application Programming Interface (API) released by the the U.S Census Bureau, into an R package to facilitate access to census data using R.\ntidycensus main functions:\n\nget_decennial()\nget_acs()\nget_estimates()\nget_pums()\nget_flows()\n\nMore details about these functions in the Intro to tidycensus lesson.\nDuring this lesson we will used get_acs() to access and map data from the American Community Survey (ACS).\n\n1.1.1 American Community Survey (ACS) recap\nProvides detailed demographic information about US population. Covers topics not available in decennial US Census data (e.g. income, education, language, housing characteristics). It is an annual survey of 3.5 million US households. Data is updated annually through the 1-year estimates (for geographies of population 65,000 and greater). And it is also provided as a 5-year estimate. The 5-year ACS is a moving average of data over a 5-year period that covers geographies down to the Census block group. ACS data represent estimates rather than precise counts, therefore data includes margin of error.\n\n\n\n\n\n\nNote\n\n\n\n\n2020 1-year data only available as experimental estimates. Data delivered as estimates characterized by margins of error.\nTo access data of regions with population less than 65K, you have to youse the 5-year estimates ACS.\n\n\n\n\n\n1.1.2 Review on get_acs()\nget_acs() function from tidycensus streamlines the process of working with ACS data.\n\nIt wrangles Census data internally and returns queried data in “tidy” format.\nEach request includes its associated margins of error.\nYou can filter by states and counties using their name (no more looking up FIPS codes!)”\n\nAND..\n\nAutomatically downloads and merges Census geometries to data for mapping.\n\nThis packages get the data for you, it shapes it in a format ready to go for analysis following the “tidy” principles, it pre joins the census geometries this means you get your data and spatial data automatically. And is streamlines the process of doing target requests.\n\nThe functions has three main arguments\n\ngeography: The geographic area of your data\nvariable(s): Character string or vector of character strings of variable IDs. tidycensus automatically returns the estimate and the margin of error associated with the variable.\nyear: The year, or end-year, of the ACS sample. 5-year ACS data is available from 2009 through 2021; 1-year ACS data is available from 2005 through 2021, with the exception of 2020. Defaults to the 5-year estimates ACS for the most recent year of data available. As for now: 2017-2021 5-year ACS. Note: the default might update when the 2022 ACS data is released (expected to be released in December 2023).\n\n\nFor example to get the median income for all counties in the U.S:\n\n## Median income by county. Defaults to 5-year estimates 2017-2021\"\nmedian_income_5yr &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\")\n\nIn addition yu can filter by a specific State or County by adding the argument state= and/or county= and specify a region by their name.\nFollowing the example above, if we want to get the median income for all counties in California, we have to add the agument state = \"CA\".\n\n## Median income in California by county. Defaults to 5-year estimates 2017-2021\"\nmedian_income_5yr &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  state = \"CA\")\n\nFor more information about geographies and variables in tidycensus check out Walker, 2023.\n\n\n\n\n\n\nDecennial Census\n\n\n\nComplete enumeration of the US population to assist with apportionment. It asks a limited set of questions on race, ethnicity, age, sex, and housing tenure. Data from 2000, 2010, and available data from 2020.\nAccess this data using get_decennial()"
  },
  {
    "objectID": "session_01.html#spatial-census-data-in-tidycensus",
    "href": "session_01.html#spatial-census-data-in-tidycensus",
    "title": "1  Mapping Census Data",
    "section": "1.2 Spatial Census Data in tidycensus",
    "text": "1.2 Spatial Census Data in tidycensus\nTo be able to work with “spatial” Census data you would generally have to go and find shapefiles on the Census website, download a CSV with the data, clean and format the data, load the geometries and data to your spatial data software of choice, then align the key fields and join your data with the geometries.\nAgain, tidycensus to the rescue! This packages combines all these steps and makes it very easy to get census data nd its geometries ready for analysis. Let’s see how this work.\n\n1.2.1 Spatial Census data with get_acs()\nAs usual we start by loading the libraries we are going to use today.\n\nlibrary(tidycensus)\nlibrary(mapview)\nlibrary(tigris)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)\n\n\n\n\n\n\n\nAPI Key\n\n\n\nRemember that the first time you work with tidycensus you have to connect your session with the Census data using am API key.\n\nGo to https://api.census.gov/data/key_signup.html\nFill out the form\nCheck your email for your key.\nUse the census_api_key() function to set your key. Note: install = TRUE forces r to write this key to a file in our R environment that will be read every time you use R. This means, by setting this argument to TRUE, you only have to do it once in any computer you are working. If you see this argument as FALSE, R will not remember this key next time you come back.\n\n\ncensus_api_key(\"YOUR KEY GOES HERE\", install = TRUE)\n\n\nLastly, restart R.\n\nNOTE: WE DID THIS LAST TIME SO OUR SERVER SESSION SHOULD BE GOOD TO GO.\n\n\nSo now, if we want to retrieve data for income estimates by county for California with it’s associated geometries we need to know the variable for income estimates (“B19013_001”), call get_acs() with all the necessary information and add the argument geometry = TRUE to get the spatial data for each geography.\n\n## defaults to most recent 5year estimates (2017-2021 5-year ACS)\nca_income &lt;- get_acs(\n    geography = \"county\",\n    variables = \"B19013_001\",\n    state = \"CA\",\n    year = 2021,\n    geometry = TRUE) ## This argument does all of the steps mentioned above.\n\n\n\n\n\n\n\nMissleading error message\n\n\n\nIf you are getting an error about issues with the API, MAKE SURE YOU HAVE A VALID VARIABLE CODE. If the variable code is not valid, functions in tidycensus() can not access the API.\n\n\nAnd that’s it!! Now we have the corresponding spatial data bind to our variable of interest. We can plot this data using the base r plot() function.\n\nplot(ca_income[\"estimate\"])\n\n\n\n\nNow we have our data ready to start exploring!\n\n\n1.2.2 What’s under the hood\nThe sf package. As we learned during the last training, the sf package implements a simple features data model for vector spatial data in R. This means that vector geometries: points, lines, and polygons stored in a list-column of a data frame. Making it very easy to work withe spatial data in R, just like you work with any other type of data, in a tabular format.\nLet’s take a look at our data\n\nhead(ca_income)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.4096 ymin: 33.21473 xmax: -117.4133 ymax: 42.00076\nGeodetic CRS:  NAD83\n  GEOID                         NAME   variable estimate  moe\n1 06059    Orange County, California B19013_001   100485  718\n2 06111   Ventura County, California B19013_001    94150 1310\n3 06063    Plumas County, California B19013_001    57885 4555\n4 06015 Del Norte County, California B19013_001    53280 5046\n5 06023  Humboldt County, California B19013_001    53350 2424\n6 06043  Mariposa County, California B19013_001    53304 4026\n                        geometry\n1 MULTIPOLYGON (((-118.1144 3...\n2 MULTIPOLYGON (((-119.4412 3...\n3 MULTIPOLYGON (((-121.497 40...\n4 MULTIPOLYGON (((-124.2175 4...\n5 MULTIPOLYGON (((-124.4086 4...\n6 MULTIPOLYGON (((-120.3944 3...\n\n\nWe can see that this is a Simple feature collection with 6 features and 5 fields. For those of you familiar with GIS, probably this is known terminology. But for those of you that this is all new, you can think of a feature as a simple shape on your data layer. Generally in a GIS perspective feature means a row in the data. In this case for example, Ventura County is a county and the shape of that county it self is a feature. And then, a field in GIS terminology means an attribute of the data or a column.\nSimilar how we saw last time in the Spatial Data lesson, we have a geometry column. This contains all the spatial information we need to map out data.\nA polygon is a two dimensional shape that has a perimeter and an area. A multipolygon are multiple shapes that belong to the same feature. For example if we have census data for the state of Hawaii, we will have multiple polygon, one for each island, representing that row or feature. We also can see the CRS associated to this data and the bounding box that indicates the extension of our data set.\n\n\n\n\n\n\nCoordinate Reference System (CRS)\n\n\n\nHow are the coordinates in our polygons referenced to the earth surface. It handles how the mapping of our data to the actual earth. For more information on CRS and tidycensus checkout Walker 2023\n\n\nIf we look at the other columns in our data, we have the data it self. GEOID, NAME, estimate and moe(margin of error, interpreted at 90% confidence level).\n\n\n\n\n\n\nNote on missing data\n\n\n\nRemember that 5 year ACS data are projections from a sample. Counties with no data means that the population in those counties is not large enough to make these projections.\n\n\n\n\n1.2.3 Adding interactivity\nFor a lot of GIS users it is hard to transition from GIS to working with spatial data in R because GIS provides nice interactive tools. One easy way to make your R maps interactive is the mapview() package. This package wraps up different interactive mapping tools and allows you to explore your data just by running a single line of code. Let’s try this.\n\nmapview(ca_income, zcol = \"estimate\")\n\n\n\n\n\n\nThe zcol = argument, allows us to easily plot data to this interactive map. We can explore the data using the RStudio Viewer.\nLet’s look at another example at a smaller census geography. tidycensus is really helpful to look at spatial data in smaller geography.\n\nsolano_income &lt;- get_acs(\n    geography = \"tract\",\n    variables = \"B19013_001\",\n    state = \"CA\",\n    county = \"Solano\",\n    geometry = \"TRUE\")\n\nhead(solano_income)\n\nWe can again use mapview() to check out our data.\n\nmapview(solano_income, zcol = \"estimate\")\n\n\n\n\n\n\nWith these two packages we can almost instantly explore different data from the ACS surveys. Note that for census tracts, the MOE will be much higher than for county data as estimates are extrapolated to a finer scale.\n\n\n1.2.4 Spatial data structure in tidycensus (long versus wide)\nThe default of tidycensus is to return a data frame in a “long” format. This is generally the preferred way to work and analyze data in R. But, if you rather have a “wide” data frame as the output (GIS users are generally used to wide format) you can do that by adding the argument output = wide. This will return a data frame where each variable is in a different column. For example:\n\nrace_var &lt;- c(\n    Hispanic = \"DP05_0071P\",\n    White = \"DP05_0077P\",\n    Black = \"DP05_0078P\",\n    Asian = \"DP05_0080P\")\n\n## Default long\nalameda_race &lt;- get_acs(\n  geography = \"tract\",\n  variables = race_var,\n  state = \"CA\",\n  county = \"Alameda\",\n  geometry = TRUE)\n\nhead(alameda_race)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.2588 ymin: 37.65598 xmax: -121.7804 ymax: 37.7547\nGeodetic CRS:  NAD83\n        GEOID                                             NAME variable\n1 06001451704 Census Tract 4517.04, Alameda County, California Hispanic\n2 06001451704 Census Tract 4517.04, Alameda County, California    White\n3 06001451704 Census Tract 4517.04, Alameda County, California    Black\n4 06001451704 Census Tract 4517.04, Alameda County, California    Asian\n5 06001428301 Census Tract 4283.01, Alameda County, California Hispanic\n6 06001428301 Census Tract 4283.01, Alameda County, California    White\n  estimate moe                       geometry\n1     11.5 4.3 MULTIPOLYGON (((-121.7985 3...\n2     68.8 8.1 MULTIPOLYGON (((-121.7985 3...\n3      0.0 0.9 MULTIPOLYGON (((-121.7985 3...\n4     14.6 6.6 MULTIPOLYGON (((-121.7985 3...\n5      9.9 6.1 MULTIPOLYGON (((-122.2588 3...\n6     34.2 5.3 MULTIPOLYGON (((-122.2588 3...\n\n\nAnd now in wide format. Every variable (Hispanic, White, Black and Asian) is in a different column as opposed to being stacked into one column named variable.\n\nalameda_race_wide &lt;- get_acs(\n  geography = \"tract\",\n  variables = race_var,\n  state = \"CA\",\n  county = \"Alameda\",\n  geometry = TRUE,\n  output = \"wide\")\n\nhead(alameda_race_wide)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.2978 ymin: 37.5333 xmax: -121.7804 ymax: 37.87881\nGeodetic CRS:  NAD83\n        GEOID                                             NAME HispanicE\n1 06001451704 Census Tract 4517.04, Alameda County, California      11.5\n2 06001428301 Census Tract 4283.01, Alameda County, California       9.9\n3 06001407000    Census Tract 4070, Alameda County, California      37.2\n4 06001422400    Census Tract 4224, Alameda County, California      15.1\n5 06001423200    Census Tract 4232, Alameda County, California      29.0\n6 06001444200    Census Tract 4442, Alameda County, California      32.9\n  HispanicM WhiteE WhiteM BlackE BlackM AsianE AsianM\n1       4.3   68.8    8.1    0.0    0.9   14.6    6.6\n2       6.1   34.2    5.3    4.8    4.1   43.0    6.8\n3      10.4   18.3    6.7   15.2    5.3   18.2    7.5\n4       4.4   36.5    6.6    2.7    2.0   39.5    9.7\n5       7.9   33.3    8.2   20.5    8.6   11.4    5.6\n6       6.5   22.3    4.5    0.4    0.5   37.6    5.8\n                        geometry\n1 MULTIPOLYGON (((-121.7985 3...\n2 MULTIPOLYGON (((-122.2588 3...\n3 MULTIPOLYGON (((-122.2098 3...\n4 MULTIPOLYGON (((-122.2737 3...\n5 MULTIPOLYGON (((-122.2978 3...\n6 MULTIPOLYGON (((-122.0552 3...\n\n\nBoth data frames alameda_race and alameda_race_wide have the same exact information. They are just in a different shape. Depending on what are you want to do with the data which one you should retrieve."
  },
  {
    "objectID": "session_01.html#working-with-census-geometry",
    "href": "session_01.html#working-with-census-geometry",
    "title": "1  Mapping Census Data",
    "section": "1.3 Working with Census Geometry",
    "text": "1.3 Working with Census Geometry\n\n\n\n\n\n\n\n\n“Census and ACS data are associated with geographies, which are units at which the data are aggregated. These defined geographies are represented in the US Census Bureau’s TIGER/Line database, where the acronym TIGER stands for Topologically Integrated Geographic Encoding and Referencing. This database includes a high-quality series of geographic datasets suitable for both spatial analysis and cartographic visualization . Spatial datasets are made available as shapefiles, a common format for encoding geographic data.” (Walker 2023, Chapter 5)\n\n\ntidycensus uses the tigris R package internally to acquire Census shapefiles\nBy default, the Cartographic Boundary shapefiles are used, which are pre-clipped to the US shoreline\ntigris offers a number of features to help with acquisition and display of spatial ACS data. Making your work with ACS data better.\n\nLet’s go back to our map of Solano County.\n\nsolano_income &lt;- get_acs(\n    geography = \"tract\",\n    variables = \"B19013_001\",\n    state = \"CA\",\n    county = \"Solano\",\n    geometry = \"TRUE\")\n\nmapview(solano_income, zcol = \"estimate\")\n\n\n\n\n\n\nWe can see that there are some issues with the interior water areas. They are often not removed from the Cartographic Boundary shapefiles. What can we do about it? We can again leverage on how powerful these tools are on making complex process simple.\nThere is a function in the tigris package that “erase water”! It finds water areas in a shapefile and removes those water areas, giving you a result that allows you to better display your data. Let’s take a look on how this works.\n\nsf_use_s2(FALSE) ## Need to run this so that mapview works.\n\nsolano_erase &lt;- erase_water(solano_income,\n                            year = 2021) ## year to use for water layer\n\nmapview(solano_erase, zcol = \"estimate\")\n\n\n\n\n\n\nAnd just like that! We get a much more accurate map of Solano County."
  },
  {
    "objectID": "session_01.html#mapping-acs-data",
    "href": "session_01.html#mapping-acs-data",
    "title": "1  Mapping Census Data",
    "section": "1.4 Mapping ACS data",
    "text": "1.4 Mapping ACS data\nThere are a several extraordinary packages in R to visualize cartographic data. Today we are going to be using our good ol’ friend ggplot2. In the last section of this lesson you can find resources to other cartography mapping packages like tmap.\nThere is a reason why we use ggplot2 over and over throughout the lessons in this course. It is a very powerful data visualization tool! In fact, is one of the most downloaded packages in R. And, as we learned in the “working with spatial data” lesson, there is a function called geom_sf() that allows us to easily plot spatial data.\nHow do we plot ACS data using ggplot?\nLets make a map with the Hispanic population in Alameda County by Census tract.\nSe we are going to use the alameda_race object we created earlier. And we are going to start by filtering the data for Hispanic population.\n\nalameda_hispanic &lt;- filter(alameda_race,\n                           variable == \"Hispanic\")\n\nggplot(alameda_hispanic,\n       aes(fill = estimate))+\n    geom_sf() ## plots polygons!\n\n\n\n\nHere we have our choropleth map with the Hispanic population in Alameda County! A choropleth plot provides a shade or color to a polygon (or shape) according to a giving attribute (e.g. The percentage of Hispanic population)\nHere we are mapping the estimate column to fill the shape we are plotting, in this case the tract polygon. The geom_sf() plots polygons!\nA choropleth is a map that uses shading to show variation in some sort of data attribute. In this case, the lighter colors represent higher values, this means that tract with lighter shades of blue have higher Hispanic population. And the darker ares represent the lower values, fewer presence of Hispanic/Latino population.\nAs we know, with ggplot2 we can heavily style our plot. Here an example of customization.\n\nggplot(alameda_hispanic, aes(fill = estimate)) + \n  geom_sf() + \n  theme_void() + \n  scale_fill_viridis_c(option = \"rocket\") + \n  labs(title = \"Percent Hispanic by Census tract\",\n       subtitle = \"Alameda County, California\",\n       fill = \"ACS estimate\",\n       caption = \"2017-2021 ACS | tidycensus R package\")\n\n\n\n\nYou can also plot you data in bins instead of a continuous scale.\n\nggplot(alameda_hispanic, aes(fill = estimate)) + \n  geom_sf() + \n  theme_void() + \n  scale_fill_viridis_b(option = \"rocket\", n.breaks = 6) + \n  labs(title = \"Percent Hispanic by Census tract\",\n       subtitle = \"Alameda County, California\",\n       fill = \"ACS estimate\",\n       caption = \"2017-2021 ACS | tidycensus R package\")\n\n\n\n\nWhich style to use will depends on what you want to achieve. We can see that in the plot with bins we loose some resolution. On the other hand the continuous scale can provide a little of a color over load.\nWe can keep leveraging on ggplot2 power and plot more variables of our data. For example create a map for each of the difference races on our data.\n\nggplot(alameda_race, aes(fill = estimate)) + \n  geom_sf(color = NA) +  ## removes delimitation of each tract\n  theme_void() + \n  scale_fill_viridis_c(option = \"rocket\") + \n  facet_wrap(~variable) +\n  labs(title = \"Race / ethnicity by Census tract\",\n       subtitle = \"Alameda County, California\",\n       fill = \"ACS estimate (%)\",\n       caption = \"2017-2021 ACS | tidycensus R package\")\n\n\n\n\n\n\n1.4.1 Mapping Count Data\nSo far we have been mapping percentage. But what if your data is not percentage but count? Choropleth are great for mapping ratios and percentage, but not so great for mapping counts. When you are working with count data, you wanna have a way to represent the extent of the count through symbols. We are going to show this with an example.\nWe start by getting the count data for race/ethnicity. Note that the process is practically the same that we did above, but there is a slight difference in the variable codes we are going to use. Generally, variables that end in “P” means the estimate is in percentage. Variable with out the “P” at the end are count data.\n\nalameda_race_counts &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    Hispanic = \"DP05_0071\",\n    White = \"DP05_0077\",\n    Black = \"DP05_0078\",\n    Asian = \"DP05_0080\"),\n  state = \"CA\",\n  county = \"Alameda\",\n  geometry = TRUE)\n\n## Checking our data. Estimates are in counts not in %\nhead(alameda_race_counts)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.2588 ymin: 37.65598 xmax: -121.7804 ymax: 37.7547\nGeodetic CRS:  NAD83\n        GEOID                                             NAME variable\n1 06001451704 Census Tract 4517.04, Alameda County, California Hispanic\n2 06001451704 Census Tract 4517.04, Alameda County, California    White\n3 06001451704 Census Tract 4517.04, Alameda County, California    Black\n4 06001451704 Census Tract 4517.04, Alameda County, California    Asian\n5 06001428301 Census Tract 4283.01, Alameda County, California Hispanic\n6 06001428301 Census Tract 4283.01, Alameda County, California    White\n  estimate moe                       geometry\n1      494 181 MULTIPOLYGON (((-121.7985 3...\n2     2945 551 MULTIPOLYGON (((-121.7985 3...\n3        0  13 MULTIPOLYGON (((-121.7985 3...\n4      627 277 MULTIPOLYGON (((-121.7985 3...\n5      729 501 MULTIPOLYGON (((-122.2588 3...\n6     2523 485 MULTIPOLYGON (((-122.2588 3...\n\n\nThe first map we are going to plot is a graduate symbol map. This kind of maps are good for count data because the comparison we are making are between symbols of the same shape. The size of the symbol is proportional to the underlying data value. The most common shape to use for this kind of plots are circles.\nThe tricky thing here, and this also speaks to really understanding our data and what we are trying to plot, is that our data is represented as polygons and we want to map points or circle. So we need to convert our data from polygons to circle.\nAs a reminder, polygons are closed shapes with a perimeter and an area. We have to convert this shape to a single point and draw a circle proportional to the corresponding data value.\nThere is a function from the sf package that allows us to do this. This function is st_centroid(). This function converts a shape, for example the shape of a census tract to a point, right in the center of that tract. So lets convert part of our Alameda race data to centroids. We are going to filter for the Asian population.\n\nalameda_asian &lt;- alameda_race_counts %&gt;% \n    filter(variable == \"Asian\")\n\n\ncentroids &lt;- st_centroid(alameda_asian)\n\n\n\n\n\n\n\nWarning message:\n\n\n\nst_centroid assumes attributes are constant over geometries\nThis message is letting us know that you are converting a polygon to a single point, and this point might not truly represent where people in this tract live. Just a heads up of what is happening.\n\n\nNow we plot. Note that we are plotting two layers. One with the polygons to provide context to our data and the other one with the actual data transformed into centroids.\n\nggplot() + \n  geom_sf(data = alameda_asian, color = \"black\", fill = \"lightgrey\") + \n  geom_sf(data = centroids, aes(size = estimate),\n          alpha = 0.7, color = \"navy\") + \n  theme_void() + \n  labs(title = \"Asian population by Census tract\",\n       subtitle = \"2017-2021 ACS, Alameda County, California\",\n       size = \"ACS estimate\") + \n  scale_size_area(max_size = 6)\n\n\n\n\nscale_size_area() argument makes the area of the circles proportional. In this case the area representing 2500 is about half of the area of the 5000 circle. Overall, areas with smaller circles are areas with less Asian population and areas with larger circles have a larger Asian population. This kind of maps makes it easier to visualize change across the different area. For example, larger counts with a very low Asian population are represented with a small circle instead of painting the whole are with a color that represents a low population.\nWe can compere location of a point and size according to the estimated value of the population.\nAnother way of plotting count data is with a dot-density map. This kind of maps excel at plotting multiple variables in one map. On the graduate symbol map, we were able to plot the Asian population and clearly see how it changes among census tract. However, the graduate symbol map doesn’t really allow as to to plot heterogeneity, or the mixing of different racial groups in this case. For example to see how different groups live together or apart. Dot-density maps scatter dots proportionally to data size; dots can be colored to show mixing of categories.\nThe as_dot_density() function allows you to calculate these density dots based on your data. It is design for categorical mapping of ACS and Census Decennial data, taking data in a long format as an input. The other arguments relevant to this function are:\n\nvalue = assigning the column in our data frame that has the values we want to transform to dots. In this case our estimate column.\nvalues_per_dot = dot to data ratio, how many data points does each dot represent. In this case values_per_dot = 200 means that each dot represents 200 people.\ngroup = is an argument that allow as to group out data. In this case group = \"variable\" groups the data by each of the categories in the variable column and creates a dot for each of those categories.\n\n\nalameda_race_dots &lt;- as_dot_density(\n  alameda_race_counts,\n  value = \"estimate\",\n  values_per_dot = 200,\n  group = \"variable\"\n)\n\nalthough coordinates are longitude/latitude, st_intersects assumes that they\nare planar\n\n\nThere are a lot of calculations happening under the hood here. What this function is doing is scattering dots with each census tract proportional to the number of people that are in each group (in this case groups are defined by the categories in the variable column).\nLet’s look at the outcome data. We can already see that this data frame has many more rows than our input data. This is because, each row in this case just represents up to 200 people, as we defined in the values_per_dot argument. We can also see that we have a geometry type POINT.\n\nhead(alameda_race_dots)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -122.2881 ymin: 37.59466 xmax: -121.8896 ymax: 37.89278\nGeodetic CRS:  NAD83\n        GEOID                                             NAME variable\n1 06001450607 Census Tract 4506.07, Alameda County, California    Asian\n2 06001432100    Census Tract 4321, Alameda County, California    White\n3 06001403900    Census Tract 4039, Alameda County, California    White\n4 06001420100    Census Tract 4201, Alameda County, California    White\n5 06001405800    Census Tract 4058, Alameda County, California    White\n6 06001440331 Census Tract 4403.31, Alameda County, California    Asian\n  estimate moe                   geometry\n1     2484 672 POINT (-121.8896 37.66138)\n2     1349 201 POINT (-122.1489 37.73378)\n3     2430 707 POINT (-122.2467 37.81675)\n4     1331 224 POINT (-122.2881 37.89278)\n5      459 198 POINT (-122.2313 37.79098)\n6     1708 288 POINT (-122.0834 37.59466)\n\n\nNow we can plot this data using the same workflow than our previews map.\n\nggplot() + \n  geom_sf(data = alameda_race_counts, color = \"lightgrey\", fill = \"white\") + \n  geom_sf(data = alameda_race_dots, aes(color = variable), size = 0.5, alpha = 0.8) +\n  scale_color_brewer(palette = \"Set2\") + \n  guides(color = guide_legend(override.aes = list(size = 3))) + ## overrides the size of the dots in the legend to make it more visible\n  theme_void() + \n  labs(color = \"Race / Ethnicity\",\n       caption = \"2017-2021 ACS | 1 dot = approximately 200 people\")\n\n\n\n\nHere we have our map. Each dot represent 200 people and each color a race or ethnicity. This allows us to wee the distribution through out the county and the areas with more racial mix and areas where one race predominates.\nThese are some of the examples of plotting census data or ACS in this case using static maps. There is a lot more to cover when talking about maps! So we encourage you to check out the resources linked below."
  },
  {
    "objectID": "session_01.html#resources",
    "href": "session_01.html#resources",
    "title": "1  Mapping Census Data",
    "section": "1.5 Resources",
    "text": "1.5 Resources\n\nThe tmap package (Tennekes 2018) is an alternative to ggplot2 for creating custom maps. T stands for “Thematic”, refering to the phenomena that is shown or plotted, for example demographical, social, cultural, or economic phenomena. This package includes a wide range of functionality for custom cartography. Example of tmap and tidycensus in Walker 2023, Chapter 6\nReactive mapping with Shiny\nSpatial Analysis with Census Data, Walker 2023, Chapter 7\nModeling Census Data, Walker 2023 Chapter 8. Indices for segregation and diversity are addresed in this chapter."
  },
  {
    "objectID": "session_01.html#your-turn",
    "href": "session_01.html#your-turn",
    "title": "1  Mapping Census Data",
    "section": "1.6 Your Turn",
    "text": "1.6 Your Turn\nNow is your turn to make some maps!\n\n\n\n\n\n\nExercise\n\n\n\n\nUse the load_variables() function to find one or more variables of your interest.\n\n\n\nAnswer\nvars_acs5 &lt;- load_variables(2021, \"acs5\")\n\n\n\nUse get_acs() to get spatial ACS data for the variable you selected in a location and geography of your choice.\n\n\n\nAnswer\n## Data for median gross rent by county in CA\nca_rent &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B25031_001\",\n  state = \"CA\",\n  year = 2021,\n  geometry = TRUE)\n\n## Data for median household income by county in CS\nca_income_county &lt;- get_acs(\n    geography = \"county\",\n    variables = \"B19013E_001\",\n    state = \"CA\",\n    year = 2021,\n    geometry = TRUE)\n\n\n\nUse any of the resources presented above to map the data.\nShare your maps on Slack!"
  },
  {
    "objectID": "session_02.html#learning-objective",
    "href": "session_02.html#learning-objective",
    "title": "2  Communicating Your Science",
    "section": "Learning Objective",
    "text": "Learning Objective\n\nDiscuss about the importance of science communication.\nDistinguish between how scientist communicate sciences vs how the rest of the world communicates.\nIntroduce and practice using the Message Box as a tool to communicate science to a specific audience."
  },
  {
    "objectID": "session_02.html#communicating-science",
    "href": "session_02.html#communicating-science",
    "title": "2  Communicating Your Science",
    "section": "2.1 Communicating Science",
    "text": "2.1 Communicating Science\nHow we communicate our science with the rest of the world matters! There are many different reasons and ways to engage and practice science communication. From writing a blog post as a creative outlet to posting in social media to make science accessible to a broader audience. Science communication is more and more becoming a formal responsibility within researchers as funding organizations encourage or require scientist to do outreach of their work (Borowiec (2023)).\nCommunication can come easy, but doing it well is hard. As scientist, it is important to spread the word of our discoveries, engage with non-scientist audiences, and build empathy and trust on what we do (PLOS SciCom). This way make sure the world understand how important, necessary and meaningful science is.\n\n\n\nJarreau, Paige B (2015): #MySciBlog Interviewee Motivations to Blog about Science\n\n\n\n\n2.1.1 Scholarly publications\n\n“The ingredients of good science are obvious—novelty of research topic, comprehensive coverage of the relevant literature, good data, good analysis including strong statistical support, and a thought-provoking discussion. The ingredients of good science reporting are obvious—good organization, the appropriate use of tables and figures, the right length, writing to the intended audience— do not ignore the obvious” - Bourne 2005\n\nPeer-reviewed publication is the primary mechanism for direct and efficient communication of research findings. Other scholarly communications include abstracts, technical reports, books and book chapters. These communications are largely directed towards students and peers; individuals learning about or engaged in the process of scientific research whether in a university, non-profit, agency, commercial or other setting.\nThe following tabling is a good summary of ‘10 Simple Rules’ for writing research papers (adapted from Zhang 2014, published in Budden and Michener, 2017)\n\n\n\n\n\n\n\n\nMake it a Driving Force\n\n“Design a project with an ultimate paper firmly in mind”\n\n\n\nLess Is More\n\n“Fewer but more significant papers serve both the research community and one’s career better than more papers of less significance”\n\n\n\nPick the Right Audience\n\n“This is critical for determining the organization of the paper and the level of detail of the story, so as to write the paper with the audience in mind.”\n\n\n\nBe Logical\n\n“The foundation of ‘’lively’’ writing for smooth reading is a sound and clear logic underlying the story of the paper.” “An effective tactic to help develop a sound logical flow is to imaginatively create a set of figures and tables, which will ultimately be developed from experimental results, and order them in a logical way based on the information flow through the experiments.”\n\n\n\nBe Thorough and Make It Complete\n\nPresent the central underlying hypotheses; interpret the insights gleaned from figures and tables and discuss their implications; provide sufficient context so the paper is self-contained; provide explicit results so readers do not need to perform their own calculations; and include self-contained figures and tables that are described in clear legends\n\n\n\nBe Concise\n\n“the delivery of a message is more rigorous if the writing is precise and concise”\n\n\n\nBe Artistic\n\n“concentrate on spelling, grammar, usage, and a ‘’lively’’ writing style that avoids successions of simple, boring, declarative sentences”\n\n\n\nBe Your Own Judge\n\nReview, revise and reiterate. “…put yourself completely in the shoes of a referee and scrutinize all the pieces—the significance of the work, the logic of the story, the correctness of the results and conclusions, the organization of the paper, and the presentation of the materials.”\n\n\n\nTest the Water in Your Own Backyard\n\n“…collect feedback and critiques from others, e.g., colleagues and collaborators.”\n\n\n\nBuild a Virtual Team of Collaborators\n\nTreat reviewers as collaborators and respond objectively to their criticisms and recommendations. This may entail redoing research and thoroughly re-writing a paper.\n\n\n\nEven though reporting our results in a clear and efficient way through scholarly publications is important, the ways scientist communicate is not how the rest of the rest of the world typically communicates. In a scientific paper, we establish credibility in the introduction and methods, provide detailed data and results, and then share the significance of our work in the discussion and conclusions. But the rest of the world leads with the impact, the take home message. An example of this are newspaper headlines.\n“But the rest of the world leads with the conclusions, because that’s what people want to know. What are your findings and why is this relevant to them? In other words, what’s your bottom line?”\n\n\n\n2.1.2 Other communications\nCommunicating your research outside of peer-reviewed journal articles is increasingly common, and important. These non academic communications can reach a more broad and diverse audience than traditional publications (and should be tailored for specific audience groups accordingly), are not subject to the same pay-walls as journal articles, and can augment and amplify scholarly publications. For example, this twitter announcement from Dr Heather Kropp, providing a synopsis of their synthesis publication in Environmental Research (note the reference to a repository where the data are located, though a DOI would be better).\n\nWhether this communication occurs through blogs, social media, or via interviews with others, developing practices to refine your messaging is critical for successful communication. One tool to support your communication practice is ‘The Message Box’ developed by COMPASS, an organization that helps scientists develop communications skills in order to share their knowledge and research across broad audiences without compromising the accuracy of their research."
  },
  {
    "objectID": "session_02.html#the-message-box",
    "href": "session_02.html#the-message-box",
    "title": "2  Communicating Your Science",
    "section": "2.2 The Message Box",
    "text": "2.2 The Message Box\n\n\nThe Message Box is a tool that helps researchers take the information they hold about their research and communicate it in a way that resonates with the chosen audience. It can be used to help prepare for interviews with journalists or employers, plan for a presentation, outline a paper or lecture, prepare a grant proposal, or clearly, and with relevancy, communicate your work to others. While the message box can be used in all these ways, you must first identify the audience for your communication.\n“Whether a journalist, a policymaker, a room of colleagues at a professional meeting, or a class of second-graders—doesn’t have deep knowledge of your subject matter. But that doesn’t mean you should explain everything you know in a fire hose of information. In fact, cognitive research tells us that the human brain can only absorb three to five pieces of information at a time. So prioritize what you share based on what your audience needs to know. Your goal as you fill out your Message Box is to identify the information that is critical to your audience—what really matters to them—and share that. Effective science communication is less about expounding on all the exciting details that you might want to convey, and more about knowing your audience and providing them with what they need or want to know from you.”\nThe Message Box comprises five sections to help you sort and distill your knowledge in a way that will resonate with your (chosen) audience.\nThe five sections of the Message Box are provided below. For a detailed explanation of the sections and guidance on how to use the Message Box, work through the Message Box Workbook\n\n2.2.1 Message Box Sections\nThe Issue\nThe Issue section in the center of the box identifies and describes the overarching issue or topic that you’re addressing in broad terms. It’s the big-picture context of your work. This should be very concise and clear; no more than a short phrase. You might find you revisit the Issue after you’ve filled out your Message Box, to see if your thinking on the overarching topic has changed since you started.\n\nDescribes the overarching issue or topic: Big Picture\nBroad enough to cover key points\nSpecific enough to set up what’s to come\nConcise and clear\n‘Frames’ the rest of the message box\n\nThe Problem\nThe Problem is the chunk of the broader issue that you’re addressing in your area of expertise. It’s your piece of the pie, reflecting your work and expert knowledge. Think about your research questions and what aspect of the specific problem you’re addressing would matter to your audience. The Problem is also where you set up the So What and describe the situation you see and want to address.\n\nThe part of the broader issue that your work is addressing\nBuilds upon your work and expert knowledge\nTry to focus on one problem per audience\nOften the Problem is your research question\nThis section sets you up for So What\n\nThe So What\nThe crux of the Message Box, and the critical question the COMPASS team seeks to help scientists answer, is “So what?” Why should your audience care? What about your research or work is important for them to know? Why are you talking to them about it? The answer to this question may change from audience to audience, and you’ll want to be able to adjust based on their interests and needs. We like to use the analogy of putting a message through a prism that clarifies the importance to different audiences. Each audience will be interested in different facets of your work, and you want your message to reflect their interests and accommodate their needs. The prism below includes a spectrum of audiences you might want to reach, and some of the questions they might have about your work.\n\nThis is the crux of the message box\nWhy should you audience care?\nWhat about your research is important for them to know?\nWhy are you talking to them about it?\n\n\nThe Solution\nThe Solution section outlines the options for solving the problem you identified. When presenting possible solutions, consider whether they are something your audience can influence or act upon. And remind yourself of your communication goals: Why are you communicating with this audience? What do you want to accomplish?\n\nOutlines the options for solving the Problem\nCan your audience influence or act upon this?\nThere may be multiple solutions\nMake sure your Solution relates back to the Problem. Edit one or both as needed\n\nThe Benefit\nIn the Benefit section, you list the benefits of addressing the Problem — all the good things that could happen if your Solution section is implemented. This ties into the So What of why your audience cares, but focuses on the positive results of taking action (the So What may be a negative thing — for example, inaction could lead to consequences that your audience cares about). If possible, it can be helpful to be specific here — concrete examples are more compelling than abstract. Who is likely to benefit, and where, and when?\n\nWhat are the benefits of addressing the Problem?\nWhat good things come from implementing your Solution?\nMake sure it connects with your So What\nBenefits and So What may be similar\n\nFinally, to make you message more memorable you should:\n\nSupport your message with data\nLimit the use of numbers and statistics\nUse specific examples\nCompare numbers to concepts, help people relate\nAvoid jargon\nLead with what you know\n\n\nIn addition to the Message Box Workbook, COMPASS have resources on how to increase the impact of your message (include important statistics, draw comparisons, reduce jargon, use examples), exercises for practicing and refining your message and published examples."
  },
  {
    "objectID": "session_02.html#resources",
    "href": "session_02.html#resources",
    "title": "2  Communicating Your Science",
    "section": "2.3 Resources",
    "text": "2.3 Resources\n\n\n\nDataONE Webinar: Communication Strategies to Increase Your Impact from DataONE on Vimeo.\n\n\nBudden, AE and Michener, W (2017) Communicating and Disseminating Research Findings. In: Ecological Informatics, 3rd Edition. Recknagel, F, Michener, W (eds.) Springer-Verlag\nCOMPASS Core Principles of Science Communication\nExample Message Boxes"
  },
  {
    "objectID": "session_02.html#your-turn",
    "href": "session_02.html#your-turn",
    "title": "2  Communicating Your Science",
    "section": "2.4 Your Turn",
    "text": "2.4 Your Turn\nLet’s take a look on how the Message Box looks in practice.\n\n\n\n\n\n\nExercise\n\n\n\n\nLook into real examples of scienctist using the message box here.\nThink about your synthesis project and a potential audience you would like to communicate the results. Define your audience and start filling in the different components of the Message Box.\n\nNote: This is just the first iteration to help your think about your work in a different way.\n\n\n\n\n\n\nBorowiec, Brittney G. 2023. “Ten Simple Rules for Scientists Engaging in Science Communication.” PLOS Computational Biology 19 (7): 1–8. https://doi.org/10.1371/journal.pcbi.1011251."
  },
  {
    "objectID": "session_03.html",
    "href": "session_03.html",
    "title": "3  Hands On Synthesis",
    "section": "",
    "text": "Note\n\n\n\nIn this session, groups convene to collaborate on their respective synthesis projects. The objective for the week is to establish a well-defined roadmap for advancing each project and determine the methods for effectively communicating the results."
  },
  {
    "objectID": "session_04.html#learning-objectives",
    "href": "session_04.html#learning-objectives",
    "title": "4  Flexdashboard",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCreate and design customized dashboards using the R package flexdashboard\nBecome familiar with different flexdashboard components and flexdashboard syntax\nApply Markdown syntax, Shiny elements, and additional R packages like plotly to create visually appealing and interactive flexdashboards"
  },
  {
    "objectID": "session_04.html#what-is-a-flexdashboard",
    "href": "session_04.html#what-is-a-flexdashboard",
    "title": "4  Flexdashboard",
    "section": "4.1 What is a flexdashboard?",
    "text": "4.1 What is a flexdashboard?\nflexdashboard is an R package from RStudio that is built on top of R Markdown and Shiny. It allows us to create flexible, interactive dashboards using simple Markdown documents and syntax. Flexdashboards are designed to be easy to create, and support a wide variety of visualizations and interactive components. We can incorporate Shiny widgets and functionality into flexdashboards, making it a powerful tool for creating interactive reports and dashboards that can be shared with others.\n\n\n\n\n\n\nFlexdashboard is for R Markdown Only\n\n\n\nflexdashboard is only compatible with R Markdown documents, meaning we can’t use Quarto markdown documents (any files ending with .qmd). And that’s okay! Remember just because Quarto exists now, doesn’t mean R Markdown is going away or won’t be maintained - see Yihui Xie’s blog post With Quarto Coming, is R Markdown Going Away? No..\nIf you’re invested in only using Quarto tools - keep an eye on this discussion. Quarto developers are actively working on a dashboarding feature for Quarto as well."
  },
  {
    "objectID": "session_04.html#flexdashboard-vs-shiny",
    "href": "session_04.html#flexdashboard-vs-shiny",
    "title": "4  Flexdashboard",
    "section": "4.2 Flexdashboard vs Shiny",
    "text": "4.2 Flexdashboard vs Shiny\n\n\n\n\n\n\nWhen should I make a flexdashboard?\n\n\n\nFlexdashboards are great for creating lightweight interactive dashboards that require minimal coding expertise (must be familiar with Markdown!). Ultimately, it depends on what your final product is and what skillset your team has. Check out the diagram below and see what scenarios resonate best with you and your project goals.\n\n\n\n\n\n\nflowchart TD\n    A[Goal: Create a web-based application for data interaction]\n    A --&gt; B{Shiny App}\n    A --&gt; C{Flexdashboard}\n    C --&gt; D(Build a Flexdashboard if:)\n    B --&gt; E(Build a Shiny App if:)\n    D --&gt; F[Interested in quickly creating a dashboard prototype]\n    F --&gt; G[Have a preference for R Markdown]\n    G --&gt; H[There are non-programmers who need to create or maintain dashboards]\n    H --&gt; I[Want to blend narrative text with interactivity]\n    I --&gt; J[Prefer a simpler, code-light approach]\n    J --&gt; K[Dashboard requirements are relatively static]\n    E --&gt; L[Dashboard requires a highly customized user interface]\n    L --&gt; M[Dashboard needs to perform complex data analysis with user inputs]\n    M --&gt; N[Dashboard requires real-time data updates]\n    N --&gt; O[There are programmers familiar with reactive programming in R to create or maintain dashboards]\n    O --&gt; P[Dashboard requires a multi-page app with navigation]"
  },
  {
    "objectID": "session_04.html#flexdashboard-layout-features",
    "href": "session_04.html#flexdashboard-layout-features",
    "title": "4  Flexdashboard",
    "section": "4.3 Flexdashboard Layout + Features",
    "text": "4.3 Flexdashboard Layout + Features\nNow let’s familiarize ourselves with how an .Rmd is structured to create a flexdashboard and what the dashboard output looks like using the default template.\nThere are a two default templates for flexdashboard in RStudio - one with a theme and one without. We’ll first look at the template without a theme. To create a flexdashboard .Rmd from a template click:\nFile -&gt; New File -&gt; R Markdown -&gt; From Template -&gt; Flex Dashboard\n\n\n\n\n\n\n\nFlexdashboard Syntax\nIn the flexdashboard template to create the different sections in the dashboard, dashes (---) and equal signs (===) are being used (we’ll see the equal signs in action during the demo). The series of dashes and equal signs were a design choice by the flexdashboard creators to make the different sections stand out in the .Rmd, but are not mandatory to use.\n\n\n\n\n\n\nThe rule of thumb is that level-one headers create pages, level-two headers create columns or rows, and level-three headers create boxes.\n\n\n\n\n\n\n\n\n\n\nFlexdashboard Syntax\nEquivalent Markdown Header Syntax\n\n\n\n\nPage\n==========================\n# Page\n\n\nColumn\n--------------------------\n## Column\n\n\n### Box\n### Box\n\n\n\n\n\nFlexdashboard Attributes\nIn flexdashboard, we can add certain attributes to columns, rows, and boxes. This is similar to adding attributes to headings in typical .Rmd documents. For example, if we didn’t want a subheading to be numbered in a rendered HTML of a .Rmd, we would use ## My Subheading {.unnumbered}.\nIn both typical .Rmd and flexdashboard, the syntax for attributes is {key=value} or {.attribute}.\nSome attributes to add to columns, rows, or boxes include:\n\n\n\n\n\n\n\n{data-width=} and {data-height=}\nboth of these attributes set the relative size of columns, rows, and boxes. See complete size documentation on the flexdashboard website\n\n\n{data-orientation=}\nsets the dashboard layout to either rows or columns. This is a global option set in the YAML. However, if your dashboard has multiple pages and you want to specify the orientation for each page, remove orientation: from the YAML and use this attribute instead\n\n\n{.tabset}\ndivide columns, rows, or charts into tabs\n\n\n{.sidebar}\ncreates a sidebar on the left side. This sidebar is typically used to place Shiny inputs and is an optional step to add Shiny elements to a flexdashboard. See full documentation and steps in section 5.3.1 Getting Started in the R Markdown: Definitive Guide\n\n\n{data-navmenu=\"name of page\"}\nthis attribute creates a new navigation bar heading with the specified page as an item in a drop-down menu. When clicked, the menu item takes you to the associated page. For example, if the syntax is # Foo {data-navmenu=\"Bar\"}, “Bar” becomes a new heading in the navigation bar, and “Foo” is a page with dashboard components listed as a drop-down menu item under “Bar”\n\n\n{.hidden}\nexcludes a specific page from the navigation bar\n\n\n\n\n\n\n\n\n\nTo add multiple attributes within a set of curly braces {} by separating the attributes by either a space or a comma.\n\n\n\n\n\nFlexdashboard Components\nThe different components that can be added to a flexdashboard are:\n\n\n\n\n\n\n\nHTML Widgets\nincorporates JavaScript data visualization tools and libraries into a flexdashboard. This includes features like interactive plots, maps and more. At this time there are 130 htmlwidgets available to use in R, check out the gallery of widgets\n\n\nR Graphics\nany chart, plot or graph that is created using any R package\n\n\nTabular Data\nadd tables using knitr::kable() for simple tables or use the DT package for interactive tables\n\n\nValue Boxes\nuse valueBox() to display single values along with a title and optional icon\n\n\nGauges\ngauges are a type of data visualization that displays values on a meter within a specified range\n\n\nNavigation Bar\nthe navigation bar automatically includes the title, author, and date (if specified in the YAML). New pages are added to the navigation bar starting on the left side. There is also an option to add links to social media and the source code (specify this in the YAML)\n\n\nText\ntext can be added either at the top of the .Rmd before the setup chunk or in a box\n\n\n\n\n\n\n\n\n\nLearn more about flexdashboard components in the flexdashboard documentation for components on the flexdashboard website."
  },
  {
    "objectID": "session_04.html#demo-creating-a-flexdashboard",
    "href": "session_04.html#demo-creating-a-flexdashboard",
    "title": "4  Flexdashboard",
    "section": "4.4 Demo: Creating a flexdashboard",
    "text": "4.4 Demo: Creating a flexdashboard\n\n\n\n\n\n\nSetup\n\n\n\nFork the NCEAS/flexdashboard-demo-lh repository from the NCEAS GitHub organization and use the materials in the Git repo to follow along with the demonstration of flexdashboard examples.\n\n\nThe demonstration will include examples that showcase different flexdashboard features:\n\nBasic Flexdashboard from Template\nInteractive and Multiple Pages Flexdashboard\nReactive Flexdashboard using shiny elements\nThemed Flexdashboard using bslib\n\n\n\n\n\n\n\nExercise: Your turn!\n\n\n\nIn the Themed Flexdashboard, use the palmerpenguins data to complete the following tasks:\n\nFill in the boxes:\n\nIn the Chart A box, add a scatterplot of your choosing.\nIn the Chart B box, add a table using either kable() or DT.\nIn the Chart C box, add a valueBox.\nOptional Explore the htmlwidgets for R gallery, choose one you like and replace Chart D with that widget.\n\nChange the theme using bslib::bs_themer(). To activate the Theme Customizer, complete these steps:\n\nAdd to the YAML runtime: shiny\nAdd bslib::bs_themer() to the setup chunk\nSave the .Rmd\nClick “Run Document” and open the dashboard in a browser window for optimal experience\n\nAdd a new page then create a second Page using {data-navmenu}.\n\nNote: You’re welcome to use the code from the demo so you can quickly start playing with the different flexdashboard features."
  },
  {
    "objectID": "session_04.html#publishing-a-flexdashboard",
    "href": "session_04.html#publishing-a-flexdashboard",
    "title": "4  Flexdashboard",
    "section": "4.5 Publishing a Flexdashboard",
    "text": "4.5 Publishing a Flexdashboard\n\n\n\n\n\nPublish Button in RStudio IDE\n\n\n\nIf your flexdashboard does not have any Shiny components you can publish your flexdashboard using:\n\nRStudio IDE using the Publish Button and select a destination to publish to. See Posit’s documentation.\nGitHub Pages. Recall the lesson from Week Two’s coursebook, Publishing your analysis to the web with GitHub Pages.\n\n\n\nIf your flexdashboard does have Shiny components you will need to publish to shinyapps.io. This can be done using:\n\nRStudio IDE using the Publish Button.\nThe rsconnect package using rsconnect::deployApp().\n\n\n\n\n\n\n\nNote: You will need to create shinyapps.io Account first to publish to shinyapps.io."
  },
  {
    "objectID": "session_04.html#additional-resources",
    "href": "session_04.html#additional-resources",
    "title": "4  Flexdashboard",
    "section": "4.6 Additional Resources",
    "text": "4.6 Additional Resources\n\nRStudio flexdashboard vingettes (The articles under the “Articles” dropdown menu are particularly helpful!)\nRStudio flexdashboard Examples\nR Markdown: The Definitive Guide Chapter 5: Dashboards by Yihui Xie, J. J. Allaire, and Garrett Grolemund\nhtmlwidgets for R: Check out widgets featured either in the gallery or the showcase"
  },
  {
    "objectID": "session_05.html#learning-objectives",
    "href": "session_05.html#learning-objectives",
    "title": "5  Shiny",
    "section": "5.1 Learning Objectives",
    "text": "5.1 Learning Objectives\nIn this lesson we will:\n\nreview the capabilities in Shiny applications\nlearn about the basic layout for Shiny interfaces\nlearn about the server component for Shiny applications\nbuild a simple shiny application for interactive plotting"
  },
  {
    "objectID": "session_05.html#overview",
    "href": "session_05.html#overview",
    "title": "5  Shiny",
    "section": "5.2 Overview",
    "text": "5.2 Overview\nShiny is an R package for creating interactive data visualizations embedded in a web application that you and your colleagues can view with just a web browser. Shiny apps are relatively easy to construct, and provide interactive features for letting others share and explore data and analyses.\nThere are some really great examples of what Shiny can do on the RStudio webite like this one exploring movie metadata. A more scientific example is a tool from the SASAP project exploring proposal data from the Alaska Board of Fisheries. There is also an app for Delta monitoring efforts.\n\nMost any kind of analysis and visualization that you can do in R can be turned into a useful interactive visualization for the web that lets people explore your data more intuitively But, a Shiny application is not the best way to preserve or archive your data. Instead, for preservation use a repository that is archival in its mission like the KNB Data Repository, Zenodo, or Dryad. This will assign a citable identifier to the specific version of your data, which you can then read in an interactive visualiztion with Shiny.\nFor example, the data for the Alaska Board of Fisheries application is published on the KNB and is citable as:\nMeagan Krupa, Molly Cunfer, and Jeanette Clark. 2017. Alaska Board of Fisheries Proposals 1959-2016. Knowledge Network for Biocomplexity. doi:10.5063/F1QN652R.\nWhile that is the best citation and archival location of the dataset, using Shiny, one can also provide an easy-to-use exploratory web application that you use to make your point that directly loads the data from the archival site. For example, the Board of Fisheries application above lets people who are not inherently familiar with the data to generate graphs showing the relationships between the variables in the dataset.\nWe’re going to create a simple shiny app with two sliders so we can interactively control inputs to an R function. These sliders will allow us to interactively control a plot."
  },
  {
    "objectID": "session_05.html#create-a-sample-shiny-application",
    "href": "session_05.html#create-a-sample-shiny-application",
    "title": "5  Shiny",
    "section": "5.3 Create a sample shiny application",
    "text": "5.3 Create a sample shiny application\n\nFile &gt; New &gt; Shiny Web App…\nSet some fields: \n\nName it “myapp” or something else\nSelect “Single File”\nChoose to create it in a new folder called ‘shiny-demo’\nClick Create\n\n\nRStudio will create a new file called app.R that contains the Shiny application.\nRun it by choosing Run App from the RStudio editor header bar. This will bring up the default demo Shiny application, which plots a histogram and lets you control the number of bins in the plot.\n\nNote that you can drag the slider to change the number of bins in the histogram."
  },
  {
    "objectID": "session_05.html#shiny-architecture",
    "href": "session_05.html#shiny-architecture",
    "title": "5  Shiny",
    "section": "5.4 Shiny architecture",
    "text": "5.4 Shiny architecture\nA Shiny application consists of two functions, the ui and the server. The ui function is responsible for drawing the web page, while the server is responsible for any calculations and for creating any dynamic components to be rendered.\nEach time that a user makes a change to one of the interactive widgets, the ui grabs the new value (say, the new slider min and max) and sends a request to the server to re-render the output, passing it the new input values that the user had set. These interactions can sometimes happen on one computer (e.g., if the application is running in your local RStudio instance). Other times, the ui runs on the web browser on one computer, while the server runs on a remote computer somewhere else on the Internet (e.g., if the application is deployed to a web server)."
  },
  {
    "objectID": "session_05.html#interactive-scatterplots",
    "href": "session_05.html#interactive-scatterplots",
    "title": "5  Shiny",
    "section": "5.5 Interactive scatterplots",
    "text": "5.5 Interactive scatterplots\nLet’s modify this application to plot Yolo bypass secchi disk data in a time-series, and allow aspects of the plot to be interactively changed.\n\n5.5.1 Load data for the example\nUse this code to load the data at the top of your app.R script. Note we are using contentId again, and we have filtered for some species of interest.\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\nnames(delta_data)\n\n\n\n5.5.2 Add a simple timeseries using ggplot\nWe know there has been a lot of variation through time in the delta, so let’s plot a time-series of Secchi depth. We do so by switching out the histogram code for a simple ggplot, like so:\n\nserver &lt;- function(input, output) {\n    \n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"salmon\", size=4) +\n            theme_light()\n    })\n}\n\nIf you now reload the app, it will display the simple time-series instead of the histogram. At this point, we haven’t added any interactivity.\nIn a Shiny application, the server function provides the part of the application that creates our interactive components, and returns them to the user interface (ui) to be displayed on the page.\n\n\n5.5.3 Add sliders to set the start and end date for the X axis\nTo make the plot interactive, first we need to modify our user interface to include widgits that we’ll use to control the plot. Specifically, we will add a new slider for setting the minDate parameter, and modify the existing slider to be used for the maxDate parameter. To do so, modify the sidebarPanel() call to include two sliderInput() function calls:\n\nsidebarPanel(\n    sliderInput(\"minDate\",\n                \"Min Date:\",\n                min = as.Date(\"1998-01-01\"),\n                max = as.Date(\"2020-01-01\"),\n                value = as.Date(\"1998-01-01\")),\n    sliderInput(\"maxDate\",\n                \"Max Date:\",\n                min = as.Date(\"1998-01-01\"),\n                max = as.Date(\"2020-01-01\"),\n                value = as.Date(\"2005-01-01\"))\n)\n\nIf you reload the app, you’ll see two new sliders, but if you change them, they don’t make any changes to the plot. Let’s fix that.\n\n\n5.5.4 Connect the slider values to the plot\nFinally, to make the plot interactive, we can use the input and output variables that are passed into the server function to access the current values of the sliders. In Shiny, each UI component is given an input identifier when it is created, which is used as the name of the value in the input list. So, we can access the minimum depth as input$minDate and the max as input$maxDate. Let’s use these values now by adding limits to our X axis in the ggplot:\n\n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"salmon\", size=4) +\n            xlim(c(input$minDate,input$maxDate)) +\n            theme_light()\n\nAt this point, we have a fully interactive plot, and the sliders can be used to change the min and max of the Depth axis.\n\nLooks so shiny!\n\n\n5.5.5 Reversed Axes?\nWhat happens if a clever user sets the minimum for the X axis at a greater value than the maximum? You’ll see that the direction of the X axis becomes reversed, and the plotted points display right to left. This is really an error condition. Rather than use two independent sliders, we can modify the first slider to output a range of values, which will prevent the min from being greater than the max. You do so by setting the value of the slider to a vector of length 2, representing the default min and max date for the slider, such as c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")). So, delete the second slider, rename the first, and provide a vector for the value, like this:\n\nsliderInput(\"date\",\n            \"Date:\",\n            min = as.Date(\"1998-01-01\"),\n            max = as.Date(\"2020-01-01\"),\n            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n)\n\nNow, modify the ggplot to use this new date slider value, which now will be returned as a vector of length 2. The first element of the depth vector is the min, and the second is the max value on the slider.\n\n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"salmon\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            theme_light()"
  },
  {
    "objectID": "session_05.html#extending-the-user-interface-with-dynamic-plots",
    "href": "session_05.html#extending-the-user-interface-with-dynamic-plots",
    "title": "5  Shiny",
    "section": "5.6 Extending the user interface with dynamic plots",
    "text": "5.6 Extending the user interface with dynamic plots\nIf you want to display more than one plot in your application, and provide a different set of controls for each plot, the current layout would be too simple. Next we will extend the application to break the page up into vertical sections, and add a new plot in which the user can choose which variables are plotted. The current layout is set up such that the FluidPage contains the title element, and then a sidebarLayout, which is divided horizontally into a sidebarPanel and a mainPanel.\n\n\n5.6.1 Vertical layout\nTo extend the layout, we will first nest the existing sidebarLayout in a new verticalLayout, which simply flows components down the page vertically. Then we will add a new sidebarLayout to contain the bottom controls and graph.\n\nThis mechanism of alternately nesting vertical and horizontal panels can be used to segment the screen into boxes with rules about how each of the panels is resized, and how the content flows when the browser window is resized. The sidebarLayout works to keep the sidebar about 1/3 of the box, and the main panel about 2/3, which is a good proportion for our controls and plots. Add the verticalLayout, and the second sidebarLayout for the second plot as follows:\n\n    verticalLayout(\n        # Sidebar with a slider input for depth axis\n        sidebarLayout(\n            sidebarPanel(\n                sliderInput(\"date\",\n                            \"Date:\",\n                            min = as.Date(\"1998-01-01\"),\n                            max = as.Date(\"2020-01-01\"),\n                            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n            ),\n            # Show a plot of the generated distribution\n            mainPanel(\n               plotOutput(\"distPlot\")\n            )\n        ),\n\n        tags$hr(),\n\n        sidebarLayout(\n            sidebarPanel(\n                selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n            ),\n\n            # Show a plot with configurable axes\n            mainPanel(\n                plotOutput(\"varPlot\")\n            )\n        ),\n        tags$hr()\n\nNote that the second sidebarPanel uses three selectInput elements to provide dropdown menus with the variable columns (cols) from our data frame. To manage that, we need to first set up the cols variable, which we do by saving the variables names from the delta_data data frame to a variable:\n\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\ncols &lt;- names(delta_data)\n\n\n\n5.6.2 Add the dynamic plot\nBecause we named the second plot varPlot in our UI section, we now need to modify the server to produce this plot. Its very similar to the first plot, but this time we want to use the selected variables from the user controls to choose which variables are plotted. These variable names from the $input are character strings, and so would not be recognized as symbols in the aes mapping in ggplot. As recommended by the tidyverse authors, we use the non-standard evaluation syntax of .data[[\"colname\"]] to access the variables.\n\n    output$varPlot &lt;- renderPlot({\n      ggplot(delta_data, aes(x = .data[[input$x_variable]],\n                             y = .data[[input$y_variable]],\n                             color = .data[[input$color_variable]])) +\n        geom_point(size = 4)+\n        theme_light()\n    })"
  },
  {
    "objectID": "session_05.html#finishing-touches-data-citation",
    "href": "session_05.html#finishing-touches-data-citation",
    "title": "5  Shiny",
    "section": "5.7 Finishing touches: data citation",
    "text": "5.7 Finishing touches: data citation\nCiting the data that we used for this application is the right thing to do, and easy. You can add arbitrary HTML to the layout using utility functions in the tags list.\n\n    # Application title\n    titlePanel(\"Yolo Bypass Fish and Water Quality Data\"),\n    p(\"Data for this application are from: \"),\n    tags$ul(\n        tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n        )\n    ),\n    tags$br(),\n    tags$hr(),\n\nThe final application shows the data citation, the depth plot, and the configurable scatterplot in three distinct panels."
  },
  {
    "objectID": "session_05.html#publishing-shiny-applications",
    "href": "session_05.html#publishing-shiny-applications",
    "title": "5  Shiny",
    "section": "5.8 Publishing Shiny applications",
    "text": "5.8 Publishing Shiny applications\nOnce you’ve finished your app, you’ll want to share it with others. To do so, you need to publish it to a server that is set up to handle Shiny apps.\nYour main choices are:\n\nshinyapps.io (Hosted by RStudio)\n\nThis is a service offered by RStudio, which is initially free for 5 or fewer apps and for limited run time, but has paid tiers to support more demanding apps. You can deploy your app using a single button push from within RStudio.\n\nShiny server (On premises)\n\nThis is an open source server which you can deploy for free on your own hardware. It requires more setup and configuration, but it can be used without a fee.\n\nPosit Connect (On premises)\n\nThis is a paid product you install on your local hardware, and that contains the most advanced suite of services for hosting apps and RMarkdown reports. You can publish using a single button click from RStudio.\n\n\nA comparison of publishing features is available from RStudio.\n\n5.8.1 Publishing to shinyapps.io\nThe easiest path is to create an account on shinyapps.io, and then configure RStudio to use that account for publishing. Instructions for enabling your local RStudio to publish to your account are displayed when you first log into shinyapps.io:\n\nOnce your account is configured locally, you can simply use the Publish button from the application window in RStudio, and your app will be live before you know it!"
  },
  {
    "objectID": "session_05.html#summary",
    "href": "session_05.html#summary",
    "title": "5  Shiny",
    "section": "5.9 Summary",
    "text": "5.9 Summary\nShiny is a fantastic way to quickly and efficiently provide data exploration for your data and code. We highly recommend it for its interactivity, but an archival-quality repository is the best long-term home for your data and products. In this example, we used data drawn directly from the EDI repository in our Shiny app, which offers both the preservation guarantees of an archive, plus the interactive data exploration from Shiny. You can utilize the full power of R and the tidyverse for writing your interactive applications."
  },
  {
    "objectID": "session_05.html#bonus-activity-a-shinier-app-with-tabs-and-a-map",
    "href": "session_05.html#bonus-activity-a-shinier-app-with-tabs-and-a-map",
    "title": "5  Shiny",
    "section": "5.10 Bonus activity: a shinier app with tabs and a map!",
    "text": "5.10 Bonus activity: a shinier app with tabs and a map!\nLet’s build a shiny app with a tabbed interface and a map!\nBecause Shiny apps are web apps, and R is just generating standard web page content for display, we can take full advantage of the power of HTML and CSS in designing our web application. While we won’t dive deeply in how to layout and format web with HTML, we’ll show a few approaches to get you started.\nOne of the main things you may want is a multi-page application, with different types of dynamically geenrated content on different pages or tabs. In web pages, this is often done with a “tabbed” layout design, in which each page of content is hidden behind other “pages” and only displayed when a “tab” is clicked in a navigation bar or menu bar. Keeping it simple, we’ll build a web page with two tabs, one showing an interactive map of sampling locations, and the other a data exploration tab for plotting data. Check it out:\n\n\n5.10.1 Setup by loading data\nFirst, we need to start a new R script. As shown in the earlier sections, you can do this in RStudio and it will pre-populate a template of the Shiny app for you. While that works fine, in this section we will slowly build up the application from the ground up, starting with data.\nShiny apps need data. And for small apps, it is convenient to load the data into data frames that are accessible throughout the app. We will use the contentid package to reliably load a data file from the EDI data repository as we do above, and then process it to another smaller data frame listing just the sites, and with a geometry column for later plotting on the map.\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(shinythemes)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(snakecase)\n\n# read in the data from EDI\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName)) %&gt;% \n    rename(DissolvedOxygen = DO,\n           Ph = pH,\n           SpecificConductivity = SpCnd)\n\ncols &lt;- names(delta_data)\n\nsites &lt;- delta_data %&gt;% \n    distinct(StationCode, Latitude, Longitude) %&gt;% \n    drop_na() %&gt;% \n    st_as_sf(coords = c('Longitude','Latitude'), crs = 4269,  remove = FALSE)\n\nWhile this approach to loading data works, as your data sizes grow, you may find that it begins to be too sluggish rather than snappy. There are several approaches to improving this performance, but they mainly center around loading data in smaller chunks from the network as it is needed. And it is helpful to use Shiny’s built-in reactive function to load these data only when first requested or when the request changes, and to use cached copies of the data whenever possible. That’s for another day – let’s build our tabbed UI first.\n\n\n5.10.2 Basic structure for a tabbed app\nTo build a tabbed UI, we start with a user interface component using Shiny’s built-in functions for rendering web pages, such as fluidPage(). Within that page, we can create a navbarPage(), which arranges a set of tabPanel children such that only one tab is displayed at a time. Like other shiny UI components, we will assign a key identifier to be used to reference each of these UI components so that our server can gather input and return output.\nLet’s start by creating a navbarPage that sets a few options using its function arguments, and then consists of a series of web-displayed components. These include a formatted HTML header with a link, and then two tab panels, one showing our application’s “Data Sources” and one to “Explore” the data through plots.Right now we’ll just stub these out.\n\nui &lt;- fluidPage(\n    navbarPage(theme = shinytheme(\"flatly\"), \n               collapsible = TRUE,\n               HTML('&lt;a style=\"text-decoration:none;cursor:default;color:#FFFFFF;\" class=\"active\" href=\"#\"&gt;Sacramento River Floodplain Data&lt;/a&gt;'), \n               id=\"nav\",\n               windowTitle = \"Sacramento River floodplain fish and water quality data\",\n               \n               tabPanel(\"Data Sources\",\n                        verticalLayout(\n                            # Application title and data  source\n                            titlePanel(\"Sacramento River floodplain fish and water quality data\")\n                        )\n               ),\n               \n               tabPanel(\n                   \"Explore\",\n                   verticalLayout(\n                       p(\"Analysis will go here...\")\n                   )\n               )\n    )\n)\n\nOnce we have the UI skeleton in place, we can create a placeholder for the server component, but we don’t build any outputs yet. Once you’ve added the code below, you can click the Run App button in RStudio to launch the skeleton Shiny app.\n\n# Build our server\nserver &lt;- function(input, output) {\n    # Server implementation will go here\n}\n\n# Launch the application\nshinyApp(ui = ui, server = server)\n\nWhen you run the shiny app, you see the basic tabbed structure of the shiny app:\n\n\n\n5.10.3 Create the Data Sources tab\nNow let’s add dynamic content in the tabs. In the first tab, we will create a map with leaflet. Begin by modifying the Data Sources tab panel and add a new leaflet map to the panel. Note how we can mix HTML formatting functions for paragraphs (p()) and horizontal lines (tags$hr()) with a subpanel of type mainPanel that will contain the leafletOutput which is our map.\n\n               tabPanel(\"Data Sources\",\n                        verticalLayout(\n                            # Application title and data  source\n                            titlePanel(\"Sacramento River floodplain fish and water quality data\"),\n                            tags$hr(),\n                            p(\"Map of sampling locations\"),\n                            mainPanel(leafletOutput(\"map\"))\n                        )\n\nBecause we assigned the key map to the leafletOutput in the UI, we now need to create the map in the server and assign it to that output$map key. This map shows the sampling locations and the station codes using the sites geospatial data frame that we created above. But you can plot any geospatial data of interest.\n\nserver &lt;- function(input, output) {\n    \n    output$map &lt;- renderLeaflet({leaflet(sites) %&gt;% \n            addTiles() %&gt;% \n            addCircleMarkers(data = sites,\n                             lat = ~Latitude,\n                             lng = ~Longitude,\n                             radius = 10, # arbitrary scaling\n                             fillColor = \"gray\",\n                             fillOpacity = 1,\n                             weight = 0.25,\n                             color = \"black\",\n                             label = ~StationCode)\n    })\n}\n\nRerun the app, and you now have a map!\n\n\n\n5.10.4 Create the Explore tab\nCreating the content for the second tab is just like the first, but this time we’ll generate dynamic plots like we did earlier in the lesson.\nFirst, we need to add new UI components for the scatterplot (with key distPlot) and the mix and match plot (with key varPlot). Note how this is arranged as a vertical layout, with the first row containing the main panel for the distPlot, and then the second row containing a sidebarLayout(), which arranges a sidebar for the controls to the left of a second main pain panel for the varPlot.\n\n               tabPanel(\n                   \"Explore\",\n                   verticalLayout(\n                       p(\"Analysis will go here...\"),\n                       mainPanel(\n                           plotOutput(\"distPlot\"),\n                           width =  12,\n                           absolutePanel(id = \"controls\",\n                                         class = \"panel panel-default\",\n                                         top = 175, left = 75, width = 300, fixed=TRUE,\n                                         draggable = TRUE, height = \"auto\",\n                                         sliderInput(\"date\",\n                                                     \"Date:\",\n                                                     min = as.Date(\"1998-01-01\"),\n                                                     max = as.Date(\"2020-01-01\"),\n                                                     value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n                           )\n                       ),\n                       \n                       tags$hr(),\n                       \n                       sidebarLayout(\n                           sidebarPanel(\n                               selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                               selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                               selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n                           ),\n                           \n                           # Show a plot with configurable axes\n                           mainPanel(\n                               plotOutput(\"varPlot\")\n                           )\n                       ),\n                       tags$hr()\n                   )\n               )\n\nOnce we have the UI components, in place, we also need to build the plots in the server function. Two plots need to be added. First, the turbidity plot that we saw earlier in the lesson:\n\n    #  turbidity plot\n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"salmon\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            labs(x = \"Sample Date\", y = \"Secchi Depth (m)\") +\n            theme_light()\n    })\n\nLike earlier, note how values from the UI get delivered to the app in the input variable, keyed based on the widget name. In this case, we use input$date[1] to find where the minimum slider value is set, and input$date[2] for the maximum, and use these to constrain our plot axis limits.\nFinally, add in the plot code for the mix and match plot, enabling the user to choose which variables to plot. This plot is just like the one from the earlier section.\n\n    # mix and  match plot\n    output$varPlot &lt;- renderPlot({\n        ggplot(delta_data, mapping = aes(x = .data[[input$x_variable]],\n                                         y = .data[[input$y_variable]],\n                                         color = .data[[input$color_variable]])) +\n            labs(x = to_any_case(input$x_variable, case = \"title\"),\n                 y = to_any_case(input$y_variable, case = \"title\"),\n                 color = to_any_case(input$color_variable, case = \"title\")) +\n            geom_point(size=4) +\n            theme_light()\n    })\n\nAnd now we have our analysis plots in their own tab as well.\n When it comes time to style your application, keep in mind that you can use the full set of elements from HTML, and they can be styled using CSS styles just as you would any web application. The examples above show a few CSS properties being applied, but many more options are available through CSS."
  },
  {
    "objectID": "session_05.html#appendix-1-full-source-code-for-the-final-application",
    "href": "session_05.html#appendix-1-full-source-code-for-the-final-application",
    "title": "5  Shiny",
    "section": "5.11 Appendix 1: Full source code for the final application",
    "text": "5.11 Appendix 1: Full source code for the final application\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n# read in the data from EDI\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\ncols &lt;- names(delta_data)\n    \n\n\n# Define UI for application that draws a two plots\nui &lt;- fluidPage(\n    \n    # Application title and data  source\n    titlePanel(\"Sacramento River floodplain fish and water quality dataa\"),\n    p(\"Data for this application are from: \"),\n    tags$ul(\n        tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n        )\n    ),\n    tags$br(),\n    tags$hr(),\n    \n    verticalLayout(\n        # Sidebar with a slider input for time axis\n        sidebarLayout(\n            sidebarPanel(\n                sliderInput(\"date\",\n                            \"Date:\",\n                            min = as.Date(\"1998-01-01\"),\n                            max = as.Date(\"2020-01-01\"),\n                            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n            ),\n            # Show a plot of the generated timeseries\n            mainPanel(\n                plotOutput(\"distPlot\")\n            )\n        ),\n        \n        tags$hr(),\n        \n        sidebarLayout(\n            sidebarPanel(\n                selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n            ),\n            \n            # Show a plot with configurable axes\n            mainPanel(\n                plotOutput(\"varPlot\")\n            )\n        ),\n        tags$hr()\n    )\n)\n\n# Define server logic required to draw the two plots\nserver &lt;- function(input, output) {\n    \n    #  turbidity plot\n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"salmon\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            theme_light()\n    })\n    \n    # mix and  match plot\n    output$varPlot &lt;- renderPlot({\n      ggplot(delta_data, aes(x = .data[[input$x_variable]],\n                             y = .data[[input$y_variable]],\n                             color = .data[[input$color_variable]])) +\n        geom_point(size = 4) +\n        theme_light()\n    })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "session_05.html#appendix-2-full-code-for-navbar-app",
    "href": "session_05.html#appendix-2-full-code-for-navbar-app",
    "title": "5  Shiny",
    "section": "5.12 Appendix 2: Full code for navbar app",
    "text": "5.12 Appendix 2: Full code for navbar app\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(shinythemes)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(snakecase)\n\n# read in the data from EDI\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName)) %&gt;% \n    rename(DissolvedOxygen = DO,\n           Ph = pH,\n           SpecificConductivity = SpCnd)\n\ncols &lt;- names(delta_data)\n\nsites &lt;- delta_data %&gt;% \n    distinct(StationCode, Latitude, Longitude) %&gt;% \n    drop_na() %&gt;% \n    st_as_sf(coords = c('Longitude','Latitude'), crs = 4269,  remove = FALSE)\n\n\n\n# Define UI for application\nui &lt;- fluidPage(\n    navbarPage(theme = shinytheme(\"flatly\"), \n               collapsible = TRUE,\n               HTML('&lt;a style=\"text-decoration:none;cursor:default;color:#FFFFFF;\" class=\"active\" href=\"#\"&gt;Sacramento River Floodplain Data&lt;/a&gt;'), \n               id=\"nav\",\n               windowTitle = \"Sacramento River floodplain fish and water quality data\",\n               \n               tabPanel(\"Data Sources\",\n                        verticalLayout(\n                            # Application title and data  source\n                            titlePanel(\"Sacramento River floodplain fish and water quality data\"),\n                            p(\"Data for this application are from: \"),\n                            tags$ul(\n                                tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                                        tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n                                )\n                            ),\n                            tags$br(),\n                            tags$hr(),\n                            p(\"Map of sampling locations\"),\n                            mainPanel(leafletOutput(\"map\"))\n                        )\n               ),\n               \n               tabPanel(\n                   \"Explore\",\n                   verticalLayout(\n                       mainPanel(\n                           plotOutput(\"distPlot\"),\n                           width =  12,\n                           absolutePanel(id = \"controls\",\n                                         class = \"panel panel-default\",\n                                         top = 175, left = 75, width = 300, fixed=TRUE,\n                                         draggable = TRUE, height = \"auto\",\n                                         sliderInput(\"date\",\n                                                     \"Date:\",\n                                                     min = as.Date(\"1998-01-01\"),\n                                                     max = as.Date(\"2020-01-01\"),\n                                                     value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n                           )\n                       ),\n                       \n                       tags$hr(),\n                       \n                       sidebarLayout(\n                           sidebarPanel(\n                               selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                               selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                               selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n                           ),\n                           \n                           # Show a plot with configurable axes\n                           mainPanel(\n                               plotOutput(\"varPlot\")\n                           )\n                       ),\n                       tags$hr()\n                   )\n               )\n    )\n)\n\n# Define server logic required to draw the two plots\nserver &lt;- function(input, output) {\n    \n    output$map &lt;- renderLeaflet({leaflet(sites) %&gt;% \n            addTiles() %&gt;% \n            addCircleMarkers(data = sites,\n                             lat = ~Latitude,\n                             lng = ~Longitude,\n                             radius = 10, # arbitrary scaling\n                             fillColor = \"gray\",\n                             fillOpacity = 1,\n                             weight = 0.25,\n                             color = \"black\",\n                             label = ~StationCode)\n    })\n    \n    #  turbidity plot\n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"salmon\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            labs(x = \"Sample Date\", y = \"Secchi Depth (m)\") +\n            theme_light()\n    })\n    \n    # mix and  match plot\n    output$varPlot &lt;- renderPlot({\n        ggplot(delta_data, mapping = aes(x = .data[[input$x_variable]],\n                                         y = .data[[input$y_variable]],\n                                         color = .data[[input$color_variable]])) +\n            labs(x = to_any_case(input$x_variable, case = \"title\"),\n                 y = to_any_case(input$y_variable, case = \"title\"),\n                 color = to_any_case(input$color_variable, case = \"title\")) +\n            geom_point(size=4) +\n            theme_light()\n    })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "session_05.html#resources",
    "href": "session_05.html#resources",
    "title": "5  Shiny",
    "section": "5.13 Resources",
    "text": "5.13 Resources\n\nMain Shiny site\nOfficial Shiny Tutorial"
  },
  {
    "objectID": "session_06.html#learning-objectives",
    "href": "session_06.html#learning-objectives",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nApply design thinking approaches and tools to dashboard development\nEvaluate whether to build a web application using flexdashboard or shiny\nPractice developing a web application using flexdashboard or shiny\nDemonstrate design knowledge including structure of layout, incorporation of data visualization, and choice of interactive elements\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThis lesson has been adapted by the Women in Data Science (WiDS) Workshop: Dashboard Design Thinking by Jenn Schilling.\nWe highly encourage you to watch the full workshop for more details on applying design thinking to dashboards."
  },
  {
    "objectID": "session_06.html#what-is-design-thinking",
    "href": "session_06.html#what-is-design-thinking",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "6.1 What is Design Thinking?",
    "text": "6.1 What is Design Thinking?\nDesign thinking in data science centers the user’s perspective throughout the entire development of a dashboard. It starts with the user by gaining an understanding on what the user needs and ends with the user by involving them in focus groups or other testing methods to get their feedback. Centering the user means we create better dashboards and improve data-informed decision making.\nDesign thinking is inherently an innovative and collaborative process because it involves exploring multiple options for the same problem.\nThe design thinking process can be encapsulated within these stages:\n\nEmpathize and Define: In these stages, we use tools to understand the problem and our audiences.\nIdeate and Prototype: We use these stages to explore solutions.\nTest and Implement: In these final stages, we use tools to materialize the final product.\n\nThese stages are not meant to be in any particular order. While there is an inherent order to them, you should feel empowered to move between the stages as needed — depending on the development of your dashboard and the needs of your users."
  },
  {
    "objectID": "session_06.html#empathize-and-define",
    "href": "session_06.html#empathize-and-define",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "6.2 Empathize and Define",
    "text": "6.2 Empathize and Define\n\n\n\n\n\n\nEmpathize: Gather information and survey your users to understand what they need to know from the data, what are the use cases for the dashboard and its overall purpose.\n\n\n\n\nEmpathize Tool: Empathy Mapping\nDuring the Empathize and Define stages, you can gather information through internal conversations with your team or conduct interviews with potential users.\nThe goal of an Empathy Map is to capture what a user says, thinks, does, and feels — additionally consider what their goals are (and do their goals align the goals you have in mind for the dashboard you’re developing?). Check out the empathy map the Nielsen Norman Group created for a customer, Jamie, who is buying a television.\n\n\n\nSource: Nielsen Norman Group\n\n\n\n\n\n\n\n\nCreate Your Own Empathy Map\n\n\n\nUse the NCEAS Learning Hub Empathy Map template to get started — adapt it as needed for your project/user(s).\n\n\n\n\n\n\n\n\n\nDefine: Identify the core problems your users have in accessing and interpreting the data in its current state.\n\n\n\n\n\nDefine Tool: Project Brief Wiki\nThe Empathize Stage helped us understand our audience. Now, we need to understand our problem. In the Define Stage, write down problem statements and document the specifications of the dashboard including the scope, timeline, and roles. It’s best to contain all this information within a Project Brief — a condensed version of a project plan.\nYou should feel comfortable sharing this document with not only your team, but your users as well. This ensures that everyone is on the same page. Keeping this in mind, consider having your Project Brief as a Wiki on GitHub in the Git repository for your dashboard.\nWhen defining the dashboard you want to create, consider these types:\n\nExecutive Dashboard: shows the big picture; used for high-level overview\nOperational Dashboard: shows what’s happening right now; used for monitoring\nStrategic Dashboard: shows progress toward a goal; used to inform decisions\nAnalytical Dashboard: shows detailed analysis; used for identifying opportunities and deep analysis\n\n\n\n\n\n\n\nWriting a Project Brief\n\n\n\nCheck out Asana’s article 5 steps to writing a clear project brief as guidance — use only what you need for your project."
  },
  {
    "objectID": "session_06.html#ideate-and-protoype",
    "href": "session_06.html#ideate-and-protoype",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "6.3 Ideate and Protoype",
    "text": "6.3 Ideate and Protoype\n\n\n\n\n\n\nIdeate: Challenge your existing assumptions and create ideas. Look for alternative ways to view the problem and identify solutions.\n\n\n\n\nIdeate Tool: Host a Brainstorm Session\nNow that we’ve learned about our audiences and problems from the Empathize & Define stages, we can start to think about what ideas for dashboard solutions. The primary goal of this stage is to generate as many ideas as possible without judgement and evaluation — it’s all about quantity over quality. Once you’ve generated many ideas, then you can start to narrow it down to a single dashboard solution.\nThere are many ways to ideate individually or as a group, but here are some tips:\n\nUse mind mapping tools like Miro for virtual collaboration.\nIf you’re working together in-person, use a white board, sticky notes, or a large piece of construction paper to capture as many ideas from as many team members as possible.\nTake a break!\nGroup related ideas together.\nSet a time limit. This can vary from a few hours to multiple sessions throughout a week. Either way setting a time limit and making a plan to regroup and narrow down on ideas is helpful so that the ideation period doesn’t feel so nebulous.\n\n\n\n\n\n\n\nShiny vs Flexdashboard\n\n\n\nThis is a good time to consider which R dashboard package may be more useful for your final dashboard product. Recall the diagram from the Flexdashboard Lesson to help determine which tool makes the most sense for your project.\n\n\n\n\n\n\n\n\n\nPrototype: Start to create the solutions. It’s important to experiment here. Produce some inexpensive and low-intensive versions of the product.\n\n\n\n\n\nPrototype Tool: Create a Minimal Viable Dashboard\nAt the Prototype Stage, we can experiment with a few of the ideas from the Ideate Stage to identify what is the best solution. Here we can create a quick, scaled down version of the dashboard. The primary goal is to create something more refined from the Ideate Stage, but not a final or completely usable dashboard. We want the prototype to be a minimal viable product where you can test out some functionality without committing to multiple iterations.\n\n\n\n\n\n\nTools to use for Prototyping\n\n\n\n\nGoogle Slides\nMicrosoft PowerPoint\nR Flexdashboard (this is a great option for prototyping if you’re set on creating a Shiny App since you can insert some Shiny elements in a Flexdashboard)\nGood ole’ pen and paper!"
  },
  {
    "objectID": "session_06.html#test-and-implement",
    "href": "session_06.html#test-and-implement",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "6.4 Test and Implement",
    "text": "6.4 Test and Implement\n\n\n\n\n\n\nTest: Try your solutions and evaluate the results. Return to your users for feedback to incorporate.\n\n\n\n\nTest Tool: Run Tests and Review Work from Previous Stages\nYou’ve completed your prototype — it’s time to test it out and receive feedback on it! Review your project brief and confirm your prototype reflects your users’ needs and the use cases you’re trying to achieve.\nIf you don’t have access to your users’, return to your Empathy Maps to make sure your prototype is aligned with the information you have gathered there.\nOther tests to consider:\n\nDoes your dashboard work across different devices?\nDoes it meet accessibility requirements?\nDoes the users’ dashboard experience align with how you think their experience should go?\n\n\n\n\n\n\n\nInteraction Design Foundation Testing Prototype Guidelines\n\n\n\nRead the Interaction Design Foundation’s article Test Your Prototypes: How to Gather Feedback and Maximize Learning for more tips, tricks, and templates for testing your dashboard.\n\n\n\n\n\n\n\n\n\nImplement: Put your vision into effect! Remember that the process doesn’t have to end here at implementation. Return to your users to gain more feedback and to guide the refinement of your solutions.\n\n\n\nIf the test you run reveal that your dashboard is not meeting the goals of your team or your users, then it’s time to go back to the Prototype Stage (or even a different stage) and iterate before you complete the Implementation stage. You will also want to return to your Project Brief, Empathy Map, and additional successful metrics to ensure that your dashboard meets most of these goals and needs.\nWhen implementing the dashboard:\n\nAdd context and definitions to the dashboard.\nTest again!\nInternally: validate your data.\nExternally: users test and provide final feedback.\nHow are you going to communicate your dashboard and make it accessible to your audience?\n\n\n\n\n\n\n\nThat’s Not All Folks!\n\n\n\nDesign Thinking is an approach that centers the user in the development of a product. When developing dashboards or other data products there are definitely more resources and approaches out there.\nIt’s worth doing additional research on aspects like testing, design, and data visualization to create the most robust data product as possible."
  },
  {
    "objectID": "session_07.html",
    "href": "session_07.html",
    "title": "7  Hands On Synthesis",
    "section": "",
    "text": "Note\n\n\n\nTime for groups to get together and advance in their synthesis projects."
  },
  {
    "objectID": "session_08.html#learning-objectives",
    "href": "session_08.html#learning-objectives",
    "title": "Reproducible Workflows Using targets",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIllustrate importance of utilizing reproducible workflows to increase project efficiency and sharing of analyses, as well as reducing computational resources.\nApply the R package targets workflow framework to scientific projects.\nOrganize projects in modular steps to improve understanding and computational flexibility.\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThis lesson is adapted from the following resources:\n\nJournal article Improving ecological data science with workflow management software by Brousil et al\nBrousil et al’s accompanying example, A worked targets example for ecologists\nRLadies Santa Barbara Chapter Workshop: An introduction to targets for R"
  },
  {
    "objectID": "session_08.html#challenges-of-workflows",
    "href": "session_08.html#challenges-of-workflows",
    "title": "Reproducible Workflows Using targets",
    "section": "8.1 Challenges of Workflows",
    "text": "8.1 Challenges of Workflows\nAll research projects have a workflow of some kind and typically includes steps like: data preparation and harmonization, running analyses or models, creating visualizations, and more.\n\n\n\nExample of ideal straightforward data workflow\n\n\nHowever, many environmental research projects are becoming increasingly more complex as researchers are utilizing larger datasets that require complicated analytical methods. More complexity means more steps, and more room for error or poor organizational methods that make projects difficult to reproduce. More complex analysis may also mean longer run times, which can make updating functions and analysis time-consuming.\n\n\n\nExample of a more realistic data workflow…\n\n\nThis is where reproducible workflow tools and packages, like the R package targets, can play a huge role in streamlining complex workflows and ease the organization and sharing of projects.\nOther interchangeable terms for workflows are:\n\nWorkflow Management Systems (WMS)\nData pipelines\nData workflow"
  },
  {
    "objectID": "session_08.html#benefits-of-reproducible-workflows",
    "href": "session_08.html#benefits-of-reproducible-workflows",
    "title": "Reproducible Workflows Using targets",
    "section": "8.2 Benefits of Reproducible Workflows",
    "text": "8.2 Benefits of Reproducible Workflows\nUsing a reproducible workflow allows us to:\n\ntrack the status of all required files and functions which makes it easier to keep all steps in the overall workflow up-to-date (Brousil et al. 2023)\nbreak our analysis and data processing steps into smaller functions that are modular which results in more computational flexibility (Brousil et al. 2023) and makes it easier to debug when errors occur\nreduce the computational tasks to only run as necessary as opposed to anytime there is an update in on the steps in the workflow (Brousil et al. 2023)\nutilize continuous integration (automating tasks) so that we spend less time on manual work and are less prone to simple errors (e.g. misspellings) (Brousil et al. 2023)\n\nOverall a reproducible workflow enhances our research projects because it improves our understanding of our work for ourselves and for collaborators, makes our work more efficient and automated, and increases reproducibility.\n\n\n\n\n\n\nChallenges of Reproducible Workflows\n\n\n\nWhile the benefits of reproducible workflows are immense, workflows and the utilizing workflow management tools can be intimidating at the start due to:\n\nhigh learning-curve for implementing reproducible workflow tools (Brousil et al. 2023)\nlimited training resources and opportunities to applying WMS for environmental researchers and professionals (Brousil et al. 2023)\ninfrequent use of WMS and reproducible workflows in the environmental field means there are less examples to learn from and a lack of standardized methods for using WMS (Brousil et al. 2023)"
  },
  {
    "objectID": "session_08.html#leveraging-targets-for-reproducible-workflows",
    "href": "session_08.html#leveraging-targets-for-reproducible-workflows",
    "title": "Reproducible Workflows Using targets",
    "section": "8.3 Leveraging targets for Reproducible Workflows",
    "text": "8.3 Leveraging targets for Reproducible Workflows\nWMS or tools like targets may not be needed by most beginners, but learning about these tools give researchers the foundational capabilities to scale their projects in size and complexity. While it takes time to learn these tools to create reproducible workflows, it saves time and frustrations in the long run (Brousil et al. 2023).\n\n\n\nA workflow visualized by targets using tar_visnetwork(). Source: The {targets} R package user manual\n\n\n\n\n\n\n\n\nWhat is the targets package?\n\n\n\ntargets is a data pipeline tool specifically for R. It coordinates and keeps track of an entire workflow. It can also help users build, visualize, and manage workflows from raw files to outputs.\n\n\n\nWhat does the targets package do?\n\nKeeps track of entire workflow\nAutomatically detects when files or functions change\nSaves time by only running steps, or targets, that are no longer up to date\nEnsures that the pipeline is run in the correct order (meaning you don’t have to keep track of this after you set it up the first time)\nCan integrate with high performance computing, like parallel processing\nEnsures reproducibility: When targets are up to date, this is evidence that the outputs match the code and inputs\nMore trustworthy and reproducible results\n\n\n\n\n\n\n\nThe R package drake was the predecessor to the targets package.\n\n\n\n\n\nHow does the targets package work?\nFor targets to be successful, at a bare minimum it needs 1) a script with the different functions you’re using for analysis, and 2) a _targets.R script which is a special file that targets uses to coordinate, connect, and keep track of the steps in your workflow aka “targets”.\nA good “target”:\n\nis a meaningful step in your workflow\nlarge enough to subtract a decent amount of runtime when skipped\nsmall enough that some targets can be skipped even if others need to run\n\n\n\n\n\n\n\nYou use a target as if it is an R object available to your in your Environment. Learn more about targets in the The {targets} R package user manual Ch 6 Targets.\n\n\n\nThe _targets.R script is where you define “targets” aka analysis steps. To define your targets, you do so using the list() and tar_targets() functions:\n\n# targets syntax\nlist(\n\ntar_target(name = first_target,\n           command = some_code),\ntar_target(name = second_target,\n           command = some_code),\ntar_target(name = third_target,\n           command = some_code),\n)\n\n# minimal example\nlist(\n\ntar_target(name = read_data,\n           command = read_csv(\"path/to/data.csv\")),\ntar_target(name = clean_data,\n           command = my_cleaning_function(read_data)),\ntar_target(name = analysis_model,\n           command = my_modeling_function(clean_data)),\n)"
  },
  {
    "objectID": "session_08.html#exercise-creating-a-pipeline-using-targets",
    "href": "session_08.html#exercise-creating-a-pipeline-using-targets",
    "title": "Reproducible Workflows Using targets",
    "section": "8.4 Exercise: Creating a Pipeline using targets",
    "text": "8.4 Exercise: Creating a Pipeline using targets\nIn this exercise, we are going to use the palmerpenguins data to recreate the pipeline below.\n\n\n\n\n\n\n\nSetup\n\n\n\n\nCreate a new project called demo_targets\nCreate the following directories:\n\ndata\nR\nfigs\n\nCreate a new R script called functions.R and save it inside the R folder\nCreate a new _targets.R script using targets::tar_script()\n\n\n\n\n\nCode for functions.R\n# create_penguin_data ----\ncreate_penguin_data &lt;- function(out_path) {\n\n  penguin_data &lt;- palmerpenguins::penguins_raw %&gt;% clean_names()\n\n  write_csv(penguin_data, out_path)\n  \n  return(out_path)\n\n} # EO penguin_data\n\n# clean_data ----\nclean_data &lt;- function(file_path) {\n  \n  clean_data &lt;- read_csv(file_path) %&gt;% \n    # keep only common species name\n    mutate(\n      species = str_extract(string = species,\n                            pattern = \"Chinstrap|Adelie|Gentoo\"),\n      year = year(date_egg)\n    ) %&gt;% \n    # select cols of interest\n    select(species,\n           island,\n           flipper_length_mm,\n           body_mass_g, \n           sex) %&gt;% \n    drop_na()\n  \n  return(clean_data)\n}\n\n# exploratory_plot ----\nexploratory_plot &lt;- function(clean_data) {\n  \n  ggplot(data = clean_data, aes(x = flipper_length_mm,\n                                y = body_mass_g)) +\n    geom_point(aes(color = species)) +\n    scale_color_manual(values = c(\n      \"Adelie\" = \"purple2\",\n      \"Chinstrap\" = \"orange\",\n      \"Gentoo\" = \"cyan4\"\n    )) +\n    labs(\n      title = NULL,\n      x = \"Flipper Length (mm)\",\n      y = \"Body Mass (g)\",\n      color = \"Species\"\n    ) +\n    theme_minimal()\n  \n  ggsave(\"figs/exploratory_plot.png\", width = 5, height = 5)\n}\n\n\n\n\nCode for _targets.R\nlibrary(targets)\n\nsource(\"R/functions.R\")\n\n# Set target-specific options such as packages:\ntar_option_set(packages = \"tidyverse\")\n\n# End this file with a list of target objects.\nlist(\n  # create data\n  tar_target(name = file,\n             command = create_penguin_data(out_path = \"data/penguin_data.csv\"),\n             packages = c(\"readr\", \"janitor\")),\n  # clean data\n  tar_target(name = data,\n             command = clean_data(file_path = file),\n             packages = c(\"readr\", \"dplyr\", \"tidyr\", \"stringr\", \"lubridate\")),\n  # plot data\n  tar_target(name = plot_data,\n             command = exploratory_plot(clean_data = data),\n             packages = \"ggplot2\")\n\n)"
  },
  {
    "objectID": "session_08.html#additional-resources",
    "href": "session_08.html#additional-resources",
    "title": "Reproducible Workflows Using targets",
    "section": "8.5 Additional Resources",
    "text": "8.5 Additional Resources\n\nThe targets R Package User Manual by the targets creator Will Landau\nGet started with targets in 4 minutes video by Will Landau\n\n\n\n\n\nBrousil, Matthew R., Alessandro Filazzola, Michael F. Meyer, Sapna Sharma, and Stephanie E. Hampton. 2023. “Improving Ecological Data Science with Workflow Management Software.” Methods in Ecology and Evolution 14 (6): 1381–88. https://doi.org/10.1111/2041-210x.14113."
  },
  {
    "objectID": "session_09.html#learning-objectives",
    "href": "session_09.html#learning-objectives",
    "title": "9  Parallel Processing",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand what parallel computing is and when it may be useful\nUnderstand how parallelism can work\nReview sequential loops and *apply functions\nUnderstand and use the parallel package multicore functions\nUnderstand and use the foreach package functions\nReview asynchronous futures and the furrr package"
  },
  {
    "objectID": "session_09.html#introduction",
    "href": "session_09.html#introduction",
    "title": "9  Parallel Processing",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nProcessing large amounts of data with complex models can be time consuming. New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America:\n\n\n\nLevy et al. 2016. doi:10.5063/F1Z899CZ\n\n\nThere are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization.\nAlternatively, think of remote sensing data. Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years.\n\n\n\nNEON Data Cube"
  },
  {
    "objectID": "session_09.html#why-parallelism",
    "href": "session_09.html#why-parallelism",
    "title": "9  Parallel Processing",
    "section": "9.2 Why parallelism?",
    "text": "9.2 Why parallelism?\nMuch R code runs fast and fine on a single processor. But at times, computations can be:\n\ncpu-bound: Take too much cpu time\nmemory-bound: Take too much memory\nI/O-bound: Take too much time to read/write from disk\nnetwork-bound: Take too much time to transfer\n\nTo help with cpu-bound computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire set of those processors. Plus, these machines also have large amounts of memory to avoid memory-bound computing jobs."
  },
  {
    "objectID": "session_09.html#processors-cpus-cores-and-threads",
    "href": "session_09.html#processors-cpus-cores-and-threads",
    "title": "9  Parallel Processing",
    "section": "9.3 Processors (CPUs), Cores, and Threads",
    "text": "9.3 Processors (CPUs), Cores, and Threads\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with mutliple processors, each of which in turn can contain multiple cores. These processors and cores are available to perform computations. But, just what’s the difference between processors and cores? A computer with one processor may still have 4 cores (quad-core), allowing 4 (or possibly more) computations to be executed at the same time.\n\n\nMicroprocessor: an integrated circuit that contains the data processing logic and control for a computer.\nMulti-core processor: a microprocessor containing multiple processing units (cores) on a single integrated circuit. Each core in a multi-core processor can execute program instructions at the same time.\nProcess: an instance of a computer program (including instructions, memory, and other resources) that is executed on a microprocessor.\nThread: a thread of execution is the smallest sequence of program instructions that can be executed independently, and is typically a component of a process. The threads in a process can be executed concurrently and typically share the same memory space. They are faster to create than a process.\nCluster: a set of multiple, physically distinct computing systems, each with its own microprocessors, memory, and storage resources, connected together by a (fast) network that allows the nodes to be viewed as a single system.\n\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here we show four quad-core processors for a total of 16 cores in this machine.\n\nYou can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later).\nHistorically, many languages only utilized one processor, which makes them single-threaded. Which is a shame, because the 2019 MacBook Pro that I am writing this on is much more powerful than that, and has mutliple cores that would support concurrent execution of multiple threads:\njones@powder:~$ sysctl hw.ncpu hw.physicalcpu\nhw.ncpu: 12\nhw.physicalcpu: 6\nTo interpret that output, this machine powder has 6 physical CPUs, each of which has two processing cores, for a total of 12 cores for computation. I’d sure like my computations to use all of that processing power. Because its all on one machine, we can easily use multicore processing tools to make use of those cores. Now let’s look at the computational server included-crab at NCEAS:\njones@included-crab:~$ lscpu | egrep 'CPU\\(s\\)|per core|per socket' \nCPU(s):                          88\nOn-line CPU(s) list:             0-87\nThread(s) per core:              1\nCore(s) per socket:              1\nNUMA node0 CPU(s):               0-87\nNow that’s more compute power! included-crab has 384 GB of RAM, and ample storage. All still under the control of a single operating system.\nFinally, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now:\n\n\n\nStampede2 at TACC\n\n4200 KNL nodes: 285,600 cores\n1736 SKX nodes: 83,328 cores\n224 ICX nodes: 17,920 cores\nTOTAL: 386,848 cores\n\nDelta at NCSA\n\n124 CPU Milan nodes (15,872 cores)\n100 quad A100 GPU nodes (6400 cores + 400 GPUs)\n100 quad A40 GPU nodes (6400 cores + 400 GPUs)\n5 eight-way A100 GPU nodes (640 cores + 40 GPUs):\n1 MI100 GPU node (128 cores + 8 GPUs)\n7 PB of disk-based Lustre storage\n3 PB of flash based storage\nTOTAL: 29,440 cores, 848 gpus\n\n\n\n\n\n\n\nDelta Supercomputer\n\n\n\n\nNote that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster. One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine. But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster."
  },
  {
    "objectID": "session_09.html#modes-of-parallelization",
    "href": "session_09.html#modes-of-parallelization",
    "title": "9  Parallel Processing",
    "section": "9.4 Modes of parallelization",
    "text": "9.4 Modes of parallelization\nSeveral different approaches can be taken to structuring a computer program to take advantage of the hardware capabilities of multi-core processors. In the typical, and simplest, case, each task in a computation is executed serially in order of first to last. The total computation time is the sum of the time of all of the subtasks that are executed. In the next figure, a single core of the processor is used to sequentially execute each of the five tasks, with time flowing from left to right.\n\n\n\nSerial and parallel execution of tasks using threads and processes.\n\n\nIn comparison, the middle panel shows two approaches to parallelization on a single computer: Parallel Threads and Parallel Processes. With multi-threaded execution, a separate thread of execution is created for each of the 5 tasks, and these are executed concurrently on 5 of the cores of the processor. All of the threads are in the same process and share the same memory and resources, so one must take care that they do not interfere with each other.\nWith multi-process execution, a separate process is created for each of the 5 tasks, and these are executed concurrently on the cores of the processor. The difference is that each process has it’s own copy of the program memory, and changes are merged when each child process completes. Because each child process must be created and resources for that process must be marshalled and unmarshalled, there is more overhead in creating a process than a thread. “Marshalling” is the process of transforming the memory representation of an object into another format, which allows communication between remote objects by converting an object into serialized form.\nFinally, cluster parallel execution is shown in the last panel, in which a cluster with multiple computers is used to execute multiple processes for each task. Again, there is a setup task associated with creating and mashaling resources for the task, which now includes the overhead of moving data from one machine to the others in the cluster over the network. This further increases the cost of creating and executing multiple processes, but can be highly advantageous when accessing exceedingly large numbers of processing cores on clusters.\nThe key to performance gains is to ensure that the overhead associated with creating new threads or processes is small relative to the time it takes to perform a task. Somewhat unintuitively, when the setup overhead time exceeds the task time, parallel execution will likely be slower than serial."
  },
  {
    "objectID": "session_09.html#when-to-parallelize",
    "href": "session_09.html#when-to-parallelize",
    "title": "9  Parallel Processing",
    "section": "9.5 When to parallelize",
    "text": "9.5 When to parallelize\nIt’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!\nIn addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced. Some propose that this may follow Amdahl’s Law, where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines):\n\n\n\n\n\nSo, its important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done. With that, let’s do some parallel computing…"
  },
  {
    "objectID": "session_09.html#pleasingly-parallel-with-palmer-penguins",
    "href": "session_09.html#pleasingly-parallel-with-palmer-penguins",
    "title": "9  Parallel Processing",
    "section": "9.6 Pleasingly Parallel with Palmer Penguins",
    "text": "9.6 Pleasingly Parallel with Palmer Penguins\n\n\n\nWhen you have a list of repetitive tasks, you may be able to speed it up by adding more computing power. If each task is completely independent of the others, then it is a prime candidate for executing those tasks in parallel, each on its own core. For example, let’s build a simple loop that uses sample with replacement to do a bootstrap analysis. In this case, we select bill_length_mm and species from the palmerpenguins dataset, randomly subset it to 100 observations, and then iterate across 3,000 trials, each time resampling the observations with replacement. We then run a logistic regression fitting species as a function of length, and record the coefficients for each trial to be returned.\n\n\n\n\n\n\n\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\nbill_length &lt;- penguins %&gt;% \n    select(species, bill_length_mm) %&gt;% \n    drop_na() %&gt;% \n    as.data.frame()\nhead(bill_length)\n\n  species bill_length_mm\n1  Adelie           39.1\n2  Adelie           39.5\n3  Adelie           40.3\n4  Adelie           36.7\n5  Adelie           39.3\n6  Adelie           38.9\n\n\nGiven this data, we can use a general linear model to estimate bill length as a function of species, where we use a loop to bootstrap over repeated subsamples of this dataset.\n\ntrials &lt;- 3000\nres &lt;- data.frame()\nsystem.time({\n  trial &lt;- 1\n  while(trial &lt;= trials) {\n    index &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(bill_length[index,1]~bill_length[index,2], family=binomial(logit))\n    r &lt;- coefficients(result1)\n    res &lt;- rbind(res, r)\n    trial &lt;- trial + 1\n  }\n})\n\n   user  system elapsed \n  8.537   0.048   8.586 \n\n\nThe issue with this loop is that we execute each trial sequentially, which means that only one of our processors on this machine are in use. In order to exploit parallelism, we need to be able to dispatch our tasks as functions, with one task going to each processor. To do that, we need to convert our task to a function, and then use the *apply() family of R functions to apply that function to all of the members of a set. In R, using apply used to be faster than the equivalent code in a loop, but now they are similar due to optimizations in R loop handling. However, using the function allows us to later take advantage of other approaches to parallelization. Here’s the same code rewritten to use lapply(), which applies a function to each of the members of a list (in this case the trials we want to run):\n\nboot_fx &lt;- function(trial) {\n  index &lt;- sample(100, 100, replace=TRUE)\n  result1 &lt;- glm(bill_length[index,1]~bill_length[index,2], family=binomial(logit))\n  r &lt;- coefficients(result1)\n  res &lt;- rbind(data.frame(), r)\n}\n\ntrials &lt;- seq(1, trials)\nsystem.time({\n  results &lt;- lapply(trials, boot_fx)\n})\n\n   user  system elapsed \n  9.228   0.008   9.238"
  },
  {
    "objectID": "session_09.html#approaches-to-parallelization",
    "href": "session_09.html#approaches-to-parallelization",
    "title": "9  Parallel Processing",
    "section": "9.7 Approaches to parallelization",
    "text": "9.7 Approaches to parallelization\nWhen parallelizing jobs, one can:\n\nUse the multiple cores on a local computer through mclapply\nUse multiple processors on local (and remote) machines using makeCluster and clusterApply\n\nIn this approach, one has to manually copy data and code to each cluster member using clusterExport\nThis is extra work, but sometimes gaining access to a large cluster is worth it\n\n\n\n9.7.1 Parallelize using: mclapply\nThe parallel library can be used to send tasks (encoded as function calls) to each of the processing cores on your machine in parallel. This is done by using the parallel::mclapply function, which is analogous to lapply, but distributes the tasks to multiple processor cores. mclapply gathers up the responses from each of these function calls, and returns a list of responses that is the same length as the list or vector of input data (one return per input item). Now let’s demonstrate with our bootstrap example. First, determine how many cores are available on this machine:\n\nlibrary(parallel)\nnumCores &lt;- detectCores()\nnumCores\n\n[1] 2\n\n\nThen, using that, run our 3000 bootstrap samples with the same function, but this time parallel on those 2 cores.\n\nsystem.time({\n  res_mca &lt;- mclapply(trials, boot_fx, mc.cores = numCores)\n})\n\n   user  system elapsed \n  4.634   0.132   4.807 \n\n\n\n\n9.7.2 Parallelize using: foreach and doParallel\nThe normal for loop in R looks like:\n\nfor (i in 1:3) {\n  print(sqrt(i))\n}\n\n[1] 1\n[1] 1.414214\n[1] 1.732051\n\n\nThe foreach method is similar, but uses the sequential %do% operator to indicate an expression to run. Note the difference in the returned data structure.\n\nlibrary(foreach)\nforeach (i=1:3) %do% {\n  sqrt(i)\n}\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n\nIn addition, foreach supports a parallelizable operator %dopar% from the doParallel package. This allows each iteration through the loop to use different cores or different machines in a cluster. Here, we demonstrate with using all the cores on the current machine:\n\nlibrary(foreach)\nlibrary(doParallel)\n\nLoading required package: iterators\n\nregisterDoParallel(numCores)  # use multicore, set to the number of our cores\nforeach (i=1:3) %dopar% {\n  sqrt(i)\n}\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n# To simplify output, foreach has the .combine parameter that can simplify return values\n\n# Return a vector\nforeach (i=1:3, .combine=c) %dopar% {\n  sqrt(i)\n}\n\n[1] 1.000000 1.414214 1.732051\n\n# Return a data frame\nforeach (i=1:3, .combine=rbind) %dopar% {\n  sqrt(i)\n}\n\n             [,1]\nresult.1 1.000000\nresult.2 1.414214\nresult.3 1.732051\n\n\nThe doParallel vignette on CRAN shows a much more realistic example, where one can use `%dopar% to parallelize a bootstrap analysis where a data set is resampled 10,000 times and the analysis is rerun on each sample, and then the results combined:\n\n# Let's use the palmerpenguins data set to do a parallel bootstrap\n# From the doParallel vignette, but slightly modified\nnum_trials &lt;- 3000\nsystem.time({\n  r &lt;- foreach(icount(num_trials), .combine=rbind) %dopar% {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(bill_length[ind,1]~bill_length[ind,2], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n\n   user  system elapsed \n  8.597   0.219   4.677 \n\n# And compare that to what it takes to do the same analysis in serial\nsystem.time({\n  r &lt;- foreach(icount(num_trials), .combine=rbind) %do% {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(bill_length[ind,1]~bill_length[ind,2], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n\n   user  system elapsed \n  8.791   0.004   8.797 \n\n# When you're done, clean up the cluster\nstopImplicitCluster()"
  },
  {
    "objectID": "session_09.html#futures-with-furrr",
    "href": "session_09.html#futures-with-furrr",
    "title": "9  Parallel Processing",
    "section": "9.8 Futures with furrr",
    "text": "9.8 Futures with furrr\n\n\n\nWhile parallel and mclapply have been reliably working in R for years for multicore parallel processing, different approaches like clusterApply have been needed to run tasks across multiple nodes in larger clusters. The future package has emerged in R as a powerful mechanism to support many types of asynchronous execution, both within a single node and across a cluster of nodes, but all using a uniform evaluation mechanism across different processing backends. The basic idea behind future is that you can either implicitly or explicitly create a future expression, and control is returned to calling code while the expression is evaluated asynchronously, and possibly in parallel depending on the backend chosen.\n\n\n\n\n\n\nIn a familiar form, you can define a future expression analogously to how you define a function, using an expression such as:\n\nlibrary(future)\nf &lt;- future({\n    cat(\"Hello world!\\n\")\n    6.28\n})\nv &lt;- value(f)\n\nHello world!\n\nv\n\n[1] 6.28\n\n\nThis creates an expression that is evaluated and will have a result available sometime in the future, but we don’t know when… it could be seconds, minutes, or hours later, depending on the task complexity and available resources. What we can do is to ask the future expression to return its result when it is available using value(), which will block until the expression has been evaluated.\nAlternatively, we can check if the expression has completed its evaluation without blocking using the resolved() function, which returns TRUE when the future has been evaluated and the result value is ready to be retrieved. This lets us do other useful work while our process is waiting for the future expression to finish its work. For example, imagine the hypothetical model_run future, where we can do some other useful things while we wait for the model to run. To do this, we need to use a processing plan that uses multiple cores, such as multisession:\n\nplan(multisession)\ndownload_data &lt;- function() {\n    # Sleep, and just pretend to go get the data\n    Sys.sleep(0.5)\n    return(c(1,2,3))\n}\n\nrun_model &lt;- function(d) {\n    # Sleep, and just pretend to run a complicated model\n    Sys.sleep(0.5)\n    return(42)\n}\nmodel_result &lt;- future({\n    d &lt;- download_data()\n    result &lt;- run_model(d)\n    result\n})\n\nwhile(!resolved(model_result)) {\n    cat(\"Waiting for model task to complete...\\n\")\n    Sys.sleep(0.2)\n    cat(\"Do some more work now, like print this message...\\n\")\n}\n\nvalue(model_result)\n\nThe multisession futures plan is one in which each future task is evaluated in a separate, background R session that runs on the same host that launched the process. If the host has multiple cores available, then multisession will make use of these and create background sessions on each of the available cores. If all background sessions are busy, then the creation of new future expressions wil be blocked until one is available.\nSo, what’s the point of all of this? Basically, the future package provides a mechanism for evaluating expressions asynchronously, which we can leverage to launch many sessions on available cores. We can harness this with another package, furrr, which functions as an asynchrobous analogue to purrr.\n\n\n\nWith furrr, you can use the map() pattern to execute an expression across a set of inputs using asynchronous futures. Most of the details here are hidden under the hood, so this usage pattern should feel really familiar to purrr users. For example, let’s return to our bootstrap linear model, and reimplement it, first using purrr, and then with furrr.\n\n\n\n\n\n\n\nlibrary(purrr)\n\n\nAttaching package: 'purrr'\n\n\nThe following objects are masked from 'package:foreach':\n\n    accumulate, when\n\nsystem.time({\n  res_purrr &lt;- map(trials, boot_fx)\n})\n\n   user  system elapsed \n  9.394   0.000   9.396 \n\n\n\nlibrary(furrr)\nplan(multisession, workers=8)\nsystem.time({\n  res_furrr &lt;- future_map(trials, boot_fx, .options = furrr_options(seed = TRUE))\n})\n\n   user  system elapsed \n  0.684   0.008   8.054 \n\n\nSo basically by dropping in furrr::future_map as a replacement for purrr::map, we can see an immediate decrease in execution time, on my machine from 22 seconds down to 8 seconds. This is not as good as the improvement we saw with the other methods, which I attribute to the overhead of starting all of the background R sessions."
  },
  {
    "objectID": "session_09.html#summary",
    "href": "session_09.html#summary",
    "title": "9  Parallel Processing",
    "section": "9.9 Summary",
    "text": "9.9 Summary\nIn this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores. Next, we reviewed the way in which traditional for loops in R can be rewritten as functions that are applied to a list serially using lapply, and then how the parallel package mclapply function can be substituted in order to utilize multiple cores on the local computer to speed up computations. We also installed and reviewed the use of the foreach package with the %dopar operator to accomplish a similar parallelization using multiple cores. And finally, we reviewed the use of the furrr::future_map() function as a drop in replacement for map operations using asynchronous futures."
  },
  {
    "objectID": "session_09.html#readings-and-tutorials",
    "href": "session_09.html#readings-and-tutorials",
    "title": "9  Parallel Processing",
    "section": "9.10 Readings and tutorials",
    "text": "9.10 Readings and tutorials\n\nMulticore Data Science with R and Python\nBeyond Single-Core R by Jonoathan Dursi (also see GitHub repo for slide source)\nThe venerable Parallel R by McCallum and Weston (a bit dated on the tooling, but conceptually solid)\nThe doParallel Vignette\nfuture: Unified Parallel and Distributed Processing in R for Everyone\nfurrr"
  },
  {
    "objectID": "session_10.html",
    "href": "session_10.html",
    "title": "10  Hands On Synthesis",
    "section": "",
    "text": "Note\n\n\n\nTime for groups to get together and advance in their synthesis projects."
  },
  {
    "objectID": "session_11.html#learning-objectives",
    "href": "session_11.html#learning-objectives",
    "title": "11  Publishing Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nOverview best practices for organizing data for publication\nReview what science metadata is and how it can be used\nDemonstrate how data and code can be documented and published in open data archives"
  },
  {
    "objectID": "session_11.html#the-data-life-cycle",
    "href": "session_11.html#the-data-life-cycle",
    "title": "11  Publishing Data",
    "section": "11.1 The Data Life Cycle",
    "text": "11.1 The Data Life Cycle\nThe Data Life Cycle gives you an overview of meaningful steps data goes through in a research project, from planning to archival. This step-by-step breakdown facilitates overseeing individual actions, operations and processes required at each stage.\n\n\n\n\nStep\nDescription\n\n\n\n\nPlan\nMap out the processes and resources for the entire data life cycle. Start with the project goals (desired outputs, outcomes, and impacts) and work backwards to build a data management plan, supporting data policies, and sustainability plans.\n\n\nCollect\nObservations are made either by hand or with sensors or other instruments and the data are placed a into digital form. You can structure the process of collecting data up front to better implement data management.\n\n\nAssure\nEmploy quality assurance and quality control procedures that enhance the quality of data (e.g., training participants, routine instrument calibration) and identify potential errors and techniques to address them.\n\n\nDescribe\nDocument data by describing the why, who, what, when, where, and how of the data. Metadata, or data about data, are key to data sharing and reuse, and many tools such as standards and software are available to help describe data.\n\n\nPreserve\nPlan to preserve data in the short term to minimize potential losses (e.g., via accidents), and in the long term so that project stakeholders and others can access, interpret, and use the data in the future. Decide what data to preserve, where to preserve it, and what documentation needs to accompany the data.\n\n\nDiscover\nIdentify complementary data sets that can add value to project data. Strategies to help endure the data have maximum impact include registering the project on a project directory site, depositing data in an open repository, and adding data descriptions to metadata clearing houses.\n\n\nIntegrate\nData from multiple sources are combined into a form that can be readily analyzed. For example, you could combine citizen science project data with other sources of data to enable new analyses and investigations. Successful data integration depends on documentation of the integration process, clearly citing and making accessable the data you are using, and employing good data management practices throughout the Data Life Cycle.\n\n\nAnalyze\nCreate analyses and visualizations to identify patterns, test hypotheses, and illustrate finding. During this process record your methods, document data processing steps, and ensure your data are reproduceable. Learn about these best practices and more.\n\n\n\nIn this lesson we focus on the Describe and Preserve stages of this cycle. However, best practices on how to organize and document your data, apply to all stages."
  },
  {
    "objectID": "session_11.html#organizing-data",
    "href": "session_11.html#organizing-data",
    "title": "11  Publishing Data",
    "section": "11.2 Organizing Data",
    "text": "11.2 Organizing Data\nThe goal is to operate through the data life cycle with the FAIR and CARE principles in mind and making sure our data is in a tidy format.\n\n\n\nArtwork by Allison Horst\n\n\nBenefits of having clean and tidy data and complete metadata:\n\nDecreases errors from redundant updates\nEnforces data integrity\nHelps you and future researchers to handle large, complex datasets\nEnables powerful search filtering\n\nSome of the best practices to follow are (Borer et al. (2009), White et al. (2013)):\n\nHave scripts for all data wrangling that start with the uncorrected raw data file and clean the data programmatically before analysis.\nDesign your tables to add rows, not columns. A column should be only one variable and a row should be only one observation.\nInclude header lines in your tables\nUse non-proprietary file formats (ie, open source) with descriptive file names without spaces.\n\nNon-proprietary file formats are essential to ensure that your data can still be machine readable long into the future. Open formats include text files and binary formats such as NetCDF.\n\nCommon switches:\n\n\n\n\n\n\n\nProprietary format\nExport to…\n\n\n\n\nMicrosoft Excel (.xlsx) files\ntext (.txt) or comma separated values (.csv)\n\n\nGIS files\nESRI shapefiles (.shp)\n\n\nMATLAB/IDL\nNetCDF\n\n\n\n\n\n\n\n\n\nLarge Data Packages\n\n\n\nWhen you have or are going to generate large data packages (in the terabytes or larger), it’s important to establish a relationship with the data center early on.\nThe data center can help come up with a strategy to tile data structures by subset, such as by spatial region, by temporal window, or by measured variable. They can also help with choosing an efficient tool to store the data (ie NetCDF or HDF), which is a compact data format that helps parallel read and write libraries of data."
  },
  {
    "objectID": "session_11.html#metadata",
    "href": "session_11.html#metadata",
    "title": "11  Publishing Data",
    "section": "11.3 Metadata",
    "text": "11.3 Metadata\nWithin the data life cycle you can be collecting data (creating new data) or integrating data that has all ready been collected. Either way, metadata plays a major role to successfully spin around the cycle because it enables data reuse long after the original collection.\nImagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files?\nThe goal is to have enough information for the researcher to understand the data, interpret the data, and then reuse the data in another study.\nOne way to think about metadata is to answer the following questions with the documentation:\n\nWhat was measured?\nWho measured it?\nWhen was it measured?\nWhere was it measured?\nHow was it measured?\nHow is the data structured?\nWhy was the data collected?\nWho should get credit for this data (researcher AND funding agency)?\nHow can this data be reused (licensing)?\n\nReview detailed metadata guidelines on course material from week 1\nWe also know that it is important to keep in mind how will computers organize and integrate this information. There are a number of metadata standards that make your metadata machine readable and therefore easier for data curators to publish your data (Ecological Metadata Language, Geospatial Metadata Standards, Biological Data Profile, Darwin Core, Metadata Encoding Transmission Standard, etc.)\nToday we are going to be focusing on the Ecological Metadata Language also known as EML. Which is widespread use in the earth and environmental sciences.\n\n“The Ecological Metadata Language (EML) defines a comprehensive vocabulary and a readable XML markup syntax for documenting research data” (https://eml.ecoinformatics.org/)\n\nEML or XML? It’s confusing. EML or Ecological Metadata Language is the name of the metadata standard. EML are stored in an XML file. XML (Extensible Markup Language), is a markup language that provides rules to define any data. XML file extension is .xml. So an EML file will be somthing like metadata.xml, and it will look like this:\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;eml:eml packageId=\"df35d.442.6\" system=\"knb\" \n    xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.1\"&gt;\n    &lt;dataset&gt;\n        &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through \n            Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt;\n        &lt;creator id=\"1385594069457\"&gt;\n            &lt;individualName&gt;\n                &lt;givenName&gt;Mark&lt;/givenName&gt;\n                &lt;surName&gt;Willette&lt;/surName&gt;\n            &lt;/individualName&gt;\n            &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt;\n            &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt;\n            &lt;address&gt;\n                &lt;city&gt;Soldotna&lt;/city&gt;\n                &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt;\n                &lt;country&gt;USA&lt;/country&gt;\n            &lt;/address&gt;\n            &lt;phone phonetype=\"voice\"&gt;(907)260-2911&lt;/phone&gt;\n            &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt;\n        &lt;/creator&gt;\n        ...\n    &lt;/dataset&gt;\n&lt;/eml:eml&gt;"
  },
  {
    "objectID": "session_11.html#data-identifiers-citation",
    "href": "session_11.html#data-identifiers-citation",
    "title": "11  Publishing Data",
    "section": "11.4 Data Identifiers & Citation",
    "text": "11.4 Data Identifiers & Citation\nMany journals require a DOI - a digital object identifier - be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to.\nKeep in mind that generally, if the data package needs to be updated (which happens in many cases), each version of the package will get its own identifier. This way, researchers can and should cite the exact version of the data set that they used in their analysis. Having the data identified in this manner allows us to accurately track the dataset usage metrics.\nFinally, stressed that researchers should get in the habit of citing the data that they use (even if it’s their own data!) in each publication that uses that data. This is important for correct attribution, provenance of your work and ultimately transparency in the scientific process."
  },
  {
    "objectID": "session_11.html#provenance-and-computational-workflow",
    "href": "session_11.html#provenance-and-computational-workflow",
    "title": "11  Publishing Data",
    "section": "11.5 Provenance and Computational Workflow",
    "text": "11.5 Provenance and Computational Workflow\n\nWhile the Knowledge Network for Biocomplexity, and similar repositories do focus on preserving data, we really set our sights much more broadly on preserving entire computational workflows that are instrumental to advances in science. A computational workflow represents the sequence of computational tasks that are performed from raw data acquisition through data quality control, integration, analysis, modeling, and visualization.\nFor example, a data acquisition and cleaning workflow often creates a derived and integrated data product that is then picked up and used by multiple downstream analytical workflows that produce specific scientific findings. These workflows can each be archived as distinct data packages, with the output of the first workflow becoming the input of the second and subsequent workflows.\n\nAdding provenance within your work makes it more reproducible and compliant with the FAIR principles. It is also useful for building on the work of others; you can produce similar visualizations for another location, for example, using the same code.\nTools like R can be used as a provenance tool, as well - by starting with the raw data and cleaning it programmatically, rather than manually, you preserve the steps that you went through and your workflow is reproducible."
  },
  {
    "objectID": "session_11.html#preserving-your-data",
    "href": "session_11.html#preserving-your-data",
    "title": "11  Publishing Data",
    "section": "11.6 Preserving your data",
    "text": "11.6 Preserving your data\nThroughout this course we have mentioned over and over the importance of preserving your data so you or other can access it years down the line. Imagine how helpful it would be if each of the data sets you are trying to get from multiple people and agencies would be publish in a data package for reuse. How much time would you safe? How many emails back and forth could be avoided. Data is a valuable asset. It takes time, effort and resources to collect. It is important to preserve our data for transparency in our work and making sure we and other can re use it.\n\n\n\n\n11.6.1 Data repositories: built for data (and code)\n\nGitHub is not an archival location\nDedicated data repositories: KNB, Arctic Data Center, Zenodo, FigShare\n\nRich metadata\nArchival in their mission\n\nData papers, e.g., Scientific Data\nList of data repositories: http://re3data.org\n\n\nDataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses.\nDataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data form the dozens of members of DataONE, rather than visiting each of the currently 43 repositories one at a time.\n\n\n\n11.6.2 Structure of a data package\nWe define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. The rule of thumb is to package your data as you would like to receive it. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\n\nThese data repositories all assign a unique identifier to every version of every data file, Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets an internal identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4."
  },
  {
    "objectID": "session_11.html#publishing-data-from-the-web",
    "href": "session_11.html#publishing-data-from-the-web",
    "title": "11  Publishing Data",
    "section": "11.7 Publishing data from the web",
    "text": "11.7 Publishing data from the web\nEach data repository tends to have its own mechanism for submitting data and providing metadata. Repositories like the Knowledge Network for Biocomplexity (KNB) and Environmenta Data Initiative (EDI), provide some easy to use web forms for editing and submitting a data package.\n\n\n\n\n\n\nStable repositories for storing data during analysis\n\n\n\nNote that you can upload data to for example the KNB repository even if you are not ready to publish. This repository can serve as “storage” while you are working with your data. By storing your data in a repository you and your collaborators can access these data either through a url or data content id.\nWhile you are working on your project. The data package where you store your data will be privet. Only people that are granted access can see data package.\n\n\n\n11.7.1 Publishing Data to EDI\nThe Environmental Data Initiative (EDI) provides lots of great resources about data publication (Resources &gt; Data Authors) and data reuse (Resources &gt; Data Users).\nTo publish a data package on EDI you have to go to https://edirepository.org/ and click on “Publish Data”.\n\nThis will direct you to their Publish Data site where you can click on Login to ezEML metadata editor. ezEML is EDI’s web based tool to interactively fill in your data’s metadata.\nThis will direct you to the ezEML site, where you can log in with different methods. All methods are totally fine. We recommend ORCID. ORCID is a free and unique identifier for researchers. This identifier allows to distinguish researchers with similar names, or researcher who change their name through their professional career. Ensuring right attribution of their scientific contribution (papers, data packages, etc.) (Track Attribution with an ORCID iD, EDI.\nOnce you are logged in, you can create a New EML Document and start documenting your knew data package filling out all the required information.\n The left side bar menu allows you to easily navigate through the forms and fill in your metadata as you like. Note that at any point you can save and come back to it later.\n\n\n\n11.7.2 Publishing Data to KNB\nSimilarly, you can publish your data to the KNB repository by going to https://knb.ecoinformatics.org/ and signing in with your ORCID.\n\nOnce you sign in you click on “Submit Data” and it will direct you to the page where you can start filling in your metadata and uploading your data files.\n\nTo further familiarize our selves with the data publication process, we are going to practice “Submitting” a data package to KNB’s demo site."
  },
  {
    "objectID": "session_11.html#practice-publish-dummy-data-to-knb-demo-site",
    "href": "session_11.html#practice-publish-dummy-data-to-knb-demo-site",
    "title": "11  Publishing Data",
    "section": "11.8 Practice: Publish Dummy Data to KNB Demo Site",
    "text": "11.8 Practice: Publish Dummy Data to KNB Demo Site\n\n\n\n\n\n\nContext\n\n\n\n\nDuring Week 2 we explored and visualized the following data package EDI Data Repository Sacramento-San Joaquin Delta Socioecological Monitoring.\nImagine that you decided to replicate this study and collected data on visits to different restoration sites on the Sacramento-San Joaquin Delta during 2022-2023 (original data is from 2017-2018).\nYou have been keeping the data on GitHub, but you know that ultimately that is not a great place to store the data.\nIt is time to upload the data to the KNB repository and start the process that will lead you to publish your data and script in a data package.\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\nNote the first 2 steps are just for the purpose of this exercise\n\nGo to this GitHub Repo and “Fork” the repository to your GitHub Account by clicking on the Fork button on the upper right side of the\nBy Forking you are creating a new repo under your user name that contains all the information of the “Forked” Repo.\nContinuing with our hypothetical scenario, let’s imagine that this repo that we just forked is the repo we have been working all along, cleaning and visualizing our data.\nClone this repo to a new Rproject in the server by clicking on Code and copying the HTTPS URL. In RStudio, create a new Rproject, with version control and paste the URL you just copied.\nFinally, in this hypothetical case, you have been somewhat diligent on documenting the metadata for these data sets. You set up a metadata spreadsheet that allow you to keep track of the information about your data. This spreadsheet lives in the SharePoint folder. Follow the link or navigate to 2023 &gt; 2023-10-week-3 &gt; publishing-data-practice.\n\n\n\n\n11.8.1 Exercise\n\nGo to KNB Demo site and login via ORCID\nWe will walk through web submission on https://demo.nceas.ucsb.edu, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button.\n\nWhen you sign in, you will be redirected to orcid.org, where you can either provide your existing ORCID credentials, or create a new account. ORCID provides multiple ways to login, including using your email address, institutional logins from many universities, and logins from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login.\n\n\n\nCreate and submit the data set\nAfter signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. Required sections are listed with a red asterisk.\n\n\nClick Add Files to choose the data files for your package\nYou can select multiple files at a time to efficiently upload many files.\n\nThe files will upload showing a progress indicator. You can continue editing metadata while they upload.\n\n\n\nEnter Overview information\nThis includes a descriptive title, abstract, and keywords.\nThe title is the first way a potential user will get information about your dataset. It should be descriptive but succinct, lack acronyms, and include some indication of the temporal and geospatial coverage of the data.\nThe abstract should be sufficiently descriptive for a general scientific audience to understand your dataset at a high level. It should provide an overview of the scientific context/project/hypotheses, how this data package fits into the larger context, a synopsis of the experimental or sampling design, and a summary of the data contents.\n\nKeywords, while not required, can help increase the searchability of your dataset, particularly if they come from a semantically defined keyword thesaurus.\n\nOptionally, you can also enter funding information, including a funding number, which can help link multiple datasets collected under the same grant.\nSelecting a distribution license - either CC-0 or CC-BY is required.\n\n\n\nPeople Information\nInformation about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people:\n\nCreators - all the people who should be in the citation for the dataset\nContacts - one is required, but defaults to the first Creator if omitted\nPrincipal Investigators\nand any other that are relevant\n\nFor each, please strive to provide their ORCID identifier, which helps link this dataset to their other scholarly works.\n\n\n\nTemporal Information\nAdd the temporal coverage of the data, which represents the time period to which data apply.\n\n\n\nLocation Information\nThe geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that a spatial search will be more precise.\n\nNote that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global. For example:\n\nMission Canyon, Santa Barbara, California, USA\n\n\n\n\nMethods\nMethods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, include details of sampling methods, experimental design, quality assurance procedures, and computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data.\n\n\n\nSave a first version with Submit \nWhen finished, click the Submit Dataset button at the bottom.\nIf there are errors or missing fields, they will be highlighted.\nCorrect those, and then try submitting again. When you are successful, you should see a green banner with a link to the current dataset view. Click the X to close that banner, if you want to continue editing metadata.\n\nSuccess!\n\n\nFile and variable level metadata\nThe final major section of metadata concerns the structure and contents of your data files (Entities). In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure.\nFor example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information.\n\nThe Attributes tab is where you enter variable (aka attribute) information. In the case of tabular data (such as csv files) each column is an attribute, thus there should be one attribute defined for every column in your dataset. Attribute metadata includes:\n\nvariable name (for programs)\nvariable label (for display)\n\n\n\nvariable definition (be specific)\ntype of measurement\n\n\n\nunits & code definitions\n\n\nYou’ll need to add these definitions for every variable (column) in the file. When done, click Done.\n\nNow the list of data files will show a green checkbox indicating that you have full described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes.\nNote that attribute information is not relevant for data files that do not contain variables, such as the R script in this example. Other examples of data files that might not need attributes are images, pdfs, and non-tabular text documents (such as readme files). The yellow circle in the editor indicates that attributes have not been filled out for a data file, and serves as a warning that they might be needed, depending on the file.\n\nAfter you get the green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes.\n\n\nAdd workflow provenance\nUnderstanding the relationships between files in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, that are often then used in analysis and visualization code to produce final outputs. In DataONE, we support structured descriptions of these relationships, so one can see the flow of data from raw data to derived to outputs.\nOnce you have saved your data package, you can add provenance by navigating to the data table descriptions, and selecting the Add buttons to link the data and scripts that were used in your computational workflow. On the left side, select the Add circle to add an input data source to the delta_restoration_locations_visits_2022_2023.csv file. This starts building the provenance graph to explain the origin and history of each data object.\n\nThe linkage to the source dataset should appear.\n\nThen you can add the link to the source code that handled the conversion between the data files by clicking on Add arrow and selecting the R script. Note that you have to add the scrip as a file in your data package to be able to reference on the workflow.\n\nSelect the R script and click “Done.”\n\n\nThe diagram now shows the relationships among the data files and the R script, so click Submit to save another version of the package.\n\nEt voilà! A beautifully preserved data package!\n\n\n\n\nBorer, Elizabeth, Eric Seabloom, Matthew B. Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” Bulletin of the Ecological Society of America 90: 205–14. https://doi.org/10.1890/0012-9623-90.2.205.\n\n\nWhite, Ethan, Elita Baldridge, Zachary Brym, Kenneth Locey, Daniel McGlinn, and Sarah Supp. 2013. “Nine Simple Ways to Make It Easier to (Re)use Your Data.” Ideas in Ecology and Evolution 6 (2). https://doi.org/10.4033/iee.2013.6b.6.f."
  },
  {
    "objectID": "session_12.html#learning-objectives",
    "href": "session_12.html#learning-objectives",
    "title": "12  Git Workflows",
    "section": "12.1 Learning Objectives",
    "text": "12.1 Learning Objectives\nIn this lesson, you will learn:\n\nWhat is a branch in Git?\nHow to use a branch to organize code\nWhat is a tag in Git and how is it useful for collaboration?\nNew mechanisms to collaborate using Git\nWhat is a Pull Request in GitHub?\nHow to contribute code to colleague’s repository using Pull Requests"
  },
  {
    "objectID": "session_12.html#branches",
    "href": "session_12.html#branches",
    "title": "12  Git Workflows",
    "section": "12.2 Branches",
    "text": "12.2 Branches\nBranches are a mechanism to isolate a set of changes in their own thread, allowing multiple types of work to happen in parallel on a repository at the same time. These are most often used for separating work from multiple collaborators so they don’t conflict, or trying out experimental work, or for managing bug fixes for historical releases of software. The default branch in most new repositories is called main, and it is the branch that is typically shown in the GitHub interface and elsewhere.\n\n\n\n\n%%{init: { 'theme': 'neutral' } }%%\ngitGraph\n    checkout main\n    commit\n    commit\n    commit\n\n\n\n\n\nThe main branch of a repository is shown above with three commits, each representing one specific version of the repository linked to its parent commit.\nBut main isn’t the only branch you can use in a repository. You can create more. Maybe you’re not so sure that a new idea will work out in code, and this is where a tool like Git shines. Without a tool like Git, we might copy analysis.R to another file called analysis-ml.R which might end up having mostly the same code except for a few lines. This isn’t particularly problematic until you want to make a change to a bit of shared code and now you have to make changes in two files, if you even remember to.\nInstead, with Git, we can start a branch. Branches allow us to confidently experiment on our code, all while leaving the old code intact and recoverable.\n\n\n\n\n%%{init: { 'theme': 'neutral' } }%%\ngitGraph\n    checkout main\n    commit\n    commit\n    commit\n    branch new-analysis-ml\n    commit\n    commit\n\n\n\n\n\nSo you’ve been working in a branch and have made a few commits on it and your boss emails again asking you to update the model in some way. If you weren’t using a tool like Git, you might panic at this point because you’ve rewritten much of your analysis to use a different method but your boss wants the change to the old method.\nBut with Git and branches, we can continue developing our main analysis at the same time as we are working on any experimental branches. Using git checkout main, you can return the state of your working copy to the main branch, and make the needed change to the original code, all while saving the code in new-analysis-ml for later. Branches are great for experiments but also great for organizing your work generally, especially when each branch is tied to a ticket to discuss progress and issues on that line of work.\n\n\n\n\n%%{init: { 'theme': 'neutral' } }%%\ngitGraph\n    checkout main\n    commit\n    commit\n    commit\n    branch new-analysis-ml\n    commit\n    commit\n    checkout main\n    commit\n\n\n\n\n\nAfter all that hard work on the machine learning experiment, you and your colleague could decide to scrap it. It’s perfectly fine to leave branches around and switch back to the main line of development, but we can also delete them to tidy up.\nIf, instead, you and your colleague had decided you liked the machine learning experiment, you could also merge the branch with your main development line. Merging branches is analogous to accepting a change in Word’s Track Changes feature but way more powerful and useful. With the merge, we take all of the changes from the branch, and apply them to the files in the target main branch (or whatever branch we are merging into). This might merge smoothly, or may generate a conflict that will need to be resolved if the same files had already been changed on main. Either way, when complete, there is a new merge commit showing where the two branches came together.\n\n\n\n\n%%{init: { 'theme': 'neutral' } }%%\ngitGraph\n    checkout main\n    commit\n    commit\n    commit\n    branch new-analysis-ml\n    commit\n    commit\n    checkout main\n    commit\n    checkout new-analysis-ml\n    commit\n    checkout main\n    merge new-analysis-ml tag: \"final-paper-version\"\n    commit\n\n\n\n\n\nA key takeaway here is that Git can drastically increase your confidence and willingness to make changes to your code and help you avoid problems down the road. Analysis rarely follows a linear path and we need a tool that respects this. The key to that confidence, however, is in understanding what git does with your versions, and how to get back to a previous state when needed.\nFinally, imagine that, years later, your colleague asks you to make sure the model you reported in a paper you published together was actually the one you used. Another really powerful feature of Git is tags which allow us to label a particular version of our code with a meaningful name. In this case, we are lucky because we tagged the version of our code we used to run the analysis at the time the paper was submitted. Even if we continued to develop beyond last commit (above) after we submitted our manuscript, we can always go back and run the analysis as it was in the past.\n\n12.2.1 Exercise\nCreate a new branch in your training repository called new-feature, and then make changes to the RMarkdown files in the directory. Commit and push those changes to the branch. Now you can switch between branches using the GitHub interface.\nThis can be done directly in the RStudio interface within the Git pane using the new branch icon. Alternatively, you can run this line in the Terminal which will create a new branch and switch to it: git checkout -b new-feature\n\n\n\n\n\nAfter you click that icon, a new dialog box appears where you can create a new branch. Keep the setting to “origin” and keep the box “Sync branch with remote” checked.\n\n\n\n\n\nYou can now work on the new-feature branch in this new workspace independently of the codebase in main, and without affecting it. When you have finished and completed your changes, you can either decide to leave it there, delete it, or merge it back to main. To merge this new feature branch back, first checkout the main branch with git checkout main, and make a change to one of the files in your main branch – commit that change, and you will see how you can have different changes stored in different branches. Your repository history should look something like this:\n\nWhen you are ready to merge the feature branch back into main, all you have to do is run git merge new-feature from the commandline. After that is complete, your new feature will now be merged back into main and visible from the git history:"
  },
  {
    "objectID": "session_12.html#why-git-workflows-are-important",
    "href": "session_12.html#why-git-workflows-are-important",
    "title": "12  Git Workflows",
    "section": "12.3 Why Git workflows are important",
    "text": "12.3 Why Git workflows are important\n\n\n\nSource: Atlassian\n\n\nWhen working with a team on a Git-managed project, it’s important to make sure the team is in agreement on how the flow of changes will be applied. To ensure the team is on the same page, an agreed-upon Git workflow should be developed or selected (maybe something to add your DMP). There are several documented Git workflows that may be a good fit for your team. In this lesson, we will discuss two of these Git workflow options:\n\nBranch Workflow\nForking Workflow\n\nRemember that these workflows are designed to be guidelines rather than concrete rules. We want to show you what’s possible, so you can mix and match aspects from different workflows to suit your project’s needs."
  },
  {
    "objectID": "session_12.html#pull-requests",
    "href": "session_12.html#pull-requests",
    "title": "12  Git Workflows",
    "section": "12.4 Pull Requests",
    "text": "12.4 Pull Requests\n\n\n\nSource: Atlassian\n\n\nIn previous chapters, we’ve gone over how to directly collaborate on a repository with colleagues by granting them write privileges as a Collaborator to your repository. This is useful with close collaborators, but also grants them tremendous latitude to change files and analyses, to remove files from the working copy, and to modify all files in the repository.\nPull requests represent a mechanism to more judiciously collaborate, one in which:\n\nFirst, a Collaborator can suggest changes to a repository\nThen, the Owner and Collaborator can discuss those changes in a structured way\nFinally, the Owner can review and accept all or only some of those changes to the repository\n\nThis is useful with open source code where a community is contributing to shared analytical software, to students in a lab working on related but not identical projects, and to others who want the capability to review changes as they are submitted.\nWe’ll be using Pull Requests in both the branch and forking workflow. Using pull requests with each of these workflows is slightly different, but the general process is as follows:\n\nA Collaborator creates a feature in their local repository\nCollaborator pushes the branch to the GitHub repository they’re collaborating on\nCollaborator opens a Pull Request on GitHub\nOwner or the rest of the team reviews the code, discusses it, and alters it (as neded)\nOwner or project maintainer merges the feature into the repository and closes the Pull Request"
  },
  {
    "objectID": "session_12.html#branch-workflow",
    "href": "session_12.html#branch-workflow",
    "title": "12  Git Workflows",
    "section": "12.5 Branch Workflow",
    "text": "12.5 Branch Workflow\nBranches are a mechanism to isolate a set of changes in their own thread, allowing multiple types of work to happen in parallel on a repository at the same time. These are most often used for trying out experimental work, or for managing bug fixes for historical releases of software. Here’s an example graph showing a branch2.1 that has changes in parallel to the main branch of development:\n\n\n\n\n\nThe default branch in almost all repositories is called main, and it is the branch that is typically shown in the GitHub interface and elsewhere. There are many mechanisms to create branches. The one we will try is through RStudio, in which we use the branch dialog to create and switch between branches.\n\n12.5.1 How it Works\nThe core idea behind the Branch Workflow is that all project development should take place in a dedicated branch instead of the main branch. This makes it easy for multiple Collaborators to work on a particular part of the project without disturbing the main codebase. It also means the main branch will never contain broken code.\nIn general a branch workflow follows these steps:\n\n\n\n\n\n\nBIG IDEA\n\n\n\nOwner and Collaborator work in the same repository, but in different branches outside of main\n\n\n\n12.5.1.1 Collaborator, in their workspace, creates a new branch for the feature or development they’re working on\nThis can be done directly in the RStudio interface within the Git pane using the new branch icon. Alternatively, you can run this line in the Terminal which will create a new branch and switch to it: git checkout -b ＜new-branch＞\n\n\n\n\n\nAfter you click that icon, a new dialog box appears where you can create a new branch. Keep the setting to “origin” and keep the box “Sync branch with remote” checked.\n\n\n\n\n\nThis new branch has effectively created a copy of the main in this new workspace where you can work on a new feature independently of the codebase in main without affecting it.\n\n\n12.5.1.2 When Collaborator has finalized their work, the will open and submit a Pull Request to the main branch\nWhen you go to GitHub to open a Pull Request, GitHub will recognize that a new change has been pushed to the remote repository and will suggest opening a pull request.\n\nWhen you open a new Pull Request, you will be pushing your changes from your new branch to main. The Pull Request will automatically compare the codebase for both workspaces to determine if the two can be successfully merged. If not, you will need to reconcile the merge conflict before completing the Pull Request. If you see that you are “Able to merge” the two branches, then move forward by clicking “Create pull request.\n\n\n\n12.5.1.3 Ideally, Owner reviews the Pull Request and discusses new code changes as necessary with Collaborator\nIt’s important to have a review process to avoid potential issues. In this case, the author of the Pull Request is being closed (and therefore not merged) since the pull request does not target the correct branch of the workflow. Ultimately, these discussions lead to greater successfull and smoother workflows on a collaborative project.\n\n\n\n\n\n\n\n12.5.1.4 Owner merges the new feature or development into main and closes the Pull Request\nOnce the Pull Request has been reviewed, the reviewer can merge in the new feature which automatically closes the pull request.\n\n\n\n12.5.1.5 Everyone switches to main in their local workspace and Pulls so that they’re in sync with the new changes in the remote main\nIf you have merged all your new features into main from the new branch and will no longer need use the new branch, pull to sync up your local workspace (because merging creates a new commit and this commit exists on GitHub since it was created in GitHub). And to keep your repository clean, delete the branch. And note that you can always restore the branch if you need to return to it.\nNow pair up with a partner and try this flow out in the Branch exercise.\n\n\n\n12.5.2 Exercise: create a branch, make a change in a file, and submit a Pull Request\n\n\n\n\n\n\nInstructions\n\n\n\n\nMake sure you’re in your {ownerName}-{collaboratorName} repository that you created in the R Practice: Collaborating on, Wrangling & Visualizing Data session\nChoose who is the Owner and who is the Collaborator\nHave the Collaborator make a new branch called new-data-viz\nIn the new, new-data-viz branch, the Collaborator will open the lobster-report.Rmd file and add some new visualization code to the file.\nCollaborator uses the Git workflow: Save -&gt; Stage -&gt; Commit -&gt; Pull -&gt; Push\nCollaborator submits a Pull Request where they want to merge changes from new-data-viz to main\nOwner reviews the Pull Request and discusses new code with Collaborator, if needed\nAfter the review, Owner accepts the Pull Request, which will merge the new changes and close it\nSwitch roles and repeat steps 1-8, where the Owner will add new vizualization code to the lobster-report.Rmd"
  },
  {
    "objectID": "session_12.html#forking-workflow",
    "href": "session_12.html#forking-workflow",
    "title": "12  Git Workflows",
    "section": "12.6 Forking Workflow",
    "text": "12.6 Forking Workflow\nThe Forking Workflow is fundamentally different than other popular Git workflows. Instead of using a single Git repository to act as the “central” codebase, it gives every Collaborator their own repository to work in. This means they are not a Collaborator on the repository they have forked from and therefore do not have write access to that original repository.\nThe Forking Workflow is most often seen in public open source projects.\nThe main advantage of the Forking Workflow is that contributions can be integrated without the need for everybody to push to a single central repository. Collaborators push to their own repositories, and only the Owner can push to the official repository. This allows the Owner to accept commits from any Collaborator without giving them write access to the official codebase.\nThe result is a distributed workflow that provides a flexible way for large, organic teams (including untrusted third-parties) to collaborate securely. This also makes it an ideal workflow for open source projects.\n\n12.6.1 How it Works\n\n\n\n\n\n\nBIG IDEA\n\n\n\nOwner and Collaborator work in their own different repositories, where the Collaborator has forked the Owner’s repository (aka the Owner repository is the “original” repository). The fork is a link to the original repository that allows the Collaborator to submit Pull Requests.\n\n\nIn the following example, mbjones will be the repository Owner, and metamattj will be the Collaborator.\n\n12.6.1.1 Collaborator first creates a fork of the owner’s repository\nThis fork is a cloned copy of the Owner’s repository that is separate but linked to the Owner’s repository. Here we see that the mbjones/training-test repository has been forked once.\nTo create a forked repository, visit the GitHub page for the Owner’s repository that you’d like to make changes to, and click the Fork button. This will create a clone of that repository in your own GitHub account that can be cloned to your local computer.\n\n\n\n12.6.1.2 Collaborator clones their new forked repository into their local workspace\nThis cloned copy is in the Collaborator’s GitHub account, which means they have the ability to make changes to it. But they don’t have the right to change the original owner’s copy. So instead, they clone their GitHub copy onto their local machine, which makes the collaborator’s GitHub copy the origin as far as they are concerned. In this scenario, we generally refer to the Collaborator’s repository as the remote origin, and the Owner’s repository as upstream.\n\n\n12.6.1.3 Collaborator makes changes to forked repository\nAgain, these changes are independent of the Owner’s repository that the Collaborator forked from. When they have completed their changes, they complete the Git workflow and push the changes to their remote forked repository.\nAt this point, the Collaborator’s local repository and their GitHub copy both have the changes that they made, but the Owner’s repository has not yet been changed.\n\n\n\n12.6.1.4 Open a Pull Request to the upstream repository\nSimilar to the branch workflow, Collaborator submits a Pull Request (there may be be a notification on their repository from GitHub saying to compare and open a pull request). But dissimilar from the branch workflow, the Pull Request will be asking to submit changes from the Collaborator’s forked repository to the Owner’s upstream repository.\nWhen the Collaborator’s visits their respository on GitHub, they will also see a message that says: This branch is 1 commit ahead of mbjones:main.\n\nWhen the Collaborator clicks the named Pull Request button to create a pull request for the Owner to review, the Collaborator can provide a brief summary of the request, and a more detailed message to start a conversation about what you are requesting. It’s helpful to be polite and concise while providing adequate context for your request. This will start a conversation with the Owner in which you can discuss your changes, they can easily review the changes, and they can ask for further changes before the accept and pull them in. The Owner of the repository is in control and determines if and when the changes are merged.\n\n\n\n12.6.1.5 Owner reviews Pull Request\nOwner will get an email or GitHub notification that the Pull Request was created, and can see the pull request listed in their “Pull requests” tab of their repository.\n\nThe Owner can initiate a conversation about the change, or request further changes. The GitHub interface indicates whether there are any conflicts with the changes, and if not, gives the Owner the option to “Merge pull request”.\n\n\n\n12.6.1.6 Owner merges Pull Request\nOnce the Owner thinks the changes look good, they can click the “Merge pull request” button to accept the changes and pull them into their repository copy. The Owner can edit the message, and then click “Confirm merge”.\n\nYay, the Pull Request has now been merged into the Owner’s copy, and has been closed with a note indicating that the changes have been made!\n\n\n\n12.6.1.7 Collaborator syncs their forked repository with Owner’s repository\nNow that the pull request has been merged, there is a new merge commit in the Owner’s repository that is not present in either of the Collaborator’s repositories. To fix that, Collaborator needs to sync changes from the upstream repository into the Collaborator’s forked repository on GitHub, and then pull those changes into their local repository.\n\n\n\n\n\n\n\n\n\n\n\nUsing Git in the Terminal to set and pull from upstream\n\n\n\nTo add a reference to the upstream remote (the repository you made your fork from) run this in the Terminal: git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git\nThen to pull from the main branch of the upstream repository run in the Terminal: git pull upstream main\n\n\nAt this point, the collaborator is fully up to date.\nAs a final overview, when working with forked pull requests, there are four copies of the git repository (2 on the GitHub server for the owner and collaborator, and two on their local machines). In this diagram, you can see that, from the collaborator’s point of view, a git push will affect their origin repository, but they also need to git pull from the Owner’s upstream repository in order to sync their changes.\n\nRemember: there is more than one way to do it!\nThere is no one-size-fits-all Git workflow, but ultimately a workflow should be simple and enhance the productivity of your team."
  },
  {
    "objectID": "session_12.html#resources",
    "href": "session_12.html#resources",
    "title": "12  Git Workflows",
    "section": "12.7 Resources",
    "text": "12.7 Resources\n\nAtlassian’s Comparing Git Workflows"
  },
  {
    "objectID": "session_13.html",
    "href": "session_13.html",
    "title": "13  Hands On Synthesis",
    "section": "",
    "text": "Note\n\n\n\nTime for groups to get together and advance in their synthesis projects."
  },
  {
    "objectID": "session_14.html#material-to-include",
    "href": "session_14.html#material-to-include",
    "title": "14  Wrap up and Presentations",
    "section": "14.1 Material to include:",
    "text": "14.1 Material to include:\n\nProject Messaging\nMethods / Analytical Process\nNext Steps\n\n\n14.1.1 Project Messaging\nPresent your message box. High Level. You can structure it within the envelope style visual format or as a section based document.\nMake sure to include:\n\nAudience\nIssue\nProblem\nSo What?\nSolution\nBenefits\n\n\n\n\n14.1.2 Methods / Analytical Process\nProvide an update on your approaches to solving your ‘Problem’. How are you tackling this? If multiple elements, describe each. Present the workflow for your synthesis.\n\n\n\n14.1.3 Next Steps\nArticulate your plan for the next steps of the project. Some things to consider as you plan:"
  }
]