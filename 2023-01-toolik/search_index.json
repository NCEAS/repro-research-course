[["index.html", "Toolik Workshop: Intro to Data Publishing and Data Portals About", " Toolik Workshop: Intro to Data Publishing and Data Portals January 18, 2023 About This half-day in-person workshop will provide researchers with an overview of reproducible and ethical research practices, steps and methods for more easily documenting and preserving their data at the Arctic Data Center, and an introduction to data portals. Responsible and reproducible data management practices will be discussed as they apply to all aspects of the data life cycle. This includes ethical data collection and data sharing, data sovereignty, and the CARE principles. The CARE principles are guidelines that help ensure open data practices (like the FAIR principles) appropriately engage with Indigenous Peoples’ rights and interests. Schedule Code of Conduct Please note that by participating in this activity you agree to abide by the NCEAS Code of Conduct. About this book These written materials reflect the continuous development of learning materials at the Arctic Data Center and NCEAS to support individuals to understand, adopt, and apply ethical open science practices. In bringing these materials together we recognize that many individuals have contributed to their development. The primary authors are listed alphabetically in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials. This work is licensed under a Creative Commons Attribution 4.0 International License. Citation: Amber E. Budden, S. Jeanette Clark, Natasha Haycock-Chavez, Noor Johnson, Matthew B. Jones, Daphne Virlar-Knight. 2023. Toolik Workshop: Intro to Data Publishing and Data Portals. Additional contributors: Stephanie Hampton, Jim Regetz, Bryce Mecum, Julien Brun, Julie Lowndes, Erin McLean, Andrew Barrett, David LeBauer, Jessica Guo. "],["welcome-and-introductions.html", "1 Welcome and Introductions 1.1 Introduction to the Arctic Data Center and NSF Standards and Policies", " 1 Welcome and Introductions In addition to individual workshops, the Arctic Data Center also teaches three week-long workshops throughout the year. These courses cover the fundamentals of open data sharing, reproducible research techniques, ethical data use and reuse, and scalable computing for reusing large data sets. 1.1 Introduction to the Arctic Data Center and NSF Standards and Policies 1.1.1 Learning Objectives In this lesson, we will discuss: The mission and structure of the Arctic Data Center How the Arctic Data Center supports the research community About data policies from the NSF Arctic program 1.1.2 Arctic Data Center - History and Introduction The Arctic Data Center is the primary data and software repository for the Arctic section of National Science Foundation’s Office of Polar Programs (NSF OPP). We’re best known in the research community as a data archive – researchers upload their data to preserve it for the future and make it available for re-use. This isn’t the end of that data’s life, though. These data can then be downloaded for different analyses or synthesis projects. In addition to being a data discovery portal, we also offer top-notch tools, support services, and training opportunities. We also provide data rescue services. NSF has long had a commitment to data reuse and sharing. Since our start in 2016, we’ve grown substantially – from that original 4 TB of data from ACADIS to now over 65 TB. In 2021 alone, we saw 16% growth in dataset count, and about 30% growth in data volume. This increase has come from advances in tools – both ours and of the scientific community, plus active community outreach and a strong culture of data preservation from NSF and from researchers. We plan to add more storage capacity in the coming months, as researchers are coming to us with datasets in the terabytes, and we’re excited to preserve these research products in our archive. We’re projecting our growth to be around several hundred TB this year, which has a big impact on processing time. Give us a heads up if you’re planning on having larger submissions so that we can work with you and be prepared for a large influx of data. The data that we have in the Arctic Data Center comes from a wide variety of disciplines. These different programs within NSF all have different focuses – the Arctic Observing Network supports scientific and community-based observations of biodiversity, ecosystems, human societies, land, ice, marine and freshwater systems, and the atmosphere as well as their social, natural, and/or physical environments, so that encompasses a lot right there in just that one program. As of 2022, we’ve begun classifying datasets by discipline using an ontology system. Along with that diversity of disciplines comes a diversity of file types. The most common file type we have are image files in four different file types. Probably less than 200-300 of the datasets have the majority of those images – we have some large datasets that have image and/or audio files from drones. Most of those 6600+ datasets are tabular datasets. There’s a large diversity of data files, though, whether you want to look at remote sensing images, listen to passive acoustic audio files, or run applications – or something else entirely. We also cover a long period of time, at least by human standards. The data represented in our repository spans across centuries. We also have data that spans the entire Arctic, as well as the sub-Arctic, regions. 1.1.3 Data Discovery Portal To browse the data catalog, navigate to arcticdata.io. Go to the top of the page and under data, go to search. Right now, you’re looking at the whole catalog. You can narrow your search down by the map area, a general search, or searching by an attribute. Clicking on a dataset brings you to this page. You have the option to download all the files by clicking the green “Download All” button, which will zip together all the files in the dataset to your Downloads folder. You can also pick and choose to download just specific files. All the raw data is in open formats to make it easily accessible and compliant with FAIR principles – for example, tabular documents are in .csv (comma separated values) rather than Excel documents. The metrics at the top give info about the number of citations with this data, the number of downloads, and the number of views. This is what it looks like when you click on the Downloads tab for more information. Scroll down for more info about the dataset – abstract, keywords. Then you’ll see more info about the data itself. This shows the data with a description, as well as info about the attributes (or variables or parameters) that were measured. The green check mark indicates that those attributes have been annotated, which means the measurements have a precise definition to them. Scrolling further, we also see who collected the data, where they collected it, and when they collected it, as well as any funding information like a grant number. For biological data, there is the option to add taxa. 1.1.4 Tools and Infrastructure Across all our services and partnership, we are strongly aligned with the community principles of making data FAIR (Findable, Accesible, Interoperable and Reusable). We have a number of tools available to submitters and researchers who are there to download data. We also partner with other organizations, like Make Data Count and DataONE, and leverage those partnerships to create a better data experience. One of those tools is provenance tracking. With provenance tracking, users of the Arctic Data Center can see exactly what datasets led to what product, using the particular script that the researcher ran. Another tool are our Metadata Quality Checks. We know that data quality is important for researchers to find datasets and to have trust in them to use them for another analysis. For every submitted dataset, the metadata is run through a quality check to increase the completeness of submitted metadata records. These checks are seen by the submitter as well as are available to those that view the data, which helps to increase knowledge of how complete their metadata is before submission. That way, the metadata that is uploaded to the Arctic Data Center is as complete as possible, and close to following the guideline of being understandable to any reasonable scientist. 1.1.5 Support Services Metadata quality checks are the automatic way that we ensure quality of data in the repository, but the real quality and curation support is done by our curation team. The process by which data gets into the Arctic Data Center is iterative, meaning that our team works with the submitter to ensure good quality and completeness of data. When a submitter submits data, our team gets a notification and beings to evaluate the data for upload. They then go in and format it for input into the catalog, communicating back and forth with the researcher if anything is incomplete or not communicated well. This process can take anywhere from a few days or a few weeks, depending on the size of the dataset and how quickly the researcher gets back to us. Once that process has been completed, the dataset is published with a DOI (digital object identifier). 1.1.6 Training and Outreach In addition to the tools and support services, we also interact with the community via trainings like this one and outreach events. We run workshops at conferences like the American Geophysical Union, Arctic Science Summit Week and others. We also run an intern and fellows program, and webinars with different organizations. We’re invested in helping the Arctic science community learn reproducible techniques, since it facilitates a more open culture of data sharing and reuse. We strive to keep our fingers on the pulse of what researchers like yourselves are looking for in terms of support. We’re active on Twitter to share Arctic updates, data science updates, and specifically Arctic Data Center updates, but we’re also happy to feature new papers or successes that you all have had with working with the data. We can also take data science questions if you’re running into those in the course of your research, or how to make a quality data management plan. Follow us on Twitter and interact with us – we love to be involved in your research as it’s happening as well as after it’s completed. 1.1.7 Data Rescue We also run data rescue operations. We digitiazed Autin Post’s collection of glacier photos that were taken from 1964 to 1997. There were 100,000+ files and almost 5 TB of data to ingest, and we reconstructed flight paths, digitized the images of his notes, and documented image metadata, including the camera specifications. 1.1.8 Who Must Submit Projects that have to submit their data include all Arctic Research Opportunities through the NSF Office of Polar Programs. That data has to be uploaded within two years of collection. The Arctic Observing Network has a shorter timeline – their data products must be uploaded within 6 months of collection. Additionally, we have social science data, though that data often has special exceptions due to sensitive human subjects data. At the very least, the metadata has to be deposited with us. Arctic Research Opportunities (ARC) Complete metadata and all appropriate data and derived products Within 2 years of collection or before the end of the award, whichever comes first ARC Arctic Observation Network (AON) Complete metadata and all data Real-time data made public immediately Within 6 months of collection Arctic Social Sciences Program (ASSP) NSF policies include special exceptions for ASSP and other awards that contain sensitive data Human subjects, governed by an Institutional Review Board, ethically or legally sensitive, at risk of decontextualization Metadata record that documents non-sensitive aspects of the project and data Title, Contact information, Abstract, Methods For more complete information see our “Who Must Submit” webpage Recognizing the importance of sensitive data handling and of ethical treatment of all data, the Arctic Data Center submission system provides the opportunity for researchers to document the ethical treatment of data and how collection is aligned with community principles (such as the CARE principles). Submitters may also tag the metadata according to community develop data sensitivity tags. We will go over these features in more detail shortly. 1.1.9 Summary All the above information can be found on our website or if you need help, ask our support team at support@arcticdata.io or tweet us @arcticdatactr! "],["open-data-and-reproducibility.html", "2 Open Data and Reproducibility 2.1 Introduction to Reproducible Research 2.2 Related resources and references", " 2 Open Data and Reproducibility 2.1 Introduction to Reproducible Research 2.1.1 Learning Objectives Shared understanding of open and research reproducibility Introduction to computational reproducibility Introduction to FAIR and CARE as they relate to open data Familiarity with metadata best practices 2.1.2 What do we mean by open science and research reproducibility? Throughout this course you will hear us emphasize open practices and research reproducibility. Indeed, as a data repository charged with both preserving and making accessible the products of NSF funded research, it is no surprise that these topics are central to our mission. However, these terms, while readily parsed, can mean different things to different individuals depending on their frame of reference and research activities. Further, even with a shared definition variation exists in the degree to which openness and research reproducibility can (or should) be implemented. Before we begin, we are going to take the time to discuss and possibly develop a shared understanding of the terms ‘open’ and ‘reproducible’ as they pertain to research. Discuss In small groups answer the following questions: What does open research mean to you? Do you consider your own research practices open? If not, why not? If so, how so? What does reproducible research mean to you? Who should be able to reproduce research? Is your own research reproducible? If not, why not? If so, how so? Identify an individual to summarize and report back to the full group. 2.1.3 What is research reproducibility and how does it relate to open science? Reproducibility is a hallmark of the scientific research process, which is based on empirical observations coupled with explanatory models. Whether integrating data from across multiple studies and sources, or working with your own data, the data life cycle typically involves some degree of data collection/integration, quality assurance practices, analysis and synthesis. Operating in a reproducible fashion means that each of these steps can be easily re-executed to achieve the same result, ideally as part of a single workflow. Reproducibility means different things to different researchers. For our purposes, practical reproducibility looks like: Preserving the data Preserving the software workflow Documenting what you did Describing how to interpret it all Reproducibility does not, by definition, require openness. Reproducibility can be achieved within a single research activity or across a research program with a closed group of collaborators. However, when working in an OPEN and REPRODUCIBLE manner, we are better able to transform knowledge into benefits for society. In this section we will expand on the benefits of reproducible research and open science before highlighting some best practices. 2.1.4 Open Science To enable full reproducibility by the broader community; researchers, practitioners, policy makers etc, all products of the research activity need to be accessible - open data, open code, and open publications. Further, full research transparency also requires open peer review. There are, of course, data sensitivities and ethical considerations regarding open everything and these will be discussed later. At its core, the aims of Open Science are to: Increase transparency of the research process Enable reproducibility of results and conclusions Accelerate discovery Enhance and facilitate collaboration Increase diversity, equity and inclusion Transform knowledge into benefits for society So, why is reproducible research important? Working in a reproducible manner builds efficiencies into your own research practices. The ability to automate processes and rerun analyses as you collect more data, or share your full workflow (including data, code and products) with colleagues, will accelerate the pace of your research and collaborations. However, beyond these direct benefits, reproducible research builds trust in science with the public, policy makers and others. What data were used in this study? What methods applied? What were the parameter settings? What documentation or code are available to us to evaluate the results? Can we trust these data and methods? Are the results reproducible? Ionnidis (2005) contends that “Most research findings are false for most research designs and for most fields”, and a study of replicability in psychology experiments found that “Most replication effects were smaller than the original results” (Open Science Collaboration, 2015). In the case of ‘climategate’, it took three years, and over 300 personnel, to gather the necessary provenance information in order to document how results, figures and other outputs were derived from input sources. Time and effort that could have been significantly reduced with appropriate documentation and reproducible practices. Moving forward, through reproducible research training, practices, and infrastructure, the need to manually chase this information will be reduced enabling replication studies and great trust in science. Computational reproducibility Computational reproducibility is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions. To be able to evaluate the data, analyses, and models on which conclusions are drawn, computational reproducibility requires open science approaches, including straightforward steps for archiving data and code openly along with the scientific workflows describing the provenance of scientific results (e.g., Hampton et al. (2015), Munafò et al. (2017)). Scientific workflows encapsulate all of the steps from data acquisition, cleaning, transformation, integration, analysis, and visualization. Workflows can range in detail from simple flowcharts to fully executable scripts. R scripts and python scripts are a textual form of a workflow, and when researchers publish specific versions of the scripts and data used in an analysis, it becomes far easier to repeat their computations and understand the provenance of their conclusions. Preserving computational workflows enables understanding, evaluation, and reuse for the benefit of future you and your collaborators and colleagues across disciplines. 2.1.5 A Note on Community Principles: CARE and FAIR In facilitating use of data resources, the community have converged on principles surrounding best practices for open data management. One set of these principles is the FAIR principles: Findable, Accessible, Interoperable, and Reproducible. FAIR principles and open science are overlapping, but distinct concepts. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. Note, that accessibility in FAIR does not require the data to be open. The guiding principles put forward by FORCE 11 for accessibility are that: (meta)data are retrievable by their identifier using a standard communicaitons protocol the protocol is open, free, and universally implementable the protocol allows for an authentication and authorization procedure, where neccesary metadata are accessible, even when the data are no longer available The CARE Principles for Indigenous Data Governance complement the more data-centric approach of the FAIR principles, introducing social responsibility to open data management practices. The CARE Principles stand for: Collective Benefit Authority to Control Responsibility Ethics These will be discussed more comprehensively during the data ethics section of the course 2.1.6 Metadata Best Practices Well structured and described data unpin the FAIR principles and we’ll visit how best to organize your data in a tidy and effective manner (in both theory and practice) shortly. Here we focus on metadata best practices. Metadata (data about data) is an important part of the data life cycle because it enables data reuse long after the original collection. Imagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files? The goal is to have enough information for the researcher to understand the data, interpret the data, and then re-use the data in another study. Another way to think about it is to answer the following questions with the documentation: What was measured? Who measured it? When was it measured? Where was it measured? How was it measured? How is the data structured? Why was the data collected? Who should get credit for this data (researcher AND funding agency)? How can this data be reused (licensing)? Bibliographic Details The details that will help your data be cited correctly are: a global identifier like a digital object identifier (DOI); a descriptive title that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data a descriptive abstract that serves as a brief overview off the specific contents and purpose of the data package funding information like the award number and the sponsor; the people and organizations like the creator of the dataset (ie who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset Discovery Details The details that will help your data be discovered correctly are: the geospatial coverage of the data, including the field and laboratory sampling locations, place names and precise coordinates; the temporal coverage of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to; the taxonomic coverage of the data, including what species were measured and what taxonomy standards and procedures were followed; as well as any other contextual information as needed. Interpretation Details The details that will help your data be interpreted correctly are: the collection methods for both field and laboratory data; the full experimental and project design as well as how the data in the dataset fits into the overall project; the processing methods for both field and laboratory samples IN FULL; all sample quality control procedures; the provenance information to support your analysis and modelling methods; information about the hardware and software used to process your data, including the make, model, and version; and the computing quality control procedures like any testing or code review. Data Structure and Contents Well constructed metadata also includes information about the data structure and contents. Everything needs a description: the data model, the data objects (like tables, images, matricies, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation. Variable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (such as 0 = not collected), and any missing values (such as 999 = NA). Not only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable). Using an example from the natural sciences, if you were to search for the character string carbon dioxide flux in the general search box at the Arctic Data Center, not all relevant results will be shown due to varying vocabulary conventions (ie, CO2 flux instead of carbon dioxide flux) across disciplines — only datasets containing the exact words carbon dioxide flux are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search. Above left demonstrates a typical search for “carbon dioxide flux”, yielding 20 datasets. Above right illustrates an annotated search for “carbon dioxide flux”, yielding 29 datasets. Note that if you were to interact with the site and explore the results of the figure on the right, the dataset in red of Figure 3 will not appear in the typical search for “carbon dioxide flux.” Rights and Attribution Correctly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like: the citation format to be used when giving credit for the data; the attribution expectations for the dataset; the reuse rights, which describe who may use the data and for what purpose; the redistribution rights, which describe who may copy and redistribute the metadata and the data; and the legal terms and conditions like how the data are licensed for reuse. Ethical Data Practices Additionally, at the Arctic Data Center, we now require data submissions to include information on the level of data sensitivity and to provide a statement of ethical research practice. As the primary repository for the NSF Office of Polar Programs Arctic Section, the Arctic Data Center accepts data from all disciplines. This includes data from social science research that may include sensitive data. Sharing sensitive data can pose challenges to researchers, however sharing metadata or anonymized data contributes to discovery, supports open science principles, and helps reduce duplicate research efforts. To help mitigate the challenges of sharing sensitive data, researchers submitting data now have the option to choose between varying levels of sensitivity that best represent their dataset. Within the research methods section, submitters are now asked to describe the ethical data practices used throughout their research. The information provided will be visible as part of the metadata record and this field has been added to encourage transparency in data ethics. Transparency in data ethics is a vital part of open science and sharing ethical practices encourages deeper discussion about data reuse and ethics. So, how do you organize all this information? There are a number of metadata standards (think, templates) that you could use, including the Ecological Metadata Language (EML), Geospatial Metadata Standards like ISO 19115 and ISO 19139, the Biological Data Profile (BDP), Dublin Core, Darwin Core, PREMIS, the Metadata Encoding and Transmission Standard (METS), and the list goes on and on. The Arctic Data Center runs on EML and has a simple to use interface that guides you through the process of creating your metadata record. 2.1.7 Data Identifiers Many journals require a DOI - a digital object identifier - be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to. At the Arctic Data Center, we assign a DOI to each published dataset. But, sometimes datasets need to be updated. Each version of a dataset published with the Arctic Data Center has a unique identifier associated with it. Researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available. When there is a newer version available, that will be clearly marked on the original dataset page with a yellow banner indicating as such. Having the data identified in this manner allows us to accurately track the dataset usage metrics. The Arctic Data Center tracks the number of citations, the number of downloads, and the number of views of each dataset in the catalog. 2.1.8 Data Citation Data citation best practices are focused on providing credit where credit is due and indexing and exposing data citations across international repository networks. In 2014, Force 11 established a Joint Declaration of Data Citation Principles that includes: Importance of data citation Credit and Attribution Evidence Unique Identification Access Persistence Specificity and Verifiability Interoperability and Flexibility Transitive Credit We want to move towards a model such that when a user cites a research publication we will also know: Which data produced it What software produced it What was derived from it Who to credit down the attribution stack This is transitive credit. And it changes the way in which we think about science communication and traditional publications. 2.2 Related resources and references [1] J. et al. Spies. The reproducibility of psychological science. Tech. rep. Report of the Open Science Collaboration, 2012. [2] O. B. Amaral and K. Neves. “Reproducibility: expect less of the scientific paper”. En. In: Nature 597.7876 (Sep. 2021). Bandiera_ abtest: a Cg_ type: Comment Number: 7876 Publisher: Nature Publishing Group Subject_ term: Publishing, Research data, Research management, pp. 329-331. DOI: 10.1038/d41586-021-02486-7. https://www.nature.com/articles/d41586-021-02486-7 (visited on 09/19/2021). [3] S. G. Baker, A. K. Darke, P. Pinsky, et al. “Transparency and reproducibility in data analysis: the Prostate Cancer Prevention Trial”. In: Biostatistics Oxford England 11.3 (2010), pp. 413-418. http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2883301&amp;tool=pmcentrez&amp;rendertype=abstract. [4] P. Cassey and T. M. Blackburn. “Reproducibility and Repeatability in Ecology”. In: BioScience 56.12 (2006), pp. 958-959. ISSN: 00063568. DOI: 10.1641/0006-3568(2006)56[958:RARIE]2.0.CO;2. http://caliber.ucpress.net/doi/full/10.1641/0006-3568%282006%2956%5B958%3ARARIE%5D2.0.CO%3B2}. [5] R. Gentleman. “Reproducible research: a bioinformatics case study.” In: Statistical applications in genetics and molecular biology 4.1 (2005), p. Article2. http://www.bepress.com/sagmb/vol4/iss1/art2. [6] C. Goble. What is reproducibility? The R* Brouhaha. Science. Feb. 25, 2019. https://www.slideshare.net/carolegoble/what-is-reproducibility-gobleclean (visited on 02/25/2019). [7] B. Marwick. “Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation”. En. In: Journal of Archaeological Method and Theory 24.2 (Jun. 2017), pp. 424-450. ISSN: 1072-5369, 1573-7764. DOI: 10.1007/s10816-015-9272-9. http://link.springer.com/10.1007/s10816-015-9272-9 (visited on 08/28/2017). [8] B. Marwick, C. Boettiger, and L. Mullen. “Packaging Data Analytical Work Reproducibly Using R (and Friends)”. In: The American Statistician 72.1 (Jan. 2018), pp. 80-88. ISSN: 0003-1305. DOI: 10.1080/00031305.2017.1375986. https://doi.org/10.1080/00031305.2017.1375986 (visited on 02/14/2019). [9] D. Nüst and E. Pebesma. “Practical Reproducibility in Geography and Geosciences”. In: Annals of the American Association of Geographers 0.0 (Oct. 2020). Publisher: Taylor &amp; Francis _ eprint: https://doi.org/10.1080/24694452.2020.1806028, pp. 1-11. ISSN: 2469-4452. DOI: 10.1080/24694452.2020.1806028. https://doi.org/10.1080/24694452.2020.1806028 (visited on 06/14/2021). [10] D. Nüst, V. Sochat, B. Marwick, et al. “Ten simple rules for writing Dockerfiles for reproducible data science”. En. In: PLOS Computational Biology 16.11 (Nov. 2020). Publisher: Public Library of Science, p. e1008316. ISSN: 1553-7358. DOI: 10.1371/journal.pcbi.1008316. https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316 (visited on 11/18/2020). [11] S. M. Powers and S. E. Hampton. “Open science, reproducibility, and transparency in ecology”. En. In: Ecological Applications 29.1 (2019). _ eprint: https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/eap.1822, p. e01822. ISSN: 1939-5582. DOI: 10.1002/eap.1822. https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/eap.1822 (visited on 07/01/2021). [12] G. K. Sandve, A. Nekrutenko, J. Taylor, et al. “Ten Simple Rules for Reproducible Computational Research”. In: PLoS Computational Biology 9.10 (Oct. 2013). Ed. by P. E. Bourne, p. e1003285. ISSN: 1553-7358. DOI: 10.1371/journal.pcbi.1003285. http://dx.plos.org/10.1371/journal.pcbi.1003285 (visited on 10/28/2013). [13] M. Schwab, M. Karrenbach, and J. Claerbout. “Making Scientific Computations Reproducible”. In: Computing in Science Engineering 2.6 (2000), pp. 61-67. ISSN: 15219615. DOI: 10.1109/5992.881708. http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=881708. [14] V. Stodden. “The Legal Framework for Reproducible Scientific Research: Licensing and Copyright”. In: Computing in Science &amp; Engineering 11.1 (Jan. 2009), pp. 35-40. ISSN: 1521-9615. DOI: 10.1109/MCSE.2009.19. http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4720221. [15] J. P. Wilson, K. Butler, S. Gao, et al. “A Five-Star Guide for Achieving Replicability and Reproducibility When Working with GIS Software and Algorithms”. In: Annals of the American Association of Geographers 111.5 (Jul. 2021). Publisher: Taylor &amp; Francis _ eprint: https://doi.org/10.1080/24694452.2020.1806026, pp. 1311-1317. ISSN: 2469-4452. DOI: 10.1080/24694452.2020.1806026. https://doi.org/10.1080/24694452.2020.1806026 (visited on 09/10/2021). "],["data-publishing.html", "3 Data Publishing 3.1 Data Documentation and Publishing", " 3 Data Publishing 3.1 Data Documentation and Publishing 3.1.1 Learning Objectives In this lesson, you will learn: About open data archives, especially the Arctic Data Center What science metadata are and how they can be used How data and code can be documented and published in open data archives Web-based submission 3.1.2 Data sharing and preservation 3.1.3 Data repositories: built for data (and code) GitHub is not an archival location Examples of dedicated data repositories: KNB, Arctic Data Center, tDAR, EDI, Zenodo Rich metadata Archival in their mission Certification for repositories: https://www.coretrustseal.org/ Data papers, e.g., Scientific Data List of data repositories: http://re3data.org Repository finder tool: https://repositoryfinder.datacite.org/ 3.1.4 Metadata Metadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where they were collected, and why they were collected. For consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the metadata for a sockeye salmon dataset: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; That same metadata document can be converted to HTML format and displayed in a much more readable form on the web: https://knb.ecoinformatics.org/#view/doi:10.5063/F1F18WN4 And, as you can see, the whole dataset or its components can be downloaded and reused. Also note that the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data. 3.1.5 Structure of a data package Note that the dataset above lists a collection of files that are contained within the dataset. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document. These data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4,and each of the individual files have their own identifiers as well. 3.1.6 DataONE Federation DataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses. DataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the (currently 44!) repositories one at a time. 3.1.7 Publishing data from the web Each data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the Arctic Data Center, we provide some easy to use web forms for editing and submitting a data package. This section provides a brief overview of some highlights within the data submission process, in advance of a more comprehensive hands-on activity. ORCiDs We will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button. ORCID is a non-profit organization made up of research institutions, funders, publishers and other stakeholders in the research space. ORCID stands for Open Researcher and Contributor ID. The purpose of ORCID is to give researchers a unique identifier which then helps highlight and give credit to researchers for their work. If you click on someone’s ORCID, their work and research contributions will show up (as long as the researcher used ORCID to publish or post their work). After signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. Sensitive Data Handling Underneath the Title field, you will see a section titled “Data Sensitivity”. As the primary repository for the NSF Office of Polar Programs Arctic Section, the Arctic Data Center accepts data from all disciplines. This includes data from social science research that may include sensitive data, meaning data that contains personal or identifiable information. Sharing sensitive data can pose challenges to researchers, however sharing metadata or anonymized data contributes to discovery, supports open science principles, and helps reduce duplicate research efforts. To help mitigate the challenges of sharing sensitive data, the Arctic Data Center has added new features to the data submission process influenced by the CARE Principles for Indigenous Data Governance (Collective benefit, Authority to control, Responsibility, Ethics). Researchers submitting data now have the option to choose between varying levels of sensitivity that best represent their dataset. Data submitters can select one of three sensitivity level data tags that best fit their data and/or metadata. Based on the level of sensitivity, guidelines for submission are provided. The data tags range from non-confidential information to maximally sensitive information. The purpose of these tags is to ethically contribute to open science by making the richest set of data available for future research. The first tag, “non-sensitive data”, represents data that does not contain potentially harmful information, and can be submitted without further precaution. Data or metadata that is “sensitive with minimal risk” means that either the sensitive data has been anonymized and shared with consent, or that publishing it will not cause any harm. The third option, “some or all data is sensitive with significant risk” represents data that contains potentially harmful or identifiable information, and the data submitter will be asked to hold off submitting the data until further notice. In the case where sharing anonymized sensitive data is not possible due to ethical considerations, sharing anonymized metadata still aligns with FAIR (Findable, Accessible, Interoperable, Reproducible) principles because it increases the visibility of the research which helps reduce duplicate research efforts. Hence, it is important to share metadata, and to publish or share sensitive data only when consent from participants is given, in alignment with the CARE principles and any IRB requirements. You will continue to be prompted to enter information about your research, and in doing so, create your metadata record. We recommend taking your time because the richer your metadata is, the more easily reproducible and usable your data and research will be for both your future self and other researchers. Detailed instructions are provided below for the hands-on activity. Research Methods Methods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software. As part of a recent update, researchers are now asked to describe the ethical data practices used throughout their research. The information provided will be visible as part of the metadata record. This feature was added to the data submission process to encourage transparency in data ethics. Transparency in data ethics is a vital part of open science and sharing ethical practices encourages deeper discussion about data reuse and ethics. We encourage you to think about the ethical data and research practices that were utilized during your research, even if they don’t seem obvious at first. File and Variable Level Metadata In addition to providing information about, (or a description of) your dataset, you can also provide information about each file and the variables within the file. By clicking the “Describe” button you can add comprehensive information about each of your measurements, such as the name, measurement type, standard units etc. Provenance The data submission system also provides the opportunity for you to provide provenance information, describe the relationship between package elements. When viewing your dataset followinng submission, After completing your data description and submitting your dataset you will see the option to add source data and code, and derived data and code. These are just some of the features and functionality of the Arctic Data Center submission system and we will go through them in more detail below as part of a hands-on activity. 3.1.7.1 Download the data to be used for the tutorial I’ve already uploaded the test data package, and so you can access the data here: https://demo.arcticdata.io/view/urn%3Auuid%3A98c799ef-d7c9-4658-b432-4d486221fca3 Grab both CSV files, and the R script, and store them in a convenient folder. 3.1.7.2 Login via ORCID We will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button. When you sign in, you will be redirected to orcid.org, where you can either provide your existing ORCID credentials or create a new account. ORCID provides multiple ways to login, including using your email address, an institutional login from many universities, and/or a login from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login. 3.1.7.3 Create and submit the dataset After signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. 3.1.7.3.1 Click Add Files to choose the data files for your package You can select multiple files at a time to efficiently upload many files. The files will upload showing a progress indicator. You can continue editing metadata while they upload. 3.1.7.3.2 Enter Overview information This includes a descriptive title, abstract, and keywords. You also must enter a funding award number and choose a license. The funding field will search for an NSF award identifier based on words in its title or the number itself. The licensing options are CC-0 and CC-BY, which both allow your data to be downloaded and re-used by other researchers. CC-0 Public Domain Dedication: “…can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.” CC-BY: Attribution 4.0 International License: “…free to…copy,…redistribute,…remix, transform, and build upon the material for any purpose, even commercially,…[but] must give appropriate credit, provide a link to the license, and indicate if changes were made.” 3.1.7.3.3 People Information Information about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people: Creators - all the people who should be in the citation for the dataset Contacts - one is required, but defaults to the first Creator if omitted Principal Investigators Any others that are relevant For each, please provide their ORCID identifier, which helps link this dataset to their other scholarly works. 3.1.7.3.4 Location Information The geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that spatial search will be more precise. Note that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global: Mission Canyon, Santa Barbara, California, USA 3.1.7.3.5 Temporal Information Add the temporal coverage of the data, which represents the time period to which data apply. Again, use multiple date ranges if your sampling was discontinuous. 3.1.7.3.6 Methods Methods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data. 3.1.7.3.7 Save a first version with Submit When finished, click the Submit Dataset button at the bottom. If there are errors or missing fields, they will be highlighted. Correct those, and then try submitting again. When you are successful, you should see a large green banner with a link to the current dataset view. Click the X to close that banner if you want to continue editing metadata. Success! 3.1.7.4 File and variable level metadata The final major section of metadata concerns the structure and content of your data files. In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure. For example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information. The Attributes tab is where you enter variable (aka attribute) information, including: variable name (for programs) variable label (for display) - variable definition (be specific) - type of measurement - units &amp; code definitions You’ll need to add these definitions for every variable (column) in the file. When done, click Done. Now, the list of data files will show a green checkbox indicating that you have fully described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes. After you get the big green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes. 3.1.7.5 Add workflow provenance Understanding the relationships between files (aka provenance) in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, which are often then used in analysis and visualization code to produce final outputs. In the DataONE network, we support structured descriptions of these relationships, so researchers can see the flow of data from raw data to derived to outputs. You add provenance by navigating to the data table descriptions and selecting the Add buttons to link the data and scripts that were used in your computational workflow. On the left side, select the Add circle to add an input data source to the filteredSpecies.csv file. This starts building the provenance graph to explain the origin and history of each data object. The linkage to the source dataset should appear. Then you can add the link to the source code that handled the conversion between the data files by clicking on Add arrow and selecting the R script: The diagram now shows the relationships among the data files and the R script, so click Submit to save another version of the package. Et voilà! A beautifully preserved data package! "],["data-portals.html", "4 Data Portals 4.1 What is a Portal? 4.2 Portal Uses 4.3 Portal Features 4.4 Enhancing Access to Social Science Research Data 4.5 Relationship between Arctic Data Center portals and DataONE 4.6 Creating Portals 4.7 How to Publish Portals 4.8 Sharing Portals 4.9 Tutorial Videos 4.10 Acknowledgements", " 4 Data Portals Data portals are a new feature on the Arctic Data Center. Researchers can now easily view project information and datasets all in one place. 4.1 What is a Portal? A portal is a collection of Arctic Data Center data packages on a unique webpage. Typically, a research project’s website won’t be maintained beyond the life of the project and all the information on the website that provides context for the data collection is lost. Arctic Data Center portals can provide a means to preserve information regarding the projects’ objectives, scopes, and organization and couple this with the data files so it’s clear how to use and interpret the data for years to come. Portals also leverage Arctic Data Center’s metric features, which create statistics describing the project’s data packages. Information such as total size of data, proportion of data file types, and data collection periods are immediately available from the portal webpage. 4.2 Portal Uses Portals allow users to bundle supplementary information about their group, data, or project along with the data packages. Data contributors can organize their project specific data packages into a unique portal and customize the portal’s theme and structure according to the needs of that project. Researchers can also use portals to compare their public data packages and highlight and share them with other teams, as well as the broader Arctic research audience. To see an example of a portal, please view the Toolik Field Station’s portal. 4.3 Portal Features Published portals vary in their design according to the needs and preferences of the individual or group. However, when constructing a portal there are three core elements: a data page, a metrics page, and customizable free-form pages. Flexible Content Creation Portals can be constructed as a website providing information about the research and products. Using our flexible ‘free-form’ pages (written in markdown), you can add and re-order pages to meet your needs. These pages might be used as a home, or ‘landing’ page with general project information. They are also used to showcase research products, and communicate news and upcoming events. Curated Collections of Data The data page is arguably the most important component of the Arctic Data Center portal system. This is where users will display the data packages of their choice. Whether these data reflect research products from your research group (collated based on contributor IDs), or thematic areas of research interest (collated based on keyword and search terms), will depend upon the intended use of the portal but in all cases, you are refining the suite of data viewed by your audience. The data page looks and performs just like the main Arctic Data Center catalog - with some added bonuses, see below. Customized Search Capabilities You can also build more precise search capabilities into your portal, leveraging the rich metadata associated with data products preserved at the Arctic Data Center. For example, in the example below the research group have identified seven primary search categories and within these, enable users to search within specific metadata fields or else select from a drop down list of options. In doing so, your audience can construct refined search queries drilling down to the data of interest. (Note that although the SASAP portal is hosted by DataONE, the same functionality exists at the Arctic Data Center. More on this later). Metrics, Metrics, Metrics As with the full Arctic Data Center catalog, we aggregate metrics for the collection of data packages within a portal. This page is not customizable - it comes as a default with the portal - but you can choose to delete it. The metrics provided include a summary of the holdings: number, volume, time period, format of datasets, metadata assessment scores, citations across all packages, and counts of downloads and views. These latter metrics can be particularly useful if wanting to track the usage or reach of your project or group’s activities. 4.4 Enhancing Access to Social Science Research Data Many of the portal examples provided above are organizational, individual or project portals created by members of the Arctic research community. The ability to group relevant datasets and customize search criteria, increases data discoverability and accessibility among target audiences. The Arctic Data Center has leveraged these features to create a specific portal for social science data. Within this portal users can subset by social science discipline, data type and other metadata fields built into the portal. These search features depend on sufficient user contributed metadata describing the dataset, and as can be seen from the ‘with data’ toggle, the portal does not require the data themselves to be uploaded to the Arctic Data Center. 4.5 Relationship between Arctic Data Center portals and DataONE Both DataONE and the Arctic Data Center use Metacat and MetacatUI software and both have the capability for individuals and groups to develop portals. This is true of other repositories running this software. The difference between a portal at teh Arctic Data Center and one through DataONE is the corpus of data that can be pulled into your portal. Arctic Data Center portals expose only data held in the Arctic Data Center repository. A portal in DataONE can expose data from across the full network of data repositories - including DataONE. This is particularly useful for interdisciplinary research projects, labs that have published data to multiple repositories etc. However, unlike at the Arctic Data Center, there is a cost associated with a DataONE portal as they are part of the organizations sustainability model. 4.6 Creating Portals A step-by-step guide on how to navigate the Arctic Data Center and create a new portal. For video tutorials on how to create your first portal, please visit the Arctic Data Center’s website. Getting Started with Portals If you are on the Arctic Data Center’s primary website, select the button on the top right titled ‘Create Portal’, this will take you to sign in with your ORCID id if you are not already signed in. Sign in with your ORCID, which will then take you directly to the page where you can start customizing your portal. Alternatively, if you are logged in with your ORCID and on the Dataset Search page, you can also get to the page to create a new portal by hovering on your name in the upper right hand corner. From there, a dropdown will appear and you can select ‘My Portals’. On your profile settings page, select ‘My Portals’. After the page loads select the green button ‘+ New Portal’ to add a new portal, you’ll automatically be directed to a fresh edit session. Portal Settings Page In a new edit session, the first page you’ll be taken to is the settings page where you’ll be able to add details about your portal. Portal URL Identify a short name for your portal that will become part of the URL. If the name is available, a label will indicate it’s available and if the name is taken already, it will note that the name is already taken. This feature ensures the portals are unique. Portal description Sharing options For the purposes of this training, please leave your portal in ‘Private’ status. You are welcome to return and make the portal public when the portal is complete and is useful to you and others. Permissions Adding collaborators to help you create your portal is as straightforward as copying and pasting in their ORCID into the box below the permissions section. You can choose whether the collaborator can view, edit, or is an owner of the portal. You can have multiples of each role. Partner Logos Adding Data to Portals When selecting the data tab you will see a page with two sections. These are titled with instructive statements to help explain their function: Add filters to help people find data within your collection build search features for others to use within your portal Add data to your collection construct a search to populate your portal with data We’re going to start with the second section. When adding data to your collection, you can include any of the datasets that are available at the Arctic Data Center. You build rules based on metadata to define which datasets should be included. Of course, where metadata is incomplete or abbreviated, this will impact your results. Data added to the network in the future that match these rules will also be added to your collection. The first thing we notice is the ability to include/exclude data based on all/any metadata criteria. This setting applies across all rules. In the default view you have the starting point for a single rule, and a view showing 0 datasets. As we build rules, the page will refresh to show how many datasets are included based on your rule structure. You can continue to add rules (and rule groups) to create complex queries that are specific to your needs. The metadata fields available for rule construction are easily visible in the dropdown option (don’t forget to scroll!), and grouped for ease of use. You also have the option to define ‘any metadata field’ though your results may be less precise. A recently added feature allows users to search for an attributes’ semantic annotation. Building Search Options for Your Audience This section covers the first part of the ‘data’ page. “Add filters to help people find data within your collection”. Although it appears first, I recommend constructing these filters after you have defined your data as you will have a better understanding of the metadata field that are relevant to your portal collection. It appears at the top of this editor page as when published, it will be at the top for users, hence the editor page reflects the layout of the published page. When selecting “add a search filter” you will be presented with a pop-up that comprises three primary elements. The metadata filed you will be querying The way in which you want the user to interact with that metadata Language settings for the filter you are building As you select options for number 1 - the metadata field, the pop-up will refresh to show only those relevant options. Save this filter to close the pop-up, return to the main editor and add another search filter. Data Package Metrics As stated above, the metrics page is a default function provided by the Arctic Data Center. This page cannot be edited and cannot be viewed while editing. Users do have the option to delete the page if they’d like. To delete the page, select the arrow next to the word ‘Metrics’ in the tab and choose ‘Delete’ from the dropdown list. You can alwys change your mind and add a metrics page with the ‘+’ tab. To see metric summaries, navigate to your portal in view mode. See Saving and Editing Portals for more information on how to view portals. Creating Unique Freeform Pages To watch a tutorial on creating a new freeform page see this video:Creating a Freeform Text Page To add a freeform page to a portal, select the ‘+’ tab next to the data and metric tabs and then choose the freeform option that appears on screen. A freeform page will then populate. Easily customize your banner with a unique image, title, and page description. To change the name of the tab, click on the arrow in the ‘Untitled’ tab and select ‘Rename’ from the dropdown list. Below the banner, there is a markdown text box with some examples on how to use the markdown formatting directives to customize the text display. There is also a formatting header at the top to assist if you’re unfamiliar with markdown. As you write, toggle through the Edit and Preview modes in the markdown text box to make sure your information is displaying as intended. Portals are flexible and can accommodate as many additional freeform pages as needed. The markdown header structure helps to generate the table of contents for the page. Please see these additional resources for help with markdown: Markdown reference Ten minute tutorial For a longer example where you can also preview the results, checkout the Showdown Live Editor Saving and Editing Portals Be sure to save your portal when you complete a page to ensure your progress is retained. Whenever a portal is saved, a dialogue box will pop up at the top of the page prompting users to view their private portal in view mode. You can choose to ignore this and continue editing. To delete a page from your portal, select the arrow in the tab and choose ‘Delete’ from the dropdown. Users can view and edit their portal from their ‘My Portals’ tab. First, click the arrow your name in the top-right corner to drop down your menu options. Then, select ‘My Portals’ from the dropdown underneath your name. See the section on Getting Started with Portals for more details. Click on the portal title to view it or select the edit button to make changes. 4.7 How to Publish Portals New portals are automatically set to private and only visible to the portal creator. The portal will remain private until the owner decides to make it public. To make your portal public, go into the settings of your portal. Under the description, you’ll see a new section called ‘Sharing Options’. You can toggle between your portal being private and your portal being public there. 4.8 Sharing Portals In order to share a published portal, users can simply direct recipients to their unique identifier. Portal identifiers are embedded into the Arctic Data Center’s portal URL: https://arcticdata.io/catalog/portals/portal-identifier To view or edit your portal identifier, go into edit mode in your portal. In the settings page, your portal URL will be the first item on the page. 4.9 Tutorial Videos For video tutorials on how to create your first portal, please visit the Arctic Data Center video tutorial page. 4.10 Acknowledgements Much of this documentation was composed by ESS-DIVE, which can be found here. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
