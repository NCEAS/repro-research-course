[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#about-this-training",
    "href": "index.html#about-this-training",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "About this training",
    "text": "About this training\nThe NCEAS openS Program consists of three 1-week long workshops, geared towards early career researchers. Participants engage in a mix of lectures, exercises, and synthesis research activities to conduct synthesis science and implement best practices in open data science.\nopenS has been adapted to coincide with each USGS CAP training, for a total of four training sessions across 2 years.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#why-nceas",
    "href": "index.html#why-nceas",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "Why NCEAS",
    "text": "Why NCEAS\nThe National Center for Ecological Analysis and Synthesis (NCEAS), a research affiliate of the University of California, Santa Barbara, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science.\nNCEAS has over 25 years of success with this model among working groups and environmental professionals, and we are excited to pass on this cumulative expertise to you.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#week-one-successful-synthesis-and-data-management-best-practices",
    "href": "index.html#week-one-successful-synthesis-and-data-management-best-practices",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "Week One: Successful Synthesis and Data Management Best Practices",
    "text": "Week One: Successful Synthesis and Data Management Best Practices\n\nLearning Objectives:\n\nImplement reproducible scientific workflows throughout all aspects of a project\nIncrease your familiarity and confidence with data science tools\nEffectively manage and wrangle data using tidy data practices\nAccessing, interpreting and developing metadata for synthesis research\nOrganize and initiate synthesis projects\n\n\n\nSchedule\n\n\n\n\n\n\n\n\n\nDate\nTime\nSession Title\nLead Facilitator\n\n\n\n\nMonday 08/21\nBlock 3\n1:45PM-3:15PM\nSuccessful Synthesis Science\nHalina Do-Linh, Data Training Program Manager\n\n\nTuesday 08/22\nBlock 2\n10:45AM-12:15PM\nPanel for PIs: Successful Mentorship in Synthesis Science\nHalina Do-Linh, Data Training Program Manager\n\n\nWednesday 08/23\nBlock 1\n9AM-10:30AM\nData Management Essentials\nCamila Vargas Poulsen, Data Training Coordinator",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "NCEAS openS for the Climate Adaptation Postdoctoral (CAP) Fellows: Future of Aquatic Flows Cohort",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS with the support of DataONE, to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors for this version are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Halina Do-Linh, Camila Vargas Poulsen. 2023. openS for USGS Climate Adaptation Postdoctoral (CAP) Fellows Program. Week One. NCEAS Learning Hub & USGS Climate Adaptation Science Centers (CASCs).\nAdditional contributors: Ben Bolker, Julien Brun, Amber E. Budden, Jeanette Clark, Samantha Csik, Carmen Galaz García, Stephanie Hampton, Natasha Haycock-Chavez, Matthew B. Jones, Samanta Katz, Julie Lowndes, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "session_01.html",
    "href": "session_01.html",
    "title": "1  Successful Synthesis Science",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Successful Synthesis Science</span>"
    ]
  },
  {
    "objectID": "session_01.html#learning-objectives",
    "href": "session_01.html#learning-objectives",
    "title": "1  Successful Synthesis Science",
    "section": "",
    "text": "Identify synthesis science research and different synthesis science approaches\nImplement synthesis science approaches to your research",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Successful Synthesis Science</span>"
    ]
  },
  {
    "objectID": "session_01.html#what-is-synthesis-science",
    "href": "session_01.html#what-is-synthesis-science",
    "title": "1  Successful Synthesis Science",
    "section": "1.1 What is Synthesis Science?",
    "text": "1.1 What is Synthesis Science?\n\n\n\nGraphic by Judith Sitters\n\n\n\n\n\n\n\n\nSynthesis Science in Ecological and Environmental Sciences\n\n\n\nSynthesis science is the process of distilling existing data, ideas, theories, or methods drawn from many sources, across multiple fields of inquiry, to accelerate the generation of new scientific knowledge, often at a broad scale.\n\n\nTo break that down further, think of it this way: synthesis science utilizes existing data and knowledge to generate new insights. It’s like putting together puzzle pieces from different sources to create a bigger picture.\nSynthesis science is fundamentally different from other types of academic research in that it does not involve collecting new or original data to conduct research or to answer a research question.\nSince the ecological and environmental sciences are complex and often interdisciplinary, synthesis approaches are ideal for “understanding complexity across scales, leveraging data from various disciplines, facilitating the discovery of general patterns in natural systems, and informing policy” (Halpern et al. 2023) in the ecological and environmental sciences.\n\n1.1.1 The Value and Impact of Synthesis Research\n\n\n\nGraphic adapted from Engingeering Ideas newsletter on Substack\n\n\nData is everywhere and information is more accessible than ever, but how can we leverage both data and knowledge to further our understanding of a topic and turn those insights into meaningful action? This is where synthesis research shines.\nThere are many studies that value the importance of synthesis for varying reasons including:\n\nElevating scientific research as evidence that is influential in policy-making (Dicks, Walsh, and Sutherland 2014)\nAddressing the challenge of “information overload” (Hampton and Parker 2011)\nDelivering research products that enhance scientific knowledge in decision-making (Hampton and Parker 2011)\nProviding critical knowledge to solve environmental issues (Carpenter et al. 2009)\nInitiating new collaborations and new insights across disciplines and non-academic sectors (Hampton and Parker 2011)\n\nAs synthesis science and research synthesis approaches have been gaining prominence in the ecological and environmental fields, researchers have sought to understand the impacts of synthesis. In particular, Wyborn et al. (2018) identified the following types of impacts from research synthesis:\n\nKnowledge is used to change understanding of a system or phenomena\nKnowledge changes understanding of the interconnections between actors, sectors or systems\nKnowledge is used to support a particular view of a problem\nKnowledge is used as the prime source of information to inform policy change or reform\nKnowledge use is dependent on the capacity to understand or apply knowledge in a given context",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Successful Synthesis Science</span>"
    ]
  },
  {
    "objectID": "session_01.html#research-synthesis-approaches",
    "href": "session_01.html#research-synthesis-approaches",
    "title": "1  Successful Synthesis Science",
    "section": "1.2 Research Synthesis Approaches",
    "text": "1.2 Research Synthesis Approaches\nIn general, research synthesis approaches include utilizing existing research on a particular topic with the intention of integrating all the findings together.\nThere are multiple different types of approaches such as narrative reviews, vote counting (this quantitative method has been discredited, but you may find examples of it in your research), case studies, and more. However, systematic reviews and meta-analysis are considered by some to be the “gold standard” of research synthesis and are often used at NCEAS, which is why we’re highlighting them here.\n\n\n\n\n\n\n\nSystematic Review\nA form of synthesis to study a specific question or problem by following clear steps: finding and choosing studies, checking them carefully, and gathering information from them to create a reliable review.\nNote: Not all systematic reviews contain meta-analysis\n\n\nMeta-Analysis\nA form of synthesis that uses statistical and quantitative methods to integrate and compare (quantitatively) results from a large number of individual studies.\n\n\n\n\n1.2.1 What about data?\nData is essential to synthesis in ecological and environmental research. Synthesizing data can involve combining and analyzing data from multiple sources to address research questions, to conduct a systematic review or meta-analysis, to identify patterns, and draw meaningful conclusions that don’t appear in siloed approaches.\nAt NCEAS, our approach tends to include combining multiple datasets from various sources, then we leverage specific expertise from diverse disciplines and researchers, and together we use those two elements to unleash new insights to answer large-scale questions.\nWe often see this approach with working groups at NCEAS. Working groups are teams that include researchers, agencies and organizational practitioners, and more who come together (and typically at NCEAS, a neutral ground) for focused and intensive working sessions around their research questions.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Successful Synthesis Science</span>"
    ]
  },
  {
    "objectID": "session_01.html#nceas-and-synthesis-science",
    "href": "session_01.html#nceas-and-synthesis-science",
    "title": "1  Successful Synthesis Science",
    "section": "1.3 NCEAS and Synthesis Science",
    "text": "1.3 NCEAS and Synthesis Science\n\n\n\n\n\nIn 1995, the National Science Foundation (NSF) responded to the increasing calls by environmental researchers and professionals for interdisciplinary and transdisciplinary explanations, methods to connect science to applications, and ways to advance research technologies by creating NCEAS, the first synthesis center.\nThere are multiple other synthesis centers that have been established since, but each one has a foundational goal to use synthesis to “advance basic science, organize ecological information for decision makers concerned with pressing national issues, and make cost-effective use of the nation’s extant and accumulating database” (Hampton and Parker 2011).\nSynthesis centers are specifically designed to catalyze collaboration leading to breakthrough ideas, and one way NCEAS (among other synthesis centers) achieves this is through working groups. Since its inception, NCEAS has hosted hundreds of working groups and has supported them in varying ways including: structuring group size or composition, facilitating of working sessions, supplying computing and informatics support, offering training opportunities for data-focused team members, providing a neutral space for meetings and co-working, and more. In short, NCEAS lowers the “activation energy” for working group members so that they can focus on the cross-pollination of ideas, creative thinking, and emergent research (Rodrigo et al. 2013).",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Successful Synthesis Science</span>"
    ]
  },
  {
    "objectID": "session_01.html#working-groups-at-nceas",
    "href": "session_01.html#working-groups-at-nceas",
    "title": "1  Successful Synthesis Science",
    "section": "1.4 Working Groups at NCEAS",
    "text": "1.4 Working Groups at NCEAS\n\n1.4.1 Ocean Health Index (OHI)\n\n\n\n\n\nThe Ocean Health Index brought together a diverse group of experts including marine scientists, economists, policy experts, and more. This collaborative effort aimed to comprehensively assess the health of oceans worldwide by integrating data from various sources, including physical, biological, and socioeconomic factors.\nThe success of the Ocean Health Index highlights several key aspects of effective synthesis science in working groups:\n\nInterdisciplinary Collaboration: The working group included experts from different disciplines, fostering a holistic understanding of ocean health that goes beyond individual scientific perspectives.\nData Integration: The Index integrated data from multiple sources and scales, ranging from local to global, to provide a comprehensive view of ocean health.\nStandardized Metrics: The working group developed standardized metrics that allowed for consistent assessment and comparison of ocean health across regions and over time.\nPolicy Relevance: The synthesis findings were designed to inform policy and management decisions, demonstrating the practical applications of synthesis science in addressing real-world challenges.\nCommunication and Outreach: The Index effectively communicated its findings to a wide range of stakeholders, raising awareness about ocean health and fostering engagement in conservation efforts.\n\n\n\n1.4.2 State of Alaska’s Salmon and People (SASAP)\n\n\n\n\n\nThe SASAP project was a partnership between NCEAS and Anchorage-based Nautilus Impact Investing to create an equitable decision-making platform for salmon management through information synthesis, collaboration, and stakeholder engagement.\nThe project sought to address the need for accessible, up-to-date, and complete information about salmon and the factors shaping their future. It convened eight collaborative working groups consisting of leading experts at the University of Alaska and other universities, local indigenous leaders, and specialists across resource sectors – an intentional integration of western scientific perspectives and indigenous knowledge.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Successful Synthesis Science</span>"
    ]
  },
  {
    "objectID": "session_01.html#priorities-of-synthesis-here-now-and-beyond",
    "href": "session_01.html#priorities-of-synthesis-here-now-and-beyond",
    "title": "1  Successful Synthesis Science",
    "section": "1.5 Priorities of Synthesis Here, Now, and Beyond",
    "text": "1.5 Priorities of Synthesis Here, Now, and Beyond\nIn February 2021, NCEAS hosted a virtual workshop where 127 ecologists, environmental and natural scientists convened to discuss the future priorities of synthesis science. Participants across career stages, institutions, backgrounds, and geographies were in attendance and together they identified seven research topics, and two pressing needs for conducting research synthesis practices.\nLater in January 2023, dozens of scientists authored and published Priorities for synthesis research in ecology and environmental science the results of the workshop. We’ve summarized these results in the tables below.\n\n\n\n\n\n\n\nResearch Topic\nDescription\n\n\n\n\nDiversity, Equity, Inclusion, and Justice (DEIJ)\nEnvironmental justice shows the link between nature and fairness for people, especially those who are disadvantaged. Unequal distribution of nature due to factors like racism impacts cities and ecosystems. Integrating DEIJ into research can improve knowledge and involve underrepresented groups, while engaging diverse communities can make research more impactful and inclusive.\n\n\nHuman and Natural systems\nEnvironmental synthesis has given us valuable insights into environmental change, but it needs to also look at how human actions and decisions affect the environment. For example, decisions about things like population growth and greenhouse gas policies impact species and nature. Bringing together information from different fields is challenging, especially when considering different scales and practices. Synthesis science can help us better understand how humans and nature are connected, but it takes time and effort to combine ideas from different disciplines.\n\n\nActionable and Use-inspired science\nSynthesis science in ecology and environmental science is great for helping decision makers. Methods like systematic reviews and predictive models can simplify research for environmental policy. To be more effective, scientists need to work closely with practitioners and use different types of knowledge. Communication, teamwork, and using various knowledge sources can improve how decisions are made and carried out based on scientific findings.\n\n\nScale\nUnderstanding ecosystems across different sizes and time periods is hard for social-ecological research. Synthesis helps by combining information and methods from different scales, and connecting these findings to policy decisions. The challenges come from not having enough long-term data, difficulty mixing different information for analysis, and lack of skills and cooperation across scales. To overcome these challenges, more efforts are needed to collect, organize, and combine data over long periods and big areas.\n\n\nGenerality\nScience looks for common rules to explain how things work, but sometimes it’s hard to find them. Synthesis science can help find these general rules and figure out when they apply. Two problems are not having enough similar studies to compare and bias in what gets studied and reported. To fix this, scientists can share their work openly, use the same rules for reporting, and make sure all types of results are published. Working together across different areas can also help find these general rules.\n\n\nComplexity and Resilience\nEcosystems are very complicated, especially when considering things like society and economics. This complexity can make systems strong or weak. Using this complex view helps solve problems like water and food. But putting all this information together (synthesis) is hard because it depends on the situation, and finding general rules is tough. To make it work better, more scientists need to learn to work together across different areas and share their data. The goal is to understand how complexity affects ecosystems and find ways to help both people and the environment.\n\n\nPredictability\nThis time of big changes in the environment makes it important to predict how ecosystems will be in the future and how they’ll affect people. Predicting things in the short term (like months or years) is common in some fields but not in ecology. Some people worry that the predictions might not be perfect, but even imperfect predictions can help us learn and make better decisions. To make predictions better in ecology, scientists can work together, use standardized methods and data, and talk to the people who use the predictions. This will help us manage natural resources more wisely.\n\n\n\n\n\n\n\n\n\n\nPressing Need\nDescription\n\n\n\n\nNeed for increased participant diversity and inclusive research practices\nSynthesis science is most effective when it combines data, perspectives, and insights from different regions and ecosystems. By including diverse participants and creating inclusive research environments, we can better address important questions and find fair solutions to challenges. Ignoring diversity and inclusion can lead to biased syntheses and unfair outcomes. The large scope of synthesis science calls for involving people, ideas, and data from various backgrounds and languages. Efforts should be made to ensure representation, provide support, and promote open dialogue to increase diversity and inclusivity in synthesis research, ultimately fostering better understanding and more equitable outcomes.\n\n\nIncreased and improved data flow, access, and skill-building\nThe amount of data for synthesis science is increasing, and there are efforts to organize and combine data from various sources to answer ecological questions. Funding is needed to manage and store data properly. It’s important to think about ethical issues when using different datasets, especially if some people can’t share their data openly. Some data may be hard to access due to language or quality issues, and including diverse types of data, like traditional knowledge, can be challenging. Tools like machine learning can help make sense of different data sources. To improve data synthesis, global collaboration among diverse groups is essential.\n\n\n\n\n\n\n\nCarpenter, Stephen R., E. Virginia Armbrust, Peter W. Arzberger, F. Stuart Chapin, James J. Elser, Edward J. Hackett, Anthony R. Ives, et al. 2009. “Accelerate Synthesis in Ecology and Environmental Sciences.” BioScience 59 (8): 699–701. https://doi.org/10.1525/bio.2009.59.8.11.\n\n\nDicks, Lynn V., Jessica C. Walsh, and William J. Sutherland. 2014. “Organising Evidence for Environmental Management Decisions: A ‘4S’ Hierarchy.” Trends in Ecology & Evolution 29 (11): 607–13. https://doi.org/10.1016/j.tree.2014.09.004.\n\n\nHalpern, Benjamin S., Carl Boettiger, Michael C. Dietze, Jessica A. Gephart, Patrick Gonzalez, Nancy B. Grimm, Peter M. Groffman, et al. 2023. “Priorities for Synthesis Research in Ecology and Environmental Science.” Ecosphere 14 (1). https://doi.org/10.1002/ecs2.4342.\n\n\nHampton, Stephanie E., and John N. Parker. 2011. “Collaboration and Productivity in Scientific Synthesis.” BioScience 61 (11): 900–910. https://doi.org/10.1525/bio.2011.61.11.9.\n\n\nRodrigo, Allen, Susan Alberts, Karen Cranston, Joel Kingsolver, Hilmar Lapp, Craig McClain, Robin Smith, Todd Vision, Jory Weintraub, and Brian Wiegmann. 2013. “Science Incubators: Synthesis Centers and Their Role in the Research Ecosystem.” PLoS Biology 11 (1): e1001468. https://doi.org/10.1371/journal.pbio.1001468.\n\n\nWyborn, Carina, Elena Louder, Jerry Harrison, Jensen Montambault, Jasper Montana, Melanie Ryan, Angela Bednarek, et al. 2018. “Understanding the Impacts of Research Synthesis.” Environmental Science & Policy 86 (August): 72–84. https://doi.org/10.1016/j.envsci.2018.04.013.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Successful Synthesis Science</span>"
    ]
  },
  {
    "objectID": "session_02.html",
    "href": "session_02.html",
    "title": "2  Logic Model",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logic Model</span>"
    ]
  },
  {
    "objectID": "session_02.html#learning-objectives",
    "href": "session_02.html#learning-objectives",
    "title": "2  Logic Model",
    "section": "",
    "text": "Become familiar with Logic Models\nApply the principles of Logic Models to synthesis development\nRefine synthesis group research question",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logic Model</span>"
    ]
  },
  {
    "objectID": "session_02.html#logic-models",
    "href": "session_02.html#logic-models",
    "title": "2  Logic Model",
    "section": "2.1 Logic Models",
    "text": "2.1 Logic Models\nLogic models are a planning tool that is designed to support program development by depicting the flow of resources and processes leading to a desired result. They are also used for outcomes-based evaluation of a program and are often requested as part of an evaluation planning process by funders or stakeholders.\nA simplified logic models comprise three main parts: Inputs, Outputs and Outcomes.\n\nInputs reflect what is invested, outputs are what is done and outcomes are the results of the program.\nIn a more detailed logic model, outputs and outcomes are further broken down. Outputs are often represented as ‘Activities’ and ‘Participants’. By including participation (or participants), the logic model is explicitly considering the intended audience, or stakeholders, impacted by the program. Engagement of this audience is an output. In the case of outcomes, these can be split into short, medium and long-term outcomes. Sometimes this last category may be labeled ‘Impact’\n\nDefining the inputs, outputs and outcomes early in a planning process enables teams to visualize the workflow from activity to results and can help mitigate potential challenges. Logic models can be thought of as having an ‘if this then that’ structure where inputs -&gt; outputs -&gt; outcomes.\n\nIn the example below we have constructed a simple logic model for a hypothetical project where training materials are being developed for a group of educators to implement at their respective institutions.\n\nLinkages are not always sequential and can be within categories, bi-directional and/or include feedback loops. Detailing this complexity of relationships, or theory of action, can be time-consuming but is a valuable part of the thought process for project planning. In exploring all relationships, logic modeling also allows for assessing program feasibility.\n\nThe above graphics include two sections within Outputs - Activities and Participants - and this is quite common. There is variation in logic model templates, including versions with a third type of output - “Products’. Sometimes description of these products is contained within the Activities section - for example, ‘develop curricula’, ‘produce a report’ - however calling these out explicitly is beneficial for teams focused on product development.\nProgram development (and logic modeling) occurs in response to a given ‘Situation’ or need, and exploring this is the first step in modeling. The situation defines the objective, or problem, that the program is designed to solve hence some logic models may omit the left-hand situation column but be framed with Problem and Solution statements. Finally, comprehensive logic modeling takes into consideration assumptions that are made with respect to the resources available, the people involved, or the way the program will work and also recognizes that there are external factors that can impact the program’s success.\n\nIn summary:\nLogic models support program development and evaluation and comprise three primary steps in the workflow:\n\nInputs: Resources, contributions, and investments required for a program;\nOutputs: Activities conducted, participants reached, and products produced; and\nOutcomes: Results or expected changes arising from the program structured as short-, medium- and long-term.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logic Model</span>"
    ]
  },
  {
    "objectID": "session_02.html#logic-models-for-synthesis-research",
    "href": "session_02.html#logic-models-for-synthesis-research",
    "title": "2  Logic Model",
    "section": "2.2 Logic Models for Synthesis Research",
    "text": "2.2 Logic Models for Synthesis Research\nLogic models are one tool for program development and have sufficient flexibility for a variety of situations, including planning for synthesis research. While some logic model categories may feel less relevant, the process of articulating a research objective, associated resources and activities and proposed outcome(s) has value.\nBelow is a table of logic model elements and example questions for each element framed for both a typical logic model and synthesis research.\n\n\n\n\n\n\n\n\nLogic Model Element\nLogic Model Framing\nSynthesis Research Framing\n\n\n\n\nProblem Statement\nWhat is the problem? Why is this a problem? Who does this impact?\nWhat is the current state of knowledge? What gaps exists in understanding? Why is more information / synthesis important?\n\n\nInputs\nWhat resources are needed for the program (e.g. personnel, money, time, equipment, partnerships, etc.)?\nWhat is needed to undertake the synthesis research? For personnel, think in terms of the roles that are needed (e.g. data manager, statistician, writer, editor etc.) Consider the time frame. Condsider what data are needed and what already exists?\n\n\nOutputs - Activities\nWhat will be done (e.g. development, design, workshops, conferences, counseling, outreach, etc.)?\nWhat activities are needed to conduct the research?\nThis could be high level or it could be broken down into details such as the types of statistical approaches.\n\n\nOutputs - Participants\nWho will we reach (e.g. clients, participants, customers, etc.)?\nWho is the target audience? Who will be impacted by this work? Who is positioned to leverage this work?\n\n\nOutputs - Products\nWhat will you create (e.g. publications, websites, media communications, etc.)?\nWhat research products are planned or expected? Consider this in relation to the intended audience. Is a peer-reviewed publication, report or white paper most appropriate? How will derived data be handled? Will documentation, workflows, or code be published?\n\n\nOutcomes - Short-term\nWhat short-term outcomes are anticipated among participants?\nThese might include changes in awareness, knowledge, skills, attitudes, opinions or intent.\nWill this work represent a significant contribution to current understanding?\n\n\nOutcomes - Medium-term\nWhat medium-term outcomes are predicted among participants?\nThese might include changes in behaviors, decision-making and actions.\nWill this work promote increased research activity or open new avenues of inquiry?\n\n\nOutcomes - Long-term\nWhat long-term benefits, or impacts, are expected?\nThis might include changes in social, economic, civic, or environmental conditions.\nWill this work result in local, regional or national policy change? What will be the long-term impact of increased investment in the ecosystem?",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logic Model</span>"
    ]
  },
  {
    "objectID": "session_02.html#activity-synthesis-planning-with-logic-models",
    "href": "session_02.html#activity-synthesis-planning-with-logic-models",
    "title": "2  Logic Model",
    "section": "2.3 Activity: Synthesis Planning with Logic Models",
    "text": "2.3 Activity: Synthesis Planning with Logic Models\n\n\n\n\n\n\nSetup\n\n\n\nBreakout into your research synthesis groups to refine your ideas for potential synthesis topics. Use either the Logic Model Template or a Mermaid Flowchart to create your logic model.\nThe goal for this activity is to develop one or more high-level logic models that:\n\nSummarize the synthesis research question\nDefine the inputs needed to approach the synthesis research question\nDefine the outputs including the activities, participants, and products needed to address the research question\nDefine at least the short-term outcomes and long-term outcomes of the research\n\nOften it is helpful to start by working backwards. Consider what outcomes are you trying to achieve, what outputs would help you get to those outcomes, and finally what inputs do you need to create those outputs.\n\n\n\nLogic Model Template\nUse our Logic Model Template to create your Logic Model. When you access the link, you must create your own copy to start editing.\n\n\n\nLogic Model Template\n\n\n\n\nMermaid Flowchart\nUse a Mermaid Flowchart to create your Logic Model. These is a tool that is embedded in Quarto documents.\n\n\n\n\n\nflowchart LR\n    INPUTS --&gt; ACTIVITIES --&gt; OUTPUTS --&gt; OUTCOMES/IMPACTS\n\n    Scenario{{Accelerate synthesis via data science training}}\n\n    R1[Instructor] & R2[Classroom space] & R3[Projector] --&gt; B{Data Science Workshop}\n    B --&gt; C(Workshop Curriculum)\n    B --&gt; D(Presentations and Practice)\n    \n    C & D --&gt; E[/Improved Delta management/] & F[/Increased analytic efficiency/]\n\n\n\n\n\n\nSource Code for Mermaid Flowhart Example (above):\n\n```{mermaid}\n\nflowchart LR\n    INPUTS --&gt; ACTIVITIES --&gt; OUTPUTS --&gt; OUTCOMES/IMPACTS\n\n    Scenario{{Accelerate synthesis via data science training}}\n\n    R1[Instructor] & R2[Classroom space] & R3[Projector] --&gt; B{Data Science Workshop}\n    B --&gt; C(Workshop Curriculum)\n    B --&gt; D(Presentations and Practice)\n    \n    C & D --&gt; E[/Improved Delta management/] & F[/Increased analytic efficiency/]\n```",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logic Model</span>"
    ]
  },
  {
    "objectID": "session_03.html",
    "href": "session_03.html",
    "title": "3  Data Management Essentials",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Essentials</span>"
    ]
  },
  {
    "objectID": "session_03.html#learning-objectives",
    "href": "session_03.html#learning-objectives",
    "title": "3  Data Management Essentials",
    "section": "",
    "text": "Understand the importance of data management for successfully preserving data\nReview the Data Life Cycle and how it can guide the data management in a project\nFamiliarize with data management plans and start thinking about one for your project\nLearn about metadata guidelines and best practices for reproducibility\nBecome familiar with environmental data repositories for accessing and publishing data",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Essentials</span>"
    ]
  },
  {
    "objectID": "session_03.html#the-big-idea",
    "href": "session_03.html#the-big-idea",
    "title": "3  Data Management Essentials",
    "section": "3.1 The Big Idea",
    "text": "3.1 The Big Idea\nThis lesson aims to get you thinking about how are you going to manage your data. Even though you are in the early stages of defining you research project, we believe that when it comes to data management, the earlier you start thinking about it the better.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Essentials</span>"
    ]
  },
  {
    "objectID": "session_03.html#introduction",
    "href": "session_03.html#introduction",
    "title": "3  Data Management Essentials",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nData management is the process of handling, organizing, documenting, and preserving data used in a research project. This is particularly important in synthesis science given the nature of synthesis, which involves combining data and information from multiple sources to answer broader questions, generate knowledge and provide insights into a particular problem or phenomenon.\n\n3.2.1 Benefits of managing your data\nSuccessfully managing your data throughout a research project helps ensures its preservation for future use. It also facilitates collaboration within your team, and it helps advance your scientific outcomes.\nFrom a researcher perspective\n\nKeep yourself organized – be able to find your files (data inputs, analytic scripts, outputs at various stages of the analytic process, etc.)\nTrack your science processes for reproducibility – be able to match up your outputs with exact inputs and transformations that produced them\nBetter control versions of data – easily identify versions that can be periodically purged\nQuality control your data more efficiently\nTo avoid data loss (e.g. making backups)\nFormat your data for re-use (by yourself or others)\nBe prepared to document your data for your own recollection, accountability, and re-use (by yourself or others)\nGain credibility and recognition for your science efforts through data sharing!\n\nAdvancement of science\n\nData is a valuable asset – it is expensive and time consuming to collect\nMaximize the effective use and value of data and information assets\nContinually improve the quality including: data accuracy, integrity, integration, timeliness of data capture and presentation, relevance, and usefulness\nEnsure appropriate use of data and information\nFacilitate data sharing\nEnsure sustainability and accessibility in long term for re-use in science",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Essentials</span>"
    ]
  },
  {
    "objectID": "session_03.html#the-data-life-cycle",
    "href": "session_03.html#the-data-life-cycle",
    "title": "3  Data Management Essentials",
    "section": "3.3 The Data Life Cycle",
    "text": "3.3 The Data Life Cycle\nThe Data Life Cycle gives you an overview of meaningful steps data goes through in a research project, from planning to archival. This step-by-step breakdown facilitates overseeing individual actions, operations and processes required at each stage. This is a visual tool that aims o help scientists plan and anticipate what will be the “data needs” for a specific project (Faundeen et al 2013) .\n\n\n\nSource: Adapted from Faundeen et al 2013, USGS & DataONE\n\n\n\n3.3.1 Primary Elements\n\n\n\n\n\n\n\n\nStep\nDescription\nTool\n\n\n\n\nPlan\nMap out the processes and resources for all activities related to the handling of the project’s data assets. Start with the project goals (desired outputs, outcomes, and impacts) and work backwards to build a data management plan, supporting data policies, and sustainability plans for each step.\nData Management Plan (DPM)\n\n\nAcquire & Discover\nActivities needed to collect new or existing data. You can structure the process of collecting data upfront to better implement data management. Consider data policies and best practices that maintain the provenance and integrity of the data.\nIdentifying data sources and mechanisms to access data\n\n\nProcess\nEvery step needed to prepare new or existing data to be able to use it as an input for synthesis. Consider the structure of the data, unit transformation, extrapolations, etc\nCleaning & Wrangling data skills\n\n\nIntegrate\nData from multiple sources are combined into a form that can be readily analyzed. Successful data integration depends on documentation of the integration process, clearly citing and making accessible the data you are using, and employing good data management practices throughout the Data Life Cycle.\nModeling & Interpretation\n\n\nAnalyze\nCreate analyses and visualizations to identify patterns, test hypotheses, and illustrate findings. During this process, record your methods, document data processing steps, and ensure your data are reproducible. Learn about these best practices and more.\nModeling, Interpretation & Statistics\n\n\nPreserve\nPlan on how you are going to store your data for long-term use and accessibility so others can access, interpret, and use the data in the future. Decide what data to preserve, where to preserve it, and what documentation needs to accompany the data.\nData packages & repositories\n\n\nPublish and Share\nPublication and distribution of your data through the web or in data catalogs, social media or other venues to increase the chances of your data being discovered. Data is a research product as publications are.\nDOIs and citations\n\n\n\n\n\n3.3.2 Cross-Cutting Elements\nThese elements are involved across all stages describes above. They need to constantly by addressed throughout all the Data Life Cycle, making sure effective data management is in place.\n\n\n\n\n\n\n\n\nStep\nDescription\nTool\n\n\n\n\nDescribe\nDocument data and each of the data stages by describing the why, who, what, when, where, and how of the data and processes. Metadata, or data about data, is key to data sharing and reuse. Documentation such as software code comments, data models, and workflows facilitate indexing, acquiring, understanding, and future uses of the data\nMetadata and documentation\n\n\nManage Quality\nEmploy quality assurance and quality control procedures that enhance the quality of data, making sure the measurements or outputs within expected values.Identify potential errors and techniques to address them.\nQuality Control and Quality Assurance techniques\n\n\nBackup and Secure\nPlan to preserve data in the short term to minimize potential losses (e.g., via software failure, human error, natural disaster). This avoids risk and ensures data is accessible to collaborators. This applies to raw and process data, original science plan, data management plan, data acquisition strategy, processing procedures, versioning, analysis methods, published products, and associated metadata\nServers, secure data sharing services",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Essentials</span>"
    ]
  },
  {
    "objectID": "session_03.html#data-management-plans",
    "href": "session_03.html#data-management-plans",
    "title": "3  Data Management Essentials",
    "section": "3.4 Data Management Plans",
    "text": "3.4 Data Management Plans\nAs you can see there is a lot happening around the Data Life Cycle. This is why PLANNING is a key first step. It is advisable to initiate your data management planning at the beginning of your research process before any data has been collected or discovered.\nIn order to better plan and keep track of all the moving pieces when working with data, a good place to start is creating a Data Management Plan. However, this is not only the starting point. This is a “living” document that should be consulted and updated throughout the project.\nA Data Management Plan (DMP) is a document that describes how you will use your data during a research project, as well as what you will do with your data long after the project ends. DMPs should be updated as research plans change to ensure new data management practices are captured (Environmental Data Initiative).\nA well-thought-out plan means you are more likely to:\n\nstay organized\nwork efficiently\ntruly share data\nengage your team\nmeet funder requirements as DMPs are becoming common in the submission process for proposals\n\nA DMP is both a straightforward blueprint for how you manage your data, and provides guidelines for you and your team on policies, roles and responsibilities. While it is important to plan, it is equally important to recognize that no plan is perfect, as change is inevitable. To make your DMP as robust as possible, review it periodically with your team and adjust as the needs of the project change.\n\n\n\n\n\n\nHow to plan\n\n\n\n\nPlan early - information gets lost over time. Think about your data needs as you are starting your project.\nPlan in collaboration - engaging all the team makes your plan more resilient, including diverse expertise and perspectives.\nMake revision part of the process - adapt as needed, revising your plan helps you make sure your are on track.\nInclude a tidy data and data ethic lens \n\n\n\n\n\n3.4.1 Creating a Good Data management Plan\nThe article Ten Simple Rules for Creating a Good Data Management Plan (Michener (2015)) outlines big picture ideas to keep in mind when you start your “planning stage”. Here we summarize each point and provide useful resources or examples to help you achieve this “rules” and write an awesome DMP.\n\n3.4.1.1 Determine what are the organization and/or sponsor requirements\n\nGenerally, each organization or funding agency have specific expectations on how to manage and disseminate data. Even though a DMP is a good idea to keep you organize. It will save you significant time and effort by first understanding the specific requirements set by the organization you are working for. Each organization often provide templates on how to structure your DMP.\nResources\n\nThe DMPTool provides templates for different funding agencies plan requirements.\nUSGS has multiple resources on DMPs. Here the Data Policy and Guidance for the Climate Adaptation Science Centers\n\n\n\n\n3.4.1.2 Identify the desired/necessary data sets for the project\n\nData is the ultimate reason why we create a DMP. Identifying what data will be use is crucial to planning. Key aspects of the data to consider are:\n\nType (text, spatial, images, tabular, etc)\nSource (where does the data currently live?, is it proprietary data?)\nVolume (10 terabytes, 10 megabytes?)\nFormat (csv, xlsx, shapefiles, etc)\n\nResource\n\nHere is a template spreadsheet to collect all information about the data set you intent to use for your synthesis project. Please make a copy and adapt as needed.\n\n\n\n\n3.4.1.3 Define how the data will be organized\n\nOnce you know the data you will be using (rule #2) it is time to define how are you going to work with your data. Where will the raw data live? How are the different collaborators going to access the data? The needs vary widely from one project to another depending on the data. When drafting your DMP is helpful to focus on identifying what products and software you will be using. When collaborating with a team it is important to identify f there are any limitations to accessing any software or tool.\nResource\n\nHere is an example from the LTER Scientific Computing Support Team on working on NCEAS Server.\n\n\n\n\n3.4.1.4 Explain how the data will be documented\n\nWe know documenting data is very important. To successfully achieve this we need a plan in place. Three main steps to plan accordingly are:\n\nIdentifying the type of information you want/need to collect to document your data thoroughly\nDetermine if the is a metadata standard or schema (organized set of elements) you will follow (eg. EML, Dublin Core, ISO 19115, ect). In many cases this relates with what data repository you intend to archive your data.\nEstablish tools that can help you create and manage metadata content.\n\nResource\n\nExcel-to EML by Li Kui is a workflow that provides a spreadsheet template to collect metadata based on the Ecological Metadata Language Schema (EML)\nThe Environmental Data Repository (EDI) providees a tool called ezEML. A form-based web application that helps streamline the process of creating EML metadata.\n\n\n\n\n3.4.1.5 Describe how data quality will be assured\n\nQuality assurance and quality control (QA/QC) are the procedures taken to ensure data looks how we expect it to be. The ultimate goal is to improve the quality of the data products. Some fields of study, data types or funding organizations have specific set of guidelines for QA/QCing data. However, when writing your DMP it is important to describe what measures you plan to take to QA/QC the data (e.g: instrument calibration, verification tests, visualization approaches for error detection, etc.)\nResources\n\nEnvironmental Data Initiative (EDI) description and examples of Quality Assurance and Quality Control\n\n\n\n\n3.4.1.6 Have a data storage strategy (short and long term)\n\nPapers get lost, hardware disk crash, URLs break, different media format degrade. It’s inevitable! Plan ahead and think on where your data will live in the short and long-term to ensure the access and use of this data during and long after the project. It is important to have a backup mechanism in place during the project to avoid losing any information.\nResource\n\nRemote locations to store your data during your project are: institutional repositories or servers or commercial services such as Amazon, Dropbox, Google, Microsoft, Box, etc.\nLong-term storage: identify an appropriate and stable data repository for your research domain (See section 3.6 Data Preservation and Sharing)\n\n\n\n\n3.4.1.7 Define the project’s data policies\n\nMany organizations and institutions require to include an explicit policy statement about how data will be managed and shared. Including licensing or sharing arrangements and legal and ethical restrictions on access and use of human subject and other sensitive data. It is valuable to establish a shared agreement around handling of data when embarking on collaborative projects. Collaborative research brings together data from across research projects with different data management plans and can include publicly accessible data from repositories where no management plan is available. For these reasons, a discussion and agreement around the handling of data brought into and resulting from the collaboration is warranted, and management of this new data may benefit from going through a data management planning process.\nResource\n\nTemplate provided by the Arctic Data Center, including sections for individual data not in the public domain, individual data with public access, derived data resulting from the project.\n\n\n\n\n3.4.1.8 What data products will be made available and how?\n\nThis portion of the DMP tries to ensure that the data products of your project will be disseminated. This can be achieved by stating how, when and where these products will be available. We encourage open data practices, this means making data extensively available and with the least restrictions possible.\nExamples\n\nPublishing the data in an open repository or archive\nSubmitting the data (or subsets thereof) as appendices or supplements to journal articles\nPublishing the data, metadata, and relevant code as a “data paper”\n\n\n\n\n3.4.1.9 Assign roles and responsibilities\n\nIt is important to clearly determine the roles and responsibilities of each group member of the project. Roles may include data collection, data entry, QA/QC, metadata creation and management, backup, data preparation and submission to an archive, and systems administration.\n\n\n\n3.4.1.10 Prepare a realistic budget\n\nGenerally overlooked, but preparing a realistic budget it’s an important part when planning for your data management. Data management takes time and it may have cost associates to it for example access to software, hardware, and personnel. Make sure you plan considers budget to support people involved as well as software or data fees or other services as needed.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Essentials</span>"
    ]
  },
  {
    "objectID": "session_03.html#metadata-best-practices",
    "href": "session_03.html#metadata-best-practices",
    "title": "3  Data Management Essentials",
    "section": "3.5 Metadata Best Practices",
    "text": "3.5 Metadata Best Practices\nWithin the data life cycle you can be collecting data (creating new data) or integrating data that has all ready been collected. Either way, metadata plays plays a major role to successfully spin around the cycle because it enables data reuse long after the original collection.\nImagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files?\nThe goal is to have enough information for the researcher to understand the data, interpret the data, and then reuse the data in another study.\n\n3.5.1 Overall Guidelines\nAnother way to think about metadata is to answer the following questions with the documentation:\n\nWhat was measured?\nWho measured it?\nWhen was it measured?\nWhere was it measured?\nHow was it measured?\nHow is the data structured?\nWhy was the data collected?\nWho should get credit for this data (researcher AND funding agency)?\nHow can this data be reused (licensing)?\n\n\n\n3.5.2 Bibliographic Guidelines\nThe details that will help your data be cited correctly are:\n\nGlobal identifier like a digital object identifier (DOI)\nDescriptive title that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data\nDescriptive abstract that serves as a brief overview off the specific contents and purpose of the data package\nFunding information like the award number and the sponsor\nPeople and organizations like the creator of the dataset (i.e. who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset\n\n\n\n3.5.3 Discovery Guidelines\nThe details that will help your data be discovered correctly are:\n\nGeospatial coverage of the data, including the field and laboratory sampling locations, place names and precise coordinates\nTemporal coverage of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to\nTaxonomic coverage of the data, including what species were measured and what taxonomy standards and procedures were followed\nAny other contextual information as needed\n\n\n\n3.5.4 Interpretation Guidelines\nThe details that will help your data be interpreted correctly are:\n\nCollection methods for both field and laboratory data the full experimental and project design as well as how the data in the dataset fits into the overall project\nProcessing methods for both field and laboratory samples\nAll sample quality control procedures\nProvenance information to support your analysis and modelling methods\nInformation about the hardware and software used to process your data, including the make, model, and version\nComputing quality control procedures like testing or code review\n\n\n\n3.5.5 Data Structure and Contents\n\nEverything needs a description: the data model, the data objects (like tables, images, matrices, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation.\nVariable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (i.e. 0 = not collected), and any missing values (i.e. 999 = NA).\n\nNot only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable).\nFor example, if you were to search for the character string “carbon dioxide flux” in a data repository, not all relevant results will be shown due to varying vocabulary conventions (i.e., “CO2 flux” instead of “carbon dioxide flux”) across disciplines — only datasets containing the exact words “carbon dioxide flux” are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search.\n\n\n3.5.6 Rights and Attribution\nCorrectly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like:\n\nCitation format to be used when giving credit for the data\nAttribution expectations for the dataset\nReuse rights, which describe who may use the data and for what purpose\nRedistribution rights, which describe who may copy and redistribute the metadata and the data\nLegal terms and conditions like how the data are licensed for reuse.\n\n\n\n3.5.7 Metadata Standards\nSo, how does a computer organize all this information? There are a number of metadata standards that make your metadata machine readable and therefore easier for data curators to publish your data.\n\nEcological Metadata Language (EML)\nGeospatial Metadata Standards (ISO 19115 and ISO 19139)\n\nSee NOAA’s ISO Workbook\n\nBiological Data Profile (BDP)\nDublin Core\nDarwin Core\nPREservation Metadata: Implementation Strategies (PREMIS)\nMetadata Encoding Transmission Standard (METS)\n\nNote this is not an exhaustive list.\n\n\n3.5.8 Data Identifiers\nMany journals require a DOI (a digital object identifier) be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to.\nSome data repositories assign a DOI for each dataset you publish on their repository. But, if you need to update the datasets, check the policy of the data repository. Some repositories assign a new DOI after you update the dataset. If this is the case, researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available.\n\n\n3.5.9 Data Citation\nResearchers should get in the habit of citing the data that they use (even if it’s their own data!) in each publication that uses that data.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Essentials</span>"
    ]
  },
  {
    "objectID": "session_03.html#data-preservation-sharing",
    "href": "session_03.html#data-preservation-sharing",
    "title": "3  Data Management Essentials",
    "section": "3.6 Data Preservation & Sharing",
    "text": "3.6 Data Preservation & Sharing\n\n\n3.6.1 Data Packages\n\nWe define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve.\n\nSometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\nMany data repositories assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier, often assigned to the metadata and becomes a publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. This allows to identify a digital entity within a data package.\nIn the graphic to the side, the package can be cited with the DOI doi:10.5063/F1Z1899CZ,and each of the individual files have their own identifiers as well.\n\n\n\n3.6.2 Data Repositories: Built for Data (and code)\n\nGitHub is not an archival location\nExamples of dedicated data repositories:\n\nKNB\nArctic Data Center\ntDAR\nEDI\nZenodo\n\nDedicated data repositories are:\n\nRich in metadata\nArchival in their mission\nCertified\n\nData papers, e.g., Scientific Data\nre3data is a global registry of research data repositories\nRepository Finder is a pilot project and tool to help researchers find an appropriate repository for their work\n\n\n3.6.2.1 DataOne Federation\nDataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses.\nDataONE can be searched on the web, which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the (currently 44!) repositories one at a time.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Essentials</span>"
    ]
  },
  {
    "objectID": "session_03.html#summary",
    "href": "session_03.html#summary",
    "title": "3  Data Management Essentials",
    "section": "3.7 Summary",
    "text": "3.7 Summary\n\nThe Data Life Cycle help us see the big picture of our data project.\nIt is extremely helpful to develop a data management plan describing each step of the data life cycle to stay organized.\nDocument everything. Having rich metadata is a key factor to enable data reuse. Describe your data and files and use an appropriate metadata standard.\nIdentify software and tools that will help you and your team organize and document the project’s data life cycle.\nPublish your data in a stable long live repository and assign a unique identifier.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Essentials</span>"
    ]
  },
  {
    "objectID": "session_03.html#activity",
    "href": "session_03.html#activity",
    "title": "3  Data Management Essentials",
    "section": "3.8 Activity",
    "text": "3.8 Activity\n\n\n\n\n\n\nQuestions to guide your Data Management Plan\n\n\n\nUsing the Logic Models you started on Monday, read the questions in this document. Answer as many as possible to the best of your knowledge. Identify which aspects of managing your data are easier to think about and which ones are more difficult, or you need more guidance.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Essentials</span>"
    ]
  },
  {
    "objectID": "session_03.html#resources",
    "href": "session_03.html#resources",
    "title": "3  Data Management Essentials",
    "section": "Resources",
    "text": "Resources\n\nDMP Example: Using NASA Remote Sensing Data to Reduce Uncertainty of Land-Use Transitions in Global Carbon-Climate Models\nDPM Example: USGS Coastal and Marine Science Center Data Management Plan\nUSGS Data Management Plan Checklist\nSOP for data management for Ocean Health Index assessments (2023 version)\n\n\n\n\n\nMichener, William K. 2015. “Ten Simple Rules for Creating a Good Data Management Plan.” PLOS Computational Biology 11 (10): 1–9. https://doi.org/10.1371/journal.pcbi.1004525.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management Essentials</span>"
    ]
  },
  {
    "objectID": "session_04.html",
    "href": "session_04.html",
    "title": "4  Install R, RStudio and Git",
    "section": "",
    "text": "4.1 Installation steps for MacOS",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Install R, RStudio and Git</span>"
    ]
  },
  {
    "objectID": "session_04.html#installation-steps-for-macos",
    "href": "session_04.html#installation-steps-for-macos",
    "title": "4  Install R, RStudio and Git",
    "section": "",
    "text": "4.1.1 Install or update R\nTo install R, visit cloud.r-project.org to download the most recent version for your operating system. The latest release is version 4.3.2 ( released 2023-10-31).\n\n\n4.1.2 Install or update RStudio\nWhile R is a programming language, RStudio is a software (often referred to as an IDE, Integrated Development Environment) that provides R programmers with a neat, easy-to-use interface for coding in R. There are a number of IDEs out there, but RStudio is arguably the best and definitely most popular among R programmers.\nNote: RStudio will not work without R installed, and you won’t particularly enjoy using R without having RStudio installed. Be sure to install both!\n\n\n\n\n\nImage Credit: Manny Gimond | Accessible at https://mgimond.github.io/ES218/R_vs_RStudio.html\n\n\n\n\n\nNew install: To install RStudio, visit https://posit.co/download/rstudio-desktop/. Download the free (“Open Source Edition”) Desktop version for your operating system. You should install the most up-to-date version available that is supported by your operating system.\nUpdate: If you already have RStudio and need to update: Open RStudio, and under ‘Help’ in the top menu, choose ‘Check for updates.’ If you have the most recent release, it will return ‘No update available. You are running the most recent version of RStudio.’ Otherwise, you should follow the instructions to install an updated version.\nOpen RStudio (logo you’ll click on shown below): If upon opening RStudio you are prompted to install Command Line Tools, do it.\n\n\n\n\n\n\n\n\n\n\n\nNote: you may need to install command line tools and XQuartz.\n\nTo install command line tools (if you’re not automatically prompted), run in the Terminal tab in RStudio: xcode-select --install\nVisit xquartz.org to download & install XQuartz\n\n\n\n\n4.1.3 Install Quarto\nQuarto is a scientific publishing tool built on Pandoc that allows R, Python, Julia, and ObservableJS users to create dynamic documents, websites, books and more.\nQuarto is now included with RStudio v2022.07.1+ so no need for a separate download/install if you have the latest version of RStudio! You can find all releases (current, pre, and older releases) on the Quarto website download page, should you want/need to reference them.\n\n\n4.1.4 Check for git\nYou should already have git on your device, but let’s check for it anyway.\n\nOpen RStudio\nIn the Terminal, run the following command:\n\n\nwhich git\n\n\nIf after running that you get something that looks like a file path to git on your computer, then you have git installed. For example, that might return something like this (or it could differ a bit): /usr/local/bin/git. If you instead get no response at all, you should download & install git from here: git-scm.com/downloads\n\nAn aside: Is it ncesary to have Git installed in your machine for this workshop. GitHub’s Git Guides are a really wonderful resource to start learning about this topic.\n\n\n4.1.5 Install R packages\n\nInstall the usethis and gitcreds packages in R by running the following in the RStudio Console:\n\n\ninstall.packages(“usethis”)\n\ninstall.packages(\"gitcreds\")\n\nA lot of scary looking red text will show up while this is installing - don’t panic. If you get to the end and see something like below (with no error) it’s installed successfully.\n\n\n\n\n\n\n\n\n\n\n\n4.1.6 Create a GitHub account\n\nIf you don’t already have a GitHub account, go to github.com and create one. Here are helpful considerations for choosing a username: happygitwithr.com/github-acct.html.\n\nOnce you’ve completed these steps you are ready for our workshop on Git and Github",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Install R, RStudio and Git</span>"
    ]
  },
  {
    "objectID": "session_04.html#installation-steps-for-windows",
    "href": "session_04.html#installation-steps-for-windows",
    "title": "4  Install R, RStudio and Git",
    "section": "4.2 Installation steps for Windows",
    "text": "4.2 Installation steps for Windows\n\n4.2.1 Install or update R\nTo install R, visit cloud.r-project.org to download the most recent version for your operating system. The latest release is version 4.3.2 ( released 2023-10-31).\n\n\n4.2.2 Install or update RStudio\nWhile R is a programming language, RStudio is a software (often referred to as an IDE, Integrated Development Environment) that provides R programmers with a neat, easy-to-use interface for coding in R. There are a number of IDEs out there, but RStudio is arguably the best and definitely most popular among R programmers.\nNote: RStudio will not work without R installed, and you won’t particularly enjoy using R without having RStudio installed. Be sure to install both!\n\n\n\n\n\nImage Credit: Manny Gimond | Accessible at https://mgimond.github.io/ES218/R_vs_RStudio.html\n\n\n\n\n\nNew install: To install RStudio, visit https://posit.co/download/rstudio-desktop/. Download the free (“Open Source Edition”) Desktop version for your operating system. You should install the most up-to-date version available that is supported by your operating system.\nUpdate: If you already have RStudio and need to update: Open RStudio, and under ‘Help’ in the top menu, choose ‘Check for updates.’ If you have the most recent release, it will return ‘No update available. You are running the most recent version of RStudio.’ Otherwise, you should follow the instructions to install an updated version.\nOpen RStudio (logo you’ll click on shown below): If upon opening RStudio you are prompted to install Command Line Tools, do it.\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.3 Install Quarto\nQuarto is a scientific publishing tool built on Pandoc that allows R, Python, Julia, and ObservableJS users to create dynamic documents, websites, books and more.\nQuarto is now included with RStudio v2022.07.1+ so no need for a separate download/install if you have the latest version of RStudio! You can find all releases (current, pre, and older releases) on the Quarto website download page, should you want/need to reference them.\n\n\n4.2.4 Check for git\nYou should already have git on your device, but let’s check for it anyway.\n\nOpen RStudio\nIn the Terminal, run the following command:\n\n\nwhere git\n\n\nIf after running that you get something that looks like a file path to git on your computer, then you have git installed. For example, that might return something like this (or it could differ a bit): /usr/local/bin/git. If you instead get no response at all or something along the lines “git is not installed”, you should download & install git from here: https://gitforwindows.org/.\n\nOnce you have download and installed Git, restart your computer. Then open RStudio and again run:\n\nwhere git\n\nIf you still get a message saying something like “git is not installed”, check out the Troubleshooting section below.\nAn aside: Is it necessary to have Git installed in your machine for this workshop. GitHub’s Git Guides are a really wonderful resource to start learning about this topic.\n\n\n4.2.5 Install R packages\n\nInstall the usethis and gitcreds packages in R by running the following in the RStudio Console:\n\n\ninstall.packages(\"usethis\")\n\ninstall.packages(\"gitcreds\")\n\nA lot of scary looking red text will show up while this is installing - don’t panic. If you get to the end and see something like below (with no error) it’s installed successfully.\n\n\n\n\n\n\n\n\n\n\n\n4.2.6 Create a GitHub account\n\nIf you don’t already have a GitHub account, go to github.com and create one. Here are helpful considerations for choosing a username: happygitwithr.com/github-acct.html.\n\nOnce you’ve completed these steps you are ready for our workshop on Git and Github",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Install R, RStudio and Git</span>"
    ]
  },
  {
    "objectID": "session_04.html#troubleshooting",
    "href": "session_04.html#troubleshooting",
    "title": "4  Install R, RStudio and Git",
    "section": "4.3 Troubleshooting",
    "text": "4.3 Troubleshooting\n\n\n\n\n\n\nIssues installing Git on a Windows\n\n\n\nIf you download Git and the Git commands still not recognized by your computer, check your computer’s PATHS.\nTo do this, follow the instructions in this link on how to set the right PATHS.\nRestart your computer and try running git --version on the terminal. You should get something like git version XX.XX (but with numbers instead of Xs).\nIf you see the git version printed out in your terminal, you are all set",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Install R, RStudio and Git</span>"
    ]
  },
  {
    "objectID": "session_05.html",
    "href": "session_05.html",
    "title": "5  R and GitHub Setup",
    "section": "",
    "text": "Before we start",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and GitHub Setup</span>"
    ]
  },
  {
    "objectID": "session_05.html#before-we-start",
    "href": "session_05.html#before-we-start",
    "title": "5  R and GitHub Setup",
    "section": "",
    "text": "Non-Verbal Feedback\nWe’ll be using the Zoom “Non Verbal Feedback” buttons throughout this session. We will ask you to put a green check by your name when you’re all set and ready to move on, and a red x by your name if you’re stuck or need assistance. These buttons can be found in the Reaction menu on the toolbar. When you’re asked to answer using these buttons, please ensure that you select one so that the instructor has the feedback that they need to either continue the lesson or pause until everyone gets back on the same page.\n\n\n\nQuestions and Getting Help\nWhen you need to ask a question, please do so in one of the following ways:\n\nTurn your mic on and ask. If you are uncomfortable interrupting the instructor, you may also raise your virtual hand (in the Reaction menu) and the instructor will pause at their convenience and address you to ask your question.\nAsk your question in the chat. The instructor might not see or address the questions ins the chat right away, but they will get to it whenever possible through out the lesson.\n\nIf you have an issue/error and get stuck, you can ask for help in the following ways:\n\nTurn your mic on and ask for help. See also above regarding the use of a virtual raised hand.\nLet one of the instructors know through the chat\nIf prompted to do so, put a red X next to your name as your status in the participant window.\nIf you have an issue that requires in-depth trouble shooting, please let us know and we will coordinate a time with you after this call.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and GitHub Setup</span>"
    ]
  },
  {
    "objectID": "session_05.html#r-and-rstudio-setup",
    "href": "session_05.html#r-and-rstudio-setup",
    "title": "5  R and GitHub Setup",
    "section": "5.1 R and RStudio Setup",
    "text": "5.1 R and RStudio Setup",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and GitHub Setup</span>"
    ]
  },
  {
    "objectID": "session_05.html#learning-objectives",
    "href": "session_05.html#learning-objectives",
    "title": "5  R and GitHub Setup",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPractice creating an R Project\nOrganize an R Project for effective project management\nUnderstand how to move in an R Project using paths and working directories\n\n\n5.1.1 Intro to R and RStudio\nRStudio is the main program we are going to be using today. We will access your computer’s terminal through RStudio to set up all the necessary Git configurations. Please make sure you have read and followed the instructions in Install R, RStudio and Git section so we can successfully go over the configuration steps below.\n\n\n\nArtwork by Allison Horst\n\n\nThere is a vibrant community out there that is collectively developing increasingly easy to use and powerful open source programming tools. The changing landscape of programming is making learning how to code easier than it ever has been. Incorporating programming into analysis workflows not only makes science more efficient, but also more computationally reproducible. In this course, we will use the programming language R, and the accompanying integrated development environment (IDE) RStudio. R is a great language to learn for data-oriented programming because it is widely adopted, user-friendly, and (most importantly) open source!\nSo what is the difference between R and RStudio? Here is an analogy to start us off. If you were a chef, R is a knife. You have food to prepare, and the knife is one of the tools that you’ll use to accomplish your task.\nAnd if R were a knife, RStudio is the kitchen. RStudio provides a place to do your work! Other tools, communication, community, it makes your life as a chef easier. RStudio makes your life as a researcher easier by bringing together other tools you need to do your work efficiently - like a file browser, data viewer, help pages, terminal, community, support, the list goes on. So it’s not just the infrastructure (the user interface or IDE), although it is a great way to learn and interact with your variables, files, and interact directly with git. It’s also data science philosophy, R packages, community, and more. Although you can prepare food without a kitchen and we could learn R without RStudio, that’s not what we’re going to do. We are going to take advantage of the great RStudio support, and learn R and RStudio together.\nSomething else to start us off is to mention that you are learning a new language here. It’s an ongoing process, it takes time, you’ll make mistakes, it can be frustrating, but it will be overwhelmingly awesome in the long run. We all speak at least one language; it’s a similar process, really. And no matter how fluent you are, you’ll always be learning, you’ll be trying things in new contexts, learning words that mean the same as others, etc, just like everybody else. And just like any form of communication, there will be miscommunication that can be frustrating, but hands down we are all better off because of it.\nWhile language is a familiar concept, programming languages are in a different context from spoken languages and you will understand this context with time. For example: you have a concept that there is a first meal of the day, and there is a name for that: in English it’s “breakfast.” So if you’re learning Spanish, you could expect there is a word for this concept of a first meal. (And you’d be right: “desayuno”). We will get you to expect that programming languages also have words (called functions in R) for concepts as well. You’ll soon expect that there is a way to order values numerically. Or alphabetically. Or search for patterns in text. Or calculate the median. Or reorganize columns to rows. Or subset exactly what you want. We will get you to increase your expectations and learn to ask and find what you’re looking for.\n\n\n5.1.2 RStudio IDE\nLet’s take a tour of the RStudio interface.\n\nNotice the default panes:\n\nConsole (entire left)\nEnvironment/History (tabbed in upper right)\nFiles/Plots/Packages/Help (tabbed in lower right)\n\n\n\n\n\n\n\nQuick Tip\n\n\n\nYou can change the default location of the panes, among many other things, see Customizing RStudio.\n\n\n\n\n5.1.3 Create an R Project\nIn this course, we are going to be using an R project to organize our work. An R project is tied to a directory on your local computer, and makes organizing your work and collaborating with others easier.\nThe Big Idea: using an R project is a reproducible research best practice because it bundles all your work within a working directory. Consider your current data analysis workflow. Where do you import you data? Where do you clean and wrangle it? Where do you create graphs, and ultimately, a final report? Are you going back and forth between multiple software tools like Microsoft Excel, JMP, and Google Docs? An R project and the tools in R that we will talk about today will consolidate this process because it can all be done (and updated) in using one software tool, RStudio, and within one R project.\n\n\n\n\n\n\nR Project Setup\n\n\n\n\nIn the “File” menu, select “New Project”\nClick “New Directory”\nClick “New Project”\nUnder “Directory name” type: training_{USERNAME} (i.e. training_vargas)\nLeave “Create Project as subdirectory of:” set to ~\nClick “Create Project”\n\nRStudio should open your new project automatically after creating it. One way to check this is by looking at the top right corner and checking for the project name.\n\n\n\n\n5.1.4 Organizing an R Project\nWhen starting a new research project, step 1 is to create an R Project for it (just like we have here!). The next step is to then populate that project with relevant directories. There are many tools out there that can do this automatically. Some examples are rrtools or usethis::create_package(). The goal is to organize your project so that it is a compendium of your research. This means that the project has all of the digital parts needed to replicate your analysis, like code, figures, the manuscript, and data access.\nSome common directories are:\n\n\n\n\ndata: where we store our data (often contains subdirectories for raw, processed, and metadata data)\nR: contains scripts for cleaning or wrangling, etc. (some find this name misleading if their work has other scripts beyond the R programming language, in which case they call this directory scripts)\nplots or figs: generated plots, graphs, and figures\ndocs: summaries or reports of analysis or other relevant project information\n\nDirectory organization will vary from project to project, but the ultimate goal is to create a well organized project for both reproducibility and collaboration.\n\n\n5.1.5 Moving in an R Project using Paths & Working Directories\n\nNow that we have your project created (and notice we know it’s an R Project because we see a .Rproj file in our Files pane), let’s learn how to move in a project. We do this using paths.\nThere are two types of paths in computing: absolute paths and relative paths.\n\nAn absolute path always starts with the root of your file system and locates files from there. The absolute path to my project directory is: /home/vargas-poulsen/training_vargas\nRelative paths start from some location in your file system that is below the root. Relative paths are combined with the path of that location to locate files on your system. R (and some other languages like MATLAB) refer to the location where the relative path starts as our working directory.\n\nRStudio projects automatically set the working directory to the directory of the project. This means that you can reference files from within the project without worrying about where the project directory itself is. If I want to read in a file from the data directory within my project, the code to do this would be read.csv(\"data/samples.csv\") (path relative to my R project) as opposed to read.csv(\"/home/vargas-poulsen/training_vargas/data/samples.csv\") (absolute path of my home directory).\nThis is not only convenient for you, but also when working collaboratively. For example if Matt makes a copy of my R project that I have published on GitHub, and I am using relative paths, he can run my code exactly as I have written it, without going back and changing /home/vargas-poulsen/training_vargas/data/samples.csv to /home/jones/training_jones/data/samples.csv.\nNote that once you start working in projects you should basically never need to run the setwd() command. If you are in the habit of doing this, stop and take a look at where and why you do it. Could leveraging the working directory concept of R projects eliminate this need? Almost definitely!\n\n\nsetwd() sets your working directory to specified file path (aka directory).\nSimilarly, think about how you work with absolute paths. Could you leverage the working directory of your R project to replace these with relative paths and make your code more portable? Probably!",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and GitHub Setup</span>"
    ]
  },
  {
    "objectID": "session_05.html#git-and-github-setup",
    "href": "session_05.html#git-and-github-setup",
    "title": "5  R and GitHub Setup",
    "section": "5.2 Git and GitHub Setup",
    "text": "5.2 Git and GitHub Setup",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and GitHub Setup</span>"
    ]
  },
  {
    "objectID": "session_05.html#learning-objectives-1",
    "href": "session_05.html#learning-objectives-1",
    "title": "5  R and GitHub Setup",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSet global options in your .gitconfig file\nPractice how to set up GitHub Authentication using a Personal Access Token (PAT)\n\n\n5.2.1 Set up global options in Git\nBefore using Git, you need to tell it who you are, also known as setting the global options. To do this, we will be setting the global options in the Terminal.\n\n\n\n\n\n\nWhat’s the Terminal?\n\n\n\nTechnically, the Terminal is an interface for the shell, a computer program. To put that simply, we use the Terminal to tell a computer what to do. This is different from the Console in RStudio, which interprets R code and returns a value.\n\n\nTo get started, let’s open a new Terminal window in RStudio. Do this by clicking Tools &gt; Terminal &gt; New Terminal.\nA Terminal tab should now be open where your Console usually is.\n\n\n\n\n\n\nDon’t be afraid to dip your toes in the Terminal\n\n\n\nMost of our git operations will be done in RStudio, but there are some situations where you must work in the Terminal and use command line. It may be daunting to code in the Terminal, but as your comfort increases over time, you might find you prefer it. Either way, it’s beneficial to learn enough command line and to feel comfortable in the Terminal.\n\n\nLet’s start by adding your user name to the global options. Type the following into the command prompt, with your exact GitHub username, and press enter:\ngit config --global user.name \"my_user_name\"\n\n\nNote that if it ran successfully, it will look like nothing happened. We will check at the end to make sure it worked.\nNext, enter the following line, with the email address you used when you created your account on github.com:\ngit config --global user.email \"my_email@nceas.ucsb.edu\"\n\n\n\n\n\n\nCase and spelling matters!\n\n\n\nWhen you add your username and email to the global options you must use the exact same spelling and case that you used on GitHub otherwise, Git won’t be able to sync to your account.\n\n\n\nNext, we will set the default branch name to main for any new repositories that are created moving forward. Why are we doing this? Previously, the default branch name was master and this racist terminology for git branches motivates us to update our default branch to main instead.\ngit config --global init.defaultBranch main\nFinally, check to make sure everything looks correct by entering this command, which will return the global options you have set.\ngit config --global --list\n\n\n5.2.2 GitHub Authentication\nGitHub recently deprecated password authentication for accessing repositories, so we need to set up a secure way to authenticate.\nThe book Happy Git and GitHub for the useR has a wealth of information related to working with Git in R, and these instructions are based off of Chapter 9 Personal access token for HTTPS.\nWe will be using a Personal Access Token (PAT) in this course. For better security and long term use, we recommend taking the extra steps to set up SSH keys (check out Chapter 10 Set up Keys for SSH).\n\n\n\n\n\n\nSetting up your PAT\n\n\n\n\nRun usethis::create_github_token() in the Console.\nA new browser window should open up to GitHub, showing all the scopes options. You can review the scopes, but you don’t need to worry about which ones to select this time. Using this function automatically pre-selects some recommended scopes. Go ahead and scroll to the bottom and click “Generate Token”.\nCopy the generated token.\nBack in RStudio, run gitcreds::gitcreds_set() in the Console.\nPaste your PAT when the prompt asks for it.\nLast thing, run usethis::git_sitrep() in the Console to check your Git configuration and that you’ve successful stored your PAT.\n\n\n\nCongrats! Now you’ve setup your authentication you should be able to work with GitHub in RStudio now.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and GitHub Setup</span>"
    ]
  },
  {
    "objectID": "session_06.html",
    "href": "session_06.html",
    "title": "6  Into to Git and GitHub",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Into to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_06.html#learning-objectives",
    "href": "session_06.html#learning-objectives",
    "title": "6  Into to Git and GitHub",
    "section": "",
    "text": "Apply the principles of Git to track and manage changes of a project\nUtilize the Git workflow including pulling changes, staging modified files, committing changes, pulling again to incorporate remote changes, and pushing changes to a remote repository\nCreate and configure Git repositories using different workflows",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Into to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_06.html#introduction-to-version-control",
    "href": "session_06.html#introduction-to-version-control",
    "title": "6  Into to Git and GitHub",
    "section": "6.1 Introduction to Version Control",
    "text": "6.1 Introduction to Version Control\n\n\n\n\n\nEvery file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when bugs are discovered. Sometimes those fixes lead to even more bugs, leading to more changes in the codebase. Data files get combined together. Sometimes those same files are split and combined again. All that to say - in just one research project, we can expect thousands of changes to occur.\nThese changes are important to track, and yet, we often use simplistic filenames to track them. Many of us have experienced renaming a document or script multiple times with the ingenuine addition of “final” to the filename (like the comic above demonstrates).\nYou might think there is a better way, and you’d be right: version control. Version control provides an organized and transparent way to track changes in code and additional files. This practice was designed for software development, but is easily applicable to scientific programming.\nThere are many benefits to using a version control software including:\n\nMaintain a history of your research project’s development while keeping your workspace clean\nFacilitate collaboration and transparency when working on teams\nExplore bugs or new features without disrupting your team members’ work\nand more!\n\nThe version control system we’ll be diving into is Git, the most widely used modern version control system in the world.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Into to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_06.html#introduction-to-git-github",
    "href": "session_06.html#introduction-to-git-github",
    "title": "6  Into to Git and GitHub",
    "section": "6.2 Introduction to Git + GitHub",
    "text": "6.2 Introduction to Git + GitHub\nBefore diving into the details of Git and how to use it, let’s start with a motivating example that’s representative of the types of problems Git can help us solve.\n\n6.2.1 A Motivating Example\nSay, for example, you’re working on an analysis in R and you’ve got it into a state you’re pretty happy with. We’ll call this version 1:\n\n\n\nYou come into the office the following day and you have an email from your boss, “Hey, you know what this model needs?”\n\n\n\nYou’re not entirely sure what she means but you figure there’s only one thing she could be talking about: more cowbell. So you add it to the model in order to really explore the space.\nBut you’re worried about losing track of the old model so, instead of editing the code in place, you comment out the old code and put as serious a warning as you can muster in a comment above it.\n\n\n\nCommenting out code you don’t want to lose is something probably all of us have done at one point or another but it’s really hard to understand why you did this when you come back years later or you when you send your script to a colleague. Luckily, there’s a better way: Version control. Instead of commenting out the old code, we can change the code in place and tell Git to commit our change. So now we have two distinct versions of our analysis and we can always see what the previous version(s) look like.\n\n\n\nYou may have noticed something else in the diagram above: Not only can we save a new version of our analysis, we can also write as much text as we like about the change in the commit message. In addition to the commit message, Git also tracks who, when, and where the change was made.\nImagine that some time has gone by and you’ve committed a third version of your analysis, version 3, and a colleague emails with an idea: What if you used machine learning instead?\n\n\n\nMaybe you’re not so sure the idea will work out and this is where a tool like Git shines. Without a tool like Git, we might copy analysis.R to another file called analysis-ml.R which might end up having mostly the same code except for a few lines. This isn’t particularly problematic until you want to make a change to a bit of shared code and now you have to make changes in two files, if you even remember to.\nInstead, with Git, we can start a branch. Branches allow us to confidently experiment on our code, all while leaving the old code in tact and recoverable.\n\n\n\nSo you’ve been working in a branch and have made a few commits on it and your boss emails again asking you to update the model in some way. If you weren’t using a tool like Git, you might panic at this point because you’ve rewritten much of your analysis to use a different method but your boss wants change to the old method.\n\n\n\nBut with Git and branches, we can continue developing our main analysis at the same time as we are working on any experimental branches. Branches are great for experiments but also great for organizing your work generally.\n\n\n\nAfter all that hard work on the machine learning experiment, you and your colleague could decide to scrap it. It’s perfectly fine to leave branches around and switch back to the main line of development but we can also delete them to tidy up.\n\n\n\nIf, instead, you and your colleague had decided you liked the machine learning experiment, you could also merge the branch with your main development line. Merging branches is analogous to accepting a change in Word’s Track Changes feature but way more powerful and useful.\n\n\n\nA key takeaway here is that Git can drastically increase your confidence and willingness to make changes to your code and help you avoid problems down the road. Analysis rarely follows a linear path and we need a tool that respects this.\n\n\n\nFinally, imagine that, years later, your colleague asks you to make sure the model you reported in a paper you published together was actually the one you used. Another really powerful feature of Git is tags which allow us to record a particular state of our analysis with a meaningful name. In this case, we are lucky because we tagged the version of our code we used to run the analysis. Even if we continued to develop beyond commit 5 (above) after we submitted our manuscript, we can always go back and run the analysis as it was in the past.\n\nWith Git we can enhance our workflow:\n\nEliminate the need for cryptic filenames and comments to track our work.\nProvide detailed descriptions of our changes through commits, making it easier to understand the reasons behind code modifications.\nWork on multiple branches simultaneously, allowing for parallel development, and optionally merge them together.\nUse commits to access and even execute older versions of our code.\nAssign meaningful tags to specific versions of our code.\nAdditionally, Git offers a powerful distributed feature. Multiple individuals can work on the same analysis concurrently on their own computers, with the ability to merge everyone’s changes together.\n\n\n\n\n6.2.2 What exactly are Git and GitHub?\n\nGit:\n\nan open-source distributed version control software\ndesigned to manage the versioning and tracking of source code files and project history\noperates locally on your computer, allowing you to create repositories, track changes, and collaborate with others\nprovides features such as committing changes, branching and merging code, reverting to previous versions, and managing project history\nworks directly with the files on your computer and does not require a network connection to perform most operations\nprimarily used through the command-line interface (CLI, e.g. Terminal), but also has various GUI tools available (e.g. RStudio IDE)\n\n\n\n\n\n\nGitHub:\n\nonline platform and service built around Git\nprovides a centralized hosting platform for Git repositories\nallows us to store, manage, and collaborate on their Git repositories in the cloud\noffers additional features on top of Git, such as a web-based interface, issue tracking, project management tools, pull requests, code review, and collaboration features\nenables easy sharing of code with others, facilitating collaboration and contribution to open source projects\nprovides a social aspect, allowing users to follow projects, star repositories, and discover new code\n\n\n\n\n\n\n\n6.2.3 The Git Life cycle\nAs a Git user, you’ll need to understand the basic concepts associated with versioned sets of changes, and how they are stored and moved across repositories. Any given Git repository can be cloned so that it exists both locally, and remotely. But each of these cloned repositories is simply a copy of all of the files and change history for those files, stored in Git’s particular format. For our purposes, we can consider a Git repository as a folder with a bunch of additional version-related metadata.\nIn a local Git-enabled folder, the folder contains a workspace containing the current version of all files in the repository. These working files are linked to a hidden folder containing the ‘Local repository’, which contains all of the other changes made to the files, along with the version metadata.\nSo, when working with files using Git, you can use Git commands to indicate specifically which changes to the local working files should be staged for versioning (using the git add command), and when to record those changes as a version in the local repository (using the command git commit).\nThe remaining concepts are involved in synchronizing the changes in your local repository with changes in a remote repository. The git push command is used to send local changes up to a remote repository (possibly on GitHub), and the git pull command is used to fetch changes from a remote repository and merge them into the local repository.\nA basic git workflow represented as two islands, one with “local repo” and “working directory”, and another with “remote repo.” Bunnies move file boxes from the working directory to the staging area, then with Commit move them to the local repo. Bunnies in rowboats move changes from the local repo to the remote repo (labeled “PUSH”) and from the remote repo to the working directory (labeled “PULL”).\n\n\n\n\nArtwork by Allison Horst\n\n\n\n\n6.2.4 Let’s Look at a GitHub Repository\nThis screen shows the copy of a repository stored on GitHub, with its list of files, when the files and directories were last modified, and some information on who made the most recent changes.\n\n\n\nIf we drill into the “commits” for the repository, we can see the history of changes made to all of the files. Looks like kellijohnson was working on the project and fixing errors in December:\n\n\n\nAnd finally, if we drill into one of the changes made on December 20, we can see exactly what was changed in each file:\n\n\n\nTracking these changes, how they relate to released versions of software and files is exactly what Git and GitHub are good for. And we will show how they can really be effective for tracking versions of scientific code, figures, and manuscripts to accomplish a reproducible workflow.\n\n\n6.2.5 Git Vocabulary & Commands\nWe know the world of Git and GitHub can be daunting. Use these tables as references while you use Git and GitHub, and we encourage you to build upon this list as you become more comfortable with these tools.\nThis table contains essential terms and commands that complement intro to Git skills. They will get you far on personal and individual projects.\n\nEssential Git Commands\n\n\n\n\n\n\n\nTerm\nGit Command(s)\nDefinition\n\n\n\n\nAdd\ngit add [file]\nStages or adds file changes to the next commit. git add . will stage or add all files.\n\n\nCommit\ngit commit\nRecords changes to the repository with a descriptive message.\n\n\nCommit Message\ngit commit -m\nA descriptive message explaining the changes made in a commit. The message must be within quotes (e.g. “This is my commit message.”).\n\n\nFetch\ngit fetch\nRetrieves changes from a remote repository but does not merge them.\n\n\nPull\ngit pull\nRetrieves and merges changes from a remote repository to the current branch.\n\n\nPush\ngit push\nSends local commits to a remote repository.\n\n\nStage\n-\nThe process of preparing and selecting changes to be included in the next commit.\n\n\nStatus\ngit status\nShows the current status of the repository, including changes and branch information.\n\n\n\nThis table includes more advanced Git terms and commands that are commonly used in both individual and collaborative projects.\n\nAdvanced Git Commands\n\n\n\n\n\n\n\nTerm\nGit Command(s)\nDefinition\n\n\n\n\nBranch\ngit branch\nLists existing branches or creates a new branch.\n\n\nCheckout\ngit checkout [branch]\nSwitches to a different branch or restores files from a specific commit.\n\n\nClone\ngit clone [repository]\nCreates a local copy of a remote repository.\n\n\nDiff\ngit diff\nShows differences between files, commits, or branches.\n\n\nFork\n-\nCreates a personal copy of a repository under your GitHub account for independent development.\n\n\nLog\ngit log\nDisplays the commit history of the repository.\n\n\nMerge\ngit merge [branch]\nIntegrates changes from one branch into another branch.\n\n\nMerge Conflict\n-\nOccurs when Git cannot automatically merge changes from different branches, requiring manual resolution.\n\n\nPull Request (PR)\n-\nA request to merge changes from a branch into another branch, typically in a collaborative project.\n\n\nRebase\ngit rebase\nIntegrates changes from one branch onto another by modifying commit history.\n\n\nRemote\ngit remote\nManages remote repositories linked to the local repository.\n\n\nRepository\ngit init\nA directory where Git tracks and manages files and their versions.\n\n\nStash\ngit stash\nTemporarily saves changes that are not ready to be committed.\n\n\nTag\ngit tag\nAssigns a label or tag to a specific commit.\n\n\n\nGit has a rich set of commands and features, and there are many more terms beyond either table.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Into to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_06.html#exercise-1-create-a-remote-repository-on-github",
    "href": "session_06.html#exercise-1-create-a-remote-repository-on-github",
    "title": "6  Into to Git and GitHub",
    "section": "6.3 Exercise 1: Create a remote repository on GitHub",
    "text": "6.3 Exercise 1: Create a remote repository on GitHub\n\n\n\n\n\n\nSetup\n\n\n\n\nLog into GitHub\nClick the New repository button\nName it {FIRSTNAME}_test\nAdd a short description\nCheck the box to add a README.md file\nAdd a .gitignore file using the R template\nSet the LICENSE to Apache 2.0\n\n\n\nIf you were successful, it should look something like this:\n\n\n\nYou’ve now created your first repository! It has a couple of files that GitHub created for you, like the README.md file, and the LICENSE file, and the .gitignore file.\n\n\n\nFor simple changes to text files, you can make edits right in the GitHub web interface.\n\n\n\n\n\n\nChallenge\n\n\n\nNavigate to the README.md file in the file listing, and edit it by clicking on the pencil icon. This is a regular Markdown file, so you can just add markdown text. Add a new level 2 header called “Purpose” and add some bullet points describing the purpose of the repo. When done, add a commit message, and hit the “Commit changes” button.\n\n\n\n\n\nCongratulations, you’ve now authored your first versioned commit! If you navigate back to the GitHub page for the repository, you’ll see your commit listed there, as well as the rendered README.md file.\n\n\n\nLet’s point out a few things about this window. It represents a view of the repository that you created, showing all of the files in the repository so far. For each file, it shows when the file was last modified, and the commit message that was used to last change each file. This is why it is important to write good, descriptive commit messages. In addition, the header above the file listing shows the most recent commit, along with its commit message, and its SHA identifier. That SHA identifier is the key to this set of versioned changes. If you click on the SHA identifier (6c18e0a), it will display the set of changes made in that particular commit.\n\n\n\n\n\n\nWhat should I write in my commit message?\n\n\n\nWriting effective Git commit messages is essential for creating a meaningful and helpful version history in your repository. It is crucial to avoid skipping commit messages or resorting to generic phrases like “Updates.” When it comes to following best practices, there are several guidelines to enhance the readability and maintainability of the codebase.\nHere are some guidelines for writing effective Git commit messages:\n\nBe descriptive and concise: Provide a clear and concise summary of the changes made in the commit. Aim to convey the purpose and impact of the commit in a few words.\nUse imperative tense: Write commit messages in the imperative tense, as if giving a command. For example, use “Add feature” instead of “Added feature” or “Adding feature.” This convention aligns with other Git commands and makes the messages more actionable.\nSeparate subject and body: Start with a subject line, followed by a blank line, and then provide a more detailed explanation in the body if necessary. The subject line should be a short, one-line summary, while the body can provide additional context, motivation, or details about the changes.\nLimit the subject line length: Keep the subject line within 50 characters or less. This ensures that the commit messages are easily scannable and fit well in tools like Git logs.\nCapitalize and punctuate properly: Begin the subject line with a capital letter and use proper punctuation. This adds clarity and consistency to the commit messages.\nFocus on the “what” and “why”: Explain what changes were made and why they were made. Understanding the motivation behind a commit helps future researchers and collaborators (including you!) comprehend its purpose.\nUse present tense for subject, past tense for body: Write the subject line in present tense as it represents the current state of the codebase. Use past tense in the body to describe what has been done.\nReference relevant issues: If the commit is related to a specific issue or task, include a reference to it. For example, you can mention the issue number or use keywords like “Fixes,” “Closes,” or “Resolves” followed by the issue number.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Into to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_06.html#exercise-2-clone-your-repository-and-use-git-locally-in-rstudio",
    "href": "session_06.html#exercise-2-clone-your-repository-and-use-git-locally-in-rstudio",
    "title": "6  Into to Git and GitHub",
    "section": "6.4 Exercise 2: clone your repository and use Git locally in RStudio",
    "text": "6.4 Exercise 2: clone your repository and use Git locally in RStudio\nIn this exercise, we’ll use the GitHub URL for the GitHub repository you created to clone the repository onto your local machine so that you can edit the files in RStudio.\nStart by copying the GitHub URL, which represents the repository location:\n\n\n\n\n\n\n\nRStudio knows how to work with files under version control with Git, but only if you are working within an R project folder.\nNext, let’s clone the repository created on GitHub so we have it accessible as an R project in RStudio.\n\n\n\n\n\n\nAn important distinction\n\n\n\nWe refer to the remote copy of the repository that is on GitHub as the origin repository (the one that we cloned from), and the copy on our local computer as the local repository.\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nIn the File menu, select “New Project”\nIn the dialog that pops up, select the “Version Control” option, and paste the GitHub URL that you copied into the field for the remote repository Repository URL\nWhile you can name the local copy of the repository anything, it’s typical to use the same name as the GitHub repository to maintain the correspondence\n\n\n\n\n\n\n\n\nOnce you hit “Create Project”, a new RStudio window will open with all of the files from the remote repository copied locally. Depending on how your version of RStudio is configured, the location and size of the panes may differ, but they should all be present, including a Git tab and the normal Files tab listing the files that had been created in the remote repository.\n\n\n\nYou’ll note that there is one new file halina_test.Rproj, and three files that we created earlier on GitHub (.gitignore, LICENSE, and README.md).\nIn the Git tab, you’ll note that two files are listed. This is the status pane that shows the current modification status of all of the files in the repository. In this case, the .gitignore file is listed as M for Modified, and halina_test.Rproj is listed with a ?? to indicate that the file is untracked. This means that Git has not stored any versions of this file, and knows nothing about the file. As you make version control decisions in RStudio, these icons will change to reflect the current version status of each of the files.\nInspect the history. For now, let’s click on the History button in the Git tab, which will show the log of changes that occurred, and will be identical to what we viewed on GitHub. By clicking on each row of the history, you can see exactly what was added and changed in each of the two commits in this repository.\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nLet’s make a change to the README.md file, this time from RStudio, then commit the README.md change\nAdd a new section to your README.md called “Creator” using a level 2 header, and under it include some information about yourself. Bonus: Add some contact information and link your email using Markdown syntax\n\n\n\nOnce you save, you’ll immediately see the README.md file show up in the Git tab, marked as a modification. You can select the file in the Git tab, and click Diff to see the differences that you saved (but which are not yet committed to your local repository).\n\nAnd here’s what the newly made changes look like compared to the original file. New lines are highlighted in green, while removed lines are in red.\n\nCommit the RStudio changes. To commit the changes you made to the README.md file, check the Staged checkbox next to the file (which tells Git which changes you want included in the commit), then provide a descriptive commit message, and then click “Commit”.\n\nNote that some of the changes in the repository, namely halina_test.Rproj are still listed as having not been committed. This means there are still pending changes to the repository. You can also see the note that says:\nYour branch is ahead of ‘origin/main’ by 1 commit.\nThis means that we have committed 1 change in the local repository, but that commit has not yet been pushed up to the origin repository, where origin is the typical name for our remote repository on GitHub. So, let’s commit the remaining project files by staging them and adding a commit message.\n\nWhen finished, you’ll see that no changes remain in the Git tab, and the repository is clean.\nInspect the history. Note that the message now says:\nYour branch is ahead of ‘origin/main’ by 2 commits.\nThese 2 commits are the two we just made, and have not yet been pushed to GitHub. By clicking on the “History” button, we can see that there are now a total of four commits in the local repository (while there had only been two on GitHub).\n\nPush these changes to GitHub. Now that everything has been changed as desired locally, you can push the changes to GitHub using the Push button. This will prompt you for your GitHub username and password, and upload the changes, leaving your repository in a totally clean and synchronized state. When finished, looking at the history shows all four commits, including the two that were done on GitHub and the two that were done locally on RStudio.\n\n\n\nAnd note that the labels indicate that both the local repository (HEAD) and the remote repository (origin/HEAD) are pointing at the same version in the history. So, if we go look at the commit history on GitHub, all the commits will be shown there as well.\n\n\n\n\n\n\n\n\n\nLast thing, some Git configuration\n\n\n\nWhen Git released version 2.27, a new feature they incorporated allows users to specify how to pull (essentially), otherwise a warning will appear. To suppress this warning we need to configure our Git with this line of code:\ngit config pull.rebase false\npull.rebase false is a default strategy for pulling where it will try to auto-merge the files if possible, and if it can’t it will show a merge conflict",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Into to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_06.html#exercise-3-setting-up-git-on-an-existing-project",
    "href": "session_06.html#exercise-3-setting-up-git-on-an-existing-project",
    "title": "6  Into to Git and GitHub",
    "section": "6.5 Exercise 3: Setting up Git on an existing project",
    "text": "6.5 Exercise 3: Setting up Git on an existing project\nNow you have two projects set up in your RStudio environment, training_{USERNAME} and {FIRSTNAME}_test. We set you up with the {FIRSTNAME}_test project since we think it is an easy way to introduce you to Git, but more commonly researchers will have an existing directory of code that they then want to make a Git repository out of. For the last exercise of this session, we will do this with your training_{USERNAME} project.\nFirst, switch to your training_{USERNAME} project using the RStudio project dropdown menu. The project dropdown menu is in the upper right corner of your RStudio pane. Click the dropdown next to your project name ({FIRSTNAME}_test), and then select the training_{USERNAME} project from the “recent projects” list.\n\n\n\n\n\nNext, from the Tools menu, select “Project Options.” In the dialog that pops up, select “Git/SVN” from the menu on the left. In the dropdown at the top of this page, select Git and click “Yes” in the confirmation box. Click “Yes” again to restart RStudio.\nWhen RStudio restarts, you should have a Git tab, with two untracked files (.gitignore and training_{USERNAME}.Rproj).\n\n\n\n\n\n\nChallenge\n\n\n\nAdd and commit the .gitignore and training_{USERNAME}.Rproj files to your Git repository.\n\n\nNow we have your local repository all set up. You can make as many commits as you want on this repository, and it will likely still be helpful to you, but the power in Git and GitHub is really in collaboration. As discussed, GitHub facilitates this, so let’s get this repository on GitHub.\n\n\n\n\n\n\nSetup\n\n\n\n\nGo to GitHub, and click on the “New Repository” button.\nIn the repository name field, enter the same name as your R Project. So for me, this would be training_dolinh.\nAdd a description, keep the repository public, and, most importantly: DO NOT INITIALIZE THE REPOSITORY WITH ANY FILES. We already have the repository set up locally so we don’t need to do this. Initializing the repository will only cause merge issues.\n\nHere is what your page should look like:\n\n\n\n\n\n\nClick the “Create repository” button.\n\n\n\nThis will open your empty repository with a page that conveniently gives you exactly the instructions you need. In our case, we are going to “push an existing repository from the command line.”\n\n\n\nClick the clipboard icon to copy the code for the middle option of the three on this page. It should have three lines and look like this:\ngit remote add origin https://github.com/hdolinh/training_dolinh.git\ngit branch -M main\ngit push -u origin main\nBack in RStudio, open the terminal by clicking the Terminal tab next to the Console tab. The prompt should look something like this:\ndolinh@included-crab:~/training_dolinh$\nIn the prompt, paste the code that you copied from the GitHub page and press return.\nThe code that you copied and pasted did three things:\n\nAdded the GitHub repository as the remote repository\nRenamed the default branch to main\nPushed the main branch to the remote GitHub repository\n\nIf you go back to your browser and refresh your GitHub repository page, you should now see your files appear.\n\n\n\n\n\n\nChallenge\n\n\n\nOn your repository page, GitHub has a button that will help you add a README.md file. Click the “Add a README” button and use markdown syntax to create a README.md Commit the changes to your repository.\nGo to your local repository (in RStudio) and pull the changes you made.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Into to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_06.html#go-further-with-git",
    "href": "session_06.html#go-further-with-git",
    "title": "6  Into to Git and GitHub",
    "section": "6.6 Go further with Git",
    "text": "6.6 Go further with Git\nThere’s a lot we haven’t covered in this brief tutorial. There are some great and much longer tutorials that cover advanced topics, such as:\n\nUsing Git on the command line\nResolving conflicts\nBranching and merging\nPull requests versus direct contributions for collaboration\nUsing .gitignore to protect sensitive data\nGitHub Issues - how to use them for project management and collaboration\n\nand much, much more.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Into to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_06.html#git-resources",
    "href": "session_06.html#git-resources",
    "title": "6  Into to Git and GitHub",
    "section": "6.7 Git resources",
    "text": "6.7 Git resources\n\nPro Git Book\nHappy Git and GitHub for the useR\nGitHub Documentation\nLearn Git Branching is an interactive tool to learn Git on the command line\nSoftware Carpentry Version Control with Git\nBitbucket’s tutorials on Git Workflows",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Into to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session_07.html",
    "href": "session_07.html",
    "title": "7  Thinking Preferences",
    "section": "",
    "text": "Learning Objectives\nAn activity and discussion that will provide:",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Thinking Preferences</span>"
    ]
  },
  {
    "objectID": "session_07.html#learning-objectives",
    "href": "session_07.html#learning-objectives",
    "title": "7  Thinking Preferences",
    "section": "",
    "text": "Opportunity to get to know fellow participants and trainers\nAn introduction to variation in thinking preferences\nDiscuss about important norms and polices when working in research teams",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Thinking Preferences</span>"
    ]
  },
  {
    "objectID": "session_07.html#thinking-preferences-activity",
    "href": "session_07.html#thinking-preferences-activity",
    "title": "7  Thinking Preferences",
    "section": "7.1 Thinking Preferences Activity",
    "text": "7.1 Thinking Preferences Activity\nStep 1:\n\nDon’t read ahead!! We’re headed to the patio.\n\n\n7.1.1 About the Whole Brain Thinking System\nEveryone thinks differently. The way individuals think guides the way they work, and the way groups of individuals think guides how teams work. Understanding thinking preferences facilitates effective collaboration and team work.\nThe Whole Brain Model, developed by Ned Herrmann, builds upon early conceptualizations of brain functioning. For example, the left and right hemispheres were thought to be associated with different types of information processing while our neocortex and limbic system would regulate different functions and behaviours.\n\nThe Herrmann Brain Dominance Instrument (HBDI) provides insight into dominant characteristics based on thinking preferences. There are four major thinking styles that reflect the left cerebral, left limbic, right cerebral and right limbic.\n\nAnalytical (Blue)\nPractical (Green)\nRelational (Red)\nExperimental (Yellow)\n\n\nThese four thinking styles are characterized by different traits. Those in the BLUE quadrant have a strong logical and rational side. They analyze information and may be technical in their approach to problems. They are interested in the ‘what’ of a situation. Those in the GREEN quadrant have a strong organizational and sequential side. They like to plan details and are methodical in their approach. They are interested in the ‘when’ of a situation. The RED quadrant includes those that are feelings-based in their apporach. They have strong interpersonal skills and are good communicators. They are interested in the ‘who’ of a situation. Those in the YELLOW quadrant are ideas people. They are imaginative, conceptual thinkers that explore outside the box. Yellows are interested in the ‘why’ of a situation.\n\nMost of us identify with thinking styles in more than one quadrant and these different thinking preferences reflect a complex self made up of our rational, theoretical self; our ordered, safekeeping self; our emotional, interpersonal self; and our imaginitive, experimental self.\n\nUnderstanding the complexity of how people think and process information helps us understand not only our own approach to problem solving, but also how individuals within a team can contribute. There is great value in diversity of thinking styles within collaborative teams, each type bringing strengths to different aspects of project development.\n\n\n\n7.1.2 Bonus Activity\nStep 1:\n\nRead through the statements contained within this document and determine which descriptors are most like you. Make a note of them.\nReview the descriptors again and determine which are quite like you.\nYou are working towards identifying your top 20. If you have more than 20, discard the descriptors that resonate the least.\nUsing the letter codes in the right hand column, count the number of descriptors that fall into the categories A B C and D.\n\nStep 2:\n\nScroll to the second page and copy the graphic onto a piece of paper, completing the quadrant with your scores for A, B, C and D.\n\nStep 3:\n\nReflect and share out: Do you have a dominant letter? Were some of the statements you included in your top 20 easier to resonate with than others? Were you answering based on how you are or how you wish to be?",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Thinking Preferences</span>"
    ]
  },
  {
    "objectID": "session_08.html",
    "href": "session_08.html",
    "title": "8  Science Communication: The Message Box",
    "section": "",
    "text": "Learning Objective",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: The Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#learning-objective",
    "href": "session_08.html#learning-objective",
    "title": "8  Science Communication: The Message Box",
    "section": "",
    "text": "Discuss about the importance of science communication.\nDistinguish between how scientist communicate sciences vs how the rest of the world communicates.\nIntroduce and practice using the Message Box as a tool to communicate science to a specific audience.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: The Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#communicating-science",
    "href": "session_08.html#communicating-science",
    "title": "8  Science Communication: The Message Box",
    "section": "8.1 Communicating Science",
    "text": "8.1 Communicating Science\nHow we communicate our science with the rest of the world matters! There are many different reasons and ways to engage and practice science communication. From writing a blog post as a creative outlet to posting in social media to make science accessible to a broader audience. Science communication is more and more becoming a formal responsibility within researchers as funding organizations encourage or require scientist to do outreach of their work (Borowiec (2023)).\nCommunication can come easy, but doing it well is hard. As scientist, it is important to spread the word of our discoveries, engage with non-scientist audiences, and build empathy and trust on what we do (PLOS SciCom). This way make sure the world understand how important, necessary and meaningful science is.\n\n\n\nJarreau, Paige B (2015): #MySciBlog Interviewee Motivations to Blog about Science\n\n\n\n\n8.1.1 Scholarly publications\n\n“The ingredients of good science are obvious—novelty of research topic, comprehensive coverage of the relevant literature, good data, good analysis including strong statistical support, and a thought-provoking discussion. The ingredients of good science reporting are obvious—good organization, the appropriate use of tables and figures, the right length, writing to the intended audience— do not ignore the obvious” - Bourne 2005\n\nPeer-reviewed publication is the primary mechanism for direct and efficient communication of research findings. Other scholarly communications include abstracts, technical reports, books and book chapters. These communications are largely directed towards students and peers; individuals learning about or engaged in the process of scientific research whether in a university, non-profit, agency, commercial or other setting.\nThe following tabling is a good summary of ‘10 Simple Rules’ for writing research papers (adapted from Zhang 2014, published in Budden and Michener, 2017)\n\n\n\n\n\n\n\n\nMake it a Driving Force\n\n“Design a project with an ultimate paper firmly in mind”\n\n\n\nLess Is More\n\n“Fewer but more significant papers serve both the research community and one’s career better than more papers of less significance”\n\n\n\nPick the Right Audience\n\n“This is critical for determining the organization of the paper and the level of detail of the story, so as to write the paper with the audience in mind.”\n\n\n\nBe Logical\n\n“The foundation of ‘’lively’’ writing for smooth reading is a sound and clear logic underlying the story of the paper.” “An effective tactic to help develop a sound logical flow is to imaginatively create a set of figures and tables, which will ultimately be developed from experimental results, and order them in a logical way based on the information flow through the experiments.”\n\n\n\nBe Thorough and Make It Complete\n\nPresent the central underlying hypotheses; interpret the insights gleaned from figures and tables and discuss their implications; provide sufficient context so the paper is self-contained; provide explicit results so readers do not need to perform their own calculations; and include self-contained figures and tables that are described in clear legends\n\n\n\nBe Concise\n\n“the delivery of a message is more rigorous if the writing is precise and concise”\n\n\n\nBe Artistic\n\n“concentrate on spelling, grammar, usage, and a ‘’lively’’ writing style that avoids successions of simple, boring, declarative sentences”\n\n\n\nBe Your Own Judge\n\nReview, revise and reiterate. “…put yourself completely in the shoes of a referee and scrutinize all the pieces—the significance of the work, the logic of the story, the correctness of the results and conclusions, the organization of the paper, and the presentation of the materials.”\n\n\n\nTest the Water in Your Own Backyard\n\n“…collect feedback and critiques from others, e.g., colleagues and collaborators.”\n\n\n\nBuild a Virtual Team of Collaborators\n\nTreat reviewers as collaborators and respond objectively to their criticisms and recommendations. This may entail redoing research and thoroughly re-writing a paper.\n\n\n\nEven though reporting our results in a clear and efficient way through scholarly publications is important, the ways scientist communicate is not how the rest of the rest of the world typically communicates. In a scientific paper, we establish credibility in the introduction and methods, provide detailed data and results, and then share the significance of our work in the discussion and conclusions. But the rest of the world leads with the impact, the take home message. An example of this are newspaper headlines.\n“But the rest of the world leads with the conclusions, because that’s what people want to know. What are your findings and why is this relevant to them? In other words, what’s your bottom line?”\n\n\n\n8.1.2 Other communications\nCommunicating your research outside of peer-reviewed journal articles is increasingly common, and important. These non academic communications can reach a more broad and diverse audience than traditional publications (and should be tailored for specific audience groups accordingly), are not subject to the same pay-walls as journal articles, and can augment and amplify scholarly publications. For example, this twitter announcement from Dr Heather Kropp, providing a synopsis of their synthesis publication in Environmental Research (note the reference to a repository where the data are located, though a DOI would be better).\n\nWhether this communication occurs through blogs, social media, or via interviews with others, developing practices to refine your messaging is critical for successful communication. One tool to support your communication practice is ‘The Message Box’ developed by COMPASS, an organization that helps scientists develop communications skills in order to share their knowledge and research across broad audiences without compromising the accuracy of their research.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: The Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#the-message-box",
    "href": "session_08.html#the-message-box",
    "title": "8  Science Communication: The Message Box",
    "section": "8.2 The Message Box",
    "text": "8.2 The Message Box\n\n\nThe Message Box is a tool that helps researchers take the information they hold about their research and communicate it in a way that resonates with the chosen audience. It can be used to help prepare for interviews with journalists or employers, plan for a presentation, outline a paper or lecture, prepare a grant proposal, or clearly, and with relevancy, communicate your work to others. While the message box can be used in all these ways, you must first identify the audience for your communication.\n“Whether a journalist, a policymaker, a room of colleagues at a professional meeting, or a class of second-graders—doesn’t have deep knowledge of your subject matter. But that doesn’t mean you should explain everything you know in a fire hose of information. In fact, cognitive research tells us that the human brain can only absorb three to five pieces of information at a time. So prioritize what you share based on what your audience needs to know. Your goal as you fill out your Message Box is to identify the information that is critical to your audience—what really matters to them—and share that. Effective science communication is less about expounding on all the exciting details that you might want to convey, and more about knowing your audience and providing them with what they need or want to know from you.”\nThe Message Box comprises five sections to help you sort and distill your knowledge in a way that will resonate with your (chosen) audience.\nThe five sections of the Message Box are provided below. For a detailed explanation of the sections and guidance on how to use the Message Box, work through the Message Box Workbook\n\n8.2.1 Message Box Sections\nThe Issue\nThe Issue section in the center of the box identifies and describes the overarching issue or topic that you’re addressing in broad terms. It’s the big-picture context of your work. This should be very concise and clear; no more than a short phrase. You might find you revisit the Issue after you’ve filled out your Message Box, to see if your thinking on the overarching topic has changed since you started.\n\nDescribes the overarching issue or topic: Big Picture\nBroad enough to cover key points\nSpecific enough to set up what’s to come\nConcise and clear\n‘Frames’ the rest of the message box\n\nThe Problem\nThe Problem is the chunk of the broader issue that you’re addressing in your area of expertise. It’s your piece of the pie, reflecting your work and expert knowledge. Think about your research questions and what aspect of the specific problem you’re addressing would matter to your audience. The Problem is also where you set up the So What and describe the situation you see and want to address.\n\nThe part of the broader issue that your work is addressing\nBuilds upon your work and expert knowledge\nTry to focus on one problem per audience\nOften the Problem is your research question\nThis section sets you up for So What\n\nThe So What\nThe crux of the Message Box, and the critical question the COMPASS team seeks to help scientists answer, is “So what?” Why should your audience care? What about your research or work is important for them to know? Why are you talking to them about it? The answer to this question may change from audience to audience, and you’ll want to be able to adjust based on their interests and needs. We like to use the analogy of putting a message through a prism that clarifies the importance to different audiences. Each audience will be interested in different facets of your work, and you want your message to reflect their interests and accommodate their needs. The prism below includes a spectrum of audiences you might want to reach, and some of the questions they might have about your work.\n\nThis is the crux of the message box\nWhy should you audience care?\nWhat about your research is important for them to know?\nWhy are you talking to them about it?\n\n\nThe Solution\nThe Solution section outlines the options for solving the problem you identified. When presenting possible solutions, consider whether they are something your audience can influence or act upon. And remind yourself of your communication goals: Why are you communicating with this audience? What do you want to accomplish?\n\nOutlines the options for solving the Problem\nCan your audience influence or act upon this?\nThere may be multiple solutions\nMake sure your Solution relates back to the Problem. Edit one or both as needed\n\nThe Benefit\nIn the Benefit section, you list the benefits of addressing the Problem — all the good things that could happen if your Solution section is implemented. This ties into the So What of why your audience cares, but focuses on the positive results of taking action (the So What may be a negative thing — for example, inaction could lead to consequences that your audience cares about). If possible, it can be helpful to be specific here — concrete examples are more compelling than abstract. Who is likely to benefit, and where, and when?\n\nWhat are the benefits of addressing the Problem?\nWhat good things come from implementing your Solution?\nMake sure it connects with your So What\nBenefits and So What may be similar\n\nFinally, to make you message more memorable you should:\n\nSupport your message with data\nLimit the use of numbers and statistics\nUse specific examples\nCompare numbers to concepts, help people relate\nAvoid jargon\nLead with what you know\n\n\nIn addition to the Message Box Workbook, COMPASS have resources on how to increase the impact of your message (include important statistics, draw comparisons, reduce jargon, use examples), exercises for practicing and refining your message and published examples.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: The Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#resources",
    "href": "session_08.html#resources",
    "title": "8  Science Communication: The Message Box",
    "section": "8.3 Resources",
    "text": "8.3 Resources\n\n\n\nDataONE Webinar: Communication Strategies to Increase Your Impact from DataONE on Vimeo.\n\n\nBudden, AE and Michener, W (2017) Communicating and Disseminating Research Findings. In: Ecological Informatics, 3rd Edition. Recknagel, F, Michener, W (eds.) Springer-Verlag\nCOMPASS Core Principles of Science Communication\nExample Message Boxes",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: The Message Box</span>"
    ]
  },
  {
    "objectID": "session_08.html#your-turn",
    "href": "session_08.html#your-turn",
    "title": "8  Science Communication: The Message Box",
    "section": "8.4 Your Turn",
    "text": "8.4 Your Turn\nLet’s take a look on how the Message Box looks in practice.\n\n\n\n\n\n\nExercise\n\n\n\n\nLook into real examples of scienctist using the message box here.\nThink about your synthesis project and a potential audience you would like to communicate the results. Define your audience and start filling in the different components of the Message Box.\n\nNote: This is just the first iteration to help your think about your work in a different way.\n\n\n\n\n\n\nBorowiec, Brittney G. 2023. “Ten Simple Rules for Scientists Engaging in Science Communication.” PLOS Computational Biology 19 (7): 1–8. https://doi.org/10.1371/journal.pcbi.1011251.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Science Communication: The Message Box</span>"
    ]
  }
]