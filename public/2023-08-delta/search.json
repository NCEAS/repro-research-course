[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "",
    "text": "Overview"
  },
  {
    "objectID": "index.html#about-this-training",
    "href": "index.html#about-this-training",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "About this training",
    "text": "About this training\nNCEAS Open Science Synthesis training consists of three 1-week long workshops, geared towards early career researchers. Participants engage in a mix of lectures, exercises, and synthesis research groups to undertake synthesis while learning and implementing best practices for open data science."
  },
  {
    "objectID": "index.html#why-nceas",
    "href": "index.html#why-nceas",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Why NCEAS",
    "text": "Why NCEAS\nThe National Center for Ecological Analysis and Synthesis (NCEAS), a research affiliate of UCSB, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science.\nNCEAS has over 25 years of success with this model among working groups and environmental professionals. Together with the Delta Science Program and the Delta Stewardship Council we are excited to pass along skills, workflows, mindsets learn throughout the years.\n\nWeek 2: Open Tools for Analysis and Visualization\nAug 28 - Sep 1, 2023\n\nStrengthen core knowledge of version control and workflow\nApproaches for data visualization including geospatial data\nData tools for qualitative data\nBuilding scientific websites with R and Shiny"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "index.html#next-training",
    "href": "index.html#next-training",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Next training",
    "text": "Next training\n\nWeek 3: Scaling up and presenting synthesis\nOctober 23 – 27, 2023\n\nHandling missing data\nBig data workflows and parallel computing\nGitHub Workflows\nSynthesis presentations and next steps"
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS with the support of DataONE, to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors for this version are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Halina Do-Linh, Carmen Galaz García, Matthew B. Jones, Camila Vargas Poulsen. 2023. Open Science Synthesis training Week 2. NCEAS Learning Hub & Delta Stewardship Council.\nAdditional contributors: Ben Bolker, Julien Brun, Amber E. Budden, Jeanette Clark, Samantha Csik, Stephanie Hampton, Natasha Haycock-Chavez, Samanta Katz, Julie Lowndes, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "session_01.html#learning-objectives",
    "href": "session_01.html#learning-objectives",
    "title": "1  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nApply the principles, features, and collaboration tools of Git and GitHub to effectively collaborate with colleagues on code\nAnalyze and evaluate common causes of conflicts that arise when collaborating on repositories\nDemonstrate the ability to resolve conflicts using Git conflict resolution techniques\nApply workflows and best practices that minimize conflicts on collaborative repositories"
  },
  {
    "objectID": "session_01.html#introduction-to-git-and-github-tools-for-collaboration",
    "href": "session_01.html#introduction-to-git-and-github-tools-for-collaboration",
    "title": "1  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "1.1 Introduction to Git and GitHub Tools for Collaboration",
    "text": "1.1 Introduction to Git and GitHub Tools for Collaboration\n\n\n\nArtwork by Allison Horst\n\n\nGit is not only a powerful tool for individual work but also an excellent choice for collaborating with friends and colleagues. Git ensures that after you’ve completed your contributions to a repository, you can confidently synchronize your changes with changes made by others.\nOne of the easiest and most effective ways to collaborate using Git is by utilizing a shared repository on a hosting service like GitHub. This shared repository acts as a central hub, enabling collaborators to effortlessly exchange and merge their changes. With Git and a shared repository, you can collaborate seamlessly and work confidently, knowing that your changes will be integrated smoothly with those of your collaborators.\n\n\n\nGraphic from Atlassian\n\n\nThere are many advanced techniques for synchronizing Git repositories, but let’s start with a simple example.\nIn this example, the Collaborator will clone a copy of the Owner’s repository from GitHub, and the Owner will grant them Collaborator status, enabling the Collaborator to directly pull and push from the Owner’s GitHub repository."
  },
  {
    "objectID": "session_01.html#collaborating-with-a-trusted-colleague-without-conflicts",
    "href": "session_01.html#collaborating-with-a-trusted-colleague-without-conflicts",
    "title": "1  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "1.2 Collaborating with a trusted colleague without conflicts",
    "text": "1.2 Collaborating with a trusted colleague without conflicts\nWe start our collaboration by giving a trusted colleague access to our repository on GitHub. In this example, we define the Owner as the individual who owns the repository, and the Collaborator as the person whom the Owner chooses to give permission to make changes to their repository.\nThe Collaborator will make changes to the repository and then push those changes to the shared repository on GitHub. The Owner will then use pull to retrieve the changes without encountering any conflicts. This is the most ideal workflow.\nThe instructors will demonstrate this process in the next section.\n\nStep 0: Owner adds Collaborator to shared repository\nThe Owner must change the settings of the repository and give the Collaborator access to the repository by inviting them as a collaborator to the repository. Once the Collaborator has accepted the invite, they can contribute to the repository.\n\n\n\n\n\n\n\nStep 1: Collaborator clone\nTo be able to contribute to a repository, the Collaborator must clone the repository from the Owner’s GitHub account. To do this, the Collaborator should visit the GitHub page for the Owner’s repository, and then copy the clone URL. In R Studio, the Collaborator will create a new project from version control by pasting this clone URL into the appropriate dialog (see the earlier chapter introducing GitHub).\n\n\n\nStep 2: Collaborator edit\nWith a clone copied locally, the Collaborator can now make changes to the README.md file in the repository, adding a line or statement somewhere noticeable near the top. Save your changes.\n\n\nStep 3: Collaborator commit and push\nTo sync changes, the Collaborator will need to add, commit, and push their changes to the Owner’s repository. But before doing so, it’s good practice to pull immediately before committing to ensure you have the most recent changes from the Owner. So, in RStudio’s Git tab, first click the “Diff” button to open the Git window, and then press the green “Pull” down arrow button. This will fetch any recent changes from the origin repository and merge them. Next, add the changed README.Rmd file to be committed by clicking the check box next to it, type in a commit message, and click “Commit”. Once that finishes, then the Collaborator can immediately click “Push” to send the commits to the Owner’s GitHub repository.\n\n\n\n\n\n\n\nStep 4: Owner pull\nNow, the Owner can open their local working copy of the code in RStudio, and pull those changes down to their local copy.\nCongrats, the Owner now has your changes!\n\n\nStep 5: Owner edits, commit, and push\nNext, the Owner should do the same. Make changes to a file in the repository, save it, pull to make sure no new changes have been made while editing, and then add, commit, and push the Owner changes to GitHub.\n\n\nStep 6: Collaborator pull\nThe Collaborator can now pull down those Owner changes, and all copies are once again fully synced. And you’re off to collaborating."
  },
  {
    "objectID": "session_01.html#ex1-no-conflict",
    "href": "session_01.html#ex1-no-conflict",
    "title": "1  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "1.3 Exercise 1: With a partner collaborate in a repository without a merge conflict",
    "text": "1.3 Exercise 1: With a partner collaborate in a repository without a merge conflict\n\n\n\n\n\n\nSetup\n\n\n\n\nGet into pairs, then choose one person as the Owner and one as the Collaborator\nBoth logon to GitHub\n\nThese next steps are for the Owner:\n\nNavigate to the {FIRSTNAME}_test repository\nGo to “Settings” and navigate to “Collaborators” in the “Access” section on the left-hand side\nUnder “Manage Access” click the button “Add people” and type the username of your Collaborator in the search box\nOnce you’ve found the correct username, click “Add {Collaborator username} to this repository\n\n\n\n\n\n\nNow, the Collaborator will follow this step:\n\nCheck your email for an invitation to GitHub or check your notifications (likely under “Your Organizations”) on GitHub to accept the invite to collaborate.\n\n\n\n\n\n\n\n\n\nLast thing, some Git configuration\n\n\n\nWhen Git released version 2.27, a new feature they incorporated allows users to specify how to pull, essentially, otherwise a warning will appear. To suppress this warning we need to configure our Git with this line of code:\ngit config pull.rebase false\npull.rebase false is a default strategy for pulling where it will try to auto-merge the files if possible, and if it can’t it will show a merge conflict\n\n\n\n\n\n\n\n\nInstructions\n\n\n\nYou will do the exercise twice, where each person will get to practice being both the Owner and the Collaborator roles.\n\nStep 0: Designate one person as the Owner and one as the Collaborator.\n\nRound One:\n\nStep 1: Owner adds Collaborator to {FIRSTNAME}_test repository (see Setup block above for detailed steps)\nStep 2: Collaborator clones the Owner’s {FIRSTNAME}_test repository\nStep 3: Collaborator edits the README file:\n\nCollaborator adds a new level 2 heading to README titled “Git Workflow”\n\nStep 4: Collaborator commits and pushes the README file with the new changes to GitHub\nStep 5: Owner pulls the changes that the Collaborator made\nStep 6: Owner edits the README file:\n\nUnder “Git Workflow”, Owner adds the steps of the Git workflow we’ve been practicing\n\nStep 7: Owner commits and pushes the README file with the new changes to GitHub\nStep 8: Collaborator pulls the Owners changes from GitHub\nStep 9: Go back to Step 0, switch roles, and then follow the steps in Round Two.\n\nRound Two:\n\nStep 1: Owner adds Collaborator to {FIRSTNAME}_test repository\nStep 2: Collaborator clones the Owner’s {FIRSTNAME}_test repository\nStep 3: Collaborator edits the README file:\n\nCollaborator adds a new level 2 heading to README titled “How to Create a Git Repository from an existing project” and adds the high level steps for this workflow\n\nStep 4: Collaborator commits and pushes the README file with the new changes to GitHub\nStep 5: Owner pulls the changes that the Collaborator made\nStep 6: Owner edits the README file:\n\nUnder “How to Create a Git Repository”, Owner adds the high level steps for this workflow\n\nStep 7: Owner commits and pushes the README file with the new changes to GitHub\nStep 8: Collaborator pulls the Owners changes from GitHub\n\nHint: If you don’t remember how to create a Git repository, refer to the chapter Intro to Git and GitHub where we created two Git repositories"
  },
  {
    "objectID": "session_01.html#a-note-on-advanced-collaboration-techniques",
    "href": "session_01.html#a-note-on-advanced-collaboration-techniques",
    "title": "1  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "1.4 A Note on Advanced Collaboration Techniques",
    "text": "1.4 A Note on Advanced Collaboration Techniques\nThere are many Git and GitHub collaboration techniques, some more advanced than others. We won’t be covering advanced strategies in this course. But here is a table for your reference on a few popular Git collaboration workflow strategies and tools.\n\n\n\n\n\n\n\n\n\nCollaboration Technique\nBenefits\nWhen to Use\nWhen Not to Use\n\n\n\n\nBranch Management Strategies\n1. Enables parallel development and experimentation2. Facilitates isolation of features or bug fixes3. Provides flexibility and control over project workflows\nWhen working on larger projects with multiple features or bug fixes simultaneously.When you want to maintain a stable main branch while developing new features or resolving issues on separate branches.When collaborating with teammates on different aspects of a project and later integrating their changes.\nWhen working on small projects with a single developer or limited codebase.When the project scope is simple and doesn’t require extensive branch management.When there is no need to isolate features or bug fixes.\n\n\nCode Review Practices\n1. Enhances code quality and correctness through feedback2. Promotes knowledge sharing and learning within the team3. Helps identify bugs, improve performance, and ensure adherence to coding standards\nWhen collaborating on a codebase with team members to ensure code quality and maintain best practices.When you want to receive feedback and suggestions on your code to improve its readability, efficiency, or functionality.When working on critical or complex code that requires an extra layer of scrutiny before merging it into the main branch.\nWhen working on personal projects or small codebases with no collaboration involved.When time constraints or project size make it impractical to conduct code reviews.When the codebase is less critical or has low complexity.\n\n\nForking\n1. Enables independent experimentation and development2. Provides a way to contribute to a project without direct access3. Allows for creating separate, standalone copies of a repository\nWhen you want to contribute to a project without having direct write access to the original repository.When you want to work on an independent variation or extension of an existing project.When experimenting with changes or modifications to a project while keeping the original repository intact.\nWhen collaborating on a project with direct write access to the original repository.When the project does not allow external contributions or forking.When the project size or complexity doesn’t justify the need for independent variations.\n\n\nPull Requests\n1. Facilitates code review and discussion2. Allows for collaboration and feedback from team members3. Enables better organization and tracking of proposed changes\nWhen working on a shared repository with a team and wanting to contribute changes in a controlled and collaborative manner.When you want to propose changes to a project managed by others and seek review and approval before merging them into the main codebase.\nWhen working on personal projects or individual coding tasks without the need for collaboration.When immediate changes or fixes are required without review processes.When working on projects with a small team or single developer with direct write access to the repository.\n\n\n\nThe “When Not to Use” column provides insights into situations where it may be less appropriate to use each collaboration technique, helping you make informed decisions based on the specific context and requirements of your project.\nThese techniques provide different benefits and are used in various collaboration scenarios, depending on the project’s needs and team dynamics."
  },
  {
    "objectID": "session_01.html#merge-conflicts",
    "href": "session_01.html#merge-conflicts",
    "title": "1  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "1.5 Merge conflicts",
    "text": "1.5 Merge conflicts\nMerge conflicts occur when both collaborators make conflicting changes to the same file. Resolving merge conflicts involves identifying the root of the problem and restoring the project to a normal state. Good communication, discussing file sections to work on, and avoiding overlaps can help prevent merge conflicts. However, if conflicts do arise, Git warns about potential issues and ensures that changes from different collaborators based on the same file version are not overwritten. To resolve conflicts, you need to explicitly specify whose changes should be used for each conflicting line in the file.\nIn this image, we see collaborators mbjones and metamattj have both made changes to the same line in the same README.md file. This is causing a merge conflict because Git doesn’t know whose changes came first. To resolve it, we need to tell Git whose changes to keep for that line, and whose changes to discard.\n\n\n1.5.1 Common ways to resolve a merge conflict\n1. Abort, abort, abort…\nSometimes you just made a mistake. When you get a merge conflict, the repository is placed in a “Merging” state until you resolve it. There’s a Terminal command to abort doing the merge altogether:\ngit merge --abort\nOf course, after doing that you still haven’t synced with your Collaborator’s changes, so things are still unresolved. But at least your repository is now usable on your local machine.\n2. Checkout\nThe simplest way to resolve a conflict, given that you know whose version of the file you want to keep, is to use the command line Git program to tell Git to use either your changes (the person doing the merge), or their changes (the Collaborator).\n\nkeep your Collaborator’s file: git checkout --theirs conflicted_file.Rmd\nkeep your own file: git checkout --ours conflicted_file.Rmd\n\nOnce you have run that command, then run add (staging), commit, pull, and push the changes as normal.\n3. Pull and edit the file\nBut that requires the command line. If you want to resolve from RStudio, or if you want to pick and choose some of your changes and some of your Collaborator’s, then instead you can manually edit and fix the file. When you pull the file with a conflict, Git notices that there is a conflict and modifies the file to show both your own changes and your Collaborator’s changes in the file. It also shows the file in the Git tab with an orange U icon, which indicates that the file is Unmerged, and therefore awaiting your help to resolve the conflict. It delimits these blocks with a series of less than and greater than signs, so they are easy to find:\n\n\n\n\n\nTo resolve the conflicts, simply find all of these blocks, and edit them so that the file looks how you want (either pick your lines, your Collaborator’s lines, some combination, or something altogether new), and save. Be sure you removed the delimiter lines that started with\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;,\n=======,\nand &gt;&gt;&gt;&gt;&gt;&gt;&gt;.\n\nOnce you have made those changes, you simply add (staging), commit, and push the files to resolve the conflict."
  },
  {
    "objectID": "session_01.html#producing-and-resolving-merge-conflicts",
    "href": "session_01.html#producing-and-resolving-merge-conflicts",
    "title": "1  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "1.6 Producing and resolving merge conflicts",
    "text": "1.6 Producing and resolving merge conflicts\nTo illustrate this process, the instructors are going to carefully create a merge conflict step by step, show how to resolve it, and show how to see the results of the successful merge after it is complete. First, the instructors will walk through the exercise to demonstrate the issues. Then, participants will pair up and try the exercise.\n\nStep 1: Owner and Collaborator ensure all changes are updated\nFirst, start the exercise by ensuring that both the Owner and Collaborator have all of the changes synced to their local copies of the Owner’s repository in RStudio. This includes doing a git pull to ensure that you have all changes local, and make sure that the Git tab in RStudio doesn’t show any changes needing to be committed.\n\n\nStep 2: Owner makes a change and commits\nFrom that clean slate, the Owner first modifies and commits a small change including their name on a specific line of the README.md file (we will change the first line, the title). Work to only change that one line, and add your username to the line in some form and commit the changes (but DO NOT push). We are now in a situation where the Owner has unpushed changes that the Collaborator can not yet see.\n\n\nStep 3: Collaborator makes a change and commits on the same line\nNow the Collaborator also makes changes to the same line (the first line, the title) on the README.md file in their RStudio copy of the project, adding their name to the line. They then commit. At this point, both the Owner and Collaborator have committed changes based on their shared version of the README.md file, but neither has tried to share their changes via GitHub.\n\n\nStep 4: Collaborator pushes the file to GitHub\nSharing starts when the Collaborator pushes their changes to the GitHub repo, which updates GitHub to their version of the file. The Owner is now one revision behind, but doesn’t know it yet.\n\n\nStep 5: Owner pushes their changes and gets an error\nAt this point, the Owner tries to push their change to the repository, which triggers an error from GitHub. While the error message is long, it basically tells you everything needed (that the Owner’s repository doesn’t reflect the changes on GitHub, and that they need to pull before they can push).\n\n\n\nStep 6: Owner pulls from GitHub to get Collaborator changes\nDoing what the message says, the Owner pulls the changes from GitHub, and gets another, different error message. In this case, it indicates that there is a merge conflict because of the conflicting lines.\n\nIn the Git pane of RStudio, the file is also flagged with an orange U, which stands for an unresolved merge conflict.\n\n\n\nStep 7: Owner edits the file to resolve the conflict\nTo resolve the conflict, the Owner now needs to edit the file. Again, as indicated above, Git has flagged the locations in the file where a conflict occurred with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. The Owner should edit the file, merging whatever changes are appropriate until the conflicting lines read how they should, and eliminate all of the marker lines with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;.\n\nOf course, for scripts and programs, resolving the changes means more than just merging the text – whoever is doing the merging should make sure that the code runs properly and none of the logic of the program has been broken.\n\n\n\nStep 8: Owner commits the resolved changes\nFrom this point forward, things proceed as normal. The Owner first add the file changes to be made, which changes the orange U to a blue M for modified, and then commits the changes locally. The Owner now has a resolved version of the file on their system.\n\n\n\nStep 9: Owner pushes the resolved changes to GitHub\nHave the Owner push the changes, and it should replicate the changes to GitHub without error.\n\n\n\nStep 10: Collaborator pulls the resolved changes from GitHub\nFinally, the Collaborator can pull from GitHub to get the changes the Owner made.\n\n\nStep 11: Both can view commit history\nWhen either the Collaborator or the Owner view the history, the conflict, associated branch, and the merged changes are clearly visible in the history."
  },
  {
    "objectID": "session_01.html#exercise-2-with-a-partner-collaborate-in-a-repository-and-resolve-a-merge-conflict",
    "href": "session_01.html#exercise-2-with-a-partner-collaborate-in-a-repository-and-resolve-a-merge-conflict",
    "title": "1  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "1.7 Exercise 2: With a partner collaborate in a repository and resolve a merge conflict",
    "text": "1.7 Exercise 2: With a partner collaborate in a repository and resolve a merge conflict\nNote you will only need to complete the Setup and Git configuration steps again if you are working in a new repository. Return to Exercise 1 for Setup and Git configuration steps.\n\n\n\n\n\n\nInstructions\n\n\n\nNow it’s your turn. In pairs, intentionally create a merge conflict, and then go through the steps needed to resolve the issues and continue developing with the merged files. See the sections above for help with each of the steps below. You will do the exercise twice, where each person will get to practice being both the Owner and the Collaborator roles.\n\nStep 0: Designate one person as the Owner and one as the Collaborator.\n\nRound One:\n\nStep 1: Both Owner and Collaborator pull to ensure both have the most up-to-date changes\nStep 2: Owner edits the README file and makes a change to the title and commits do not push\nStep 3: On the same line, Collaborator edits the README file and makes a change to the title and commits\nStep 4: Collaborator pushes the file to GitHub\nStep 5: Owner pushes their changes and gets an error\nStep 6: Owner pulls from GitHub to get Collaborator changes\nStep 7: Owner edits the README file to resolve the conflict\nStep 8: Owner commits the resolved changes\nStep 9: Owner pushes the resolved changes to GitHub\nStep 10: Collaborator pulls the resolved changes from GitHub\nStep 11: Both view commit history\nStep 12: Go back to Step 0, switch roles, and then follow the steps in Round Two.\n\nRound Two:\n\nStep 1: Both Owner and Collaborator pull to ensure both have the most up-to-date changes\nStep 2: Owner edits the README file and makes a change to line 2 and commits do not push\nStep 3: On the same line, Collaborator edits the README file and makes a change to line 2 and commits\nStep 4: Collaborator pushes the file to GitHub\nStep 5: Owner pushes their changes and gets an error\nStep 6: Owner pulls from GitHub to get Collaborator changes\nStep 7: Owner edits the README file to resolve the conflict\nStep 8: Owner commits the resolved changes\nStep 9: Owner pushes the resolved changes to GitHub\nStep 10: Collaborator pulls the resolved changes from GitHub\nStep 11: Both view commit history"
  },
  {
    "objectID": "session_01.html#best-practices-to-avoid-merge-conflicts",
    "href": "session_01.html#best-practices-to-avoid-merge-conflicts",
    "title": "1  Collaborating using Git and GitHub & Merge Conflicts",
    "section": "1.8 Best practices to avoid merge conflicts",
    "text": "1.8 Best practices to avoid merge conflicts\nSome basic rules of thumb can avoid the vast majority of merge conflicts, saving a lot of time and frustration. These are words our teams live by:\n\n\n\n\n\nXKCD 1597\n\n\n\nCommunicate often and set up effective communication channels\nTell each other what you are working on\nStart your working session with a pull\nPull immediately before you commit or push\nCommit often in small chunks (this helps you organize your work!)\nMake sure you and who you are collaborating with all fully understand the Git workflow you’re using aka make sure you’re on the same page before you start!\n\nA good workflow is encapsulated as follows:\nPull -&gt; Edit -&gt; Save -&gt; Add (stage) -&gt; Commit -&gt; Pull -&gt; Push\nAlways start your working sessions with a pull to get any outstanding changes, then start your work. Stage your changes, but before you commit, pull again to see if any new changes have arrived. If so, they should merge in easily if you are working in different parts of the program. You can then commit and immediately push your changes safely.\nGood luck, and try to not get frustrated. Once you figure out how to handle merge conflicts, they can be avoided or dispatched when they occur, but it does take a bit of practice."
  },
  {
    "objectID": "session_02.html#learning-objectives",
    "href": "session_02.html#learning-objectives",
    "title": "2  Publishing your analysis to the web with GitHub Pages",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nHow to use Git, GitHub (+Pages), and Quarto to publish an analysis to the web"
  },
  {
    "objectID": "session_02.html#introduction",
    "href": "session_02.html#introduction",
    "title": "2  Publishing your analysis to the web with GitHub Pages",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nSharing your work with others in engaging ways is an important part of the scientific process.\nSo far in this course, we’ve introduced a small set of powerful tools for doing open science:\n\nR and its many packages\nRStudio\nGit\nGitHub\nQuarto\n\nQuarto, in particular, is amazingly powerful for creating scientific reports but, so far, we haven’t tapped its full potential for sharing our work with others.\nIn this lesson, we’re going to take our training_{USERNAME} GitHub repository and turn it into a beautiful and easy to read web page using the tools listed above.\n\n\n\n\n\n\nSet up\n\n\n\n\nMake sure you are in training_{USERNAME} project\nAdd a new Quarto file at the top level called index.qmd\n\nGo to the RStudio menu File &gt; New File &gt; Quarto Document…\nThis will bring up a dialog box. Add the title “GitHub Pages Example”, keep the Default Output Format as “HTML”, and then click “OK”\n\nSave the Quarto file you just created. Use index.qmd as the file name\n\nBe sure to use the exact case (lower case ‘index’) as different operating systems handle case differently and it can interfere with loading your web page later\n\nPress “Render” and observe the rendered output\n\nNotice the new file in the same directory index.html\nThis is our Quarto file rendered as HTML (a web page)\n\nCommit your changes (for both index.qmd and index.html) with a commit message, pull and then push to GitHub\nOpen your web browser to the GitHub.com and navigate to the page for your training_{USERNAME} repository\nActivate GitHub Pages for the main branch\n\nGo to Settings &gt; Pages (underneath the Code and Automation section)\nKeep the “Source” as “Deploy from a branch”\nUnder “Branch” you’ll see a message that says “GitHub Pages is currently disabled”. To change this, change the branch from “None” to main. Keep the folder as the root and then click “Save”\nYou should see the message change to “Your GitHub Pages site is currently being built from the main branch”\n\n\nNote: index.qmd represents the default file for a web site, and is returned whenever you visit the web site but doesn’t specify an explicit file to be returned.\n\n\nNow, the rendered website version of your repo will show up at a special URL.\nGitHub Pages follows a convention like this:\n\nNote that it changes from github.com to github.io\n\nGo to https://{username}.github.io/{repo_name}/ (Note the trailing /)\nObserve the awesome rendered output\n\nNow that we’ve successfully published a web page from an Quarto document, let’s make a change to our document and follow the steps to publish the change on the web:\n\n\n\n\n\n\nUpdate content in your published page\n\n\n\n\nGo back to your index.qmd\nDelete all the content, except the YAML frontmatter\nType “Hello world”\nSave and render your file\nUse Git workflow: Stage &gt; Commit &gt; Pull &gt; Push\nGo back to https://{username}.github.io/{repo_name}/\n\n\n\nNext, we will show how you can link different qmds rendered into html so you can easily share different parts of your work.\n\n\n\n\n\n\nExercise\n\n\n\nIn this exercise, you’ll create a table of contents with the lessons of this course on the main page, and link some of the files we have work on so far.\n\nGo back to the RStudio server and to your index.Rmd file\nCreate a table of contents with the names of the main technical lessons of this course, like so:\n\n## coreR workshop\n\n- Introduction to Quarto \n- Cleaning and Wrangling data\n- Data Visualization\n- Spatial Analysis\n\nMake sure you have the html versions of your intro-to-rmd.Rmd and data-cleaning.Rmd files. If you only see the Rmd version, you need to “Knit” your files first\nIn your index.Rmd let’s add the links to the html files we want to show on our webpage. Do you remember the Markdown syntax to create a link?\n\n\n\nMarkdown syntax to create a link:\n\n\n[Text you want to hyperlink](link)\n\nExample: [Data wrangling and cleaning](data-wrangling-cleaning.html)\n\n\n\n\nUse Git workflow: Stage &gt; Commit &gt; Pull &gt; Push\n\nNow when you visit your web site, you’ll see the table of contents, and can navigate to the others file you linked.\n\n\nQuarto web pages are a great way to share work in progress with your colleagues. Here we showed an example with the materials we have created in this course. However, you can use these same steps to share the different files and progress of a project you’ve been working on. To do so simply requires thinking through your presentation so that it highlights the workflow to be reviewed. You can include multiple pages and build a simple web site and make your work accessible to people who aren’t set up to open your project in R. Your site could look something like this:"
  },
  {
    "objectID": "session_03.html#learning-objectives",
    "href": "session_03.html#learning-objectives",
    "title": "3  Data Visualization",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the fundamentals of how the ggplot2 package works\nUse ggplot2’s theme and other customization functions create publication-grade graphics\nIntroduce the leaflet and DT package to create interactive maps and tables respectively"
  },
  {
    "objectID": "session_03.html#overview",
    "href": "session_03.html#overview",
    "title": "3  Data Visualization",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nggplot2 is a popular package for visualizing data in R. From the home page:\n\nggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.\n\nIt’s been around for years and has pretty good documentation and tons of example code around the web (like on StackOverflow). The goal of this lesson is to explain the fundamentals of how ggplot2 work, introduce useful functions for customizing your plots and inspire you to go and explore this awesome resource for visualizing your data.\n\n\n\n\n\n\nggplot2 vs base graphics in R vs others\n\n\n\nThere are many different ways to plot your data in R. All of them work! However, ggplot2 excels at making complicated plots easy and easy plots simple enough\nBase R graphics (plot(), hist(), etc) can be helpful for simple, quick and dirty plots. ggplot2 can be used for almost everything else.\n\n\nLet’s dive into creating and customizing plots with ggplot2.\n\n\n\n\n\n\nSetup\n\n\n\n\nMake sure you’re in the right project (training_{USERNAME}) and use the Git workflow by Pulling to check for any changes. Then, create a new Quarto document, delete the default text, and save this document.\nLoad the packages we’ll need:\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(forcats) # makes working with factors easier\nlibrary(ggplot2)\nlibrary(leaflet) # interactive maps\nlibrary(DT) # interactive tables\nlibrary(scales) # scale functions for visualization\nlibrary(janitor) # expedite cleaning and exploring data\nlibrary(viridis) # colorblind friendly color pallet\n\n\nLoad the data directly from the EDI Data Repository: Sacramento-San Joaquin Delta Socioecological Monitoring. Navegate to the link above, scroll down and under Resources, hover over the “Download” button for the “Socioecological monitoring data”, right click, and select “Copy Link Address”. Note: janitor::clean_names() is an awesome function to transform all column names into the same format.\n\n\ndelta_visits &lt;- read_csv(\"https://portal.edirepository.org/nis/dataviewer?packageid=edi.587.1&entityid=cda8c1384af0089b506d51ad8507641f\") %&gt;% \n    janitor::clean_names() ## Introducing this new package!\n\n\nLearn about the data. For this session we are going to be working with data on Socioecological Monitoring on the Sacramento-San Joaquin Delta. Check out the documentation.\nFinally, let’s explore the data we just read into our working environment.\n\n\n## Check out column names\ncolnames(delta_visits)\n\n## Peak at each column and class\nglimpse(delta_visits)\n\n## From when to when\nrange(delta_visits$date)\n\n## First and last rows\nhead(delta_visits)\ntail(delta_visits)\n\n## Which time of day?\nunique(delta_visits$time_of_day)"
  },
  {
    "objectID": "session_03.html#getting-the-data-ready",
    "href": "session_03.html#getting-the-data-ready",
    "title": "3  Data Visualization",
    "section": "3.2 Getting the data ready",
    "text": "3.2 Getting the data ready\nIt is more frequent than not, that we need to do some wrangling before we can plot our data the way we want to. After reading and exploring our data, we’ll put our data wrangling skills to practice to get our data in the desired format.\nWith the tidy data principles in mind. Is this data tidy?\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nggplot2 for the most part likes data input to be in a long format (aka “tidy”). So let go ahead and make this data frame long instead of wide. Do you remember the name of the function we can use?\nLet’s refresh our memory on how this function works by accessing the help page. Type ?pivot_long() in the console to see the documentation for this function.\n\nvisits_long &lt;- delta_visits %&gt;% \n    pivot_longer(cols = c(\"sm_boat\", \"med_boat\", \"lrg_boat\", \"bank_angler\", \"scientist\", \"cars\"),\n                 names_to = \"visitor_type\",\n                 values_to = \"quantity\") %&gt;%\n    rename(restore_loc = eco_restore_approximate_location) %&gt;% \n    select(-notes)\n\n## Checking the outcome\nhead(visits_long)\n\n# A tibble: 6 × 8\n  restore_loc   reach     latitude longitude date       time_of_day visitor_type\n  &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;       &lt;chr&gt;       \n1 Decker Island Brannan …     38.1     -122. 2017-07-07 unknown     sm_boat     \n2 Decker Island Brannan …     38.1     -122. 2017-07-07 unknown     med_boat    \n3 Decker Island Brannan …     38.1     -122. 2017-07-07 unknown     lrg_boat    \n4 Decker Island Brannan …     38.1     -122. 2017-07-07 unknown     bank_angler \n5 Decker Island Brannan …     38.1     -122. 2017-07-07 unknown     scientist   \n6 Decker Island Brannan …     38.1     -122. 2017-07-07 unknown     cars        \n# ℹ 1 more variable: quantity &lt;dbl&gt;\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nCalculate the daily visits by visit_type and `restore_loc``,\n\n\n\n\ndaily_visits_loc &lt;- visits_long %&gt;%\n    group_by(restore_loc, date, visitor_type) %&gt;% \n    summarise(daily_visits = sum(quantity))\n    \nhead(daily_visits_loc)\n\n# A tibble: 6 × 4\n# Groups:   restore_loc, date [1]\n  restore_loc   date       visitor_type daily_visits\n  &lt;chr&gt;         &lt;date&gt;     &lt;chr&gt;               &lt;dbl&gt;\n1 Decker Island 2017-07-07 bank_angler             4\n2 Decker Island 2017-07-07 cars                    0\n3 Decker Island 2017-07-07 lrg_boat                0\n4 Decker Island 2017-07-07 med_boat                6\n5 Decker Island 2017-07-07 scientist               0\n6 Decker Island 2017-07-07 sm_boat                 0\n\n\nThe chunk above uses some of the dplyr functions that we’ve used in the past. We use group_by() to indicate that we want to calculate our results for the unique combinations of type of visit, restoration location proximity, and day. We next use summarise() to calculate an daily visit value for each of these groups. Note we use the %in% operator to pipe in the result of one command as an argument to the next one."
  },
  {
    "objectID": "session_03.html#plotting-with-ggplot2",
    "href": "session_03.html#plotting-with-ggplot2",
    "title": "3  Data Visualization",
    "section": "3.3 Plotting with ggplot2",
    "text": "3.3 Plotting with ggplot2\n\n3.3.1 Essentials components\nFirst, we’ll cover some ggplot2 basics to create the foundation of our plot. Then, we’ll add on to make our great customized data visualization.\n\n\n\n\n\n\nThe basics\n\n\n\n\nIndicate we are using ggplot() (call the ggplot2::ggplot() function)\nWhat data do we want to plot? (data = my_data)\nWhat is my mapping aesthetics? What variables do we want to plot? (define usingaes() function)\nDefine the geometry of our plot. This specifies the type of plot we’re making (use geom_*() to indicate the type of plot e.g: point, bar, etc.)\n\nNote To add layers to our plot, for example, additional geometries/aesthetics and theme elements or any ggplot object we use +.\n\n\nNow, let’s plot total daily visits by restoration location. We will show this by creating the same plot in 3 slightly different ways. Each of the options below have the 4 essential pieces of a ggplot.\n\n## Option 1 - data and mapping called in the ggplot() function\nggplot(data = daily_visits_loc,\n       aes(x = restore_loc, y = daily_visits))+\n    geom_col()\n\n\n## Option 2 - data called in ggplot function; mapping called in geom\nggplot(data = daily_visits_loc) +\n    geom_col(aes(x = restore_loc, y = daily_visits))\n\n\n## Option 3 - data and mapping called in geom\nggplot() +\n    geom_col(data = daily_visits_loc,\n             aes(x = restore_loc, y = daily_visits))\n\nThey all will create the same plot:\n(Apologies for the crumble text on the x-axis, we will learn how to make this look better soon)\n\n\n\n\n\n\n\n3.3.2 Looking at different geoms\nHaving the basic structure with the essential components in mind, we can easily change the type of graph by updating the geom_*().\n\n\n\n\n\n\nggplot2 and the pipe operator\n\n\n\nJust like in dplyr and tidyr, we can also pipe a data.frame directly into the first argument of the ggplot function using the %&gt;% operator.\nThis can certainly be convenient, but use it carefully! Combining too many data-tidying or subsetting operations with your ggplot call can make your code more difficult to debug and understand.\n\n\nWe will use the pipe operator to pass into ggplot() a filtered version of daily_visit_loc, and make a plot with different geometries.\nBoxplot Note: These examples are to demonstrate case uses of wrangling function prior to plotting. They are not necessarily plotting best practices.\n\ndaily_visits_loc %&gt;%\n    separate(date, c(\"year\", \"month\", \"day\"), sep = \"-\") %&gt;%\n    filter(daily_visits &lt; 30,\n           visitor_type %in% c(\"sm_boat\", \"med_boat\", \"lrg_boat\")) %&gt;%\n    ggplot(aes(x = visitor_type, y = daily_visits)) +\n    geom_boxplot()\n\n\n\n\nViolin plot\n\ndaily_visits_loc %&gt;%\n    separate(date, c(\"year\", \"month\", \"day\"), sep = \"-\") %&gt;%\n    filter(daily_visits &lt; 30,\n           visitor_type %in% c(\"sm_boat\", \"med_boat\", \"lrg_boat\")) %&gt;%\n    ggplot(aes(x = visitor_type, y = daily_visits)) +\n    geom_violin()\n\n\n\n\nLine and point\n\ndaily_visits_loc %&gt;%\n    filter(restore_loc == \"Decker Island\",\n           visitor_type == \"med_boat\") %&gt;%\n    ggplot(aes(x = date, y = daily_visits)) +\n    geom_line() +\n    geom_point()\n\n\n\n3.3.3 Customizing our plot\nLet’s go back to our base bar graph. What if we want our bars to be blue instead of gray? You might think we could run this:\n\nggplot(data = daily_visits_loc,\n       aes(x = restore_loc, y = daily_visits,\n           fill = \"blue\"))+\n    geom_col()\n\n\n\n\nWhy did that happen?\nNotice that we tried to set the fill color of the plot inside the mapping aesthetic call. What we have done, behind the scenes, is create a column filled with the word “blue” in our data frame, and then mapped it to the fill aesthetic, which then chose the default fill color of red.\nWhat we really wanted to do was just change the color of the bars. If we want do do that, we can call the color option in the geom_col() function, outside of the mapping aesthetics function call.\n\nggplot(data = daily_visits_loc,\n       aes(x = restore_loc, y = daily_visits))+\n    geom_col(fill = \"blue\")\n\n\n\n\nWhat if we did want to map the color of the bars to a variable, such as visitor_type. ggplot() is really powerful because we can easily get this plot to visualize more aspects of our data.\n\nggplot(data = daily_visits_loc,\n       aes(x = restore_loc, y = daily_visits,\n           fill = visitor_type))+\n    geom_col()\n\n\n\n\n\n\n\n\n\n\nKeep in mind\n\n\n\n\nIf you want to map a variable onto a graph aesthetic (e.g., point color should be based on a specific region), put it within aes().\nIf you want to update your plot base on a constant (e.g. “Make ALL the points BLUE”), you can add the information directly to the relevant geom_ layer.\n\n\n\n\n3.3.3.1 Setting ggplot themes\nWe have successfully plotted our data. But, this is clearly not a nice plot. Let’s work on making this plot look a bit nicer. We are going to:\n\nAdd a title, subtitle and adjust labels using labs()\nFlip the x and y axis to better read the graph\nInclude a built in theme using theme_bw()\n\n\nggplot(data = daily_visits_loc,\n       aes(x = restore_loc, y = daily_visits,\n           fill = visitor_type))+\n    geom_col()+\n    labs(x = \"Restoration Location\",\n         y = \"Number of Visits\",\n         fill = \"Type of Visitor\",\n         title = \"Total Number of Visits to Delta Restoration Areas by visitor type\",\n         subtitle = \"Sum of all visits during July 2017 and March 2018\")+\n    coord_flip()+\n    theme_bw()\n\n\n\n\nYou can see that the theme_bw() function changed a lot of the aspects of our plot! The background is white, the grid is a different color, etc. There are lots of other built in themes like this that come with the ggplot2 package that help quickly set the look of the plot. Use the RStudio auto-complete theme_ &lt;TAB&gt; to view a list of theme functions.\n\n## Useful baseline themes are\ntheme_minimal()\ntheme_light()\ntheme_classic()\n\nThe built in theme functions (theme_*()) change the default settings for many elements that can also be changed individually using thetheme() function. The theme() function is a way to further fine-tune the look of your plot. This function takes MANY arguments (just have a look at ?theme). Luckily there are many great ggplot resources online so we don’t have to remember all of these, just Google “ggplot cheat sheet” and find one you like.\nLet’s look at an example of a theme() call, where we change the position of the legend from the right side to the bottom, and remove the ticks of our Locations axis.\n\nggplot(data = daily_visits_loc,\n       aes(x = restore_loc, y = daily_visits,\n           fill = visitor_type))+\n    geom_col()+\n    labs(x = \"Restoration Location\",\n         y = \"Number of Visits\",\n         fill = \"Type of Visitor\",\n         title = \"Total Number of Visits to Delta Restoration Areas by visitor type\",\n         subtitle = \"Sum of all visits during study period\")+\n    coord_flip()+\n    theme_bw()+\n    theme(legend.position = \"bottom\",\n          axis.ticks.y = element_blank()) ## note we mention y-axis here\n\n\n\n\nNote that the theme() call needs to come after any built-in themes like theme_bw() are used. Otherwise, theme_bw() will likely override any theme elements that you changed using theme().\nYou can also save the result of a series of theme() function calls to an object to use on multiple plots. This prevents needing to copy paste the same lines over and over again!\n\nmy_theme &lt;- theme_bw(base_size = 16) +\n    theme(legend.position = \"bottom\",\n          axis.ticks.y = element_blank())\n\nSo now our code will look like this:\n\nggplot(data = daily_visits_loc,\n       aes(x = restore_loc, y = daily_visits,\n           fill = visitor_type))+\n    geom_col()+\n    labs(x = \"Restoration Location\",\n         y = \"Number of Visits\",\n         fill = \"Type of Visitor\",\n         title = \"Total Number of Visits to Delta Restoration Areas by visitor type\",\n         subtitle = \"Sum of all visits during study period\")+\n    coord_flip()+\n    my_theme\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat changes do you expect to see in your plot by adding the following line of code? Discuss with your neighbor and then try it out!\nscale_y_continuous(breaks = seq(0,120, 20))\n\n\n\n\nAnswer\nggplot(data = daily_visits_loc,\n       aes(x = restore_loc, y = daily_visits,\n           fill = visitor_type))+\n    geom_col()+\n    coord_flip()+\n    scale_y_continuous(breaks = seq(0,120, 20))+\n    labs(x = \"Restoration Location\",\n         y = \"Number of Visits\",\n         fill = \"Type of Visitor\",\n         title = \"Total Number of Visits to Delta Restoration Areas by visitor type\",\n         subtitle = \"Sum of all visits during study period\")+\n    my_theme\n\n\n\n\n\nFinally we are going to expand the bars all the way to the axis line. In other words, remove the gap between the bars and the vertical “x-axis” line.\n\nggplot(data = daily_visits_loc,\n       aes(x = restore_loc, y = daily_visits,\n           fill = visitor_type))+\n    geom_col()+\n    labs(x = \"Restoration Location\",\n         y = \"Number of Visits\",\n         fill = \"Type of Visitor\",\n         title = \"Total Number of Visits to Delta Restoration Areas by visitor type\",\n         subtitle = \"Sum of all visits during study period\")+\n    coord_flip()+\n    scale_y_continuous(breaks = seq(0,120, 20), expand = c(0,0))+\n    my_theme\n\n\n\n\n\n\n3.3.3.2 Reordering things\nggplot() loves putting things in alphabetical order. But more frequent than not, that’s not the order you actually want things to be plotted. One way to do this is to use the fct_reorder() function from the forcats package. forcats provide tools for working with categorical variables. In this case, we want to reorder or categorical variable of “Restoration Location” base on the total number of visits.\nThe fist thing we need to do is to add a column to our data with the total number of visits by location. This will be our “sorting” variable.\n\ndaily_visits_totals &lt;- daily_visits_loc %&gt;% \n    group_by(restore_loc) %&gt;%\n    mutate(n = sum(daily_visits)) %&gt;% \n    ungroup()\n\nhead(daily_visits_totals)\n\n# A tibble: 6 × 5\n  restore_loc   date       visitor_type daily_visits     n\n  &lt;chr&gt;         &lt;date&gt;     &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1 Decker Island 2017-07-07 bank_angler             4    37\n2 Decker Island 2017-07-07 cars                    0    37\n3 Decker Island 2017-07-07 lrg_boat                0    37\n4 Decker Island 2017-07-07 med_boat                6    37\n5 Decker Island 2017-07-07 scientist               0    37\n6 Decker Island 2017-07-07 sm_boat                 0    37\n\n\nNext, we will run the code for our plot adding the fct_reorder() function.\n\nggplot(data = daily_visits_totals,\n       aes(x = fct_reorder(restore_loc, n), y = daily_visits,\n           fill = visitor_type))+\n    geom_col()+\n    labs(x = \"Restoration Location\",\n         y = \"Number of Visits\",\n         fill = \"Type of Visitor\",\n         title = \"Total Number of Visits to Delta Restoration Areas by visitor type\",\n         subtitle = \"Sum of all visits during study period\")+\n    coord_flip()+\n    scale_y_continuous(breaks = seq(0,120, 20), expand = c(0,0))+\n    my_theme\n\n\n\n\nWhat if you want to plot the other way around? In this case from least to greater? We add the desc() to the variable we are sorting by.\n\nggplot(data = daily_visits_totals,\n       aes(x = fct_reorder(restore_loc, desc(n)), y = daily_visits,\n           fill = visitor_type))+\n    geom_col()+\n    theme_bw()+\n    coord_flip()+\n    scale_y_continuous(breaks = seq(0,120, 20), expand = c(0,0))+\n    labs(x = \"Restoration Location\",\n         y = \"Number of Visits\",\n         fill = \"Type of Visitor\",\n         title = \"Total Number of Visits to Delta Restoration Areas by visitor type\",\n         subtitle = \"Sum of all visits during study period\")+\n    my_theme\n\n\n\n\n\n\n\n3.3.3.3 Colors\nThe last thing we will do to our plot is change the color. To do this we are going to use a functions from the viridis package. This package provides different color pallets that are designed to improve graph readability for readers with common forms of color blindness and/or color vision deficiency. As viridis there are multiple other color pallet packages or color pallets out there that you can use to customize your graphs. We could spend a whole session talking about colors in R! For the purpose of this lesson we are just going to keep it brief and show one function of the viridis package that will make our plot colors look better.\n\nggplot(data = daily_visits_totals,\n       aes(x = fct_reorder(restore_loc, desc(n)), y = daily_visits,\n           fill = visitor_type))+\n    geom_col()+\n    theme_bw()+\n    coord_flip()+\n    scale_y_continuous(breaks = seq(0,120, 20), expand = c(0,0))+\n    scale_fill_viridis_d()+\n    labs(x = \"Restoration Location\",\n         y = \"Number of Visits\",\n         fill = \"Type of Visitor\",\n         title = \"Total Number of Visits to Delta Restoration Areas by visitor type\",\n         subtitle = \"Sum of all visits during study period\")+\n    my_theme\n\n\n\n\nThing to keep in mind when choosing a color pallet is the number of variables you have and how many colors your pallet have. And if your need a discrete or a continuous color pallet. Find more information about colors in this R color cheatsheet.\n\n\n3.3.3.4 Saving plots\nSaving plots using ggplot is easy! The ggsave() function will save either the last plot you created, or any plot that you have saved to an object. You can specify what output format you want, size, resolution, etc. See ?ggsave() for documentation.\n\nggsave(\"figures/visit_restore_site_delta.jpg\", width = 12, height = 6, units = \"in\")\n\n\n\n3.3.3.5 Creating multiple plots\nAn easy way to plot another aspect of your data is using the function facet_wrap(). This function takes a mapping to a variable using the syntax ~{variable_name}. The ~ (tilde) is a model operator which tells facet_wrap() to model each unique value within variable_name to a facet in the plot.\nThe default behavior of facet wrap is to put all facets on the same x and y scale. You can use the scales argument to specify whether to allow different scales between facet plots (e.g scales = \"free_y\" to free the y axis scale). You can also specify the number of columns using the ncol = argument or number of rows using nrow =.\n\nfacet_plot &lt;- ggplot(data = daily_visits_totals,\n       aes(x = visitor_type, y = daily_visits,\n           fill = visitor_type))+\n    geom_col()+\n    theme_bw()+\n    facet_wrap(~restore_loc,\n               scales = \"free_y\",\n               ncol = 5,\n               nrow = 2)+\n    scale_fill_viridis_d()+\n    labs(x = \"Type of visitor\",\n         y = \"Number of Visits\",\n         title = \"Total Number of Visits to Delta Restoration Areas\",\n         subtitle = \"Sum of all visits during study period\")+\n    theme_bw()+\n    theme(legend.position = \"bottom\",\n          axis.ticks.x = element_blank(),\n          axis.text.x = element_blank())\n\nWe can save this plot to our figures folder too. Note that this time we are specifically mentioning the object we want to save.\n\nggsave(\"figures/visit_restore_site_facet.jpg\", facet_plot, width = 12, height = 8, units = \"in\")"
  },
  {
    "objectID": "session_03.html#interactive-visualization",
    "href": "session_03.html#interactive-visualization",
    "title": "3  Data Visualization",
    "section": "3.4 Interactive visualization",
    "text": "3.4 Interactive visualization\n\n3.4.1 Tables with DT\nNow that we know how to make great static visualizations, let’s introduce two other packages that allow us to display our data in interactive ways. These packages really shine when used with GitHub Pages, so at the end of this lesson we will publish our figures to the website we created earlier.\nFirst let’s show an interactive table of unique sampling locations using DT. We will start by creating a data.frame containing unique sampling locations.\n\nlocations &lt;- visits_long %&gt;%\n    distinct(restore_loc, .keep_all = T) %&gt;%\n    select(restore_loc, latitude, longitude)\n\nhead(locations)\n\n# A tibble: 6 × 3\n  restore_loc     latitude longitude\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n1 Decker Island       38.1     -122.\n2 SW Suisun Marsh     38.2     -122.\n3 Grizzly Bay         38.1     -122.\n4 Prospect            38.2     -122.\n5 SJ River            38.1     -122.\n6 Wildlands           38.3     -122.\n\n\nThe dplyr::distinct() function comes pretty handy when you want to filter unique values in a column. In this case we use the .keep_all = T argument to keep all the columns of our data frame so we can have the lat and long of each of the locations. If we don’t add this argument, we would end up with a data frame with only one column: restore_loc and 10 rows, one for each of the unique locations.\nNow we can display this table as an interactive table using datatable() from the DT package.\n\ndatatable(locations)\n\n\n\n\n\n\n\n\n3.4.2 Maps with leaflet\nThe leaflet() package allows you to make basic interactive maps using just a couple lines of code. Note that unlike ggplot2, the leaflet package uses pipe operators (%&gt;%) and not the additive operator (+).\nThe addTiles() function without arguments will add base tiles to your map from OpenStreetMap. addMarkers() will add a marker at each location specified by the latitude and longitude arguments. Note that the ~ symbol is used here to model the coordinates to the map (similar to facet_wrap() in ggplot).\n\nleaflet(locations) %&gt;%\n    addTiles() %&gt;%\n    addMarkers(\n        lng = ~ longitude,\n        lat = ~ latitude,\n        popup = ~ restore_loc\n    )\n\n\n\n\n\n\nYou can also use leaflet to import Web Map Service (WMS) tiles. For example, we can use any of the base maps provided by USGS in the National Map archive. For example, let’s use the USGSTopo base map. In this example, we also demonstrate how to create a more simple circle marker, the look of which is explicitly set using a series of style-related arguments.\n\nleaflet(locations) %&gt;%\n    addWMSTiles(\n        \"https://basemap.nationalmap.gov/arcgis/services/USGSTopo/MapServer/WmsServer\",\n        layers = \"0\",\n        options = WMSTileOptions(format = \"image/png\", transparent = TRUE)) %&gt;%\n    addCircleMarkers(\n        lng = ~ longitude,\n        lat = ~ latitude,\n        popup = ~ restore_loc,\n        radius = 5,\n        # set fill properties\n        fillColor = \"salmon\",\n        fillOpacity = 1,\n        # set stroke properties\n        stroke = T,\n        weight = 0.5,\n        color = \"white\",\n        opacity = 1)\n\n\n\n\n\n\nWe can also layer base maps. In this case the USGSImageryTopo base map with the USGSHydroCached base map. Note that the url where the map is retrived is very similar for each USGS base map.\n\nleaflet(locations) %&gt;%\n    addWMSTiles(\n        \"https://basemap.nationalmap.gov/arcgis/services/USGSImageryTopo/MapServer/WmsServer\",\n        layers = \"0\",\n        options = WMSTileOptions(format = \"image/png\", transparent = TRUE)) %&gt;%\n    addWMSTiles(\n        \"https://basemap.nationalmap.gov/arcgis/services/USGSHydroCached/MapServer/WmsServer\",\n        layers = \"0\",\n        options = WMSTileOptions(format = \"image/png\", transparent = TRUE)) %&gt;%\n    addCircleMarkers(\n        lng = ~ longitude,\n        lat = ~ latitude,\n        popup = ~ restore_loc,\n        radius = 5,\n        # set fill properties\n        fillColor = \"salmon\",\n        fillOpacity = 1,\n        # set stroke properties\n        stroke = T,\n        weight = 0.5,\n        color = \"white\",\n        opacity = 1)\n\n\n\n\n\n\nLeaflet has a ton of functionality that can enable you to create some beautiful, functional maps with relative ease. Here is an example of some we created as part of the State of Alaskan Salmon and People (SASAP) project, created using the same tools we showed you here. This map hopefully gives you an idea of how powerful the combination of Quarto or RMarkdown and GitHub Pages can be."
  },
  {
    "objectID": "session_03.html#publish-the-data-visualization-lesson-to-your-webpage",
    "href": "session_03.html#publish-the-data-visualization-lesson-to-your-webpage",
    "title": "3  Data Visualization",
    "section": "3.5 Publish the Data Visualization lesson to your webpage",
    "text": "3.5 Publish the Data Visualization lesson to your webpage\n\n\n\n\n\n\nSteps\n\n\n\n\nSave the qmd you have been working on for this lesson.\n“Render” the qmd. This is a good way to test if everything in your code is working.\nGo to your index.qmd and the link to the html file with this lesson’s content.\nSave and render index.qmd to an html.\nUse the Git workflow: Stage &gt; Commit &gt; Pull &gt; Push"
  },
  {
    "objectID": "session_03.html#ggplot2-resources",
    "href": "session_03.html#ggplot2-resources",
    "title": "3  Data Visualization",
    "section": "3.6 ggplot2 Resources",
    "text": "3.6 ggplot2 Resources\n\nWhy not to use two axes, and what to use instead: The case against dual axis charts by Lisa Charlotte Rost.\nCustomized Data Visualization in ggplot2 by Allison Horst.\nA ggplot2 tutorial for beautiful plotting in R by Cedric Scherer."
  },
  {
    "objectID": "session_04.html#learning-objectives",
    "href": "session_04.html#learning-objectives",
    "title": "4  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPractice using common cleaning and wrangling functions\nPractice creating plots using common visualization functions in ggplot\nPractice saving and sharing data visualizations\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThese exercises are adapted from Allison Horst’s EDS 221: Scientific Programming Essentials Course for the Bren School’s Master of Environmental Data Science program."
  },
  {
    "objectID": "session_04.html#about-the-data",
    "href": "session_04.html#about-the-data",
    "title": "4  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "About the data",
    "text": "About the data\nThese exercises will be using data on abundance, size, and trap counts (fishing pressure) of California spiny lobster (Panulirus interruptus) and were collected along the mainland coast of the Santa Barbara Channel by Santa Barbara Coastal LTER researchers (LTER, Reed, and Miller 2022)."
  },
  {
    "objectID": "session_04.html#exercise-collaborate-on-an-analysis-and-create-a-report-to-publish-using-github-pages",
    "href": "session_04.html#exercise-collaborate-on-an-analysis-and-create-a-report-to-publish-using-github-pages",
    "title": "4  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "4.1 Exercise: Collaborate on an analysis and create a report to publish using GitHub Pages",
    "text": "4.1 Exercise: Collaborate on an analysis and create a report to publish using GitHub Pages\n\n\n\n\n\n\nSetup\n\n\n\n\nCreate a new repository with a partner\n\nDetermine who is the Owner and who is the Collaborator\nThe Owner creates a repository on GitHub titled with both your names (i.e. If Halina and Camila were partners, and Halina is the Owner, she would create a repo called halina-camila)\n\nWhen creating the repository, add a brief description (i.e. coreR R Practice Session: Collaborating on, Wrangling & Visualizing Data), keep the repo Public, and Initialize the repo with a README file and an R .gitignore template.\n\nThe Owner adds the Collaborator to the repo\nBoth the Collaborator and the Owner clone the repo into their RStudio\nBoth the Collaborator and the Owner clone the repo into their RStudio run git config pull.rebase false in the Terminal to set the Git default strategy for Pulling\n\n\nStep 2 and Step 3 are meant to be completed at the same time. Step 2 is for the Collaborator to complete, and Step 3 is for the Owner to complete.\n\nCollaborator creates new files for exercise\n\nThe Collaborator creates the following directory:\n\nanalysis\n\nAfter creating the directories, create the following Quarto files and store them in the listed folders:\n\nTitle it: “Owner Analysis”, save it as: owner-analysis.qmd, and store in analysis folder\nTitle it: “Collaborator Analysis”, save it as: collaborator-analysis.qmd, and store in analysis folder\nTitle it: “Lobster Report” and save it as: lobster-report.qmd (do not need to put in a folder)\n\nAfter creating the files, the Collaborator will stage, commit, write a commit message, pull, and push the files to the remote repository (on GitHub)\nThe Owner pulls the changes and Quarto files into their local repository (their workspace)\n\nOwner downloads data from the EDI Data Portal SBC LTER: Reef: Abundance, size and fishing effort for California Spiny Lobster (Panulirus interruptus), ongoing since 2012.\n\nCreate two new directories one called data and one called figs\nDownload the following data and upload them to the data folder:\n\nTime-series of lobster abundance and size\nTime-series of lobster trap buoy counts\n\nAfter creating the data folder and adding the data, the Owner will stage, commit, write a commit message,pull, and push the files to the remote repository (on GitHub)\nThe Collaborator pulls the changes and data into their local repository (their workspace)\n\n\n\n\n\n4.1.1 Explore, clean and wrangle data\nFor this portion of the exercise, the Owner will be working with the lobster abundance and size data, and the Collaborator will be working with the lobster trap buoy counts data\nQuestions 1-3 you will be working independently since you’re working with different data frames, but you’re welcome to check in with each other.\n\nOwnerCollaborator\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nOpen the Quarto file owner-analysis.qmd\n\nCheck the YAML and add your name to the author field\nCreate a new section with a level 2 header and title it “Exercise: Explore, Clean, and Wrangle Data”\n\nLoad the following libraries at the top of your Quarto file\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\nRead in the data and store the data frame as lobster_abundance\n\n\nlobster_abundance &lt;- read_csv(\"data/Lobster_Abundance_All_Years_20220829.csv\")\n\n\nLook at your data. Take a minute to explore what your data structure looks like, what data types are in the data frame, or use a function to get a high-level summary of the data you’re working with.\nUse the Git workflow: Stage &gt; Commit &gt; Pull &gt; Push\n\nNote: You also want to Pull when you first open a project\n\n\n\n\n\n4.2 Convert missing values using mutate() and na_if()\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe variable SIZE_MM uses -99999 as the code for missing values (see metadata or use unique()). This has the potential to cause conflicts with our analyses, so let’s convert -99999 to an NA value. Do this using mutate() and na_if(). Look up the help page to see how to use na_if(). Check your output data using unique().\n\n\n\n\nCode\nlobster_abundance &lt;- lobster_abundance %&gt;% \n    mutate(SIZE_MM = na_if(SIZE_MM, -99999))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nOpen the Quarto file collaborator-analysis.qmd\n\nCheck the YAML and add your name to the author field\nCreate a new section with a level 2 header and title it “Exercise: Explore, Clean, and Wrangle Data”\n\nLoad the following libraries at the top of your Quarto file.\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\nRead in the data and store the data frame as lobster_traps\n\n\nlobster_traps &lt;- read_csv(\"data/Lobster_Trap_Counts_All_Years_20210519.csv\")\n\n\nLook at your data. Take a minute to explore what your data structure looks like, what data types are in the data frame, or use a function to get a high-level summary of the data you’re working with.\nUse the Git workflow: Stage &gt; Commit &gt; Pull &gt; Push\n\nNote: You also want to Pull when you first open a project\n\n\n\n\n\n4.3 Convert missing values using mutate() and na_if()\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe variable TRAPS uses -99999 as the code for missing values (see metadata or use unique()). This has the potential to cause conflicts with our analyses, so let’s convert -99999 to an NA value. Do this using mutate() and na_if(). Look up the help page to see how to use na_if(). Check your output data using unique().\n\n\n\n\nCode\nlobster_traps &lt;- lobster_traps %&gt;% \n    mutate(TRAPS = na_if(TRAPS, -99999))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.1 Create visually appealing and informative data visualization\n\nOwnerCollaborator\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nStay in the Quarto file owner-analysis.qmd and create a new section with a level 2 header and title it “Exercise: Data Visualization”\n\nStructure of the data visualization exercises:\n\nQuestions 7-8 will have you create the necessary subsets to create the data visualizations, as well as the basic code to create a visualization.\nQuestion 9, return to the data visualization code you’ve written and add styling code to it. For this exercise, only add styling code to the visualization you want to include in the lobster-report.qmd (start with just one plot and if there’s time add styling code to another plot).\nLastly, save the final visualizations to the figs folder before collaborating on the lobster-report.qmd.\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nCreate a multi-panel plot of lobster carapace length (SIZE_MM) using ggplot(), geom_histogram(), and facet_wrap(). Use the variable SITE in facet_wrap(). Use the object lobster_abundance.\n\n\n\n\nCode\nggplot(data = lobster_abundance, aes(x = SIZE_MM)) +\n    geom_histogram() +\n    facet_wrap(~SITE)\n\n\n\n\nPlots\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nCreate a line graph of the number of total lobsters observed (y-axis) by year (x-axis) in the study, grouped by SITE.\n\n\nFirst, you’ll need to create a new dataset subset called lobsters_summarize:\n\nGroup the data by SITE AND YEAR\nCalculate the total number of lobsters observed using count()\n\n\n\nCode\nlobsters_summarize &lt;- lobster_abundance %&gt;% \n  group_by(SITE, YEAR) %&gt;% \n  summarize(COUNT = n())\n\n\nNext, create a line graph using ggplot() and geom_line(). Use geom_point() to make the data points more distinct, but ultimately up to you if you want to use it or not. We also want SITE information on this graph, do this by specifying the variable in the color argument. Where should the color argument go? Inside or outside of aes()? Why or why not?\n\n\nCode\n# line plot\nggplot(data = lobsters_summarize, aes(x = YEAR, y = COUNT)) +\n  geom_line(aes(color = SITE)) \n\n# line and point plot\nggplot(data = lobsters_summarize, aes(x = YEAR, y = COUNT)) +\n  geom_point(aes(color = SITE)) +\n  geom_line(aes(color = SITE)) \n\n\n\n\nPlots\n\n\n\n\n\n\n\nLine plot\n\n\n\n\n\n\n\nLine and point plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\n\nGo back to your visualization code and add some styling code (aka make your plots pretty!). Again, start with one plot and if there’s time add styling code to additional plots. Here’s a list of functions to help you get started (this is not an exhaustive list!) or revisit the data visualization lesson:\n\n\nlabs(): modifying axis, legend and plot labels\ntheme_(): add a complete theme to your plot (i.e. theme_light())\ntheme(): use to customize non-data components of a plot. We’ve listed out some parameters here, but run ?theme to see the full list (there’s a lot of customization you can do!)\n\naxis.title.y\npanel.background\nplot.background\npanel.grid.major.*\ntext\n\nscale_*_date(): use with dates and update breaks, limits, and labels\nscale_*_continuous(): use with continuous variables and update breaks, limits, and labels\nscale_*_discrete(): use with discrete variables and update breaks, limits, and labels\nscales package: use this within the above scale functions and you can do things like add percents to axes labels\ngeom_() within a geom function you can modify:\n\nfill: updates fill colors (e.g. column, density, violin, & boxplot interior fill color)\ncolor: updates point & border line colors (generally)\nshape: update point style\nalpha: update transparency (0 = transparent, 1 = opaque)\nsize: point size or line width\nlinetype: update the line type (e.g. “dotted”, “dashed”, “dotdash”, etc.)\n\n\n\nOnce you’re happy with how your plot looks, assign it to an object, and save it to the figs directory using ggsave()\n\n\n\n\n\n\n\n\n\nSave your work and use Git\n\n\n\nDon’t forget the Git workflow! After you’ve completed the exercises or reached a significant stopping point, use the workflow: Stage &gt; Commit &gt; Pull &gt; Push\n\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nStay in the Quarto file collaborator-analysis.qmd and create a new section with a level 2 header and title it “Exercise: Data Visualization”\n\nStructure of the data visualization exercises:\n\nQuestions 7-8 will have you create the necessary subsets to create the data visualizations, as well as the basic code to create a visualization.\nQuestion 9, return to the data visualization code you’ve written and add styling code to it. For this exercise, only add styling code to the visualization you want to include in the lobster-report.qmd (start with just one plot and if there’s time add styling code to another plot).\nLastly, save the final visualizations to the figs folder before collaborating on the lobster-report.qmd.\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nCreate a multi-panel plot of lobster commercial traps (TRAPS) grouped by year, using ggplot(), geom_histogram(), and facet_wrap(). Use the variable YEAR in facet_wrap(). Use the object lobster_traps.\n\n\n\n\nCode\nggplot(data = lobster_traps, aes(x = TRAPS)) +\n    geom_histogram() +\n    facet_wrap( ~ YEAR)\n\n\n\n\nPlots\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nCreate a line graph of the number of total lobster commercial traps observed (y-axis) by year (x-axis) in the study, grouped by SITE.\n\n\nFirst, you’ll need to create a new dataset subset called lobsters_traps_summarize:\n\nGroup the data by SITE AND YEAR\nCalculate the total number of lobster commercial traps observed using sum(). Look up sum() if you need to. Call the new column TOTAL_TRAPS. Don’t forget about NAs here!\n\n\n\nCode\nlobsters_traps_summarize &lt;- lobster_traps %&gt;% \n  group_by(SITE, YEAR) %&gt;% \n  summarize(TOTAL_TRAPS = sum(TRAPS, na.rm = TRUE))\n\n\nNext, create a line graph using ggplot() and geom_line(). Use geom_point() to make the data points more distinct, but ultimately up to you if you want to use it or not. We also want SITE information on this graph, do this by specifying the variable in the color argument. Where should the color argument go? Inside or outside of aes()? Why or why not?\n\n\nCode\n# line plot\nggplot(data = lobsters_traps_summarize, aes(x = YEAR, y = TOTAL_TRAPS)) +\n    geom_line(aes(color = SITE))\n\n# line and point plot\nggplot(data = lobsters_traps_summarize, aes(x = YEAR, y = TOTAL_TRAPS)) +\n    geom_point(aes(color = SITE)) +\n    geom_line(aes(color = SITE))\n\n\n\n\nPlots\n\n\n\n\n\n\n\nLine plot\n\n\n\n\n\n\n\nLine and point plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\n\nGo back to your visualization code and add some styling code (aka make your plots pretty!). Again, start with one plot and if there’s time add styling code to additional plots. Here’s a list of functions to help you get started (this is not an exhaustive list!) or revisit the data visualization lesson:\n\n\nlabs(): modifying axis, legend and plot labels\ntheme_(): add a complete theme to your plot (i.e. theme_light())\ntheme(): use to customize non-data components of a plot. We’ve listed out some parameters here, but run ?theme to see the full list (there’s a lot of customization you can do!)\n\naxis.title.y\npanel.background\nplot.background\npanel.grid.major.*\ntext\n\nscale_*_date(): use with dates and update breaks, limits, and labels\nscale_*_continuous(): use with continuous variables and update breaks, limits, and labels\nscale_*_discrete(): use with discrete variables and update breaks, limits, and labels\nscales package: use this within the above scale functions and you can do things like add percents to axes labels\ngeom_() within a geom function you can modify:\n\nfill: updates fill colors (e.g. column, density, violin, & boxplot interior fill color)\ncolor: updates point & border line colors (generally)\nshape: update point style\nalpha: update transparency (0 = transparent, 1 = opaque)\nsize: point size or line width\nlinetype: update the line type (e.g. “dotted”, “dashed”, “dotdash”, etc.)\n\n\n\nOnce you’re happy with how your plot looks, assign it to an object, and save it to the figs directory using ggsave()\n\n\n\n\n\n\n\n\n\nSave your work and use Git\n\n\n\nDon’t forget the Git workflow! After you’ve completed the exercises or reached a significant stopping point, use the workflow: Stage &gt; Commit &gt; Pull &gt; Push\n\n\n\n\n\n\n\n4.3.2 Collaborate on a report and publish using GitHub pages\nThe final step! Time to work together again. Collaborate with your partner in lobster-report.qmd to create a report to publish to GitHub pages.\n\n\n\n\n\n\nCode Review\n\n\n\nAs you’re working on the lobster-report.qmd you will be conducting two types of code reviews: (1) pair programming and (2) lightweight code review.\n\nPair programming is where two people develop code together at the same workstation. One person is the “driver” and one person is the “navigator”. The driver writes the code while the navigator observes the code being typed, points out any immediate quick fixes, and will also Google / troubleshoot if errors occur. Both the Owner and the Collaborator should experience both roles, so switch halfway through or at a meaningful stopping point.\nA lightweight code review is brief and you will be giving feedback on code readability and code logic as you’re adding Owner and Collaborator code from their respective analysis.qmds to the lobster-report.qmd. Think of it as a walk through of your the code for the data visualizations you plan to include in the report (this includes the code you wrote to create the subset for the plot and the code to create the plot) and give quick feedback.\n\n\n\nMake sure your Quarto document is well organized and includes the following elements:\n\ncitation of the data\nbrief summary of the abstract (i.e. 1-2 sentences) from the EDI Portal\nOwner analysis and visualizations (you choose which plots you want to include)\n\nadd alternative text to your plots\nplots can be added either with the data visualization code or with Markdown syntax - it’s up to you if you want to include the code or not.\n\nCollaborator analysis and visualizations (you choose which plots you want to include)\n\nadd alternative text to your plots\nplots can be added either with the data visualization code or with Markdown syntax - it’s up to you if you want to include the code or not.\n\n\nFinally, publish on GitHub pages (from Owner’s repository). Refer back to Chapter 9 for steps on how to publish using GitHub pages.\n\n\n\n\n\nLTER, Santa Barbara Coastal, Daniel C Reed, and Robert J Miller. 2022. “SBC LTER: Reef: Abundance, Size and Fishing Effort for California Spiny Lobster (Panulirus Interruptus), Ongoing Since 2012.” Environmental Data Initiative. https://doi.org/10.6073/PASTA/25AA371650A671BAFAD64DD25A39EE18."
  },
  {
    "objectID": "session_04.html#convert-missing-values-using-mutate-and-na_if",
    "href": "session_04.html#convert-missing-values-using-mutate-and-na_if",
    "title": "4  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "4.2 Convert missing values using mutate() and na_if()",
    "text": "4.2 Convert missing values using mutate() and na_if()\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe variable SIZE_MM uses -99999 as the code for missing values (see metadata or use unique()). This has the potential to cause conflicts with our analyses, so let’s convert -99999 to an NA value. Do this using mutate() and na_if(). Look up the help page to see how to use na_if(). Check your output data using unique().\n\n\n\n\nCode\nlobster_abundance &lt;- lobster_abundance %&gt;% \n    mutate(SIZE_MM = na_if(SIZE_MM, -99999))"
  },
  {
    "objectID": "session_04.html#convert-missing-values-using-mutate-and-na_if-1",
    "href": "session_04.html#convert-missing-values-using-mutate-and-na_if-1",
    "title": "4  R Practice: Collaborating on, Wrangling & Visualizing Data",
    "section": "4.3 Convert missing values using mutate() and na_if()",
    "text": "4.3 Convert missing values using mutate() and na_if()\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe variable TRAPS uses -99999 as the code for missing values (see metadata or use unique()). This has the potential to cause conflicts with our analyses, so let’s convert -99999 to an NA value. Do this using mutate() and na_if(). Look up the help page to see how to use na_if(). Check your output data using unique().\n\n\n\n\nCode\nlobster_traps &lt;- lobster_traps %&gt;% \n    mutate(TRAPS = na_if(TRAPS, -99999))"
  },
  {
    "objectID": "session_06.html#developing-a-code-of-conduct",
    "href": "session_06.html#developing-a-code-of-conduct",
    "title": "6  Social Aspects of Collaborations",
    "section": "6.1 Developing a Code of Conduct",
    "text": "6.1 Developing a Code of Conduct\nWhether you are joining a lab group or establishing a new collaboration, articulating a set of shared agreements about how people in the group will treat each other will help create the conditions for successful collaboration. If agreements or a code of conduct do not yet exist, invite a conversation among all members to create them. Co-creation of a code of conduct will foster collaboration and engagement as a process in and of itself, and is important to ensure all voices heard such that your code of conduct represents the perspectives of your community. If a code of conduct already exists, and your community will be a long-acting collaboration, you might consider revising the code of conduct. Having your group ‘sign off’ on the code of conduct, whether revised or not, supports adoption of the principles.\nWhen creating a code of conduct, consider both the behaviors you want to encourage and those that will not be tolerated. For example, the Openscapes code of conduct includes Be respectful, honest, inclusive, accommodating, appreciative, and open to learning from everyone else. Do not attack, demean, disrupt, harass, or threaten others or encourage such behavior.\nBelow are other example codes of conduct:\n\nNCEAS Code of Conduct\nCarpentries Code of Conduct\nArctic Data Center Code of Conduct\nMozilla Community Participation Guidelines\nEcological Society of America Code of Conduct"
  },
  {
    "objectID": "session_06.html#authorship-and-credit-policies",
    "href": "session_06.html#authorship-and-credit-policies",
    "title": "6  Social Aspects of Collaborations",
    "section": "6.2 Authorship and Credit Policies",
    "text": "6.2 Authorship and Credit Policies\n\nNavigating issues of intellectual property and credit can be a challenge, particularly for early career researchers. Open communication is critical to avoiding misunderstandings and conflicts. Talk to your coauthors and collaborators about authorship, credit, and data sharing early and often. This is particularly important when working with new collaborators and across lab groups or disciplines which may have divergent views on authorship and data sharing. If you feel uncomfortable talking about issues surrounding credit or intellectual property, seek the advice or assistance of a mentor to support you in having these important conversations.\nThe “Publication” section of the Ecological Society of America’s Code of Ethics is a useful starting point for discussions about co-authorship, as are the International Committee of Medical Journal Editors guidelines for authorship and contribution. You should also check guidelines published by the journal(s) to which you anticipate submitting your work.\nFor collaborative research projects, develop an authorship agreement for your group early in the project and refer to it for each product. This example authorship agreement from the Arctic Data Center provides a useful template. It builds from information contained within Weltzin et al (2006) and provides a rubric for inclusion of individuals as authors. Your collaborative team may not choose to adopt the agreement in the current form, however it will prompt thought and discussion in advance of developing a consensus. Some key questions to consider as you are working with your team to develop the agreement:\n\nWhat roles do we anticipate contributors will play? e.g., the NISO Contributor Roles Taxonomy (CRediT) identifies 14 distinct roles:\n\nConceptualization\nData curation\nFormal Analysis\nFunding acquisition\nInvestigation\nMethodology\nProject administration\nResources\nSoftware\nSupervision\nValidation\nVisualization\nWriting – original draft\nWriting – review & editing\n\nWhat are our criteria for authorship? (See the ICMJE guidelines for potential criteria)\nWill we extend the opportunity for authorship to all group members on every paper or product?\nDo we want to have an opt in or opt out policy? (In an opt out policy, all group members are considered authors from the outset and must request removal from the paper if they don’t want think they meet the criteria for authorship)\nWho has the authority to make decisions about authorship? Lead author? PI? Group?\nHow will we decide authorship order?\nIn what other ways will we acknowledge contributions and extend credit to collaborators?\nHow will we resolve conflicts if they arise?"
  },
  {
    "objectID": "session_06.html#data-sharing-and-reuse-policies",
    "href": "session_06.html#data-sharing-and-reuse-policies",
    "title": "6  Social Aspects of Collaborations",
    "section": "6.3 Data Sharing and Reuse Policies",
    "text": "6.3 Data Sharing and Reuse Policies\nAs with authorship agreements, it is valuable to establish a shared agreement around handling of data when embarking on collaborative projects. Data collected as part of a funded research activity will typically have been managed as part of the Data Management Plan (DMP) associated with that project. However, collaborative research brings together data from across research projects with different data management plans and can include publicly accessible data from repositories where no management plan is available. For these reasons, a discussion and agreement around the handling of data brought into and resulting from the collaboration is warranted and management of this new data may benefit from going through a data management planning process. Below we discuss example data agreements.\nThe example data policy template provided by the Arctic Data Center addresses three categories of data.\n\nIndividual data not in the public domain\nIndividual data with public access\nDerived data resulting from the project\n\nFor the first category, the agreement considers conditions under which those data may be used and permissions associated with use. It also addresses access and sharing. In the case of individual, publicly accessible data, the agreement stipulates that the team will abide by the attribution and usage policies that the data were published under, noting how those requirements we met. In the case of derived data, the agreement reads similar to a DMP with consideration of making the data public; management, documentation and archiving; pre-publication sharing; and public sharing and attribution. As research data objects receive a persistent identifier (PID), often a DOI, there are citable objects and consideration should be given to authorship of data, as with articles.\nThe following example lab policy from the Wolkovich Lab combines data management practices with authorship guidelines and data sharing agreements. It provides a lot of detail about how this lab approaches data use, attribution and authorship. For example:\n\nSection 6: Co-authorship & data\nIf you agree to take on existing data you cannot offer co-authorship for use of the data unless four criteria are met:\n\nThe co-author agrees to (and does) make substantial intellectual contribution to the work, which includes the reading and editing of all manuscripts on which you are a co-author through the submission-for-publication stage. This includes helping with interpretation of the data, system, study questions.\nAgreement of co-authorship is made at the start of the project.\nAgreement is approved of by Lizzie.\nAll data-sharers are given an equal opportunity at authorship. It is not allowed to offer or give authorship to one data-sharer unless all other data-sharers are offered an equal opportunity at authorship—this includes data that are publicly-available, meaning if you offer authorship to one data-sharer and were planning to use publicly-available data you must reach out to the owner of the publicly-available data and strongly offer equivalent authorship as offered to the other data-sharer. As an example, if five people share data freely with you for a meta-analysis and and a sixth wants authorship you either must strongly offer equivalent authorship to all five or deny authorship to the sixth person. Note that the above requirements must also be met in this situation. If one or more datasets are more central or critical to a paper to warrant selective authorship this must be discussed and approved by Lizzie (and has not, to date, occurred within the lab).\n\n\n\n\n\n\n6.3.0.1 Policy Preview\n\n\nThis policy is communicated with all incoming lab members, from undergraduate to postdocs and visiting scholars, and is shared here with permission from Dr Elizabeth Wolkovich.\n\n\n6.3.1 Community Principles: CARE and FAIR\nThe CARE and FAIR Principles were introduced previously in the context of introducing the Arctic Data Center and our data submission and documentation process. In this section we will dive a little deeper.\nTo recap, the Arctic Data Center is an openly-accessible data repository and the data published through the repository is open for anyone to reuse, subject to conditions of the license (at the Arctic Data Center, data is released under one of two licenses: CC-0 Public Domain and CC-By Attribution 4.0). In facilitating use of data resources, the data stewardship community have converged on principles surrounding best practices for open data management One set of these principles is the FAIR principles. FAIR stands for Findable, Accessible, Interoperable, and Reproducible.\n\nThe “Fostering FAIR Data Practices in Europe” project found that it is more monetarily and timely expensive when FAIR principles are not used, and it was estimated that 10.2 billion dollars per years are spent through “storage and license costs to more qualitative costs related to the time spent by researchers on creation, collection and management of data, and the risks of research duplication.” FAIR principles and open science are overlapping concepts, but are distinctive concepts. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data.\n\nAnother set of community developed principles surrounding open data are the CARE Principles. The CARE principles for Indigenous Data Governance complement the more data-centric approach of the FAIR principles, introducing social responsibility to open data management practices. The CARE Principles stand for:\n\nCollective Benefit - Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data\nAuthority to Control - Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data.\nResponsibility - Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples.\nEthics - Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n\nThe CARE principles align with the FAIR principles by outlining guidelines for publishing data that is findable, accessible, interoperable, and reproducible while at the same time, accounts for Indigenous’ Peoples rights and interests. Initially designed to support Indigenous data sovereignty, CARE principles are now being adopted across domains, and many researchers argue they are relevant for both Indigenous Knowledge and data, as well as data from all disciplines (Carroll et al., 2021). These principles introduce a “game changing perspective” that encourages transparency in data ethics, and encourages data reuse that is purposeful and intentional that aligns with human well-being aligns with human well-being (Carroll et al., 2021)."
  },
  {
    "objectID": "session_06.html#research-data-publishing-ethics",
    "href": "session_06.html#research-data-publishing-ethics",
    "title": "6  Social Aspects of Collaborations",
    "section": "6.4 Research Data Publishing Ethics",
    "text": "6.4 Research Data Publishing Ethics\nFor over 20 years, the Committee on Publication Ethics (COPE) has provided trusted guidance on ethical practices for scholarly publishing. The COPE guidelines have been broadly adopted by academic publishers across disciplines, and represent a common approach to identify, classify, and adjudicate potential breaches of ethics in publication such as authorship conflicts, peer review manipulation, and falsified findings, among many other areas. Despite these guidelines, there has been a lack of ethics standards, guidelines, or recommendations for data publications, even while some groups have begun to evaluate and act upon reported issues in data publication ethics.\n\n\n\nData retractions\n\n\nTo address this gap, the Force 11 Working Group on Research Data Publishing Ethics was formed as a collaboration among research data professionals and the Committee on Publication Ethics (COPE) “to develop industry-leading guidance and recommended best practices to support repositories, journal publishers, and institutions in handling the ethical responsibilities associated with publishing research data.” The group released the “Joint FORCE11 & COPE Research Data Publishing Ethics Working Group Recommendations” (Puebla, Lowenberg, and WG 2021), which outlines recommendations for four categories of potential data ethics issues:\n\n\n\nForce11/COPE\n\n\n\nAuthorship and Contribution Conflicts\n\nAuthorship omissions\nAuthorship ordering changes / conflicts\nInstitutional investigation of author finds misconduct\n\nLegal/regulatory restrictions\n\nCopyright violation\nInsufficient rights for deposit\nBreaches of national privacy laws (GPDR, CCPA)\nBreaches of biosafety and biosecurity protocols\nBreaches of contract law governing data redistribution\n\nRisks of publication or release\n\nRisks to human subjects\n\nLack of consent\nBreaches of himan rights\nRelease of personally identifiable information (PII)\n\nRisks to species, ecosystems, historical sites\n\nLocations of endangered species or historical sites\n\nRisks to communities or societies\n\nData harvested for profit or surveillance\nBreaches of data sovereignty\n\n\nRigor of published data\n\nUnintentional errors in collection, calculation, display\nUn-interpretable data due to lack of adequate documentation\nErrors of of study design and inference\nData manipulation or fabrication\n\n\nGuidelines cover what actions need to be taken, depending on whether the data are already published or not, as well as who should be involved in decisions, who should be notified of actions, and when the public should be notified. The group has also published templates for use by publishers and repositories to announce the extent to which they plan to conform to the data ethics guidelines.\n\nDiscussion: Data publishing policies\nAt the Arctic Data Center, we need to develop policies and procedures governing how we react to potential breaches of data publication ethics. In this exercise, break into groups to provide advice on how the Arctic Data Center should respond to reports of data ethics issues, and whether we should adopt the Joint FORCE11 & COPE Research Data Publishing Ethics Working Group Policy Templates for repositories. In your discussion, consider:\n\nShould the repository adopt the repository policy templates from Force11?\nWho should be involved in evaluation of the merits of ethical cases reported to ADC?\nWho should be involved in deciding the actions to take?\nWhat are the range of responses that the repository should consider for ethical breaches?\nWho should be notified when a determination has been made that a breach has occurred?\n\nYou might consider a hypothetical scenario such as the following in considering your response.\n\nThe data coordinator at the Arctic Data Center receives an email in 2022 from a prior postdoctoral fellow who was employed as part of an NSF-funded project on microbial diversity in Alaskan tundra ecosystems. The email states that a dataset from 2014 in the Arctic Data Center was published with the project PI as author, but omits two people, the postdoc and an undergraduate student, as co-authors on the dataset. The PI retired in 2019, and the postdoc asks that they be added to the author list of the dataset to correct the historical record and provide credit."
  },
  {
    "objectID": "session_06.html#extra-reading",
    "href": "session_06.html#extra-reading",
    "title": "6  Social Aspects of Collaborations",
    "section": "6.5 Extra Reading",
    "text": "6.5 Extra Reading\n\nCheruvelil, K. S., Soranno, P. A., Weathers, K. C., Hanson, P. C., Goring, S. J., Filstrup, C. T., & Read, E. K. (2014). Creating and maintaining high-performing collaborative research teams: The importance of diversity and interpersonal skills. Frontiers in Ecology and the Environment, 12(1), 31-38. DOI: 10.1890/130001\nCarroll, S. R., Garba, I., Figueroa-Rodríguez, O. L., Holbrook, J., Lovett, R., Materechera, S., … Hudson, M. (2020). The CARE Principles for Indigenous Data Governance. Data Science Journal, 19(1), 43. DOI: http://doi.org/10.5334/dsj-2020-043\n\n\n\n\n\nPuebla, Iratxe, Daniella Lowenberg, and FORCE11 Research Data Publishing Ethics WG. 2021. “Joint FORCE11 & COPE Research Data Publishing Ethics Working Group Recommendations.” Zenodo. https://doi.org/10.5281/zenodo.5391293."
  },
  {
    "objectID": "session_07.html",
    "href": "session_07.html",
    "title": "7  Thinking Preferences",
    "section": "",
    "text": "INSERT ONE OF THE THIKING PREFERENCES VERSIONS"
  },
  {
    "objectID": "session_08.html",
    "href": "session_08.html",
    "title": "8  Collaborative Synthesis: Authorship Guidlines & Data Ploicy",
    "section": "",
    "text": "ADD INSTRUCTIONS"
  },
  {
    "objectID": "session_10.html#learning-objectives",
    "href": "session_10.html#learning-objectives",
    "title": "10  Using sf for Spatia Data & Intro to Making Maps",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nHow to use the sf package to wrangle spatial data\nStatic mapping with ggplot\nAdding basemaps to static maps\nInteractive mapping with leaflet"
  },
  {
    "objectID": "session_10.html#brief-introduction-to-sf",
    "href": "session_10.html#brief-introduction-to-sf",
    "title": "10  Using sf for Spatia Data & Intro to Making Maps",
    "section": "10.1 Brief introduction to sf",
    "text": "10.1 Brief introduction to sf\nFrom the sf vignette:\n\nSimple features or simple feature access refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects. It also describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them.\n\nThe sf package is an R implementation of Simple Features. This package incorporates:\n\na new spatial data class system in R\n\nfunctions for reading and writing spatial data\n\ntools for spatial operations on vectors\n\nMost of the functions in this package starts with prefix st_ which stands for spatial and temporal.\nIn this lesson, our goal is to use a shapefile of Alaska regions and rivers, and data on population in Alaska by community to create a map that looks like this:"
  },
  {
    "objectID": "session_10.html#about-the-data",
    "href": "session_10.html#about-the-data",
    "title": "10  Using sf for Spatia Data & Intro to Making Maps",
    "section": "10.2 About the data",
    "text": "10.2 About the data\nAll of the data used in this tutorial are simplified versions of real datasets available on the KNB Data Repository. We are using simplified datasets to ease the processing burden on all our computers since the original geospatial datasets are high-resolution. These simplified versions of the datasets may contain topological errors.\nThe spatial data we will be using to create the map are:\n\n\n\nData\nOriginal datasets\n\n\n\n\nAlaska regional boundaries\nJared Kibele and Jeanette Clark. 2018. State of Alaska’s Salmon and People Regional Boundaries. Knowledge Network for Biocomplexity. doi:10.5063/F1125QWP.\n\n\nCommunity locations and population\nJeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018. Languages used in Alaskan households, 1990-2015. Knowledge Network for Biocomplexity. doi:10.5063/F11G0JHX.\n\n\nAlaska rivers\nThe rivers shapefile is a simplified version of Jared Kibele and Jeanette Clark. Rivers of Alaska grouped by SASAP region, 2018. Knowledge Network for Biocomplexity. doi:10.5063/F1SJ1HVW.\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nNavigate to this dataset on KNB’s test site and download the zip folder.\nUpload the zip folder to the data folder in the training_{USERNAME} project. You don’t need to unzip the folder ahead of time, uploading will automatically unzip the folder.\nCreate a new R Markdown file.\n\nTitle it “Intro to sf package for Spatial Data and Making Maps”\nSave the file and name it “intro-sf-spatial-data-maps”.\n\nLoad the following libraries at the top of your R Markdown file.\n\n\nlibrary(readr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(leaflet)\nlibrary(scales)\nlibrary(ggmap)\nlibrary(dplyr)"
  },
  {
    "objectID": "session_10.html#exploring-the-data-using-plot-and-st_crs",
    "href": "session_10.html#exploring-the-data-using-plot-and-st_crs",
    "title": "10  Using sf for Spatia Data & Intro to Making Maps",
    "section": "10.3 Exploring the data using plot() and st_crs()",
    "text": "10.3 Exploring the data using plot() and st_crs()\nFirst let’s read in the shapefile of regional boundaries in Alaska using read_sf() and then create a basic plot of the data plot().\n\n# read in shapefile using read_sf()\nak_regions &lt;- read_sf(\"data/ak_regions_simp.shp\")\n\n\n# quick plot\nplot(ak_regions)\n\n\n\n\nWe can also examine it’s class using class().\n\nclass(ak_regions)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nsf objects usually have two types of classes: sf and data.frame.\nUnlike a typical data.frame, an sf object has spatial metadata (geometry type, dimension, bbox, epsg (SRID), proj4string) and an additional column typically named geometry that contains the spatial data.\nSince our shapefile object has the data.frame class, viewing the contents of the object using the head() function or other exploratory functions shows similar results as if we read in data using read.csv() or read_csv().\n\nhead(ak_regions)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.2296 ymin: 51.15702 xmax: 179.8567 ymax: 71.43957\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  region_id region           mgmt_area                                  geometry\n      &lt;int&gt; &lt;chr&gt;                &lt;dbl&gt;                        &lt;MULTIPOLYGON [°]&gt;\n1         1 Aleutian Islands         3 (((-171.1345 52.44974, -171.1686 52.4174…\n2         2 Arctic                   4 (((-139.9552 68.70597, -139.9893 68.7051…\n3         3 Bristol Bay              3 (((-159.8745 58.62778, -159.8654 58.6137…\n4         4 Chignik                  3 (((-155.8282 55.84638, -155.8049 55.8655…\n5         5 Copper River             2 (((-143.8874 59.93931, -143.9165 59.9403…\n6         6 Kodiak                   3 (((-151.9997 58.83077, -152.0358 58.8271…\n\nglimpse(ak_regions)\n\nRows: 13\nColumns: 4\n$ region_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n$ region    &lt;chr&gt; \"Aleutian Islands\", \"Arctic\", \"Bristol Bay\", \"Chignik\", \"Cop…\n$ mgmt_area &lt;dbl&gt; 3, 4, 3, 3, 2, 3, 4, 4, 2, 4, 2, 1, 4\n$ geometry  &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-171.1345 5..., MULTIPOLYGON (((-139.9552 6.…\n\n\n\n10.3.1 Coordinate Reference System (CRS)\n\n\n\nSource: ESRI\n\n\nEvery sf object needs a coordinate reference system (or crs) defined in order to work with it correctly. A coordinate reference system contains both a datum and a projection. The datum is how you georeference your points (in 3 dimensions!) onto a spheroid. The projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. All coordinate reference systems require a datum. However, some coordinate reference systems are “unprojected” (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984.\nESRI has a blog post that explains these concepts in more detail with very helpful diagrams and examples.\nYou can view what crs is set by using the function st_crs().\n\nst_crs(ak_regions)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nThis is pretty confusing looking. Without getting into the details, that long string says that this data has a geographic coordinate system (WGS84) with no projection. A convenient way to reference crs quickly is by using the EPSG code, a number that represents a standard projection and datum. You can check out a list of (lots!) of EPSG codes here.\nWe will use multiple EPSG codes in this lesson. Here they are, along with their more readable names:\n\n3338: Alaska Albers (projected CRS)\n4326: WGS84 (World Geodetic System 1984), used in GPS (unprojected CRS)\n3857: Pseudo-Mercator, used in Google Maps, OpenStreetMap, Bing, ArcGIS, ESRI (projected CRS)\n\nYou will often need to transform your geospatial data from one coordinate system to another. The st_transform() function does this quickly for us. You may have noticed the maps above looked wonky because of the dateline. We might want to set a different projection for this data so it plots nicer. A good one for Alaska is called the Alaska Albers projection, with an EPSG code of 3338.\n\nak_regions_3338 &lt;- ak_regions %&gt;%\n    st_transform(crs = 3338)\n\nst_crs(ak_regions_3338)\n\nCoordinate Reference System:\n  User input: EPSG:3338 \n  wkt:\nPROJCRS[\"NAD83 / Alaska Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Alaska Albers (meters)\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-154,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",55,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",65,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Topographic mapping (small scale).\"],\n        AREA[\"United States (USA) - Alaska.\"],\n        BBOX[51.3,172.42,71.4,-129.99]],\n    ID[\"EPSG\",3338]]\n\n\n\nplot(ak_regions_3338)\n\n\n\n\nMuch better!"
  },
  {
    "objectID": "session_10.html#sf-the-tidyverse",
    "href": "session_10.html#sf-the-tidyverse",
    "title": "10  Using sf for Spatia Data & Intro to Making Maps",
    "section": "10.4 sf & the Tidyverse",
    "text": "10.4 sf & the Tidyverse\nsf objects can be used as a regular data.frame object in many operations. We already saw the results of plot() and head().\nSince sf objects are data.frames, they play nicely with packages in the tidyverse. Here are a couple of simple examples:\n\n10.4.1 select()\n\n# returns the names of all the columns in dataset\ncolnames(ak_regions_3338)\n\n[1] \"region_id\" \"region\"    \"mgmt_area\" \"geometry\" \n\n\n\nak_regions_3338 %&gt;%\n    select(region)\n\nSimple feature collection with 13 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -2175328 ymin: 405653 xmax: 1579226 ymax: 2383770\nProjected CRS: NAD83 / Alaska Albers\n# A tibble: 13 × 2\n   region                                                               geometry\n   &lt;chr&gt;                                                      &lt;MULTIPOLYGON [m]&gt;\n 1 Aleutian Islands     (((-1156666 420855.1, -1159837 417990.3, -1161898 41694…\n 2 Arctic               (((571289.9 2143072, 569941.5 2142691, 569158.2 2142146…\n 3 Bristol Bay          (((-339688.6 973904.9, -339302 972297.3, -339229.2 9710…\n 4 Chignik              (((-114381.9 649966.8, -112866.8 652065.8, -108836.8 65…\n 5 Copper River         (((561012 1148301, 559393.7 1148169, 557797.7 1148492, …\n 6 Kodiak               (((115112.5 983293, 113051.3 982825.9, 110801.3 983211.…\n 7 Kotzebue             (((-678815.3 1819519, -677555.2 1820698, -675557.8 1821…\n 8 Kuskokwim            (((-1030125 1281198, -1029858 1282333, -1028980 1284032…\n 9 Cook Inlet           (((35214.98 1002457, 36660.3 1002038, 36953.11 1001186,…\n10 Norton Sound         (((-848357 1636692, -846510 1635203, -840513.7 1632225,…\n11 Prince William Sound (((426007.1 1087250, 426562.5 1088591, 427711.6 1089991…\n12 Southeast            (((1287777 744574.1, 1290183 745970.8, 1292940 746262.7…\n13 Yukon                (((-375318 1473998, -373723.9 1473487, -373064.8 147393…\n\n\nNote the sticky geometry column! The geometry column will stay with your sf object even if it is not called explicitly.\n\n\n10.4.2 filter()\n\nunique(ak_regions_3338$region)\n\n [1] \"Aleutian Islands\"     \"Arctic\"               \"Bristol Bay\"         \n [4] \"Chignik\"              \"Copper River\"         \"Kodiak\"              \n [7] \"Kotzebue\"             \"Kuskokwim\"            \"Cook Inlet\"          \n[10] \"Norton Sound\"         \"Prince William Sound\" \"Southeast\"           \n[13] \"Yukon\"               \n\n\n\nak_regions_3338 %&gt;%\n    filter(region == \"Southeast\")\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 559475.7 ymin: 722450 xmax: 1579226 ymax: 1410576\nProjected CRS: NAD83 / Alaska Albers\n# A tibble: 1 × 4\n  region_id region    mgmt_area                                         geometry\n*     &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;                               &lt;MULTIPOLYGON [m]&gt;\n1        12 Southeast         1 (((1287777 744574.1, 1290183 745970.8, 1292940 …"
  },
  {
    "objectID": "session_10.html#spatial-joins",
    "href": "session_10.html#spatial-joins",
    "title": "10  Using sf for Spatia Data & Intro to Making Maps",
    "section": "10.5 Spatial Joins",
    "text": "10.5 Spatial Joins\nYou can also use the sf package to create spatial joins, useful for when you want to utilize two datasets together.\n\n\n\n\n\n\nExercise: How many people live in each of these Alaska regions?\n\n\n\nWe have some population data, but it gives the population by city, not by region. To determine the population per region we will need to:\n\nRead in the population data from a csv and turn it into an sf object\nUse a spatial join (st_join()) to assign each city to a region\nUse group_by() and summarize() to calculate the total population by region\nSave the spatial object you created using write_sf()\n\n\n\n1. Read in alaska_population.csv using read.csv()\n\n# read in population data\npop &lt;- read_csv(\"data/alaska_population.csv\")\n\nTurn pop into a spatial object\nThe st_join() function is a spatial left join. The arguments for both the left and right tables are objects of class sf which means we will first need to turn our population data.frame with latitude and longitude coordinates into an sf object.\nWe can do this easily using the st_as_sf() function, which takes as arguments the coordinates and the crs. The remove = F specification here ensures that when we create our geometry column, we retain our original lat lng columns, which we will need later for plotting. Although it isn’t said anywhere explicitly in the file, let’s assume that the coordinate system used to reference the latitude longitude coordinates is WGS84, which has a crs number of 4326.\n\npop_4326 &lt;- st_as_sf(pop,\n                     coords = c('lng', 'lat'),\n                     crs = 4326,\n                     remove = F)\n\nhead(pop_4326)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -176.6581 ymin: 51.88 xmax: -154.1703 ymax: 62.68889\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 6\n   year city       lat   lng population             geometry\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;          &lt;POINT [°]&gt;\n1  2015 Adak      51.9 -177.        122    (-176.6581 51.88)\n2  2015 Akhiok    56.9 -154.         84 (-154.1703 56.94556)\n3  2015 Akiachak  60.9 -161.        562 (-161.4314 60.90944)\n4  2015 Akiak     60.9 -161.        399 (-161.2139 60.91222)\n5  2015 Akutan    54.1 -166.        899 (-165.7731 54.13556)\n6  2015 Alakanuk  62.7 -165.        777 (-164.6153 62.68889)\n\n\n2. Join population data with Alaska regions data using st_join()\nNow we can do our spatial join! You can specify what geometry function the join uses (st_intersects, st_within, st_crosses, st_is_within_distance…) in the join argument. The geometry function you use will depend on what kind of operation you want to do, and the geometries of your shapefiles.\nIn this case, we want to find what region each city falls within, so we will use st_within.\n\npop_joined &lt;- st_join(pop_4326, ak_regions_3338, join = st_within)\n\nThis gives an error!\nError: st_crs(x) == st_crs(y) is not TRUE\nTurns out, this won’t work right now because our coordinate reference systems are not the same. Luckily, this is easily resolved using st_transform(), and projecting our population object into Alaska Albers.\n\npop_3338 &lt;- st_transform(pop_4326, crs = 3338)\n\n\npop_joined &lt;- st_join(pop_3338, ak_regions_3338, join = st_within)\n\nhead(pop_joined)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -1537925 ymin: 472626.9 xmax: -10340.71 ymax: 1456223\nProjected CRS: NAD83 / Alaska Albers\n# A tibble: 6 × 9\n   year city     lat   lng population             geometry region_id region     \n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;          &lt;POINT [m]&gt;     &lt;int&gt; &lt;chr&gt;      \n1  2015 Adak    51.9 -177.        122  (-1537925 472626.9)         1 Aleutian I…\n2  2015 Akhiok  56.9 -154.         84 (-10340.71 770998.4)         6 Kodiak     \n3  2015 Akiac…  60.9 -161.        562  (-400885.5 1236460)         8 Kuskokwim  \n4  2015 Akiak   60.9 -161.        399  (-389165.7 1235475)         8 Kuskokwim  \n5  2015 Akutan  54.1 -166.        899 (-766425.7 526057.8)         1 Aleutian I…\n6  2015 Alaka…  62.7 -165.        777  (-539724.9 1456223)        13 Yukon      \n# ℹ 1 more variable: mgmt_area &lt;dbl&gt;\n\n\n\n\n\n\n\n\nExploring types of joins\n\n\n\nThere are many different types of joins you can do with geospatial data. Examine the help page for these joins (?st_within() will get you there). What other joins types might be appropriate for examining the relationship between points and polygyons? What about two sets of polygons?\n\n\n3. Calculate the total population by region using group_by() and summarize()\nNext we compute the total population for each region. In this case, we want to do a group_by() and summarise() as this were a regular data.frame. Otherwise all of our point geometries would be included in the aggregation, which is not what we want. Our goal is just to get the total population by region. We remove the sticky geometry using as.data.frame(), on the advice of the sf::tidyverse help page.\n\npop_region &lt;- pop_joined %&gt;%\n    as.data.frame() %&gt;%\n    group_by(region) %&gt;%\n    summarise(total_pop = sum(population))\n\nhead(pop_region)\n\n# A tibble: 6 × 2\n  region           total_pop\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Aleutian Islands      8840\n2 Arctic                8419\n3 Bristol Bay           6947\n4 Chignik                311\n5 Cook Inlet          408254\n6 Copper River          2294\n\n\nAnd use a regular left_join() to get the information back to the Alaska region shapefile. Note that we need this step in order to regain our region geometries so that we can make some maps.\n\npop_region_3338 &lt;- left_join(ak_regions_3338, pop_region, by = \"region\")\n\n# plot to check\nplot(pop_region_3338[\"total_pop\"])\n\n\n\n\nSo far, we have learned how to use sf and dplyr to use a spatial join on two datasets and calculate a summary metric from the result of that join.\n\n\n\n\n\n\nsf and tidyverse best practices\n\n\n\nThe group_by() and summarize() functions can also be used on sf objects to summarize within a dataset and combine geometries. Many of the tidyverse functions have methods specific for sf objects, some of which have additional arguments that wouldn’t be relevant to the data.frame methods. You can run ?sf::tidyverse to get documentation on the tidyverse sf methods.\n\n\nSay we want to calculate the population by Alaska management area, as opposed to region.\n\npop_mgmt_338 &lt;- pop_region_3338 %&gt;%\n    group_by(mgmt_area) %&gt;%\n    summarize(total_pop = sum(total_pop))\n\nplot(pop_mgmt_338[\"total_pop\"])\n\n\n\n\nNotice that the region geometries were combined into a single polygon for each management area.\nIf we don’t want to combine geometries, we can specify do_union = F as an argument.\n\npop_mgmt_3338 &lt;- pop_region_3338 %&gt;%\n    group_by(mgmt_area) %&gt;%\n    summarize(total_pop = sum(total_pop), do_union = F)\n\nplot(pop_mgmt_3338[\"total_pop\"])\n\n\n\n\n4. Save the spatial object to a new file using write_sf()\nSave the spatial object to disk using write_sf() and specifying the filename. Writing your file with the extension .shp will assume an ESRI driver driver, but there are many other format options available.\n\nwrite_sf(pop_region_3338, \"data/ak_regions_population.shp\")\n\n\n10.5.1 Visualize with ggplot\nggplot2 now has integrated functionality to plot sf objects using geom_sf().\nWe can plot sf objects just like regular data.frames using geom_sf.\n\nggplot(pop_region_3338) +\n    geom_sf(aes(fill = total_pop)) +\n    labs(fill = \"Total Population\") +\n    scale_fill_continuous(low = \"khaki\",\n                          high =  \"firebrick\",\n                          labels = comma) +\n    theme_bw()\n\n\n\n\nWe can also plot multiple shapefiles in the same plot. Say if we want to visualize rivers in Alaska, in addition to the location of communities, since many communities in Alaska are on rivers. We can read in a rivers shapefile, doublecheck the crs to make sure it is what we need, and then plot all three shapefiles - the regional population (polygons), the locations of cities (points), and the rivers (linestrings).\n\n\nCoordinate Reference System:\n  User input: Albers \n  wkt:\nPROJCRS[\"Albers\",\n    BASEGEOGCRS[\"GCS_GRS 1980(IUGG, 1980)\",\n        DATUM[\"D_unknown\",\n            ELLIPSOID[\"GRS80\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1,\n                    ID[\"EPSG\",9001]]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",50,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-154,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",55,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",65,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nrivers_3338 &lt;- read_sf(\"data/ak_rivers_simp.shp\")\nst_crs(rivers_3338)\n\nNote that although no EPSG code is set explicitly, with some sluething we can determine that this is EPSG:3338. This site is helpful for looking up EPSG codes.\n\nggplot() +\n    geom_sf(data = pop_region_3338, aes(fill = total_pop)) +\n    geom_sf(data = pop_3338, size = 0.5) +\n    geom_sf(data = rivers_3338,\n            aes(linewidth = StrOrder)) +\n    scale_linewidth(range = c(0.05, 0.5), guide = \"none\") +\n    labs(title = \"Total Population by Alaska Region\",\n         fill = \"Total Population\") +\n    scale_fill_continuous(low = \"khaki\",\n                          high =  \"firebrick\",\n                          labels = comma) +\n    theme_bw()"
  },
  {
    "objectID": "session_10.html#incorporate-base-maps-into-static-maps-using-ggmap",
    "href": "session_10.html#incorporate-base-maps-into-static-maps-using-ggmap",
    "title": "10  Using sf for Spatia Data & Intro to Making Maps",
    "section": "10.6 Incorporate base maps into static maps using ggmap",
    "text": "10.6 Incorporate base maps into static maps using ggmap\nThe ggmap package has some functions that can render base maps (as raster objects) from open tile servers like Google Maps, Stamen, OpenStreetMap, and others.\nWe’ll need to transform our shapefile with population data by community to EPSG:3857 which is the crs used for rendering maps in Google Maps, Stamen, and OpenStreetMap, among others.\n\npop_3857 &lt;- pop_3338 %&gt;%\n    st_transform(crs = 3857)\n\nNext, let’s grab a base map from the Stamen map tile server covering the region of interest. First we include a function that transforms the bounding box (which starts in EPSG:4326) to also be in the EPSG:3857 CRS, which is the projection that the map raster is returned in from Stamen. This is an issue with ggmap described in more detail here\n\n# Define a function to fix the bbox to be in EPSG:3857\n# See https://github.com/dkahle/ggmap/issues/160#issuecomment-397055208\nggmap_bbox_to_3857 &lt;- function(map) {\n    if (!inherits(map, \"ggmap\"))\n        stop(\"map must be a ggmap object\")\n    # Extract the bounding box (in lat/lon) from the ggmap to a numeric vector,\n    # and set the names to what sf::st_bbox expects:\n    map_bbox &lt;- setNames(unlist(attr(map, \"bb\")),\n                         c(\"ymin\", \"xmin\", \"ymax\", \"xmax\"))\n    \n    # Coonvert the bbox to an sf polygon, transform it to 3857,\n    # and convert back to a bbox (convoluted, but it works)\n    bbox_3857 &lt;-\n        st_bbox(st_transform(st_as_sfc(st_bbox(map_bbox, crs = 4326)), 3857))\n    \n    # Overwrite the bbox of the ggmap object with the transformed coordinates\n    attr(map, \"bb\")$ll.lat &lt;- bbox_3857[\"ymin\"]\n    attr(map, \"bb\")$ll.lon &lt;- bbox_3857[\"xmin\"]\n    attr(map, \"bb\")$ur.lat &lt;- bbox_3857[\"ymax\"]\n    attr(map, \"bb\")$ur.lon &lt;- bbox_3857[\"xmax\"]\n    map\n}\n\nNext, we define the bounding box of interest, and use get_stamenmap() to get the basemap. Then we run our function defined above on the result of the get_stamenmap() call.\n\nbbox &lt;- c(-170, 52,-130, 64) # this is roughly southern Alaska\nak_map &lt;- get_stamenmap(bbox, zoom = 4) # get base map\nak_map_3857 &lt;- ggmap_bbox_to_3857(ak_map) # fix the bbox to be in EPSG:3857\n\nFinally, plot both the base raster map with the population data overlayed, which is easy now that everything is in the same projection (3857):\n\nggmap(ak_map_3857) +\n    geom_sf(data = pop_3857,\n            aes(color = population),\n            inherit.aes = F) +\n    scale_color_continuous(low = \"khaki\",\n                           high =  \"firebrick\",\n                           labels = comma)"
  },
  {
    "objectID": "session_10.html#visualize-sf-objects-with-leaflet",
    "href": "session_10.html#visualize-sf-objects-with-leaflet",
    "title": "10  Using sf for Spatia Data & Intro to Making Maps",
    "section": "10.7 Visualize sf objects with leaflet",
    "text": "10.7 Visualize sf objects with leaflet\nWe can also make an interactive map from our data above using leaflet.\nleaflet (unlike ggplot) will project data for you. The catch is that you have to give it both a projection (like Alaska Albers), and that your shapefile must use a geographic coordinate system. This means that we need to use our shapefile with the 4326 EPSG code. Remember you can always check what crs you have set using st_crs.\nHere we define a leaflet projection for Alaska Albers, and save it as a variable to use later.\n\nepsg3338 &lt;- leaflet::leafletCRS(\n    crsClass = \"L.Proj.CRS\",\n    code = \"EPSG:3338\",\n    proj4def =  \"+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\",\n    resolutions = 2 ^ (16:7)\n)\n\nYou might notice that this looks familiar! The syntax is a bit different, but most of this information is also contained within the crs of our shapefile:\n\nst_crs(pop_region_3338)\n\nCoordinate Reference System:\n  User input: EPSG:3338 \n  wkt:\nPROJCRS[\"NAD83 / Alaska Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Alaska Albers (meters)\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-154,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",55,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",65,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Topographic mapping (small scale).\"],\n        AREA[\"United States (USA) - Alaska.\"],\n        BBOX[51.3,172.42,71.4,-129.99]],\n    ID[\"EPSG\",3338]]\n\n\nSince leaflet requires that we use an unprojected coordinate system, let’s use st_transform() yet again to get back to WGS84.\n\npop_region_4326 &lt;- pop_region_3338 %&gt;% st_transform(crs = 4326)\n\n\nm &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;%\n    addPolygons(data = pop_region_4326,\n                fillColor = \"gray\",\n                weight = 1)\n\nm\n\n\n\n\n\nWe can add labels, legends, and a color scale.\n\npal &lt;- colorNumeric(palette = \"Reds\", domain = pop_region_4326$total_pop)\n\nm &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;%\n    addPolygons(\n        data = pop_region_4326,\n        fillColor = ~ pal(total_pop),\n        weight = 1,\n        color = \"black\",\n        fillOpacity = 1,\n        label = ~ region\n    ) %&gt;%\n    addLegend(\n        position = \"bottomleft\",\n        pal = pal,\n        values = range(pop_region_4326$total_pop),\n        title = \"Total Population\"\n    )\n\nm\n\n\n\n\n\nWe can also add the individual communities, with popup labels showing their population, on top of that!\n\npal &lt;- colorNumeric(palette = \"Reds\", domain = pop_region_4326$total_pop)\n\nm &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;%\n    addPolygons(\n        data = pop_region_4326,\n        fillColor = ~ pal(total_pop),\n        weight = 1,\n        color = \"black\",\n        fillOpacity = 1\n    ) %&gt;%\n    addCircleMarkers(\n        data = pop_4326,\n        lat = ~ lat,\n        lng = ~ lng,\n        radius = ~ log(population / 500),\n        # arbitrary scaling\n        fillColor = \"gray\",\n        fillOpacity = 1,\n        weight = 0.25,\n        color = \"black\",\n        label = ~ paste0(pop_4326$city, \", population \", comma(pop_4326$population))\n    ) %&gt;%\n    addLegend(\n        position = \"bottomleft\",\n        pal = pal,\n        values = range(pop_region_4326$total_pop),\n        title = \"Total Population\"\n    )\n\nm"
  },
  {
    "objectID": "session_10.html#more-spatial-resources",
    "href": "session_10.html#more-spatial-resources",
    "title": "10  Using sf for Spatia Data & Intro to Making Maps",
    "section": "10.8 More Spatial Resources",
    "text": "10.8 More Spatial Resources\nThere is a lot more functionality to sf including the ability to intersect polygons, calculate distance, create a buffer, and more. Here are some more great resources and tutorials for a deeper dive into this great package:\n\nRaster analysis in R\n\nSpatial analysis in R with the sf package\n\nIntro to Spatial Analysis\n\nsf github repo\n\nTidy spatial data in R: using dplyr, tidyr, and ggplot2 with sf\n\nmapping-fall-foliage-with-sf"
  },
  {
    "objectID": "session_11.html#learning-objectives",
    "href": "session_11.html#learning-objectives",
    "title": "11  Working with US Census Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nProvide an overview of US Census data\nIntroduce the main functions of the tidycensus package to be able to work with census data\nReview data wrangling function to get census data ready for analysis"
  },
  {
    "objectID": "session_12.html#learning-objectives",
    "href": "session_12.html#learning-objectives",
    "title": "12  Working with Text Data in R",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDescribe principles of tidy text\nEmploy strategies to wrangle unstructured text data into a tidy format using the tidytext package\nBecome familiar with text analysis (or text mining) methods and when to use them\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThis lesson has been adapted from the following resources:\n\nWelcome to Text Mining with R by Julia Silge and David Robinson. Julia and David are also the developers of the tidytext package.\nSection 7.3: (R-) Workflow for Text Analysis from Computational Social Science: Theory & Application, Version: 17 June, 2021 by Paul C. Bauer"
  },
  {
    "objectID": "session_12.html#what-is-text-data",
    "href": "session_12.html#what-is-text-data",
    "title": "12  Working with Text Data in R",
    "section": "12.1 What is text data?",
    "text": "12.1 What is text data?\nText data is information stored as character or string data types. It comes in various different forms including books, emails, social media posts, interview transcripts, newspapers, government reports, and much more.\n\n12.1.1 How do we talk about text data?\nHere is a list of text data or text analysis terms we’ll be referring to throughout this lesson. Note this is not a comprehensive list of text analysis terms that are used beyond this lesson.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nCorpus (corpora, plural)\nCollection or database of text or multiple texts. These types of objects typically contain raw strings annotated with additional metadata and details.\n\n\nDocument-term matrix\nRepresents the relationship between terms and documents, where each row stands for a term and each column for a document, and an entry is the number of occurrences of the term in the document.\n\n\nNatural Language Processing (NLP)\nNLP is an interdisciplinary field used in computer science, data science, linguistics, and others to analyze, categorize, and work with computerized text.\n\n\nString\nSpecific type of data whose values are enclosed within a set of quotes. Typically values or elements are characters (e.g. “Hello World!”).\n\n\nText analysis\nThe process of deriving high-quality information or patterns from text through evaluation and interpretation of the output. Also referred to as “text mining” or “text analytics”.\n\n\nToken\nA meaningful unit of text, such as a word, to use for analysis.\n\n\nTokenization\nThe process of splitting text into tokens.\n\n\n\n\n\n12.1.2 How is text data used in the environmental field?\nAs our knowledge about the environmental world grows, researchers will need new computational approaches for working with text data because reading and identifying all the relevant literature for literature syntheses is becoming an increasingly difficult task.\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Beyond literature syntheses, quantitative text analysis tools are extremely valuable for efficiently extracting information from texts and other text mining or text analysis tasks. ======= Beyond literature syntheses, quantitative text analysis tools are extremely valuable for efficiently extracting information from texts and other text mining or text analysis tasks. &gt;&gt;&gt;&gt;&gt;&gt;&gt; bb20f6f8311df6dfd89cc170ff212e980fc9a258"
  },
  {
    "objectID": "session_12.html#what-is-tidy-text-data",
    "href": "session_12.html#what-is-tidy-text-data",
    "title": "12  Working with Text Data in R",
    "section": "12.2 What is tidy text data?",
    "text": "12.2 What is tidy text data?\nLet’s recall what are the three tidy data principles:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nKeeping that in mind, Silge and Robinson define the tidy text format as being a table with one-token-per-row.\nThis one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix.\nFor tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph.\nBy using tidy data principles, we can apply many “tidy” R packages including dpylr, tidyr, ggplot2, and more.\n\n12.2.1 What is the tidytext R package?\n\n\n\n\n\n\n\n\ntidytext is a package that applies the tidy principles to analyzing text. \nThe package contains many useful functions to wrangle text data into tidy formats. It has been built considering other text mining R packages so that it’s easy to switch between text mining tools (e.g. quanteda, stringr, wordcloud2)."
  },
  {
    "objectID": "session_12.html#exercise-tidy-text-workflow",
    "href": "session_12.html#exercise-tidy-text-workflow",
    "title": "12  Working with Text Data in R",
    "section": "12.3 Exercise: Tidy Text Workflow",
    "text": "12.3 Exercise: Tidy Text Workflow\n\nWe are going to use the gutenbergr package to access public domain texts from Project Gutenberg (a library of free eBooks). We’ll then use the tidytext, dyplr and ggplot2 packages to practice the tidy text workflow.\nBreak out into groups and then follow the exercise setup and instructions.\n\n\n\n\n\n\nSetup and Instructions\n\n\n\n\nCreate a new qmd file and title it “Intro to Text Data”, name yourself as the author, and then save the file as intro-text-data.qmd.\nCreate a new code chunk and attach the following libraries:\n\n\nlibrary(gutenbergr) # access public domain texts from Project Gutenberg\nlibrary(tidytext) # text mining using tidy tools\nlibrary(dplyr) # wrangle data\nlibrary(ggplot2) # plot data\n\n\nDepending on which group you’re in, use one of the following public domain texts:\n\n\n# Group A\ngutenberg_works(title == \"Dracula\") # dracula text\n\n# Group B\ngutenberg_works(title == \"Frankenstein; Or, The Modern Prometheus\") # frankenstein text\n\n# Group C\ngutenberg_works(title == \"Carmilla\") # carmilla text\n\n\nGet the id number from the gutenberg_works() function so that you can download the text as a corpus using the function gutenberg_download(). Save the corpus to an object called {book-title}_corp. View the object - is the data in a tidy format?\nTokenize the corpus data using unnest_tokens(). Take a look at the data - do we need every single token for our analysis?\nRemove “stop words” or words that can be safely removed or ignored without sacrificing the meaning of the sentence (e.g. “to”, “in”, “and”) using anti_join(). Take a look at the data - are you satisfied with your data? We won’t conduct any additional cleaning steps here, but consider how you would further clean the data.\nCalculate the top 10 most frequent words using the functions count() and slice_max().\nPlot the top 10 most frequent words using ggplot(). We reccommend creating either a bar plot using geom_col() or a lollipop plot using both geom_point() and geom_segment().\nBonus: Consider elements in theme() and improve your plot.\n\n\n\n\n12.3.1 Example using Ray Bradbury’s Asleep in Armageddon\nThe code chunks below follows the instructions from above using Ray Bradbury’s Asleep in Armageddon.\n\n# get id number\ngutenberg_works(title == \"Asleep in Armageddon\")\n\n# A tibble: 1 × 8\n  gutenberg_id title     author gutenberg_author_id language gutenberg_bookshelf\n         &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              \n1        63827 Asleep i… Bradb…               41269 en       &lt;NA&gt;               \n# ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;\n\n\n\n\nSteps 4-7 Code\n# access text data using id number from `gutenberg_works()`\nbradbury_corp &lt;- gutenberg_download(63827)\n\n# tidy text data - unnest and remove stop words\ntidy_bradbury &lt;- bradbury_corp %&gt;% \n    unnest_tokens(word, text) %&gt;% \n    anti_join(stop_words, by = \"word\")\n\n# calculate top 10 most frequent words\ncount_bradbury &lt;- tidy_bradbury %&gt;%\n    count(word) %&gt;% \n    slice_max(n = 10, order_by = n)\n\n\n\n\nStep 8 Plot Code\n# visualize text data #\n# bar plot\nggplot(data = count_bradbury, aes(n, reorder(word, n))) +\n  geom_col() +\n    labs(x = \"Count\",\n         y = \"Token\")\n\n\n\n\n\n\n\nStep 9 Plot Code\n# visualize text data #\n# initial lollipop plot\n# ggplot(data = count_bradbury, aes(x=word, y=n)) +\n#     geom_point() +\n#     geom_segment(aes(x=word, xend=word, y=0, yend=n)) +\n#     coord_flip() +\n#     labs(x = \"Token\",\n#          y = \"Count\")\n\n# ascending order pretty lollipop plot\nggplot(data = count_bradbury, aes(x=reorder(word, n), y=n)) +\n    geom_point(color=\"cyan4\") +\n    geom_segment(aes(x=word, xend=word, y=0, yend=n), color=\"cyan4\") +\n    coord_flip() +\n    labs(title = \"Top Ten Words in Ray Bradbury's Asleep in Armageddon\",\n         x = NULL,\n         y = \"Count\") +\n    theme_minimal() +\n    theme(\n        panel.grid.major.y = element_blank()\n    )"
  },
  {
    "objectID": "session_13.html#learning-objectives",
    "href": "session_13.html#learning-objectives",
    "title": "13  Reproducible Surveys",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nOverview of survey tools\nGenerating a reproducible survey report with Qualtrics"
  },
  {
    "objectID": "session_13.html#introduction",
    "href": "session_13.html#introduction",
    "title": "13  Reproducible Surveys",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nSurveys and questionnaires are commonly used research methods within social science and other fields. For example, understanding regional and national population demographics, income, and education as part of the National Census activity, assessing audience perspectives on specific topics of research interest (e.g. the work by Tenopir and colleagues on Data Sharing by Scientists), evaluation of learning deliverable and outcomes, and consumer feedback on new and upcoming products. These are distinct from the use of the term survey within natural sciences, which might include geographical surveys (“the making of measurement in the field from which maps are drawn”), ecological surveys (“the process whereby a proposed development site is assess to establish any environmental impact the development may have”) or biodiversity surveys (“provide detailed information about biodiversity and community structure”) among others.\nAlthough surveys can be conducted on paper or verbally, here we focus on surveys done via software tools. Needs will vary according to the nature of the research being undertaken. However, there is fundamental functionality that survey software should provide including:\n\nThe ability to create and customize questions\nThe ability to include different types of questions\nThe ability to distribute the survey and manage response collection\nThe ability to collect, summarize, and (securely) store response data\n\nMore advanced features can include:\n\nVisual design and templates - custom design might include institutional branding or aesthetic elements. Templates allow you to save these designs and apply to other surveys\nQuestion piping - piping inserts answers from previous questions into upcoming questions and can personalize the survey experience for users\nSurvey logic - with question logic and skip logic you can control the inclusion / exclusion of questions based on previous responses\nRandomization - the ability to randomize the presentation of questions within (blocks of) the survey\nBranching - this allows for different users to take different paths through the survey. Similar to question logic but at a bigger scale\nLanguage support - automated translation or multi-language presentation support\nShared administration - enables collaboration on the survey and response analysis\nSurvey export - ability to download (export) the survey instrument\nReports - survey response visualization and reporting tools\nInstitutional IRB approved - institutional IRB policy may require certain software be used for research purposes\n\nCommonly used survey software within academic (vs market) research include Qualtrics, Survey Monkey and Google Forms. Both Qualtrics and Survey Monkey are licensed (with limited functionality available at no cost) and Google forms is free."
  },
  {
    "objectID": "session_13.html#building-workflows-using-qualtrics",
    "href": "session_13.html#building-workflows-using-qualtrics",
    "title": "13  Reproducible Surveys",
    "section": "13.2 Building workflows using Qualtrics",
    "text": "13.2 Building workflows using Qualtrics\nIn this lesson we will use the qualtRics package to reproducible access some survey results set up for this course.\n\n13.2.1 Survey Instrument\nThe survey is very short, only four questions. The first question is on it’s own page and is a consent question, after a couple of short paragraphs describing what the survey is, it’s purpose, how long it will take to complete, and who is conducting it. This type of information is required if the survey is governed by an IRB, and the content will depend on the type of research being conducted. In this case, this survey is not for research purposes, and thus is not governed by IRB, but we still include this information as it conforms to the Belmont Principles. The Belmont Principles identify the basic ethical principles that should underlie research involving human subjects.\n\nThe three main questions of the survey have three types of responses: a multiple choice answer, a multiple choice answer which also includes an “other” write in option, and a free text answer. We’ll use the results of this survey, which was sent out to NCEAS staff to fill out, to learn about how to create a reproducible survey report.\n\nFirst, open a new Quarto document and add a chunk to load the libraries we’ll need for this lesson:\n\nlibrary(qualtRics)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(dplyr)\n\nNext, we need to set the API credentials. This function modifies the .Renviron file to set your API key and base URL so that you can access Qualtrics programmatically.\nThe API key is as good as a password, so care should be taken to not share it publicly. For example, you would never want to save it in a script. The function below is the rare exception of code that should be run in the console and not saved. It works in a way that you only need to run it once, unless you are working on a new computer or your credentials changed. Note that in this book, we have not shared the actual API key, for the reasons outlined above. You should have an e-mail with the API key in it. Copy and paste it as a string to the api_key argument in the function below:\n\nqualtrics_api_credentials(api_key = \"\", base_url = \"ucsb.co1.qualtrics.com\", install = TRUE, overwrite = T)\n\n\n\n\n\n\n\nAside note\n\n\n\nThe .Renviron file is a special user controlled file that can create environment variables. Every time you open Rstudio, the variables in your environment file are loaded as…environment variables! Environment variables are named values that are accessible by your R process. They will not show up in your environment pane, but you can get a list of all of them using Sys.getenv(). Many are system defaults.\nTo view or edit your .Renviron file, you can use usethis::edit_r_environ().\n\n\nTo get a list of all the surveys in your Qualtrics instance, use the all_surveys function.\n\nsurveys &lt;- all_surveys()\nkable(surveys) %&gt;%\n    kable_styling()\n\nThis function returns a list of surveys, in this case only one, and information about each, including an identifier and it’s name. We’ll need that identifier later, so let’s go ahead and extract it using base R from the data frame.\n\ni &lt;- which(surveys$name == \"Survey for Data Science Training\")\nid &lt;- surveys$id[i]\n\nYou can retrieve a list of the questions the survey asked using the survey_questions function and the survey id.\n\nquestions &lt;- survey_questions(id)\nkable(questions) %&gt;%\n    kable_styling()\n\nThis returns a data.frame with one row per question with columns for question id, question name, question text, and whether the question was required. This is helpful to have as a reference for when you are looking at the full survey results.\nTo get the full survey results, run fetch_survey with the survey id.\n\nsurvey_results &lt;- fetch_survey(id)\n\nThe survey results table has tons of information in it, not all of which will be relevant depending on your survey. The table has identifying information for the respondents (eg: ResponseID, IPaddress, RecipientEmail, RecipientFirstName, etc), much of which will be empty for this survey since it is anonymous. It also has information about the process of taking the survey, such as the StartDate, EndDate, Progress, and Duration. Finally, there are the answers to the questions asked, with columns labeled according to the qname column in the questions table (eg: Q1, Q2, Q3). Depending on the type of question, some questions might have multiple columns associated with them. We’ll have a look at this more closely in a later example.\n\n\n13.2.2 Question 2\nLet’s look at the responses to the second question in the survey, “How long have you been programming?” Remember, the first question was the consent question.\nWe’ll use the dplyr and tidyr tools we learned earlier to extract the information. Here are the steps:\n\nselect the column we want (Q1)\ngroup_by and summarize the values\n\n\nq2 &lt;- survey_results %&gt;% \n    select(Q2) %&gt;% \n    group_by(Q2) %&gt;% \n    summarise(n = n())\n\nWe can show these results in a table using the kable function from the knitr package:\n\nkable(q2, col.names = c(\"How long have you been programming?\",\n                        \"Number of responses\")) %&gt;%\n    kable_styling()\n\n\n\n13.2.3 Question 3\nFor question 3, we’ll use a similar workflow. For this question, however there are two columns containing survey answers. One contains the answers from the controlled vocabulary, the other contains any free text answers users entered.\nTo present this information, we’ll first show the results of the controlled answers as a plot. Below the plot, we’ll include a table showing all of the free text answers for the “other” option.\n\nq3 &lt;- survey_results %&gt;% \n    select(Q3) %&gt;% \n    group_by(Q3) %&gt;% \n    summarise(n = n())\n\n\nggplot(data = q3, \n       mapping = aes(x = Q3, y = n)) +\n    geom_col() +\n    labs(x = \"What language do you currently use most frequently?\", y = \"Number of reponses\") +\n    theme_minimal()\n\nNow we’ll extract the free text responses:\n\nq3_text &lt;- survey_results %&gt;% \n    select(Q3_7_TEXT) %&gt;% \n    drop_na()\n\nkable(q3_text, col.names = c(\"Other responses to 'What language do you currently use mose frequently?'\")) %&gt;% \n    kable_styling()\n\n\n\n13.2.4 Question 4\nThe last question is just a free text question, so we can just display the results as is.\n\nq4 &lt;- survey_results %&gt;% \n    select(Q4) %&gt;% \n    rename(`What data science tool or language are you most excited to learn next?` = Q4) %&gt;% \n    drop_na()\n\nkable(q4, col.names = \"What data science tool or language are you most excited to learn next?\") %&gt;% \n    kable_styling()"
  },
  {
    "objectID": "session_13.html#other-survey-tools",
    "href": "session_13.html#other-survey-tools",
    "title": "13  Reproducible Surveys",
    "section": "13.3 Other survey tools",
    "text": "13.3 Other survey tools\n\n13.3.1 Google forms\nGoogle forms can be a great way to set up surveys, and it is very easy to interact with the results using R. The benefits of using google forms are a simple interface and easy sharing between collaborators, especially when writing the survey instrument.\nThe downside is that google forms has far fewer features than Qualtrics in terms of survey flow and appearance.\nTo show how we can link R into our survey workflows, I’ve set up a simple example survey here.\nI’ve set up the results so that they are in a new spreadsheet here:. To access them, we will use the googlesheets4 package.\nFirst, open up a new R script and load the googlesheets4 library:\n\nlibrary(googlesheets4)\n\nNext, we can read the sheet in using the same URL that you would use to share the sheet with someone else. Right now, this sheet is public\n\nresponses &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1CSG__ejXQNZdwXc1QK8dKouxphP520bjUOnZ5SzOVP8/edit?usp=sharing\")\n\n✔ Reading from \"Example Survey Form (Responses)\".\n\n\n✔ Range 'Form Responses 1'.\n\n\nThe first time you run this, you should get a popup window in your web browser asking you to confirm that you want to provide access to your google sheets via the tidyverse (googlesheets) package.\nMy dialog box looked like this:\n\nMake sure you click the third check box enabling the Tidyverse API to see, edit, create, and delete your sheets. Note that you will have to tell it to do any of these actions via the R code you write.\nWhen you come back to your R environment, you should have a data frame containing the data in your sheet! Let’s take a quick look at the structure of that sheet.\n\ndplyr::glimpse(responses)\n\nRows: 10\nColumns: 5\n$ Timestamp                                              &lt;dttm&gt; 2022-04-15 13:…\n$ `To what degree did the event meet your expectations?` &lt;chr&gt; \"Met expectatio…\n$ `To what degree did your knowledge improve?`           &lt;chr&gt; \"Increase\", \"Si…\n$ `What did you like most about the event?`              &lt;chr&gt; \"the cool instr…\n$ `What might you change about the event?`               &lt;chr&gt; \"more snacks\", …\n\n\nSo, now that we have the data in a standard R data.frame, we can easily summarize it and plot results. By default, the column names in the sheet are the long fully descriptive questions that were asked, which can be hard to type. We can save those questions into a vector for later reference, like when we want to use the question text for plot titles.\n\nquestions &lt;- colnames(responses)[2:5]\ndplyr::glimpse(questions)\n\n chr [1:4] \"To what degree did the event meet your expectations?\" ...\n\n\nWe can make the responses data frame more compact by renaming the columns of the vector with short numbered names of the form Q1. Note that, by using a sequence, this should work for sheets from just a few columns to many hundreds of columns, and provides a consistent question naming convention.\n\nnames(questions) &lt;- paste0(\"Q\", seq(1:4))\n\nquestions\n\n                                                    Q1 \n\"To what degree did the event meet your expectations?\" \n                                                    Q2 \n          \"To what degree did your knowledge improve?\" \n                                                    Q3 \n             \"What did you like most about the event?\" \n                                                    Q4 \n              \"What might you change about the event?\" \n\ncolnames(responses) &lt;- c(\"Timestamp\", names(questions))\ndplyr::glimpse(responses)\n\nRows: 10\nColumns: 5\n$ Timestamp &lt;dttm&gt; 2022-04-15 13:48:58, 2022-04-15 13:49:43, 2022-04-15 13:50:…\n$ Q1        &lt;chr&gt; \"Met expectations\", \"Above expectations\", \"Above expectation…\n$ Q2        &lt;chr&gt; \"Increase\", \"Significant increase\", \"Significant increase\", …\n$ Q3        &lt;chr&gt; \"the cool instructors\", \"R is rad!\", \"everything\", \"the pizz…\n$ Q4        &lt;chr&gt; \"more snacks\", \"no pineapple pizza!\", \"nothing\", \"needs more…\n\n\nNow that we’ve renamed our columns, let’s summarize the responses for the first question. We can use the same pattern that we usually do to split the data from Q1 into groups, then summarize it by counting the number of records in each group, and then merge the count of each group back together into a summarized data frame. We can then plot the Q1 results using ggplot:\n\nq1 &lt;- responses %&gt;% \n    dplyr::select(Q1) %&gt;% \n    dplyr::group_by(Q1) %&gt;% \n    dplyr::summarise(n = dplyr::n())\n\nggplot2::ggplot(data = q1, mapping = aes(x = Q1, y = n)) +\n    geom_col() +\n    labs(x = questions[1], \n         y = \"Number of reponses\",\n         title = \"To what degree did the course meet expectations?\") +\n    theme_minimal()\n\n\nBypassing authentication for public sheets\nIf you don’t want to go through a little interactive dialog every time you read in a sheet, and your sheet is public, you can run the function gs4_deauth() to access the sheet as a public user. This is helpful for cases when you want to run your code non-interactively. This is actually how I set it up for this book to build!\n\n\n\n\n13.3.2 Survey Monkey\nSimilar to Qualtrics and qualtRics, there is an open source R package for working with data in Survey Monkey: Rmonkey. However, the last updates were made 5 years ago, an eternity in the software world, so it may or may not still function as intended.\nThere are also commercial options available. For example, cdata have a driver and R package that enable access to an analysis of Survey Monkey data through R."
  },
  {
    "objectID": "session_16.html",
    "href": "session_16.html",
    "title": "15  Shiny",
    "section": "",
    "text": "15.0.1 Learning Objectives\nIn this lesson we will:\n\nreview the capabilities in Shiny applications\nlearn about the basic layout for Shiny interfaces\nlearn about the server component for Shiny applications\nbuild a simple shiny application for interactive plotting\n\n\n\n15.0.2 Overview\nShiny is an R package for creating interactive data visualizations embedded in a web application that you and your colleagues can view with just a web browser. Shiny apps are relatively easy to construct, and provide interactive features for letting others share and explore data and analyses.\nThere are some really great examples of what Shiny can do on the RStudio webite like this one exploring movie metadata. A more scientific example is a tool from the SASAP project exploring proposal data from the Alaska Board of Fisheries. There is also an app for Delta monitoring efforts.\n\nMost any kind of analysis and visualization that you can do in R can be turned into a useful interactive visualization for the web that lets people explore your data more intuitively But, a Shiny application is not the best way to preserve or archive your data. Instead, for preservation use a repository that is archival in its mission like the KNB Data Repository, Zenodo, or Dryad. This will assign a citable identifier to the specific version of your data, which you can then read in an interactive visualiztion with Shiny.\nFor example, the data for the Alaska Board of Fisheries application is published on the KNB and is citable as:\nMeagan Krupa, Molly Cunfer, and Jeanette Clark. 2017. Alaska Board of Fisheries Proposals 1959-2016. Knowledge Network for Biocomplexity. doi:10.5063/F1QN652R.\nWhile that is the best citation and archival location of the dataset, using Shiny, one can also provide an easy-to-use exploratory web application that you use to make your point that directly loads the data from the archival site. For example, the Board of Fisheries application above lets people who are not inherently familiar with the data to generate graphs showing the relationships between the variables in the dataset.\nWe’re going to create a simple shiny app with two sliders so we can interactively control inputs to an R function. These sliders will allow us to interactively control a plot.\n\n\n15.0.3 Create a sample shiny application\n\nFile &gt; New &gt; Shiny Web App…\nSet some fields: \n\nName it “myapp” or something else\nSelect “Single File”\nChoose to create it in a new folder called ‘shiny-demo’\nClick Create\n\n\nRStudio will create a new file called app.R that contains the Shiny application.\nRun it by choosing Run App from the RStudio editor header bar. This will bring up the default demo Shiny application, which plots a histogram and lets you control the number of bins in the plot.\n\nNote that you can drag the slider to change the number of bins in the histogram.\n\n\n15.0.4 Shiny architecture\nA Shiny application consists of two functions, the ui and the server. The ui function is responsible for drawing the web page, while the server is responsible for any calculations and for creating any dynamic components to be rendered.\nEach time that a user makes a change to one of the interactive widgets, the ui grabs the new value (say, the new slider min and max) and sends a request to the server to re-render the output, passing it the new input values that the user had set. These interactions can sometimes happen on one computer (e.g., if the application is running in your local RStudio instance). Other times, the ui runs on the web browser on one computer, while the server runs on a remote computer somewhere else on the Internet (e.g., if the application is deployed to a web server).\n\n\n\n15.0.5 Interactive scatterplots\nLet’s modify this application to plot Yolo bypass secchi disk data in a time-series, and allow aspects of the plot to be interactively changed.\n\n15.0.5.1 Load data for the example\nUse this code to load the data at the top of your app.R script. Note we are using contentId again, and we have filtered for some species of interest.\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\nnames(delta_data)\n\n\n\n15.0.5.2 Add a simple timeseries using ggplot\nWe know there has been a lot of variation through time in the delta, so let’s plot a time-series of Secchi depth. We do so by switching out the histogram code for a simple ggplot, like so:\n\nserver &lt;- function(input, output) {\n    \n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            theme_light()\n    })\n}\n\nIf you now reload the app, it will display the simple time-series instead of the histogram. At this point, we haven’t added any interactivity.\nIn a Shiny application, the server function provides the part of the application that creates our interactive components, and returns them to the user interface (ui) to be displayed on the page.\n\n\n15.0.5.3 Add sliders to set the start and end date for the X axis\nTo make the plot interactive, first we need to modify our user interface to include widgits that we’ll use to control the plot. Specifically, we will add a new slider for setting the minDate parameter, and modify the existing slider to be used for the maxDate parameter. To do so, modify the sidebarPanel() call to include two sliderInput() function calls:\n\nsidebarPanel(\n    sliderInput(\"minDate\",\n                \"Min Date:\",\n                min = as.Date(\"1998-01-01\"),\n                max = as.Date(\"2020-01-01\"),\n                value = as.Date(\"1998-01-01\")),\n    sliderInput(\"maxDate\",\n                \"Max Date:\",\n                min = as.Date(\"1998-01-01\"),\n                max = as.Date(\"2020-01-01\"),\n                value = as.Date(\"2005-01-01\"))\n)\n\nIf you reload the app, you’ll see two new sliders, but if you change them, they don’t make any changes to the plot. Let’s fix that.\n\n\n15.0.5.4 Connect the slider values to the plot\nFinally, to make the plot interactive, we can use the input and output variables that are passed into the server function to access the current values of the sliders. In Shiny, each UI component is given an input identifier when it is created, which is used as the name of the value in the input list. So, we can access the minimum depth as input$minDate and the max as input$maxDate. Let’s use these values now by adding limits to our X axis in the ggplot:\n\n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$minDate,input$maxDate)) +\n            theme_light()\n\nAt this point, we have a fully interactive plot, and the sliders can be used to change the min and max of the Depth axis.\n\nLooks so shiny!\n\n\n15.0.5.5 Reversed Axes?\nWhat happens if a clever user sets the minimum for the X axis at a greater value than the maximum? You’ll see that the direction of the X axis becomes reversed, and the plotted points display right to left. This is really an error condition. Rather than use two independent sliders, we can modify the first slider to output a range of values, which will prevent the min from being greater than the max. You do so by setting the value of the slider to a vector of length 2, representing the default min and max date for the slider, such as c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")). So, delete the second slider, rename the first, and provide a vector for the value, like this:\n\nsliderInput(\"date\",\n            \"Date:\",\n            min = as.Date(\"1998-01-01\"),\n            max = as.Date(\"2020-01-01\"),\n            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n)\n\nNow, modify the ggplot to use this new date slider value, which now will be returned as a vector of length 2. The first element of the depth vector is the min, and the second is the max value on the slider.\n\n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            theme_light()\n\n\n\n\n\n15.0.6 Extending the user interface with dynamic plots\nIf you want to display more than one plot in your application, and provide a different set of controls for each plot, the current layout would be too simple. Next we will extend the application to break the page up into vertical sections, and add a new plot in which the user can choose which variables are plotted. The current layout is set up such that the FluidPage contains the title element, and then a sidebarLayout, which is divided horizontally into a sidebarPanel and a mainPanel.\n\n\n15.0.6.1 Vertical layout\nTo extend the layout, we will first nest the existing sidebarLayout in a new verticalLayout, which simply flows components down the page vertically. Then we will add a new sidebarLayout to contain the bottom controls and graph.\n\nThis mechanism of alternately nesting vertical and horizontal panels can be used to segment the screen into boxes with rules about how each of the panels is resized, and how the content flows when the browser window is resized. The sidebarLayout works to keep the sidebar about 1/3 of the box, and the main panel about 2/3, which is a good proportion for our controls and plots. Add the verticalLayout, and the second sidebarLayout for the second plot as follows:\n\n    verticalLayout(\n        # Sidebar with a slider input for depth axis\n        sidebarLayout(\n            sidebarPanel(\n                sliderInput(\"date\",\n                            \"Date:\",\n                            min = as.Date(\"1998-01-01\"),\n                            max = as.Date(\"2020-01-01\"),\n                            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n            ),\n            # Show a plot of the generated distribution\n            mainPanel(\n               plotOutput(\"distPlot\")\n            )\n        ),\n\n        tags$hr(),\n\n        sidebarLayout(\n            sidebarPanel(\n                selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n            ),\n\n            # Show a plot with configurable axes\n            mainPanel(\n                plotOutput(\"varPlot\")\n            )\n        ),\n        tags$hr()\n\nNote that the second sidebarPanel uses three selectInput elements to provide dropdown menus with the variable columns (cols) from our data frame. To manage that, we need to first set up the cols variable, which we do by saving the variables names from the delta_data data frame to a variable:\n\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\ncols &lt;- names(delta_data)\n\n\n\n15.0.6.2 Add the dynamic plot\nBecause we named the second plot varPlot in our UI section, we now need to modify the server to produce this plot. Its very similar to the first plot, but this time we want to use the selected variables from the user controls to choose which variables are plotted. These variable names from the $input are character strings, and so would not be recognized as symbols in the aes mapping in ggplot. As recommended by the tidyverse authors, we use the non-standard evaluation syntax of .data[[\"colname\"]] to access the variables.\n\n    output$varPlot &lt;- renderPlot({\n      ggplot(delta_data, aes(x = .data[[input$x_variable]],\n                             y = .data[[input$y_variable]],\n                             color = .data[[input$color_variable]])) +\n        geom_point(size = 4)+\n        theme_light()\n    })\n\n\n\n\n15.0.7 Finishing touches: data citation\nCiting the data that we used for this application is the right thing to do, and easy. You can add arbitrary HTML to the layout using utility functions in the tags list.\n\n    # Application title\n    titlePanel(\"Yolo Bypass Fish and Water Quality Data\"),\n    p(\"Data for this application are from: \"),\n    tags$ul(\n        tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n        )\n    ),\n    tags$br(),\n    tags$hr(),\n\nThe final application shows the data citation, the depth plot, and the configurable scatterplot in three distinct panels.\n\n\n\n15.0.8 Publishing Shiny applications\nOnce you’ve finished your app, you’ll want to share it with others. To do so, you need to publish it to a server that is set up to handle Shiny apps.\nYour main choices are:\n\nshinyapps.io (Hosted by RStudio)\n\nThis is a service offered by RStudio, which is initially free for 5 or fewer apps and for limited run time, but has paid tiers to support more demanding apps. You can deploy your app using a single button push from within RStudio.\n\nShiny server (On premises)\n\nThis is an open source server which you can deploy for free on your own hardware. It requires more setup and configuration, but it can be used without a fee.\n\nRStudio connect (On premises)\n\nThis is a paid product you install on your local hardware, and that contains the most advanced suite of services for hosting apps and RMarkdown reports. You can publish using a single button click from RStudio.\n\n\nA comparison of publishing features is available from RStudio.\n\n15.0.8.1 Publishing to shinyapps.io\nThe easiest path is to create an account on shinyapps.io, and then configure RStudio to use that account for publishing. Instructions for enabling your local RStudio to publish to your account are displayed when you first log into shinyapps.io:\n\nOnce your account is configured locally, you can simply use the Publish button from the application window in RStudio, and your app will be live before you know it!\n\n\n\n\n15.0.9 Summary\nShiny is a fantastic way to quickly and efficiently provide data exploration for your data and code. We highly recommend it for its interactivity, but an archival-quality repository is the best long-term home for your data and products. In this example, we used data drawn directly from the EDI repository in our Shiny app, which offers both the preservation guarantees of an archive, plus the interactive data exploration from Shiny. You can utilize the full power of R and the tidyverse for writing your interactive applications.\n\n\n15.0.10 Full source code for the final application\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n# read in the data from EDI\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\ncols &lt;- names(delta_data)\n    \n\n\n# Define UI for application that draws a two plots\nui &lt;- fluidPage(\n    \n    # Application title and data  source\n    titlePanel(\"Sacramento River floodplain fish and water quality dataa\"),\n    p(\"Data for this application are from: \"),\n    tags$ul(\n        tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n        )\n    ),\n    tags$br(),\n    tags$hr(),\n    \n    verticalLayout(\n        # Sidebar with a slider input for time axis\n        sidebarLayout(\n            sidebarPanel(\n                sliderInput(\"date\",\n                            \"Date:\",\n                            min = as.Date(\"1998-01-01\"),\n                            max = as.Date(\"2020-01-01\"),\n                            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n            ),\n            # Show a plot of the generated timeseries\n            mainPanel(\n                plotOutput(\"distPlot\")\n            )\n        ),\n        \n        tags$hr(),\n        \n        sidebarLayout(\n            sidebarPanel(\n                selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n            ),\n            \n            # Show a plot with configurable axes\n            mainPanel(\n                plotOutput(\"varPlot\")\n            )\n        ),\n        tags$hr()\n    )\n)\n\n# Define server logic required to draw the two plots\nserver &lt;- function(input, output) {\n    \n    #  turbidity plot\n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            theme_light()\n    })\n    \n    # mix and  match plot\n    output$varPlot &lt;- renderPlot({\n      ggplot(delta_data, aes(x = .data[[input$x_variable]],\n                             y = .data[[input$y_variable]],\n                             color = .data[[input$color_variable]])) +\n        geom_point(size = 4) +\n        theme_light()\n    })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n15.0.11 A shinier app with tabs and a map!\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(shinythemes)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(snakecase)\n\n# read in the data from EDI\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName)) %&gt;% \n    rename(DissolvedOxygen = DO,\n           Ph = pH,\n           SpecificConductivity = SpCnd)\n\ncols &lt;- names(delta_data)\n\nsites &lt;- delta_data %&gt;% \n    distinct(StationCode, Latitude, Longitude) %&gt;% \n    drop_na() %&gt;% \n    st_as_sf(coords = c('Longitude','Latitude'), crs = 4269,  remove = FALSE)\n\n\n\n# Define UI for application\nui &lt;- fluidPage(\n    navbarPage(theme = shinytheme(\"flatly\"), collapsible = TRUE,\n               HTML('&lt;a style=\"text-decoration:none;cursor:default;color:#FFFFFF;\" class=\"active\" href=\"#\"&gt;Sacramento River Floodplain Data&lt;/a&gt;'), id=\"nav\",\n               windowTitle = \"Sacramento River floodplain fish and water quality data\",\n               \n               tabPanel(\"Data Sources\",\n                        verticalLayout(\n                            # Application title and data  source\n                            titlePanel(\"Sacramento River floodplain fish and water quality data\"),\n                            p(\"Data for this application are from: \"),\n                            tags$ul(\n                                tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                                        tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n                                )\n                            ),\n                            tags$br(),\n                            tags$hr(),\n                            p(\"Map of sampling locations\"),\n                            mainPanel(leafletOutput(\"map\"))\n                        )\n               ),\n               \n               tabPanel(\n                   \"Explore\",\n                   verticalLayout(\n                       mainPanel(\n                           plotOutput(\"distPlot\"),\n                           width =  12,\n                           absolutePanel(id = \"controls\",\n                                         class = \"panel panel-default\",\n                                         top = 175, left = 75, width = 300, fixed=TRUE,\n                                         draggable = TRUE, height = \"auto\",\n                                         sliderInput(\"date\",\n                                                     \"Date:\",\n                                                     min = as.Date(\"1998-01-01\"),\n                                                     max = as.Date(\"2020-01-01\"),\n                                                     value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n                                         \n                           )\n                       ),\n                       \n                       tags$hr(),\n                       \n                       sidebarLayout(\n                           sidebarPanel(\n                               selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                               selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                               selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n                           ),\n                           \n                           # Show a plot with configurable axes\n                           mainPanel(\n                               plotOutput(\"varPlot\")\n                           )\n                       ),\n                       tags$hr()\n                   )\n               )\n    )\n)\n\n# Define server logic required to draw the two plots\nserver &lt;- function(input, output) {\n    \n    \n    output$map &lt;- renderLeaflet({leaflet(sites) %&gt;% \n            addTiles() %&gt;% \n            addCircleMarkers(data = sites,\n                             lat = ~Latitude,\n                             lng = ~Longitude,\n                             radius = 10, # arbitrary scaling\n                             fillColor = \"gray\",\n                             fillOpacity = 1,\n                             weight = 0.25,\n                             color = \"black\",\n                             label = ~StationCode)\n    })\n    \n    #  turbidity plot\n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            labs(x = \"Sample Date\", y = \"Secchi Depth (m)\") +\n            theme_light()\n    })\n    \n    # mix and  match plot\n    output$varPlot &lt;- renderPlot({\n        ggplot(delta_data, mapping = aes(x = .data[[input$x_variable]],\n                                         y = .data[[input$y_variable]],\n                                         color = .data[[input$color_variable]])) +\n            labs(x = to_any_case(input$x_variable, case = \"title\"),\n                 y = to_any_case(input$y_variable, case = \"title\"),\n                 color = to_any_case(input$color_variable, case = \"title\")) +\n            geom_point(size=4) +\n            theme_light()\n    })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n15.0.12 Resources\n\nMain Shiny site\nOfficial Shiny Tutorial"
  },
  {
    "objectID": "session_17.html",
    "href": "session_17.html",
    "title": "16  Appendix",
    "section": "",
    "text": "Configuring Two-factor Authentication on GitHub"
  },
  {
    "objectID": "session_17.html#learning-objectives",
    "href": "session_17.html#learning-objectives",
    "title": "16  Appendix",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSuccessfully setup two-factor authentication on GitHub\nRecognize two-factor authentication jargon"
  },
  {
    "objectID": "session_17.html#why-set-up-two-factor-authentication-2fa",
    "href": "session_17.html#why-set-up-two-factor-authentication-2fa",
    "title": "16  Appendix",
    "section": "16.1 Why Set up Two-factor Authentication (2FA)",
    "text": "16.1 Why Set up Two-factor Authentication (2FA)\n\nPrevents unauthorized access\nStrengthens your web security, especially if you have a compromised password\nIt is an increasing requirement for most websites and online applications or services\n\nIn March 2023, GitHub announced that it will require 2FA for “all developers who contribute code on GitHub.com” (GitHub Blog). This rollout will be completed by the end of 2023.\nAll users have the flexibility to use their preferred 2FA method, including: TOTP, SMS, security keys, or GitHub Mobile app. GitHub strongly recommends using security keys and TOTPs. While SMS-based 2FA is available to use, it does not provide the same level of protection, and is no longer recommended under NIST (National Institute of Standards and Technology) 800-63B.\n\n16.1.1 Additional information about 2FA on GitHub:\n\nSecuring your account with two-factor authentication (2FA)\nConfiguring two-factor authentication\nRaising the bar for software security: GitHub 2FA begins March 13\nSoftware security starts with the developer: Securing developer accounts with 2FA\n\n\n\n\n\n\n\nFor Your Reference\n\n\n\nReview the Glossary table to see a comprehensive list of two-factor authentication related terms and definitions"
  },
  {
    "objectID": "session_17.html#steps-for-configuring-2fa-using-a-totp-app",
    "href": "session_17.html#steps-for-configuring-2fa-using-a-totp-app",
    "title": "16  Appendix",
    "section": "16.2 Steps for Configuring 2FA Using a TOTP App",
    "text": "16.2 Steps for Configuring 2FA Using a TOTP App\n\n\n\n\n\n\nAdditional Resource\n\n\n\nGitHub outlines these steps online in an article: Configuring two-factor authentication.\n\n\n\nDownload a TOTP app\nNavigate to your account Settings (click your profile photo in the top right-hand corner)\nIn the “Access” section, click “Password and Authenticate”\nIn the “Two-factor authentication” section, click Enable two-factor authentication\nUnder “Setup authenticator app”, either:\n\nScan the QR code with your TOTP app. After scanning, the app displays a six-digit code that you can enter on GitHub\nIf you can’t scan the QR code, click “enter this text code” to see a code that you can manually enter in your TOTP app instead\n\nOn GitHub, type the code into the field under “Verify the code from the app”\nUnder “Save your recovery codes”, click “Download” to download your recovery codes. Save them to a secure location because your recovery codes can help you get back into your account if you lose access.\n\nAfter saving your two-factor recovery codes, click “I have saved my recovery codes” to enable two-factor authentication for your account\n\nConfigure additional 2FA methods, if desired"
  },
  {
    "objectID": "session_17.html#glossary",
    "href": "session_17.html#glossary",
    "title": "16  Appendix",
    "section": "16.3 Glossary",
    "text": "16.3 Glossary\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nQuick Response (QR) Code\nA type of two-dimensional matrix barcode that contains specific information\n\n\nRecovery Code\nA unique code(s) used to reset passwords or regain access to accounts\n\n\nShort Message Service (SMS)\nA text messaging service that allows mobile devices to exchange short text messages\n\n\nTime-based one-time password (TOTP)\nA string of unique codes that changes based on time. Often, these appear as six-digit numbers that regenerate every 30 seconds\n\n\nTwo-factor Authentication (2FA)\nAn identity and access management security method that requires two forms of identification to access accounts, resources, or data"
  }
]