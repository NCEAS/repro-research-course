[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "",
    "text": "Overview"
  },
  {
    "objectID": "index.html#about-this-training",
    "href": "index.html#about-this-training",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "About this training",
    "text": "About this training\nNCEAS Open Science Synthesis training consists of three 1-week long workshops, geared towards early career researchers. Participants engage in a mix of lectures, exercises, and synthesis research groups to undertake synthesis while learning and implementing best practices for open data science."
  },
  {
    "objectID": "index.html#why-nceas",
    "href": "index.html#why-nceas",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Why NCEAS",
    "text": "Why NCEAS\nThe National Center for Ecological Analysis and Synthesis (NCEAS), a research affiliate of UCSB, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science.\nNCEAS has over 25 years of success with this model among working groups and environmental professionals. Together with the Delta Science Program and the Delta Stewardship Council we are excited to pass along skills, workflows, mindsets learn throughout the years.\n\nWeek 3: Scaling up and presenting synthesis\nOctober 23 – 27, 2023\n\nBig data workflows and parallel computing\nPresenting results using Shiny or flexdashboards\nRevisit reproducible and git workflows\nSynthesis presentations and next steps"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS with the support of DataONE, to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors for this version are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Halina Do-Linh, Matthew B. Jones, Camila Vargas Poulsen. 2023. Open Science Synthesis training Week 3. NCEAS Learning Hub & Delta Stewardship Council.\nAdditional contributors: Ben Bolker, Julien Brun, Amber E. Budden, Jeanette Clark, Samantha Csik, Stephanie Hampton, Natasha Haycock-Chavez, Samanta Katz, Julie Lowndes, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "session_02.html#learning-objective",
    "href": "session_02.html#learning-objective",
    "title": "2  Communicating Science",
    "section": "Learning Objective",
    "text": "Learning Objective\n\nOne"
  },
  {
    "objectID": "session_02.html#communicating-science",
    "href": "session_02.html#communicating-science",
    "title": "2  Communicating Science",
    "section": "2.1 Communicating Science",
    "text": "2.1 Communicating Science\nHow we communicate our science with the rest of the world matters! There are many different reasons and ways to engage and practice science communication. From writing a blog post as a creative outlet to posting in social media to make science accessible to a broader audience. Science communication is more and more becoming a formal responsibility within researchers as funding organizations encourage or require scientist to do outreach of their work (Borowiec (2023)).\nCommunication can come easy, but doing it well is hard. As scientist, it is important to spread the word of our discoveries, engage with non-scientist audiences, and build empathy and trust on what we do (PLOS SciCom). This way make sure the world understand how important, necessary and meaningful science is.\n(http://www.fromthelabbench.com/from-the-lab-bench-science-blog/2018/3/25/why-we-scientists-do-instagram)\n\n!!!!ADD TRANSIONTION!! As scientist our prime way of communicating our results is through peer-reviewed publications…\n\n2.1.1 Scholarly publications\n\n“The ingredients of good science are obvious—novelty of research topic, comprehensive coverage of the relevant literature, good data, good analysis including strong statistical support, and a thought-provoking discussion. The ingredients of good science reporting are obvious—good organization, the appropriate use of tables and figures, the right length, writing to the intended audience— do not ignore the obvious” - Bourne 2005\n\nPeer-reviewed publication is the primary mechanism for direct and efficient communication of research findings. Other scholarly communications include abstracts, technical reports, books and book chapters. These communications are largely directed towards students and peers; individuals learning about or engaged in the process of scientific research whether in a university, non-profit, agency, commercial or other setting.\nThe following tabling is a good summary of ‘10 Simple Rules’ for writing research papers (adapted from Zhang 2014, published in Budden and Michener, 2017)\n\n\n\n\n\n\n\n\nMake it a Driving Force\n\n“Design a project with an ultimate paper firmly in mind”\n\n\n\nLess Is More\n\n“Fewer but more significant papers serve both the research community and one’s career better than more papers of less significance”\n\n\n\nPick the Right Audience\n\n“This is critical for determining the organization of the paper and the level of detail of the story, so as to write the paper with the audience in mind.”\n\n\n\nBe Logical\n\n“The foundation of ‘’lively’’ writing for smooth reading is a sound and clear logic underlying the story of the paper.” “An effective tactic to help develop a sound logical flow is to imaginatively create a set of figures and tables, which will ultimately be developed from experimental results, and order them in a logical way based on the information flow through the experiments.”\n\n\n\nBe Thorough and Make It Complete\n\nPresent the central underlying hypotheses; interpret the insights gleaned from figures and tables and discuss their implications; provide sufficient context so the paper is self-contained; provide explicit results so readers do not need to perform their own calculations; and include self-contained figures and tables that are described in clear legends\n\n\n\nBe Concise\n\n“the delivery of a message is more rigorous if the writing is precise and concise”\n\n\n\nBe Artistic\n\n“concentrate on spelling, grammar, usage, and a ‘’lively’’ writing style that avoids successions of simple, boring, declarative sentences”\n\n\n\nBe Your Own Judge\n\nReview, revise and reiterate. “…put yourself completely in the shoes of a referee and scrutinize all the pieces—the significance of the work, the logic of the story, the correctness of the results and conclusions, the organization of the paper, and the presentation of the materials.”\n\n\n\nTest the Water in Your Own Backyard\n\n“…collect feedback and critiques from others, e.g., colleagues and collaborators.”\n\n\n\nBuild a Virtual Team of Collaborators\n\nTreat reviewers as collaborators and respond objectively to their criticisms and recommendations. This may entail redoing research and thoroughly re-writing a paper.\n\n\n\nEven though reporting our results in a clear and efficient way through scholarly publications is important, the ways scientist communicate is not how the rest of the rest of the world typically communicates. In a scientific paper, we establish credibility in the introduction and methods, provide detailed data and results, and then share the significance of our work in the discussion and conclusions. But the rest of the world leads with the impact, the take home message. An example of this are newspaper headlines.\n“But the rest of the world leads with the conclusions, because that’s what people want to know. What are your findings and why is this relevant to them? In other words, what’s your bottom line?”\n\n\n\n2.1.2 Other communications\nCommunicating your research outside of peer-reviewed journal articles is increasingly common, and important. These non academic communications can reach a more broad and diverse audience than traditional publications (and should be tailored for specific audience groups accordingly), are not subject to the same pay-walls as journal articles, and can augment and amplify scholarly publications. For example, this twitter announcement from Dr Heather Kropp, providing a synopsis of their synthesis publication in Environmental Research (note the reference to a repository where the data are located, though a DOI would be better).\n\nWhether this communication occurs through blogs, social media, or via interviews with others, developing practices to refine your messaging is critical for successful communication. One tool to support your communication practice is ‘The Message Box’ developed by COMPASS, an organization that helps scientists develop communications skills in order to share their knowledge and research across broad audiences without compromising the accuracy of their research."
  },
  {
    "objectID": "session_02.html#the-message-box",
    "href": "session_02.html#the-message-box",
    "title": "2  Communicating Science",
    "section": "2.2 The Message Box",
    "text": "2.2 The Message Box\n\n\nThe Message Box is a tool that helps researchers take the information they hold about their research and communicate it in a way that resonates with the chosen audience. It can be used to help prepare for interviews with journalists or employers, plan for a presentation, outline a paper or lecture, prepare a grant proposal, or clearly, and with relevancy, communicate your work to others. While the message box can be used in all these ways, you must first identify the audience for your communication.\n“Whether a journalist, a policymaker, a room of colleagues at a professional meeting, or a class of second-graders—doesn’t have deep knowledge of your subject matter. But that doesn’t mean you should explain everything you know in a fire hose of information. In fact, cognitive research tells us that the human brain can only absorb three to five pieces of information at a time. So prioritize what you share based on what your audience needs to know. Your goal as you fill out your Message Box is to identify the information that is critical to your audience—what really matters to them—and share that. Effective science communication is less about expounding on all the exciting details that you might want to convey, and more about knowing your audience and providing them with what they need or want to know from you.”\nThe Message Box comprises five sections to help you sort and distill your knowledge in a way that will resonate with your (chosen) audience.\nThe five sections of the Message Box are provided below. For a detailed explanation of the sections and guidance on how to use the Message Box, work through the Message Box Workbook\n\n2.2.0.1 Message Box Sections\nThe Issue\nThe Issue section in the center of the box identifies and describes the overarching issue or topic that you’re addressing in broad terms. It’s the big-picture context of your work. This should be very concise and clear; no more than a short phrase. You might find you revisit the Issue after you’ve filled out your Message Box, to see if your thinking on the overarching topic has changed since you started.\n\nDescribes the overarching issue or topic: Big Picture\nBroad enough to cover key points\nSpecific enough to set up what’s to come\nConcise and clear\n‘Frames’ the rest of the message box\n\nThe Problem\nThe Problem is the chunk of the broader issue that you’re addressing in your area of expertise. It’s your piece of the pie, reflecting your work and expert knowledge. Think about your research questions and what aspect of the specific problem you’re addressing would matter to your audience. The Problem is also where you set up the So What and describe the situation you see and want to address.\n\nThe part of the broader issue that your work is addressing\nBuilds upon your work and expert knowledge\nTry to focus on one problem per audience\nOften the Problem is your research question\nThis section sets you up for So What\n\nThe So What\nThe crux of the Message Box, and the critical question the COMPASS team seeks to help scientists answer, is “So what?” Why should your audience care? What about your research or work is important for them to know? Why are you talking to them about it? The answer to this question may change from audience to audience, and you’ll want to be able to adjust based on their interests and needs. We like to use the analogy of putting a message through a prism that clarifies the importance to different audiences. Each audience will be interested in different facets of your work, and you want your message to reflect their interests and accommodate their needs. The prism below includes a spectrum of audiences you might want to reach, and some of the questions they might have about your work.\n\nThis is the crux of the message box\nWhy should you audience care?\nWhat about your research is important for them to know?\nWhy are you talking to them about it?\n\n\nThe Solution\nThe Solution section outlines the options for solving the problem you identified. When presenting possible solutions, consider whether they are something your audience can influence or act upon. And remind yourself of your communication goals: Why are you communicating with this audience? What do you want to accomplish?\n\nOutlines the options for solving the Problem\nCan your audience influence or act upon this?\nThere may be multiple solutions\nMake sure your Solution relates back to the Problem. Edit one or both as needed\n\nThe Benefit\nIn the Benefit section, you list the benefits of addressing the Problem — all the good things that could happen if your Solution section is implemented. This ties into the So What of why your audience cares, but focuses on the positive results of taking action (the So What may be a negative thing — for example, inaction could lead to consequences that your audience cares about). If possible, it can be helpful to be specific here — concrete examples are more compelling than abstract. Who is likely to benefit, and where, and when?\n\nWhat are the benefits of addressing the Problem?\nWhat good things come from implementing your Solution?\nMake sure it connects with your So What\nBenefits and So What may be similar\n\nFinally, to make you message more memorable you should:\n\nSupport your message with data\nLimit the use of numbers and statistics\nUse specific examples\nCompare numbers to concepts, help people relate\nAvoid jargon\nLead with what you know\n\n\nIn addition to the Message Box Workbook, COMPASS have resources on how to increase the impact of your message (include important statistics, draw comparisons, reduce jargon, use examples), exercises for practicing and refining your message and published examples.\n\n\n2.2.1 Resources\n\n\n\nDataONE Webinar: Communication Strategies to Increase Your Impact from DataONE on Vimeo.\n\n\nBudden, AE and Michener, W (2017) Communicating and Disseminating Research Findings. In: Ecological Informatics, 3rd Edition. Recknagel, F, Michener, W (eds.) Springer-Verlag\nCOMPASS Core Principles of Science Communication\nExample Message Boxes\n\n\n\n\n\nBorowiec, Brittney G. 2023. “Ten Simple Rules for Scientists Engaging in Science Communication.” PLOS Computational Biology 19 (7): 1–8. https://doi.org/10.1371/journal.pcbi.1011251."
  },
  {
    "objectID": "session_03.html",
    "href": "session_03.html",
    "title": "3  Hands On Synthesis",
    "section": "",
    "text": "Note\n\n\n\nIn this session, groups convene to collaborate on their respective synthesis projects. The objective for the week is to establish a well-defined roadmap for advancing each project and determine the methods for effectively communicating the results."
  },
  {
    "objectID": "session_04.html#learning-objectives",
    "href": "session_04.html#learning-objectives",
    "title": "4  Flexdashboard",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCreate and design customized dashboards using the R package flexdashboard\nBecome familiar with different flexdashboard components and flexdashboard syntax\nApply Markdown syntax and Shiny elements to create visually appealing and interactive flexdashboards"
  },
  {
    "objectID": "session_04.html#what-is-a-flexdashboard",
    "href": "session_04.html#what-is-a-flexdashboard",
    "title": "4  Flexdashboard",
    "section": "4.1 What is a flexdashboard?",
    "text": "4.1 What is a flexdashboard?\nflexdashboard is an R package from RStudio that is built on top of R Markdown and Shiny. It allows us to create flexible, interactive dashboards using simple Markdown documents and syntax. Flexdashboards are designed to be easy to create, and support a wide variety of visualizations and interactive components. We can incorporate Shiny widgets and functionality into flexdashboards, making it a powerful tool for creating interactive reports and dashboards that can be shared with others.\n\n\n\n\n\n\nFlexdashboard is for R Markdown Only\n\n\n\nflexdashboard is only compatible with R Markdown documents, meaning we can’t use Quarto markdown documents (any files ending with .qmd). And that’s okay! Remember just because Quarto exists now, doesn’t mean R Markdown is going away or won’t be maintained - see Yihui Xie’s blog post With Quarto Coming, is R Markdown Going Away? No..\nIf you’re invested in only using Quarto tools - keep an eye on this discussion. Quarto developers are actively working on a dashboarding feature for Quarto as well."
  },
  {
    "objectID": "session_04.html#flexdashboard-vs-shiny",
    "href": "session_04.html#flexdashboard-vs-shiny",
    "title": "4  Flexdashboard",
    "section": "4.2 Flexdashboard vs Shiny",
    "text": "4.2 Flexdashboard vs Shiny\n\n\n\n\n\n\nWhen should I make a flexdashboard?\n\n\n\nFlexdashboards are great for creating lightweight interactive dashboards that require minimal coding expertise (must be familiar with Markdown!). Ultimately, it depends on what your final product is and what skillset your team has. Check out the diagram below and see what scenarios resonate best with you and your project goals.\n\n\n\n\n\n\nflowchart TD\n    A[Goal: Create a web-based application for data interaction]\n    A --&gt; B{Shiny App}\n    A --&gt; C{Flexdashboard}\n    C --&gt; D(Build a Flexdashboard if:)\n    B --&gt; E(Build a Shiny App if:)\n    D --&gt; F[Interested in quickly creating a dashboard prototype]\n    F --&gt; G[Have a preference for R Markdown]\n    G --&gt; H[There are non-programmers who need to create or maintain dashboards]\n    H --&gt; I[Want to blend narrative text with interactivity]\n    I --&gt; J[Prefer a simpler, code-light approach]\n    J --&gt; K[Dashboard requirements are relatively static]\n    E --&gt; L[Dashboard requires a highly customized user interface]\n    L --&gt; M[Dashboard needs to perform complex data analysis with user inputs]\n    M --&gt; N[Dashboard requires real-time data updates]\n    N --&gt; O[There are programmers familiar with reactive programming in R to create or maintain dashboards]\n    O --&gt; P[Dashboard requires a multi-page app with navigation]"
  },
  {
    "objectID": "session_04.html#flexdashboard-layout-features",
    "href": "session_04.html#flexdashboard-layout-features",
    "title": "4  Flexdashboard",
    "section": "4.3 Flexdashboard Layout + Features",
    "text": "4.3 Flexdashboard Layout + Features\nNow let’s familiarize ourselves with how an .Rmd is structured to create a flexdashboard and what the dashboard output looks like using the default template.\nThere are a two default templates for flexdashboard in RStudio - one with a theme and one without. We’ll first look at the template without a theme. To create a flexdashboard .Rmd from a template click:\nFile -&gt; New File -&gt; R Markdown -&gt; From Template -&gt; Flex Dashboard\n\n\n\n\n\n\n\nFlexdashboard Syntax\nIn the flexdashboard template to create the different sections in the dashboard, dashes (---) and equal signs (===) are being used (we’ll see the equal signs in action during the demo). The series of dashes and equal signs were a design choice by the flexdashboard creators to make the different sections stand out in the .Rmd, but are not mandatory to use.\n\n\n\n\n\n\nThe rule of thumb is that level-one headers create pages, level-two headers create columns or rows, and level-three headers create boxes.\n\n\n\n\n\n\n\n\n\n\nFlexdashboard Syntax\nEquivalent Markdown Header Syntax\n\n\n\n\nPage\n==========================\n# Page\n\n\nColumn\n--------------------------\n## Column\n\n\n### Box\n### Box\n\n\n\n\n\nFlexdashboard Attributes\nIn flexdashboard, we can add certain attributes to columns, rows, and boxes. This is similar to adding attributes to headings in typical .Rmd documents. For example, if we didn’t want a subheading to be numbered in a rendered HTML of a .Rmd, we would use ## My Subheading {.unnumbered}.\nIn both typical .Rmd and flexdashboard, the syntax for attributes is {key=value} or {.attribute}.\nSome attributes to add to columns, rows, or boxes include:\n\n\n\n\n\n\n\n{data-width=} and {data-height=}\nboth of these attributes set the relative size of columns, rows, and boxes. See complete size documentation on the flexdashboard website\n\n\n{data-orientation=}\nsets the dashboard layout to either rows or columns. This is a global option set in the YAML. However, if your dashboard has multiple pages and you want to specify the orientation for each page, remove orientation: from the YAML and use this attribute instead\n\n\n{.tabset}\ndivide columns, rows, or charts into tabs\n\n\n{.sidebar}\ncreates a sidebar on the left side. This sidebar is typically used to place Shiny inputs and is an optional step to add Shiny elements to a flexdashboard. See full documentation and steps in section 5.3.1 Getting Started in the R Markdown: Definitive Guide\n\n\n{data-navmenu=\"name of page\"}\nthis attribute creates a new navigation bar heading with the specified page as an item in a drop-down menu. When clicked, the menu item takes you to the associated page. For example, if the syntax is # Foo {data-navmenu=\"Bar\"}, “Bar” becomes a new heading in the navigation bar, and “Foo” is a page with dashboard components listed as a drop-down menu item under “Bar”\n\n\n{.hidden}\nexcludes a specific page from the navigation bar\n\n\n\n\n\n\n\n\n\nTo add multiple attributes within a set of curly braces {} by separating the attributes by either a space or a comma.\n\n\n\n\n\nFlexdashboard Components\nThe different components that can be added to a flexdashboard are:\n\n\n\n\n\n\n\nHTML Widgets\nincorporates JavaScript data visualization tools and libraries into a flexdashboard. This includes features like interactive plots, maps and more. At this time there are 130 htmlwidgets available to use in R, check out the gallery of widgets\n\n\nR Graphics\nany chart, plot or graph that is created using any R package\n\n\nTabular Data\nadd tables using knitr::kable() for simple tables or use the DT package for interactive tables\n\n\nValue Boxes\nuse valueBox() to display single values along with a title and optional icon\n\n\nGauges\ngauges are a type of data visualization that displays values on a meter within a specified range\n\n\nNavigation Bar\nthe navigation bar automatically includes the title, author, and date (if specified in the YAML). New pages are added to the navigation bar starting on the left side. There is also an option to add links to social media and the source code (specify this in the YAML)\n\n\nText\ntext can be added either at the top of the .Rmd before the setup chunk or in a box"
  },
  {
    "objectID": "session_04.html#demo-creating-a-flexdashboard",
    "href": "session_04.html#demo-creating-a-flexdashboard",
    "title": "4  Flexdashboard",
    "section": "4.4 Demo: Creating a flexdashboard",
    "text": "4.4 Demo: Creating a flexdashboard\n\n\n\n\n\n\nSetup\n\n\n\nFork the NCEAS/flexdashboard-demo-lh repository from the NCEAS GitHub organization and use the materials in the Git repo to follow along with the demonstration of flexdashboard examples.\n\n\nThe demonstration will include examples that showcase different flexdashboard features:\n\nBasic Flexdashboard from Template\nInteractive and Multiple Pages Flexdashboard\nReactive Flexdashboard using shiny elements\nThemed Flexdashboard using bslib\n\n\n\n\n\n\n\nExercise: Your turn!\n\n\n\nIn the Themed Flexdashboard, use the palmerpenguins data to complete the following tasks:\n\nFill in the boxes:\n\nIn the Chart A box, add a scatterplot of your choosing.\nIn the Chart B box, add a table using either kable() or DT.\nIn the Chart C box, add a valueBox.\nOptional Explore the htmlwidgets for R gallery, choose one you like and replace Chart D with that widget.\n\nChange the theme using bslib::bs_themer(). To activate the Theme Customizer, complete these steps:\n\nAdd to the YAML runtime: shiny\nAdd bslib::bs_themer() to the setup chunk\nSave the .Rmd\nClick “Run Document” and open the dashboard in a browser window for optimal experience\n\nAdd a new page then create a second Page using {data-navmenu}.\n\nNote: You’re welcome to use the code from the demo so you can quickly start playing with the different flexdashboard features."
  },
  {
    "objectID": "session_04.html#additional-resources",
    "href": "session_04.html#additional-resources",
    "title": "4  Flexdashboard",
    "section": "4.5 Additional Resources",
    "text": "4.5 Additional Resources\n\nRStudio flexdashboard vingettes (The articles under the “Articles” dropdown menu are particularly helpful!)\nRStudio flexdashboard Examples\nR Markdown: The Definitive Guide Chapter 5: Dashboards by Yihui Xie, J. J. Allaire, and Garrett Grolemund\nhtmlwidgets for R: Check out widgets featured either in the gallery or the showcase"
  },
  {
    "objectID": "session_05.html",
    "href": "session_05.html",
    "title": "5  Shiny",
    "section": "",
    "text": "5.0.1 Learning Objectives\nIn this lesson we will:\n\nreview the capabilities in Shiny applications\nlearn about the basic layout for Shiny interfaces\nlearn about the server component for Shiny applications\nbuild a simple shiny application for interactive plotting\n\n\n\n5.0.2 Overview\nShiny is an R package for creating interactive data visualizations embedded in a web application that you and your colleagues can view with just a web browser. Shiny apps are relatively easy to construct, and provide interactive features for letting others share and explore data and analyses.\nThere are some really great examples of what Shiny can do on the RStudio webite like this one exploring movie metadata. A more scientific example is a tool from the SASAP project exploring proposal data from the Alaska Board of Fisheries. There is also an app for Delta monitoring efforts.\n\nMost any kind of analysis and visualization that you can do in R can be turned into a useful interactive visualization for the web that lets people explore your data more intuitively But, a Shiny application is not the best way to preserve or archive your data. Instead, for preservation use a repository that is archival in its mission like the KNB Data Repository, Zenodo, or Dryad. This will assign a citable identifier to the specific version of your data, which you can then read in an interactive visualiztion with Shiny.\nFor example, the data for the Alaska Board of Fisheries application is published on the KNB and is citable as:\nMeagan Krupa, Molly Cunfer, and Jeanette Clark. 2017. Alaska Board of Fisheries Proposals 1959-2016. Knowledge Network for Biocomplexity. doi:10.5063/F1QN652R.\nWhile that is the best citation and archival location of the dataset, using Shiny, one can also provide an easy-to-use exploratory web application that you use to make your point that directly loads the data from the archival site. For example, the Board of Fisheries application above lets people who are not inherently familiar with the data to generate graphs showing the relationships between the variables in the dataset.\nWe’re going to create a simple shiny app with two sliders so we can interactively control inputs to an R function. These sliders will allow us to interactively control a plot.\n\n\n5.0.3 Create a sample shiny application\n\nFile &gt; New &gt; Shiny Web App…\nSet some fields: \n\nName it “myapp” or something else\nSelect “Single File”\nChoose to create it in a new folder called ‘shiny-demo’\nClick Create\n\n\nRStudio will create a new file called app.R that contains the Shiny application.\nRun it by choosing Run App from the RStudio editor header bar. This will bring up the default demo Shiny application, which plots a histogram and lets you control the number of bins in the plot.\n\nNote that you can drag the slider to change the number of bins in the histogram.\n\n\n5.0.4 Shiny architecture\nA Shiny application consists of two functions, the ui and the server. The ui function is responsible for drawing the web page, while the server is responsible for any calculations and for creating any dynamic components to be rendered.\nEach time that a user makes a change to one of the interactive widgets, the ui grabs the new value (say, the new slider min and max) and sends a request to the server to re-render the output, passing it the new input values that the user had set. These interactions can sometimes happen on one computer (e.g., if the application is running in your local RStudio instance). Other times, the ui runs on the web browser on one computer, while the server runs on a remote computer somewhere else on the Internet (e.g., if the application is deployed to a web server).\n\n\n\n5.0.5 Interactive scatterplots\nLet’s modify this application to plot Yolo bypass secchi disk data in a time-series, and allow aspects of the plot to be interactively changed.\n\n5.0.5.1 Load data for the example\nUse this code to load the data at the top of your app.R script. Note we are using contentId again, and we have filtered for some species of interest.\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\nnames(delta_data)\n\n\n\n5.0.5.2 Add a simple timeseries using ggplot\nWe know there has been a lot of variation through time in the delta, so let’s plot a time-series of Secchi depth. We do so by switching out the histogram code for a simple ggplot, like so:\n\nserver &lt;- function(input, output) {\n    \n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            theme_light()\n    })\n}\n\nIf you now reload the app, it will display the simple time-series instead of the histogram. At this point, we haven’t added any interactivity.\nIn a Shiny application, the server function provides the part of the application that creates our interactive components, and returns them to the user interface (ui) to be displayed on the page.\n\n\n5.0.5.3 Add sliders to set the start and end date for the X axis\nTo make the plot interactive, first we need to modify our user interface to include widgits that we’ll use to control the plot. Specifically, we will add a new slider for setting the minDate parameter, and modify the existing slider to be used for the maxDate parameter. To do so, modify the sidebarPanel() call to include two sliderInput() function calls:\n\nsidebarPanel(\n    sliderInput(\"minDate\",\n                \"Min Date:\",\n                min = as.Date(\"1998-01-01\"),\n                max = as.Date(\"2020-01-01\"),\n                value = as.Date(\"1998-01-01\")),\n    sliderInput(\"maxDate\",\n                \"Max Date:\",\n                min = as.Date(\"1998-01-01\"),\n                max = as.Date(\"2020-01-01\"),\n                value = as.Date(\"2005-01-01\"))\n)\n\nIf you reload the app, you’ll see two new sliders, but if you change them, they don’t make any changes to the plot. Let’s fix that.\n\n\n5.0.5.4 Connect the slider values to the plot\nFinally, to make the plot interactive, we can use the input and output variables that are passed into the server function to access the current values of the sliders. In Shiny, each UI component is given an input identifier when it is created, which is used as the name of the value in the input list. So, we can access the minimum depth as input$minDate and the max as input$maxDate. Let’s use these values now by adding limits to our X axis in the ggplot:\n\n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$minDate,input$maxDate)) +\n            theme_light()\n\nAt this point, we have a fully interactive plot, and the sliders can be used to change the min and max of the Depth axis.\n\nLooks so shiny!\n\n\n5.0.5.5 Reversed Axes?\nWhat happens if a clever user sets the minimum for the X axis at a greater value than the maximum? You’ll see that the direction of the X axis becomes reversed, and the plotted points display right to left. This is really an error condition. Rather than use two independent sliders, we can modify the first slider to output a range of values, which will prevent the min from being greater than the max. You do so by setting the value of the slider to a vector of length 2, representing the default min and max date for the slider, such as c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")). So, delete the second slider, rename the first, and provide a vector for the value, like this:\n\nsliderInput(\"date\",\n            \"Date:\",\n            min = as.Date(\"1998-01-01\"),\n            max = as.Date(\"2020-01-01\"),\n            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n)\n\nNow, modify the ggplot to use this new date slider value, which now will be returned as a vector of length 2. The first element of the depth vector is the min, and the second is the max value on the slider.\n\n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            theme_light()\n\n\n\n\n\n5.0.6 Extending the user interface with dynamic plots\nIf you want to display more than one plot in your application, and provide a different set of controls for each plot, the current layout would be too simple. Next we will extend the application to break the page up into vertical sections, and add a new plot in which the user can choose which variables are plotted. The current layout is set up such that the FluidPage contains the title element, and then a sidebarLayout, which is divided horizontally into a sidebarPanel and a mainPanel.\n\n\n5.0.6.1 Vertical layout\nTo extend the layout, we will first nest the existing sidebarLayout in a new verticalLayout, which simply flows components down the page vertically. Then we will add a new sidebarLayout to contain the bottom controls and graph.\n\nThis mechanism of alternately nesting vertical and horizontal panels can be used to segment the screen into boxes with rules about how each of the panels is resized, and how the content flows when the browser window is resized. The sidebarLayout works to keep the sidebar about 1/3 of the box, and the main panel about 2/3, which is a good proportion for our controls and plots. Add the verticalLayout, and the second sidebarLayout for the second plot as follows:\n\n    verticalLayout(\n        # Sidebar with a slider input for depth axis\n        sidebarLayout(\n            sidebarPanel(\n                sliderInput(\"date\",\n                            \"Date:\",\n                            min = as.Date(\"1998-01-01\"),\n                            max = as.Date(\"2020-01-01\"),\n                            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n            ),\n            # Show a plot of the generated distribution\n            mainPanel(\n               plotOutput(\"distPlot\")\n            )\n        ),\n\n        tags$hr(),\n\n        sidebarLayout(\n            sidebarPanel(\n                selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n            ),\n\n            # Show a plot with configurable axes\n            mainPanel(\n                plotOutput(\"varPlot\")\n            )\n        ),\n        tags$hr()\n\nNote that the second sidebarPanel uses three selectInput elements to provide dropdown menus with the variable columns (cols) from our data frame. To manage that, we need to first set up the cols variable, which we do by saving the variables names from the delta_data data frame to a variable:\n\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\ncols &lt;- names(delta_data)\n\n\n\n5.0.6.2 Add the dynamic plot\nBecause we named the second plot varPlot in our UI section, we now need to modify the server to produce this plot. Its very similar to the first plot, but this time we want to use the selected variables from the user controls to choose which variables are plotted. These variable names from the $input are character strings, and so would not be recognized as symbols in the aes mapping in ggplot. As recommended by the tidyverse authors, we use the non-standard evaluation syntax of .data[[\"colname\"]] to access the variables.\n\n    output$varPlot &lt;- renderPlot({\n      ggplot(delta_data, aes(x = .data[[input$x_variable]],\n                             y = .data[[input$y_variable]],\n                             color = .data[[input$color_variable]])) +\n        geom_point(size = 4)+\n        theme_light()\n    })\n\n\n\n\n5.0.7 Finishing touches: data citation\nCiting the data that we used for this application is the right thing to do, and easy. You can add arbitrary HTML to the layout using utility functions in the tags list.\n\n    # Application title\n    titlePanel(\"Yolo Bypass Fish and Water Quality Data\"),\n    p(\"Data for this application are from: \"),\n    tags$ul(\n        tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n        )\n    ),\n    tags$br(),\n    tags$hr(),\n\nThe final application shows the data citation, the depth plot, and the configurable scatterplot in three distinct panels.\n\n\n\n5.0.8 Publishing Shiny applications\nOnce you’ve finished your app, you’ll want to share it with others. To do so, you need to publish it to a server that is set up to handle Shiny apps.\nYour main choices are:\n\nshinyapps.io (Hosted by RStudio)\n\nThis is a service offered by RStudio, which is initially free for 5 or fewer apps and for limited run time, but has paid tiers to support more demanding apps. You can deploy your app using a single button push from within RStudio.\n\nShiny server (On premises)\n\nThis is an open source server which you can deploy for free on your own hardware. It requires more setup and configuration, but it can be used without a fee.\n\nRStudio connect (On premises)\n\nThis is a paid product you install on your local hardware, and that contains the most advanced suite of services for hosting apps and RMarkdown reports. You can publish using a single button click from RStudio.\n\n\nA comparison of publishing features is available from RStudio.\n\n5.0.8.1 Publishing to shinyapps.io\nThe easiest path is to create an account on shinyapps.io, and then configure RStudio to use that account for publishing. Instructions for enabling your local RStudio to publish to your account are displayed when you first log into shinyapps.io:\n\nOnce your account is configured locally, you can simply use the Publish button from the application window in RStudio, and your app will be live before you know it!\n\n\n\n\n5.0.9 Summary\nShiny is a fantastic way to quickly and efficiently provide data exploration for your data and code. We highly recommend it for its interactivity, but an archival-quality repository is the best long-term home for your data and products. In this example, we used data drawn directly from the EDI repository in our Shiny app, which offers both the preservation guarantees of an archive, plus the interactive data exploration from Shiny. You can utilize the full power of R and the tidyverse for writing your interactive applications.\n\n\n5.0.10 Full source code for the final application\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n# read in the data from EDI\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\ncols &lt;- names(delta_data)\n    \n\n\n# Define UI for application that draws a two plots\nui &lt;- fluidPage(\n    \n    # Application title and data  source\n    titlePanel(\"Sacramento River floodplain fish and water quality dataa\"),\n    p(\"Data for this application are from: \"),\n    tags$ul(\n        tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n        )\n    ),\n    tags$br(),\n    tags$hr(),\n    \n    verticalLayout(\n        # Sidebar with a slider input for time axis\n        sidebarLayout(\n            sidebarPanel(\n                sliderInput(\"date\",\n                            \"Date:\",\n                            min = as.Date(\"1998-01-01\"),\n                            max = as.Date(\"2020-01-01\"),\n                            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n            ),\n            # Show a plot of the generated timeseries\n            mainPanel(\n                plotOutput(\"distPlot\")\n            )\n        ),\n        \n        tags$hr(),\n        \n        sidebarLayout(\n            sidebarPanel(\n                selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n            ),\n            \n            # Show a plot with configurable axes\n            mainPanel(\n                plotOutput(\"varPlot\")\n            )\n        ),\n        tags$hr()\n    )\n)\n\n# Define server logic required to draw the two plots\nserver &lt;- function(input, output) {\n    \n    #  turbidity plot\n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            theme_light()\n    })\n    \n    # mix and  match plot\n    output$varPlot &lt;- renderPlot({\n      ggplot(delta_data, aes(x = .data[[input$x_variable]],\n                             y = .data[[input$y_variable]],\n                             color = .data[[input$color_variable]])) +\n        geom_point(size = 4) +\n        theme_light()\n    })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n5.0.11 A shinier app with tabs and a map!\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(shinythemes)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(snakecase)\n\n# read in the data from EDI\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName)) %&gt;% \n    rename(DissolvedOxygen = DO,\n           Ph = pH,\n           SpecificConductivity = SpCnd)\n\ncols &lt;- names(delta_data)\n\nsites &lt;- delta_data %&gt;% \n    distinct(StationCode, Latitude, Longitude) %&gt;% \n    drop_na() %&gt;% \n    st_as_sf(coords = c('Longitude','Latitude'), crs = 4269,  remove = FALSE)\n\n\n\n# Define UI for application\nui &lt;- fluidPage(\n    navbarPage(theme = shinytheme(\"flatly\"), collapsible = TRUE,\n               HTML('&lt;a style=\"text-decoration:none;cursor:default;color:#FFFFFF;\" class=\"active\" href=\"#\"&gt;Sacramento River Floodplain Data&lt;/a&gt;'), id=\"nav\",\n               windowTitle = \"Sacramento River floodplain fish and water quality data\",\n               \n               tabPanel(\"Data Sources\",\n                        verticalLayout(\n                            # Application title and data  source\n                            titlePanel(\"Sacramento River floodplain fish and water quality data\"),\n                            p(\"Data for this application are from: \"),\n                            tags$ul(\n                                tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                                        tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n                                )\n                            ),\n                            tags$br(),\n                            tags$hr(),\n                            p(\"Map of sampling locations\"),\n                            mainPanel(leafletOutput(\"map\"))\n                        )\n               ),\n               \n               tabPanel(\n                   \"Explore\",\n                   verticalLayout(\n                       mainPanel(\n                           plotOutput(\"distPlot\"),\n                           width =  12,\n                           absolutePanel(id = \"controls\",\n                                         class = \"panel panel-default\",\n                                         top = 175, left = 75, width = 300, fixed=TRUE,\n                                         draggable = TRUE, height = \"auto\",\n                                         sliderInput(\"date\",\n                                                     \"Date:\",\n                                                     min = as.Date(\"1998-01-01\"),\n                                                     max = as.Date(\"2020-01-01\"),\n                                                     value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n                                         \n                           )\n                       ),\n                       \n                       tags$hr(),\n                       \n                       sidebarLayout(\n                           sidebarPanel(\n                               selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                               selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                               selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n                           ),\n                           \n                           # Show a plot with configurable axes\n                           mainPanel(\n                               plotOutput(\"varPlot\")\n                           )\n                       ),\n                       tags$hr()\n                   )\n               )\n    )\n)\n\n# Define server logic required to draw the two plots\nserver &lt;- function(input, output) {\n    \n    \n    output$map &lt;- renderLeaflet({leaflet(sites) %&gt;% \n            addTiles() %&gt;% \n            addCircleMarkers(data = sites,\n                             lat = ~Latitude,\n                             lng = ~Longitude,\n                             radius = 10, # arbitrary scaling\n                             fillColor = \"gray\",\n                             fillOpacity = 1,\n                             weight = 0.25,\n                             color = \"black\",\n                             label = ~StationCode)\n    })\n    \n    #  turbidity plot\n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            labs(x = \"Sample Date\", y = \"Secchi Depth (m)\") +\n            theme_light()\n    })\n    \n    # mix and  match plot\n    output$varPlot &lt;- renderPlot({\n        ggplot(delta_data, mapping = aes(x = .data[[input$x_variable]],\n                                         y = .data[[input$y_variable]],\n                                         color = .data[[input$color_variable]])) +\n            labs(x = to_any_case(input$x_variable, case = \"title\"),\n                 y = to_any_case(input$y_variable, case = \"title\"),\n                 color = to_any_case(input$color_variable, case = \"title\")) +\n            geom_point(size=4) +\n            theme_light()\n    })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n5.0.12 Resources\n\nMain Shiny site\nOfficial Shiny Tutorial"
  },
  {
    "objectID": "session_06.html#learning-objectives",
    "href": "session_06.html#learning-objectives",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nApply design thinking approaches and tools to dashboard development\nEvaluate whether to build a web application using flexdashboard or shiny\nPractice developing a web application using flexdashboard or shiny\nDemonstrate design knowledge including structure of layout, incorporation of data visualization, and choice of interactive elements\n\n\n\n\n\n\n\nAcknowledgements\n\n\n\nThis lesson has been adapted by the Women in Data Science (WiDS) Workshop: Dashboard Design Thinking by Jenn Schilling.\nWe highly encourage you to watch the full workshop for more details on applying design thinking to dashboards."
  },
  {
    "objectID": "session_06.html#what-is-design-thinking",
    "href": "session_06.html#what-is-design-thinking",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "6.1 What is Design Thinking?",
    "text": "6.1 What is Design Thinking?\nDesign thinking in data science centers the user’s perspective throughout the entire development of a dashboard. It starts with the user by gaining an understanding on what the user needs and ends with the user by involving them in focus groups or other testing methods to get their feedback. Centering the user means we create better dashboards and improve data-informed decision making.\nDesign thinking is inherently an innovative and collaborative process because it involves exploring multiple options for the same problem.\nThe design thinking process can be encapsulated within these stages:\n\nEmpathize and Define: In these stages, we use tools to understand the problem and our audiences.\nIdeate and Prototype: We use these stages to explore solutions.\nTest and Implement: In these final stages, we use tools to materialize the final product.\n\nThese stages are not meant to be in any particular order. While there is an inherent order to them, you should feel empowered to move between the stages as needed — depending on the development of your dashboard and the needs of your users."
  },
  {
    "objectID": "session_06.html#empathize-and-define",
    "href": "session_06.html#empathize-and-define",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "6.2 Empathize and Define",
    "text": "6.2 Empathize and Define\n\n\n\n\n\n\nEmpathize: Gather information and survey your users to understand what they need to know from the data, what are the use cases for the dashboard and its overall purpose.\n\n\n\n\nEmpathize Tool: Empathy Mapping\nDuring the Empathize and Define stages, you can gather information through internal conversations with your team or conduct interviews with potential users.\nThe goal of an Empathy Map is to capture what a user says, thinks, does, and feels — additionally consider what their goals are (and do their goals align the goals you have in mind for the dashboard you’re developing?). Check out the empathy map the Nielsen Norman Group created for a customer, Jamie, who is buying a television.\n\n\n\nSource: Nielsen Norman Group\n\n\n\n\n\n\n\n\nCreate Your Own Empathy Map\n\n\n\nUse the NCEAS Learning Hub Empathy Map template to get started — adapt it as needed for your project/user(s).\n\n\n\n\n\n\n\n\n\nDefine: Identify the core problems your users have in accessing and interpreting the data in its current state.\n\n\n\n\n\nDefine Tool: Project Brief Wiki\nThe Empathize Stage helped us understand our audience. Now, we need to understand our problem. In the Define Stage, write down problem statements and document the specifications of the dashboard including the scope, timeline, and roles. It’s best to contain all this information within a Project Brief — a condensed version of a project plan.\nYou should feel comfortable sharing this document with not only your team, but your users as well. This ensures that everyone is on the same page. Keeping this in mind, consider having your Project Brief as a Wiki on GitHub in the Git repository for your dashboard.\nWhen defining the dashboard you want to create, consider these types:\n\nExecutive Dashboard: shows the big picture; used for high-level overview\nOperational Dashboard: shows what’s happening right now; used for monitoring\nStrategic Dashboard: shows progress toward a goal; used to inform decisions\nAnalytical Dashboard: shows detailed analysis; used for identifying opportunities and deep analysis\n\n\n\n\n\n\n\nWriting a Project Brief\n\n\n\nCheck out Asana’s article 5 steps to writing a clear project brief as guidance — use only what you need for your project."
  },
  {
    "objectID": "session_06.html#ideate-and-protoype",
    "href": "session_06.html#ideate-and-protoype",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "6.3 Ideate and Protoype",
    "text": "6.3 Ideate and Protoype\n\n\n\n\n\n\nIdeate: Challenge your existing assumptions and create ideas. Look for alternative ways to view the problem and identify solutions.\n\n\n\n\nIdeate Tool: Host a Brainstorm Session\nNow that we’ve learned about our audiences and problems from the Empathize & Define stages, we can start to think about what ideas for dashboard solutions. The primary goal of this stage is to generate as many ideas as possible without judgement and evaluation — it’s all about quantity over quality. Once you’ve generated many ideas, then you can start to narrow it down to a single dashboard solution.\nThere are many ways to ideate individually or as a group, but here are some tips:\n\nUse mind mapping tools like Miro for virtual collaboration.\nIf you’re working together in-person, use a white board, sticky notes, or a large piece of construction paper to capture as many ideas from as many team members as possible.\nTake a break!\nGroup related ideas together.\nSet a time limit. This can vary from a few hours to multiple sessions throughout a week. Either way setting a time limit and making a plan to regroup and narrow down on ideas is helpful so that the ideation period doesn’t feel so nebulous.\n\n\n\n\n\n\n\nShiny vs Flexdashboard\n\n\n\nThis is a good time to consider which R dashboard package may be more useful for your final dashboard product. Recall the diagram from the Flexdashboard Lesson to help determine which tool makes the most sense for your project.\n\n\n\n\n\n\n\n\n\nPrototype: Start to create the solutions. It’s important to experiment here. Produce some inexpensive and low-intensive versions of the product.\n\n\n\n\n\nPrototype Tool: Create a Minimal Viable Dashboard\nAt the Prototype Stage, we can experiment with a few of the ideas from the Ideate Stage to identify what is the best solution. Here we can create a quick, scaled down version of the dashboard. The primary goal is to create something more refined from the Ideate Stage, but not a final or completely usable dashboard. We want the prototype to be a minimal viable product where you can test out some functionality without committing to multiple iterations.\n\n\n\n\n\n\nTools to use for Prototyping\n\n\n\n\nGoogle Slides\nMicrosoft PowerPoint\nR Flexdashboard (this is a great option for prototyping if you’re set on creating a Shiny App since you can insert some Shiny elements in a Flexdashboard)\nGood ole’ pen and paper!"
  },
  {
    "objectID": "session_06.html#test-and-implement",
    "href": "session_06.html#test-and-implement",
    "title": "6  Practice Session: Shiny and Flexdashboard",
    "section": "6.4 Test and Implement",
    "text": "6.4 Test and Implement\n\n\n\n\n\n\nTest: Try your solutions and evaluate the results. Return to your users for feedback to incorporate.\n\n\n\n\nTest Tool: Run Tests and Review Work from Previous Stages\nYou’ve completed your prototype — it’s time to test it out and receive feedback on it! Review your project brief and confirm your prototype reflects your users’ needs and the use cases you’re trying to achieve.\nIf you don’t have access to your users’, return to your Empathy Maps to make sure your prototype is aligned with the information you have gathered there.\nOther tests to consider:\n\nDoes your dashboard work across different devices?\nDoes it meet accessibility requirements?\nDoes the users’ dashboard experience align with how you think their experience should go?\n\n\n\n\n\n\n\nInteraction Design Foundation Testing Prototype Guidelines\n\n\n\nRead the Interaction Design Foundation’s article Test Your Prototypes: How to Gather Feedback and Maximize Learning for more tips, tricks, and templates for testing your dashboard.\n\n\n\n\n\n\n\n\n\nImplement: Put your vision into effect! Remember that the process doesn’t have to end here at implementation. Return to your users to gain more feedback and to guide the refinement of your solutions.\n\n\n\nIf the test you run reveal that your dashboard is not meeting the goals of your team or your users, then it’s time to go back to the Prototype Stage (or even a different stage) and iterate before you complete the Implementation stage. You will also want to return to your Project Brief, Empathy Map, and additional successful metrics to ensure that your dashboard meets most of these goals and needs.\nWhen implementing the dashboard:\n\nAdd context and definitions to the dashboard.\nTest again!\nInternally: validate your data.\nExternally: users test and provide final feedback.\nHow are you going to communicate your dashboard and make it accessible to your audience?\n\n\n\n\n\n\n\nThat’s Not All Folks!\n\n\n\nDesign Thinking is an approach that centers the user in the development of a product. When developing dashboards or other data products there are definitely more resources and approaches out there.\nIt’s worth doing additional research on aspects like testing, design, and data visualization to create the most robust data product as possible."
  },
  {
    "objectID": "session_07.html",
    "href": "session_07.html",
    "title": "7  Hands On Synthesis",
    "section": "",
    "text": "Note\n\n\n\nTime for groups to get together and advance in their synthesis projects."
  },
  {
    "objectID": "session_09.html#learning-objectives",
    "href": "session_09.html#learning-objectives",
    "title": "9  Parellel Processing",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand what parallel computing is and when it may be useful\nUnderstand how parallelism can work\nReview sequential loops and *apply functions\nUnderstand and use the parallel package multicore functions\nUnderstand and use the foreach package functions"
  },
  {
    "objectID": "session_09.html#introduction",
    "href": "session_09.html#introduction",
    "title": "9  Parellel Processing",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nProcessing large amounts of data with complex models can be time consuming. New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America:\n\n\n\nLevy et al. 2016. doi:10.5063/F1Z899CZ\n\n\nThere are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization.\nAlternatively, think of remote sensing data. Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years.\n\n\n\nNEON Data Cube"
  },
  {
    "objectID": "session_09.html#why-parallelism",
    "href": "session_09.html#why-parallelism",
    "title": "9  Parellel Processing",
    "section": "9.2 Why parallelism?",
    "text": "9.2 Why parallelism?\nMuch R code runs fast and fine on a single processor. But at times, computations can be:\n\ncpu-bound: Take too much cpu time\nmemory-bound: Take too much memory\nI/O-bound: Take too much time to read/write from disk\nnetwork-bound: Take too much time to transfer\n\nTo help with cpu-bound computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire cluster of those computers. Plus, these machines also have large amounts of memory to avoid memory-bound computing jobs."
  },
  {
    "objectID": "session_09.html#processors-cpus-and-cores",
    "href": "session_09.html#processors-cpus-and-cores",
    "title": "9  Parellel Processing",
    "section": "9.3 Processors (CPUs) and Cores",
    "text": "9.3 Processors (CPUs) and Cores\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with mutliple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations.\nA computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time.\n\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here we show four quad-core processors for a total of 16 cores in this machine.\n\nYou can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later).\nHistorically, R has only utilized one processor, which makes it single-threaded. Which is a shame, because the 2017 MacBook Pro that I am writing this on is much more powerful than that:\n\njones@powder:~$ sysctl hw.ncpu hw.physicalcpu\nhw.ncpu: 8\nhw.physicalcpu: 4\n\nTo interpret that output, this machine powder has 4 physical CPUs, each of which has two processing cores, for a total of 8 cores for computation. I’d sure like my R computations to use all of that processing power. Because its all on one machine, we can easily use multicore processing tools to make use of those cores. Now let’s look at the computational server aurora at NCEAS:\n\njones@aurora:~$ lscpu | egrep 'CPU\\(s\\)|per core|per socket'\nCPU(s):                88\nOn-line CPU(s) list:   0-87\nThread(s) per core:    2\nCore(s) per socket:    22\nNUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86\nNUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87\n\nNow that’s some compute power! Aurora has 384 GB of RAM, and ample storage. All still under the control of a single operating system.\nHowever, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now:\n\nJetStream\n\n640 nodes, 15,360 cores, 80TB RAM\n\nStampede2 at TACC is coming online in 2017\n\n4200 nodes, 285,600 cores\n\n\nNote that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster. One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine. But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster."
  },
  {
    "objectID": "session_09.html#when-to-parallelize",
    "href": "session_09.html#when-to-parallelize",
    "title": "9  Parellel Processing",
    "section": "9.4 When to parallelize",
    "text": "9.4 When to parallelize\nIt’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!\nIn addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced. Some propose that this may follow Amdahl’s Law, where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines):\n\n\n\n\n\nSo, its important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done. With that, let’s do some parallel computing…"
  },
  {
    "objectID": "session_09.html#loops-and-repetitive-tasks-using-lapply",
    "href": "session_09.html#loops-and-repetitive-tasks-using-lapply",
    "title": "9  Parellel Processing",
    "section": "9.5 Loops and repetitive tasks using lapply",
    "text": "9.5 Loops and repetitive tasks using lapply\nWhen you have a list of repetitive tasks, you may be able to speed it up by adding more computing power. If each task is completely independent of the others, then it is a prime candidate for executing those tasks in parallel, each on its own core. For example, let’s build a simple loop that uses sample with replacement to do a bootstrap analysis. In this case, we select Sepal.Length and Species from the iris dataset, subset it to 100 observations, and then iterate across 10,000 trials, each time resampling the observations with replacement. We then run a logistic regression fitting species as a function of length, and record the coefficients for each trial to be returned.\n\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- 10000\nres &lt;- data.frame()\nsystem.time({\n  trial &lt;- 1\n  while(trial &lt;= trials) {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    r &lt;- coefficients(result1)\n    res &lt;- rbind(res, r)\n    trial &lt;- trial + 1\n  }\n})\n\n   user  system elapsed \n 31.009   0.136  31.163 \n\n\nThe issue with this loop is that we execute each trial sequentially, which means that only one of our 8 processors on this machine are in use. In order to exploit parallelism, we need to be able to dispatch our tasks as functions, with one task going to each processor. To do that, we need to convert our task to a function, and then use the *apply() family of R functions to apply that function to all of the members of a set. In R, using apply used to be faster than the equivalent code in a loop, but now they are similar due to optimizations in R loop handling. However, using the function allows us to later take advantage of other approaches to parallelization. Here’s the same code rewritten to use lapply(), which applies a function to each of the members of a list (in this case the trials we want to run):\n\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- seq(1, 10000)\nboot_fx &lt;- function(trial) {\n  ind &lt;- sample(100, 100, replace=TRUE)\n  result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n  r &lt;- coefficients(result1)\n  res &lt;- rbind(data.frame(), r)\n}\nsystem.time({\n  results &lt;- lapply(trials, boot_fx)\n})\n\n   user  system elapsed \n 33.385   0.024  33.429"
  },
  {
    "objectID": "session_09.html#approaches-to-parallelization",
    "href": "session_09.html#approaches-to-parallelization",
    "title": "9  Parellel Processing",
    "section": "9.6 Approaches to parallelization",
    "text": "9.6 Approaches to parallelization\nWhen parallelizing jobs, one can:\n\nUse the multiple cores on a local computer through mclapply\nUse multiple processors on local (and remote) machines using makeCluster and clusterApply\n\nIn this approach, one has to manually copy data and code to each cluster member using clusterExport\nThis is extra work, but sometimes gaining access to a large cluster is worth it\n\n\n\n9.6.1 Parallelize using: mclapply\nThe parallel library can be used to send tasks (encoded as function calls) to each of the processing cores on your machine in parallel. This is done by using the parallel::mclapply function, which is analogous to lapply, but distributes the tasks to multiple processors. mclapply gathers up the responses from each of these function calls, and returns a list of responses that is the same length as the list or vector of input data (one return per input item).\n\nlibrary(parallel)\nlibrary(MASS)\n\nstarts &lt;- rep(100, 40)\nfx &lt;- function(nstart) kmeans(Boston, 4, nstart=nstart)\nnumCores &lt;- detectCores()\nnumCores\n\n[1] 2\n\nsystem.time(\n  results &lt;- lapply(starts, fx)\n)\n\n   user  system elapsed \n  1.403   0.008   1.411 \n\nsystem.time(\n  results &lt;- mclapply(starts, fx, mc.cores = numCores)\n)\n\n   user  system elapsed \n  0.704   0.105   0.847 \n\n\nNow let’s demonstrate with our bootstrap example: ::: {.cell}\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- seq(1, 10000)\nboot_fx &lt;- function(trial) {\n  ind &lt;- sample(100, 100, replace=TRUE)\n  result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n  r &lt;- coefficients(result1)\n  res &lt;- rbind(data.frame(), r)\n}\nsystem.time({\n  results &lt;- mclapply(trials, boot_fx, mc.cores = numCores)\n})\n\n   user  system elapsed \n 16.791   0.160  17.448 \n\n:::\n\n\n9.6.2 Parallelize using: foreach and doParallel\nThe normal for loop in R looks like:\n\nfor (i in 1:3) {\n  print(sqrt(i))\n}\n\n[1] 1\n[1] 1.414214\n[1] 1.732051\n\n\nThe foreach method is similar, but uses the sequential %do% operator to indicate an expression to run. Note the difference in the returned data structure. ::: {.cell}\nlibrary(foreach)\nforeach (i=1:3) %do% {\n  sqrt(i)\n}\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n:::\nIn addition, foreach supports a parallelizable operator %dopar% from the doParallel package. This allows each iteration through the loop to use different cores or different machines in a cluster. Here, we demonstrate with using all the cores on the current machine: ::: {.cell}\nlibrary(foreach)\nlibrary(doParallel)\n\nLoading required package: iterators\n\nregisterDoParallel(numCores)  # use multicore, set to the number of our cores\nforeach (i=1:3) %dopar% {\n  sqrt(i)\n}\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n# To simplify output, foreach has the .combine parameter that can simplify return values\n\n# Return a vector\nforeach (i=1:3, .combine=c) %dopar% {\n  sqrt(i)\n}\n\n[1] 1.000000 1.414214 1.732051\n\n# Return a data frame\nforeach (i=1:3, .combine=rbind) %dopar% {\n  sqrt(i)\n}\n\n             [,1]\nresult.1 1.000000\nresult.2 1.414214\nresult.3 1.732051\n\n:::\nThe doParallel vignette on CRAN shows a much more realistic example, where one can use `%dopar% to parallelize a bootstrap analysis where a data set is resampled 10,000 times and the analysis is rerun on each sample, and then the results combined:\n\n# Let's use the iris data set to do a parallel bootstrap\n# From the doParallel vignette, but slightly modified\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- 10000\nsystem.time({\n  r &lt;- foreach(icount(trials), .combine=rbind) %dopar% {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n\n   user  system elapsed \n 30.043   0.435  16.468 \n\n# And compare that to what it takes to do the same analysis in serial\nsystem.time({\n  r &lt;- foreach(icount(trials), .combine=rbind) %do% {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n\n   user  system elapsed \n 31.568   0.012  31.591 \n\n# When you're done, clean up the cluster\nstopImplicitCluster()"
  },
  {
    "objectID": "session_09.html#summary",
    "href": "session_09.html#summary",
    "title": "9  Parellel Processing",
    "section": "9.7 Summary",
    "text": "9.7 Summary\nIn this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores. Next, we reviewed the way in which traditional for loops in R can be rewritten as functions that are applied to a list serially using lapply, and then how the parallel package mclapply function can be substituted in order to utilize multiple cores on the local computer to speed up computations. Finally, we installed and reviewed the use of the foreach package with the %dopar operator to accomplish a similar parallelization using multiple cores."
  },
  {
    "objectID": "session_09.html#readings-and-tutorials",
    "href": "session_09.html#readings-and-tutorials",
    "title": "9  Parellel Processing",
    "section": "9.8 Readings and tutorials",
    "text": "9.8 Readings and tutorials\n\nMulticore Data Science with R and Python\nBeyond Single-Core R by Jonoathan Dursi (also see GitHub repo for slide source)\nThe venerable Parallel R by McCallum and Weston (a bit dated on the tooling, but conceptually solid)\nThe doParallel Vignette"
  },
  {
    "objectID": "session_10.html",
    "href": "session_10.html",
    "title": "10  Hands On Synthesis",
    "section": "",
    "text": "Note\n\n\n\nTime for groups to get together and advance in their synthesis projects."
  },
  {
    "objectID": "session_11.html#learning-objectives",
    "href": "session_11.html#learning-objectives",
    "title": "11  Publishing Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nOverview best practices for organizing data for publication\n\nReview what science metadata is and how it can be used\nDemonstrate how data and code can be documented and published in open data archives"
  },
  {
    "objectID": "session_11.html#the-data-life-cycle",
    "href": "session_11.html#the-data-life-cycle",
    "title": "11  Publishing Data",
    "section": "11.1 The Data Life Cycle",
    "text": "11.1 The Data Life Cycle\nThe Data Life Cycle gives you an overview of meaningful steps data goes through in a research project, from planning to archival. This step-by-step breakdown facilitates overseeing individual actions, operations and processes required at each stage.\n\n\n\n\nStep\nDescription\n\n\n\n\nPlan\nMap out the processes and resources for the entire data life cycle. Start with the project goals (desired outputs, outcomes, and impacts) and work backwards to build a data management plan, supporting data policies, and sustainability plans.\n\n\nCollect\nObservations are made either by hand or with sensors or other instruments and the data are placed a into digital form. You can structure the process of collecting data up front to better implement data management.\n\n\nAssure\nEmploy quality assurance and quality control procedures that enhance the quality of data (e.g., training participants, routine instrument calibration) and identify potential errors and techniques to address them.\n\n\nDescribe\nDocument data by describing the why, who, what, when, where, and how of the data. Metadata, or data about data, are key to data sharing and reuse, and many tools such as standards and software are available to help describe data.\n\n\nPreserve\nPlan to preserve data in the short term to minimize potential losses (e.g., via accidents), and in the long term so that project stakeholders and others can access, interpret, and use the data in the future. Decide what data to preserve, where to preserve it, and what documentation needs to accompany the data.\n\n\nDiscover\nIdentify complementary data sets that can add value to project data. Strategies to help endure the data have maximum impact include registering the project on a project directory site, depositing data in an open repository, and adding data descriptions to metadata clearing houses.\n\n\nIntegrate\nData from multiple sources are combined into a form that can be readily analyzed. For example, you could combine citizen science project data with other sources of data to enable new analyses and investigations. Successful data integration depends on documentation of the integration process, clearly citing and making accessable the data you are using, and employing good data management practices throughout the Data Life Cycle.\n\n\nAnalyze\nCreate analyses and visualizations to identify patterns, test hypotheses, and illustrate finding. During this process record your methods, document data processing steps, and ensure your data are reproduceable. Learn about these best practices and more.\n\n\n\nIn this lesson we focus on the Describe and Preserve stages of this cycle. However, best practices on how to organize and document your data, apply to all stages."
  },
  {
    "objectID": "session_11.html#organizing-data",
    "href": "session_11.html#organizing-data",
    "title": "11  Publishing Data",
    "section": "11.2 Organizing Data",
    "text": "11.2 Organizing Data\nThe goal is to operate through the data life cycle with the FAIR and CARE principles in mind and making sure our data is in a tidy format.\n\n\n\nArtwork by Allison Horst\n\n\nBenefits of having clean and tidy data and complete metadata:\n\nDecreases errors from redundant updates\nEnforces data integrity\nHelps you and future researchers to handle large, complex datasets\nEnables powerful search filtering\n\nSome of the best practices to follow are (Borer et al. (2009), White et al. (2013)):\n\nHave scripts for all data wrangling that start with the uncorrected raw data file and clean the data programmatically before analysis.\nDesign your tables to add rows, not columns. A column should be only one variable and a row should be only one observation.\nInclude header lines in your tables\nUse non-proprietary file formats (ie, open source) with descriptive file names without spaces.\n\nNon-proprietary file formats are essential to ensure that your data can still be machine readable long into the future. Open formats include text files and binary formats such as NetCDF.\n\nCommon switches:\n\n\n\n\n\n\n\nProprietary format\nExport to…\n\n\n\n\nMicrosoft Excel (.xlsx) files\ntext (.txt) or comma separated values (.csv)\n\n\nGIS files\nESRI shapefiles (.shp)\n\n\nMATLAB/IDL\nNetCDF\n\n\n\n\n\n\n\n\n\nLarge Data Packages\n\n\n\nWhen you have or are going to generate large data packages (in the terabytes or larger), it’s important to establish a relationship with the data center early on.\nThe data center can help come up with a strategy to tile data structures by subset, such as by spatial region, by temporal window, or by measured variable. They can also help with choosing an efficient tool to store the data (ie NetCDF or HDF), which is a compact data format that helps parallel read and write libraries of data."
  },
  {
    "objectID": "session_11.html#metadata",
    "href": "session_11.html#metadata",
    "title": "11  Publishing Data",
    "section": "11.3 Metadata",
    "text": "11.3 Metadata\nWithin the data life cycle you can be collecting data (creating new data) or integrating data that has all ready been collected. Either way, metadata plays a major role to successfully spin around the cycle because it enables data reuse long after the original collection.\nImagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files?\nThe goal is to have enough information for the researcher to understand the data, interpret the data, and then reuse the data in another study.\nOne way to think about metadata is to answer the following questions with the documentation:\n\nWhat was measured?\nWho measured it?\nWhen was it measured?\nWhere was it measured?\nHow was it measured?\nHow is the data structured?\nWhy was the data collected?\nWho should get credit for this data (researcher AND funding agency)?\nHow can this data be reused (licensing)?\n\nWe also know that it is important to keep in mind how will computers organize and integrate this information. There are a number of metadata standards that make your metadata machine readable and therefore easier for data curators to publish your data (Ecological Metadata Language, Geospatial Metadata Standards, Biological Data Profile, Darwin Core, Metadata Encoding Transmission Standard, etc.)\nToday we are going to be focusing on the Ecological Metadata Language also known as EML. Which is widespread use in the earth and environmental sciences.\n\n“The Ecological Metadata Language (EML) defines a comprehensive vocabulary and a readable XML markup syntax for documenting research data” (https://eml.ecoinformatics.org/)\n\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;eml:eml packageId=\"df35d.442.6\" system=\"knb\" \n    xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.1\"&gt;\n    &lt;dataset&gt;\n        &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through \n            Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt;\n        &lt;creator id=\"1385594069457\"&gt;\n            &lt;individualName&gt;\n                &lt;givenName&gt;Mark&lt;/givenName&gt;\n                &lt;surName&gt;Willette&lt;/surName&gt;\n            &lt;/individualName&gt;\n            &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt;\n            &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt;\n            &lt;address&gt;\n                &lt;city&gt;Soldotna&lt;/city&gt;\n                &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt;\n                &lt;country&gt;USA&lt;/country&gt;\n            &lt;/address&gt;\n            &lt;phone phonetype=\"voice\"&gt;(907)260-2911&lt;/phone&gt;\n            &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt;\n        &lt;/creator&gt;\n        ...\n    &lt;/dataset&gt;\n&lt;/eml:eml&gt;"
  },
  {
    "objectID": "session_11.html#data-identifiers-citation",
    "href": "session_11.html#data-identifiers-citation",
    "title": "11  Publishing Data",
    "section": "11.4 Data Identifiers & Citation",
    "text": "11.4 Data Identifiers & Citation\nMany journals require a DOI - a digital object identifier - be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to.\nKeep in mind that generally, if the data package needs to be updated (which happens in many cases), each version of the package will get its own identifier. This way, researchers can and should cite the exact version of the data set that they used in their analysis. Having the data identified in this manner allows us to accurately track the dataset usage metrics.\n\nFinally, stressed that researchers should get in the habit of citing the data that they use (even if it’s their own data!) in each publication that uses that data. This is important for correct attribution, provenance of your work and ultimately transparency in the scientific process."
  },
  {
    "objectID": "session_11.html#provenance-and-computational-workflow",
    "href": "session_11.html#provenance-and-computational-workflow",
    "title": "11  Publishing Data",
    "section": "11.5 Provenance and Computational Workflow",
    "text": "11.5 Provenance and Computational Workflow\n\nWhile the Knowledge Network for Biocomplexity, and similar repositories do focus on preserving data, we really set our sights much more broadly on preserving entire computational workflows that are instrumental to advances in science. A computational workflow represents the sequence of computational tasks that are performed from raw data acquisition through data quality control, integration, analysis, modeling, and visualization.\nFor example, a data acquisition and cleaning workflow often creates a derived and integrated data product that is then picked up and used by multiple downstream analytical workflows that produce specific scientific findings. These workflows can each be archived as distinct data packages, with the output of the first workflow becoming the input of the second and subsequent workflows.\n\nAdding provenance within your work makes it more reproducible and compliant with the FAIR principles. It is also useful for building on the work of others; you can produce similar visualizations for another location, for example, using the same code.\nTools like Quarto can be used as a provenance tool, as well - by starting with the raw data and cleaning it programmatically, rather than manually, you preserve the steps that you went through and your workflow is reproducible."
  },
  {
    "objectID": "session_11.html#preserving-your-data",
    "href": "session_11.html#preserving-your-data",
    "title": "11  Publishing Data",
    "section": "11.6 Preserving your data",
    "text": "11.6 Preserving your data\n\n\n\n\n\n11.6.1 Data repositories: built for data (and code)\n\nGitHub is not an archival location\nDedicated data repositories: KNB, Arctic Data Center, Zenodo, FigShare\n\nRich metadata\nArchival in their mission\n\nData papers, e.g., Scientific Data\nList of data repositories: http://re3data.org\n\n\nDataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses.\nDataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data form the dozens of members of DataONE, rather than visiting each of the currently 43 repositories one at a time.\n\n\n\n11.6.2 Structure of a data package\nNote that the data set above lists a collection of files that are contained within the data set. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\n These data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets an internal identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4."
  },
  {
    "objectID": "session_11.html#publishing-data-from-the-web",
    "href": "session_11.html#publishing-data-from-the-web",
    "title": "11  Publishing Data",
    "section": "11.7 Publishing data from the web",
    "text": "11.7 Publishing data from the web\nEach data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the EDI, they provide some easy to use web forms for editing and submitting a data package.\n\n11.7.1 Publishing Data to EDI\nShort explanations and DEMO\n\n\n11.7.2 Publishing Data to KNB\nShort Explanation and Practice\n\n\n\n\n\n\nSetup\n\n\n\nDownload the data to be used for the tutorial\nI’ve already uploaded the test data package, and so you can access the data here:\n\nhttps://dev.nceas.ucsb.edu/view/urn:uuid:0702cc63-4483-4af4-a218-531ccc59069f\n\nGrab both CSV files, and the R script, and store them in a convenient folder.\n\n\n\n\n11.7.2.1 Login via ORCID\nWe will walk through web submission on https://demo.nceas.ucsb.edu, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button.\n\nWhen you sign in, you will be redirected to orcid.org, where you can either provide your existing ORCID credentials, or create a new account. ORCID provides multiple ways to login, including using your email address, institutional logins from many universities, and logins from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login.\n\n\n\nCreate and submit the data set\nAfter signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. Required sections are listed with a red asterisk.\n\nClick Add Files to choose the data files for your package\nYou can select multiple files at a time to efficiently upload many files.\n\nThe files will upload showing a progress indicator. You can continue editing metadata while they upload.\n\n\n\nEnter Overview information\nThis includes a descriptive title, abstract, and keywords.\nThe title is the first way a potential user will get information about your dataset. It should be descriptive but succinct, lack acronyms, and include some indication of the temporal and geospatial coverage of the data.\nThe abstract should be sufficiently descriptive for a general scientific audience to understand your dataset at a high level. It should provide an overview of the scientific context/project/hypotheses, how this data package fits into the larger context, a synopsis of the experimental or sampling design, and a summary of the data contents.\n\nKeywords, while not required, can help increase the searchability of your dataset, particularly if they come from a semantically defined keyword thesaurus.\n\nOptionally, you can also enter funding information, including a funding number, which can help link multiple datasets collected under the same grant.\nSelecting a distribution license - either CC-0 or CC-BY is required.\n\n\n\nPeople Information\nInformation about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people:\n\nCreators - all the people who should be in the citation for the dataset\nContacts - one is required, but defaults to the first Creator if omitted\nPrincipal Investigators\nand any other that are relevant\n\nFor each, please strive to provide their ORCID identifier, which helps link this dataset to their other scholarly works.\n\n\n\n11.7.2.1.1 Temporal Information\nAdd the temporal coverage of the data, which represents the time period to which data apply.\n\n\n\nLocation Information\nThe geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that a spatial search will be more precise.\n\nNote that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global. For example:\n\nMission Canyon, Santa Barbara, California, USA\n\n\n\n\nMethods\nMethods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, include details of sampling methods, experimental design, quality assurance procedures, and computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data.\n\n\n\nSave a first version with Submit\nWhen finished, click the Submit Dataset button at the bottom.\nIf there are errors or missing fields, they will be highlighted.\nCorrect those, and then try submitting again. When you are successful, you should see a green banner with a link to the current dataset view. Click the X to close that banner, if you want to continue editing metadata.\n\nSuccess!\n\n\n\nFile and variable level metadata\nThe final major section of metadata concerns the structure and contents of your data files. In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure.\nFor example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information.\n\nThe Attributes tab is where you enter variable (aka attribute) information. In the case of tabular data (such as csv files) each column is an attribute, thus there should be one attribute defined for every column in your dataset. Attribute metadata includes:\n\nvariable name (for programs)\nvariable label (for display)\n\n\n\nvariable definition (be specific)\ntype of measurement\n\n\n\nunits & code definitions\n\n\nYou’ll need to add these definitions for every variable (column) in the file. When done, click Done.\n\nNow the list of data files will show a green checkbox indicating that you have full described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes.\nNote that attribute information is not relevant for data files that do not contain variables, such as the R script in this example. Other examples of data files that might not need attributes are images, pdfs, and non-tabular text documents (such as readme files). The yellow circle in the editor indicates that attributes have not been filled out for a data file, and serves as a warning that they might be needed, depending on the file.\n\nAfter you get the green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes.\n\n\nAdd workflow provenance\nUnderstanding the relationships between files in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, that are often then used in analysis and visualization code to produce final outputs. In DataONE, we support structured descriptions of these relationships, so one can see the flow of data from raw data to derived to outputs.\nYou add provenance by navigating to the data table descriptions, and selecting the Add buttons to link the data and scripts that were used in your computational workflow. On the left side, select the Add circle to add an input data source to the filteredSpecies.csv file. This starts building the provenance graph to explain the origin and history of each data object.\n\nThe linkage to the source dataset should appear.\n\nThen you can add the link to the source code that handled the conversion between the data files by clicking on Add arrow and selecting the R script:\n\nSelect the R script and click “Done.”\n\n\nThe diagram now shows the relationships among the data files and the R script, so click Submit to save another version of the package.\n\nEt voilà! A beatifully preserved data package!\n\n\n\n\nBorer, Elizabeth, Eric Seabloom, Matthew B. Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” Bulletin of the Ecological Society of America 90: 205–14. https://doi.org/10.1890/0012-9623-90.2.205.\n\n\nWhite, Ethan, Elita Baldridge, Zachary Brym, Kenneth Locey, Daniel McGlinn, and Sarah Supp. 2013. “Nine Simple Ways to Make It Easier to (Re)use Your Data.” Ideas in Ecology and Evolution 6 (2). https://doi.org/10.4033/iee.2013.6b.6.f."
  },
  {
    "objectID": "session_12.html",
    "href": "session_12.html",
    "title": "12  Git Workflows",
    "section": "",
    "text": "12.0.1 Learning Objectives\nIn this lesson, you will learn:\n\nNew mechanisms to collaborate using Git\nWhat is a Pull Request in GitHub?\nHow to contribute code to colleague’s repository using Pull Requests\nWhat is a branch in Git?\nHow to use a branch to organize code\nWhat is a tag in Git and how is it useful for collaboration?\n\n\n\n12.0.2 Pull requests\nWe’ve shown in other chapters how to directly collaborate on a repository with colleagues by granting them write privileges as a collaborator to your repository. This is useful with close collaborators, but also grants them tremendous latitude to change files and analyses, to remove files from the working copy, and to modify all files in the repository.\nPull requests represent a mechanism to more judiciously collaborate, one in which a collaborator can suggest changes to a repository, the owner and collaborator can discuss those changes in a structured way, and the owner can then review and accept all or only some of those changes to the repository. This is useful with open source code where a community is contributing to shared analytical software, to students in a lab working on related but not identical projects, and to others who want the capability to review changes as they are submitted.\nTo use pull requests, the general procedure is as follows. The collaborator first creates a fork of the owner’s repository, which is a cloned copy of the original that is linked to the original. This cloned copy is in the collaborator’s GitHub account, which means they have the ability to make changes to it. But they don’t have the right to change the original owner’s copy. So instead, they clone their GitHub copy onto their local machine, which makes the collaborator’s GitHub copy the origin as far as they are concerned. In this scenario, we generally refer to the Collaborator’s repository as the remote origin, and the Owner’s repository as upstream.\n\nPull requests are a mechanism for someone that has a forked copy of a repository to request that the original owner review and pull in their changes. This allows them to collaborate, but keeps the owner in control of exactly what changed.\n\n\n12.0.3 Exercise: Create and merge pull requests\nIn this exercise, work in pairs. Each pair should create a fork of their partner’s training repository, and then clone that onto their local machine. Then they can make changes to that forked repository, and, from the GitHub interface, create a pull request that the owner can incorporate. We’ll walk through the process from both the owner and the collaborator’s perspectives. In the following example, mbjones will be the repository owner, and metamattj will be the collaborator.\n\nChange settings (Owner): Edit the GitHub settings file for your training-test repository, and ensure that the collaborator does not have editing permission. Also, be sure that all changes in your repository are committed and pushed to the origin server.\nFork (Collaborator): Visit the GitHub page for the owner’s GitHub repository on which you’d like to make changes, and click the Fork button. This will create a clone of that repository in your own GitHub account. You will be able to make changes to this forked copy of the repository, but you will not be able to make direct changes to the owner’s copy. After you have forked the repository, visit your GitHub page for your forked repository, copy the url, and create a new RStudio project using that repository url.\n\n\n\nEdit README.md (Collaborator): The collaborator should make one or more changes to the README.md file from their cloned copy of the repository, commit the changes, and push them to their forked copy. At this point, their local repo and GitHub copy both have the changes that they made, but the owner’s repository has not yet been changed. When you now visit your forked copy of the repository on GitHub, you will now see your change has been made, and it will say that This branch is 1 commit ahead of mbjones:main.\n\n\n\nCreate Pull Request (Collaborator): At this point, click the aptly named Pull Request button to create a pull request which will be used to ask that the owner pull in your changes to their copy.\n\n\nWhen you click Create pull request, provide a brief summary of the request, and a more detailed message to start a conversation about what you are requesting. It’s helpful to be polite and concise while providing adequate context for your request. This will start a conversation with the owner in which you can discuss your changes, they can easily review the changes, and they can ask for further changes before the accept and pull them in. The owner of the repository is in control and determines if and when the changes are merged.\n\n\nReview pull request (Owner): The owner will get an email notification that the Pull Request was created, and can see the PR listed in their Pull requests tab of their repsoitory.\n\n\nThe owner can now initiate a conversation about the change, requesting further changes. The interface indicates whether there are any conflicts with the changes, and if not, gives the owner the option to Merge pull request.\n\n\nMerge pull request (Owner): Once the owner thinks the changes look good, they can click the Merge pull request button to accept the changes and pull them into their repository copy. Edit the message, and then click Confirm merge.\n\n\nCongratulations, the PR request has now been merged into the owner’s copy, and has been closed with a note indicating that the changes have been made.\n\n\nSync with owner (Collaborator): Now that the pull request has been merged, there is a new merge commit in the Owner’s repository that is not present in either of the collaborator’s repositories. To fix that, one needs to pull changes from the upstream repository into the collaborator’s local repository, and then push those changes from that local repository to the collaborator’s origin repository.\n\nTo add a reference to the upstream remote (the repository you made your fork from), in the terminal, run:\ngit remote add upstream https://GitHub.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git\nThen to pull from the main branch of the upstream repository, in the terminal, run:\ngit pull upstream main\nAt this point, the collaborator is fully up to date.\n\n\n\n12.0.4 Branches\nBranches are a mechanism to isolate a set of changes in their own thread, allowing multiple types of work to happen in parallel on a repository at the same time. These are most often used for trying out experimental work, or for managing bug fixes for historical releases of software. Here’s an example graph showing a branch2.1 that has changes in parallel to the main branch of development:\n\nThe default branch in almost all repositories is called main, and it is the branch that is typically shown in the GitHub interface and elsewhere. There are many mechanisms to create branches. The one we will try is through RStudio, in which we use the branch dialog to create and switch between branches.\n\n12.0.4.1 Exercise:\nCreate a new branch in your training repository called exp-1, and then make changes to the RMarkdown files in the directory. Commit and push those changes to the branch. Now you can switch between branches using the GitHub interface."
  },
  {
    "objectID": "session_13.html",
    "href": "session_13.html",
    "title": "13  Hands On Synthesis",
    "section": "",
    "text": "Note\n\n\n\nTime for groups to get together and advance in their synthesis projects."
  },
  {
    "objectID": "session_14.html#material-to-include",
    "href": "session_14.html#material-to-include",
    "title": "14  Wrap up and Presentations",
    "section": "14.1 Material to include:",
    "text": "14.1 Material to include:\n\nProject Messaging\nMethods / Analytical Process\nNext Steps\n\n\n14.1.1 Project Messaging\nPresent your message box. High Level. You can structure it within the envelope style visual format or as a section based document.\nMake sure to include:\n\nAudience\nIssue\nProblem\nSo What?\nSolution\nBenefits\n\n\n\n\n14.1.2 Methods / Analytical Process\nProvide an update on your approaches to solving your ‘Problem’. How are you tackling this? If multiple elements, describe each. Present the workflow for your synthesis.\n\n\n\n14.1.3 Next Steps\nArticulate your plan for the next steps of the project. Some things to consider as you plan:"
  }
]