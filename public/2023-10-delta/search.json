[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "",
    "text": "Overview"
  },
  {
    "objectID": "index.html#about-this-training",
    "href": "index.html#about-this-training",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "About this training",
    "text": "About this training\nNCEAS Open Science Synthesis training consists of three 1-week long workshops, geared towards early career researchers. Participants engage in a mix of lectures, exercises, and synthesis research groups to undertake synthesis while learning and implementing best practices for open data science."
  },
  {
    "objectID": "index.html#why-nceas",
    "href": "index.html#why-nceas",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Why NCEAS",
    "text": "Why NCEAS\nThe National Center for Ecological Analysis and Synthesis (NCEAS), a research affiliate of UCSB, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science.\nNCEAS has over 25 years of success with this model among working groups and environmental professionals. Together with the Delta Science Program and the Delta Stewardship Council we are excited to pass along skills, workflows, mindsets learn throughout the years.\n\nWeek 3: Scaling up and presenting synthesis\nOctober 23 – 27, 2023\n\nBig data workflows and parallel computing\nPresenting results using Shiny or flexdashboards\nRevisit reproducible and git workflows\nSynthesis presentations and next steps"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Schedule",
    "text": "Schedule\nUPDATE IMAGE WITH WEEK 3 SCHEDULE"
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "NCEAS Open Science Synthesis for the Delta Science Program",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS with the support of DataONE, to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors for this version are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Halina Do-Linh, Matthew B. Jones, Camila Vargas Poulsen. 2023. Open Science Synthesis training Week 3. NCEAS Learning Hub & Delta Stewardship Council.\nAdditional contributors: Ben Bolker, Julien Brun, Amber E. Budden, Jeanette Clark, Samantha Csik, Stephanie Hampton, Natasha Haycock-Chavez, Samanta Katz, Julie Lowndes, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "session_02.html#communication-principles-and-practices",
    "href": "session_02.html#communication-principles-and-practices",
    "title": "2  Message Box",
    "section": "2.1 Communication Principles and Practices",
    "text": "2.1 Communication Principles and Practices\n“The ingredients of good science are obvious—novelty of research topic, comprehensive coverage of the relevant literature, good data, good analysis including strong statistical support, and a thought-provoking discussion. The ingredients of good science reporting are obvious—good organization, the appropriate use of tables and figures, the right length, writing to the intended audience— do not ignore the obvious” - Bourne 2005\n\n2.1.1 Scholarly publications\nPeer-reviewed publication remains a primary mechanism for direct and efficient communication of research findings. Other scholarly communications include abstracts, technical reports, books and book chapters. These communications are largely directed towards students and peers; individuals learning about or engaged in the process of scientific research whether in a university, non-profit, agency, commercial or other setting. In this section we will focus less on peer-reviewed publications and more on messaging in general, whether for publications, reports, articles or social media. That said, the following tabling is a good summary of ‘10 Simple Rules’ for writing research papers (adapted from Zhang 2014, published in Budden and Michener, 2017)\n\nMake it a Driving Force “design a project with an ultimate paper firmly in mind”\nLess Is More “fewer but more significant papers serve both the research community and one’s career better than more papers of less significance”\nPick the Right Audience “This is critical for determining the organization of the paper and the level of detail of the story, so as to write the paper with the audience in mind.”\nBe Logical “The foundation of ‘‘lively’’ writing for smooth reading is a sound and clear logic underlying the story of the paper.” “An effective tactic to help develop a sound logical flow is to imaginatively create a set of figures and tables, which will ultimately be developed from experimental results, and order them in a logical way based on the information flow through the experiments.”\nBe Thorough and Make It Complete Present the central underlying hypotheses; interpret the insights gleaned from figures and tables and discuss their implications; provide sufficient context so the paper is self-contained; provide explicit results so readers do not need to perform their own calculations; and include self-contained figures and tables that are described in clear legends\nBe Concise “the delivery of a message is more rigorous if the writing is precise and concise”\nBe Artistic “concentrate on spelling, grammar, usage, and a ‘‘lively’’ writing style that avoids successions of simple, boring, declarative sentences”\nBe Your Own Judge Review, revise and reiterate. “…put yourself completely in the shoes of a referee and scrutinize all the pieces—the significance of the work, the logic of the story, the correctness of the results and conclusions, the organization of the paper, and the presentation of the materials.”\nTest the Water in Your Own Backyard “…collect feedback and critiques from others, e.g., colleagues and collaborators.”\nBuild a Virtual Team of Collaborators Treat reviewers as collaborators and respond objectively to their criticisms and recommendations. This may entail redoing research and thoroughly re-writing a paper.\n\n\n\n2.1.2 Other communications\nCommunicating your research outside of peer-reviewed journal articles is increasingly common, and important. These non academic communications can reach a more broad and diverse audience than traditional publications (and should be tailored for specific audience groups accordingly), are not subject to the same pay-walls as journal articles, and can augment and amplify scholarly publications. For example, this twitter announcement from Dr Heather Kropp, providing a synopsis of their synthesis publication in Environmental Research (note the reference to a repository where the data are located, though a DOI would be better).\n Whether this communication occurs through blogs, social media, or via interviews with others, developing practices to refine your messaging is critical for successful communication. One tool to support your communication practice is ‘The Message Box’ developed by COMPASS, an organization that helps scientists develop communications skills in order to share their knowledge and research across broad audiences without compromising the accuracy of their research.\n\n\n2.1.3 The Message Box\n\n\nThe Message Box is a tool that helps researchers take the information they hold about their research and communicate it in a way that resonates with the chosen audience. It can be used to help prepare for interviews with journalists or employers, plan for a presentation, outline a paper or lecture, prepare a grant proposal, or clearly, and with relevancy, communicate your work to others. While the message box can be used in all these ways, you must first identify the audience for your communication.\nThe Message Box comprises five sections to help you sort and distill your knowledge in a way that will resonate with your (chosen) audience. How we communicate with other scientists (through scholarly publications) is not how the rest of the rest of the world typically communicates. In a scientific paper, we establish credibility in the introduction and methods, provide detailed data and results, and then share the significance of our work in the discussion and conclusions. But the rest of the world leads with the impact, the take home message. A quick glance of newspaper headlines demonstrates this.\n\nThe five sections of the Message Box are provided below. For a detailed explanation of the sections and guidance on how to use the Message Box, work through the Message Box Workbook\n\n2.1.3.1 Message Box Sections\nThe Issue\nThe Issue section in the center of the box identifies and describes the overarching issue or topic that you’re addressing in broad terms. It’s the big-picture context of your work. This should be very concise and clear; no more than a short phrase. You might find you revisit the Issue after you’ve filled out your Message Box, to see if your thinking on the overarching topic has changed since you started.\n\nDescribes the overarching issue or topic: Big Picture\nBroad enough to cover key points\nSpecific enough to set up what’s to come\nConcise and clear\n‘Frames’ the rest of the message box\n\nThe Problem\nThe Problem is the chunk of the broader issue that you’re addressing in your area of expertise. It’s your piece of the pie, reflecting your work and expert knowledge. Think about your research questions and what aspect of the specific problem you’re addressing would matter to your audience. The Problem is also where you set up the So What and describe the situation you see and want to address.\n\nThe part of the broader issue that your work is addressing\nBuilds upon your work and expert knowledge\nTry to focus on one problem per audience\nOften the Problem is your research question\nThis section sets you up for So What\n\nThe So What\nThe crux of the Message Box, and the critical question the COMPASS team seeks to help scientists answer, is “So what?” Why should your audience care? What about your research or work is important for them to know? Why are you talking to them about it? The answer to this question may change from audience to audience, and you’ll want to be able to adjust based on their interests and needs. We like to use the analogy of putting a message through a prism that clarifies the importance to different audiences. Each audience will be interested in different facets of your work, and you want your message to reflect their interests and accommodate their needs. The prism below includes a spectrum of audiences you might want to reach, and some of the questions they might have about your work.\n\nThis is the crux of the message box\nWhy should you audience care?\nWhat about your research is important for them to know?\nWhy are you talking to them about it?\n\n\nThe Solution\nThe Solution section outlines the options for solving the problem you identified. When presenting possible solutions, consider whether they are something your audience can influence or act upon. And remind yourself of your communication goals: Why are you communicating with this audience? What do you want to accomplish?\n\nOutlines the options for solving the Problem \nCan your audience influence or act upon this?\nThere may be multiple solutions\nMake sure your Solution relates back to the Problem. Edit one or both as needed\n\nThe Benefit\nIn the Benefit section, you list the benefits of addressing the Problem — all the good things that could happen if your Solution section is implemented. This ties into the So What of why your audience cares, but focuses on the positive results of taking action (the So What may be a negative thing — for example, inaction could lead to consequences that your audience cares about). If possible, it can be helpful to be specific here — concrete examples are more compelling than abstract. Who is likely to benefit, and where, and when?\n\nWhat are the benefits of addressing the Problem?\nWhat good things come from implementing your Solution?\nMake sure it connects with your So What\nBenefits and So What may be similar\n\nFinally, to make you message more memorable you should:\n\nSupport your message with data\nLimit the use of numbers and statistics\nUse specific examples\nCompare numbers to concepts, help people relate\nAvoid jargon\nLead with what you know\n\n\nIn addition to the Message Box Workbook, COMPASS have resources on how to increase the impact of your message (include important statistics, draw comparisons, reduce jargon, use examples), exercises for practicing and refining your message and published examples.\n\n\n\n2.1.4 Resources\n\n\n\nDataONE Webinar: Communication Strategies to Increase Your Impact from DataONE on Vimeo.\n\n\nBudden, AE and Michener, W (2017) Communicating and Disseminating Research Findings. In: Ecological Informatics, 3rd Edition. Recknagel, F, Michener, W (eds.) Springer-Verlag\nCOMPASS Core Principles of Science Communication\nExample Message Boxes"
  },
  {
    "objectID": "session_04.html",
    "href": "session_04.html",
    "title": "4  Flexdashboard",
    "section": "",
    "text": "Outline:\n\nWhat is a flexdashboard?\nHow does it compare to Shiny?\nDemo: How to make one\nTips, tricks, and features\nPractice: Create one using palmer penguins or find other relevant data"
  },
  {
    "objectID": "session_05.html",
    "href": "session_05.html",
    "title": "5  Shiny",
    "section": "",
    "text": "5.0.1 Learning Objectives\nIn this lesson we will:\n\nreview the capabilities in Shiny applications\nlearn about the basic layout for Shiny interfaces\nlearn about the server component for Shiny applications\nbuild a simple shiny application for interactive plotting\n\n\n\n5.0.2 Overview\nShiny is an R package for creating interactive data visualizations embedded in a web application that you and your colleagues can view with just a web browser. Shiny apps are relatively easy to construct, and provide interactive features for letting others share and explore data and analyses.\nThere are some really great examples of what Shiny can do on the RStudio webite like this one exploring movie metadata. A more scientific example is a tool from the SASAP project exploring proposal data from the Alaska Board of Fisheries. There is also an app for Delta monitoring efforts.\n\nMost any kind of analysis and visualization that you can do in R can be turned into a useful interactive visualization for the web that lets people explore your data more intuitively But, a Shiny application is not the best way to preserve or archive your data. Instead, for preservation use a repository that is archival in its mission like the KNB Data Repository, Zenodo, or Dryad. This will assign a citable identifier to the specific version of your data, which you can then read in an interactive visualiztion with Shiny.\nFor example, the data for the Alaska Board of Fisheries application is published on the KNB and is citable as:\nMeagan Krupa, Molly Cunfer, and Jeanette Clark. 2017. Alaska Board of Fisheries Proposals 1959-2016. Knowledge Network for Biocomplexity. doi:10.5063/F1QN652R.\nWhile that is the best citation and archival location of the dataset, using Shiny, one can also provide an easy-to-use exploratory web application that you use to make your point that directly loads the data from the archival site. For example, the Board of Fisheries application above lets people who are not inherently familiar with the data to generate graphs showing the relationships between the variables in the dataset.\nWe’re going to create a simple shiny app with two sliders so we can interactively control inputs to an R function. These sliders will allow us to interactively control a plot.\n\n\n5.0.3 Create a sample shiny application\n\nFile &gt; New &gt; Shiny Web App…\nSet some fields: \n\nName it “myapp” or something else\nSelect “Single File”\nChoose to create it in a new folder called ‘shiny-demo’\nClick Create\n\n\nRStudio will create a new file called app.R that contains the Shiny application.\nRun it by choosing Run App from the RStudio editor header bar. This will bring up the default demo Shiny application, which plots a histogram and lets you control the number of bins in the plot.\n\nNote that you can drag the slider to change the number of bins in the histogram.\n\n\n5.0.4 Shiny architecture\nA Shiny application consists of two functions, the ui and the server. The ui function is responsible for drawing the web page, while the server is responsible for any calculations and for creating any dynamic components to be rendered.\nEach time that a user makes a change to one of the interactive widgets, the ui grabs the new value (say, the new slider min and max) and sends a request to the server to re-render the output, passing it the new input values that the user had set. These interactions can sometimes happen on one computer (e.g., if the application is running in your local RStudio instance). Other times, the ui runs on the web browser on one computer, while the server runs on a remote computer somewhere else on the Internet (e.g., if the application is deployed to a web server).\n\n\n\n5.0.5 Interactive scatterplots\nLet’s modify this application to plot Yolo bypass secchi disk data in a time-series, and allow aspects of the plot to be interactively changed.\n\n5.0.5.1 Load data for the example\nUse this code to load the data at the top of your app.R script. Note we are using contentId again, and we have filtered for some species of interest.\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\nnames(delta_data)\n\n\n\n5.0.5.2 Add a simple timeseries using ggplot\nWe know there has been a lot of variation through time in the delta, so let’s plot a time-series of Secchi depth. We do so by switching out the histogram code for a simple ggplot, like so:\n\nserver &lt;- function(input, output) {\n    \n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            theme_light()\n    })\n}\n\nIf you now reload the app, it will display the simple time-series instead of the histogram. At this point, we haven’t added any interactivity.\nIn a Shiny application, the server function provides the part of the application that creates our interactive components, and returns them to the user interface (ui) to be displayed on the page.\n\n\n5.0.5.3 Add sliders to set the start and end date for the X axis\nTo make the plot interactive, first we need to modify our user interface to include widgits that we’ll use to control the plot. Specifically, we will add a new slider for setting the minDate parameter, and modify the existing slider to be used for the maxDate parameter. To do so, modify the sidebarPanel() call to include two sliderInput() function calls:\n\nsidebarPanel(\n    sliderInput(\"minDate\",\n                \"Min Date:\",\n                min = as.Date(\"1998-01-01\"),\n                max = as.Date(\"2020-01-01\"),\n                value = as.Date(\"1998-01-01\")),\n    sliderInput(\"maxDate\",\n                \"Max Date:\",\n                min = as.Date(\"1998-01-01\"),\n                max = as.Date(\"2020-01-01\"),\n                value = as.Date(\"2005-01-01\"))\n)\n\nIf you reload the app, you’ll see two new sliders, but if you change them, they don’t make any changes to the plot. Let’s fix that.\n\n\n5.0.5.4 Connect the slider values to the plot\nFinally, to make the plot interactive, we can use the input and output variables that are passed into the server function to access the current values of the sliders. In Shiny, each UI component is given an input identifier when it is created, which is used as the name of the value in the input list. So, we can access the minimum depth as input$minDate and the max as input$maxDate. Let’s use these values now by adding limits to our X axis in the ggplot:\n\n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$minDate,input$maxDate)) +\n            theme_light()\n\nAt this point, we have a fully interactive plot, and the sliders can be used to change the min and max of the Depth axis.\n\nLooks so shiny!\n\n\n5.0.5.5 Reversed Axes?\nWhat happens if a clever user sets the minimum for the X axis at a greater value than the maximum? You’ll see that the direction of the X axis becomes reversed, and the plotted points display right to left. This is really an error condition. Rather than use two independent sliders, we can modify the first slider to output a range of values, which will prevent the min from being greater than the max. You do so by setting the value of the slider to a vector of length 2, representing the default min and max date for the slider, such as c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")). So, delete the second slider, rename the first, and provide a vector for the value, like this:\n\nsliderInput(\"date\",\n            \"Date:\",\n            min = as.Date(\"1998-01-01\"),\n            max = as.Date(\"2020-01-01\"),\n            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n)\n\nNow, modify the ggplot to use this new date slider value, which now will be returned as a vector of length 2. The first element of the depth vector is the min, and the second is the max value on the slider.\n\n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            theme_light()\n\n\n\n\n\n5.0.6 Extending the user interface with dynamic plots\nIf you want to display more than one plot in your application, and provide a different set of controls for each plot, the current layout would be too simple. Next we will extend the application to break the page up into vertical sections, and add a new plot in which the user can choose which variables are plotted. The current layout is set up such that the FluidPage contains the title element, and then a sidebarLayout, which is divided horizontally into a sidebarPanel and a mainPanel.\n\n\n5.0.6.1 Vertical layout\nTo extend the layout, we will first nest the existing sidebarLayout in a new verticalLayout, which simply flows components down the page vertically. Then we will add a new sidebarLayout to contain the bottom controls and graph.\n\nThis mechanism of alternately nesting vertical and horizontal panels can be used to segment the screen into boxes with rules about how each of the panels is resized, and how the content flows when the browser window is resized. The sidebarLayout works to keep the sidebar about 1/3 of the box, and the main panel about 2/3, which is a good proportion for our controls and plots. Add the verticalLayout, and the second sidebarLayout for the second plot as follows:\n\n    verticalLayout(\n        # Sidebar with a slider input for depth axis\n        sidebarLayout(\n            sidebarPanel(\n                sliderInput(\"date\",\n                            \"Date:\",\n                            min = as.Date(\"1998-01-01\"),\n                            max = as.Date(\"2020-01-01\"),\n                            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n            ),\n            # Show a plot of the generated distribution\n            mainPanel(\n               plotOutput(\"distPlot\")\n            )\n        ),\n\n        tags$hr(),\n\n        sidebarLayout(\n            sidebarPanel(\n                selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n            ),\n\n            # Show a plot with configurable axes\n            mainPanel(\n                plotOutput(\"varPlot\")\n            )\n        ),\n        tags$hr()\n\nNote that the second sidebarPanel uses three selectInput elements to provide dropdown menus with the variable columns (cols) from our data frame. To manage that, we need to first set up the cols variable, which we do by saving the variables names from the delta_data data frame to a variable:\n\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\ncols &lt;- names(delta_data)\n\n\n\n5.0.6.2 Add the dynamic plot\nBecause we named the second plot varPlot in our UI section, we now need to modify the server to produce this plot. Its very similar to the first plot, but this time we want to use the selected variables from the user controls to choose which variables are plotted. These variable names from the $input are character strings, and so would not be recognized as symbols in the aes mapping in ggplot. As recommended by the tidyverse authors, we use the non-standard evaluation syntax of .data[[\"colname\"]] to access the variables.\n\n    output$varPlot &lt;- renderPlot({\n      ggplot(delta_data, aes(x = .data[[input$x_variable]],\n                             y = .data[[input$y_variable]],\n                             color = .data[[input$color_variable]])) +\n        geom_point(size = 4)+\n        theme_light()\n    })\n\n\n\n\n5.0.7 Finishing touches: data citation\nCiting the data that we used for this application is the right thing to do, and easy. You can add arbitrary HTML to the layout using utility functions in the tags list.\n\n    # Application title\n    titlePanel(\"Yolo Bypass Fish and Water Quality Data\"),\n    p(\"Data for this application are from: \"),\n    tags$ul(\n        tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n        )\n    ),\n    tags$br(),\n    tags$hr(),\n\nThe final application shows the data citation, the depth plot, and the configurable scatterplot in three distinct panels.\n\n\n\n5.0.8 Publishing Shiny applications\nOnce you’ve finished your app, you’ll want to share it with others. To do so, you need to publish it to a server that is set up to handle Shiny apps.\nYour main choices are:\n\nshinyapps.io (Hosted by RStudio)\n\nThis is a service offered by RStudio, which is initially free for 5 or fewer apps and for limited run time, but has paid tiers to support more demanding apps. You can deploy your app using a single button push from within RStudio.\n\nShiny server (On premises)\n\nThis is an open source server which you can deploy for free on your own hardware. It requires more setup and configuration, but it can be used without a fee.\n\nRStudio connect (On premises)\n\nThis is a paid product you install on your local hardware, and that contains the most advanced suite of services for hosting apps and RMarkdown reports. You can publish using a single button click from RStudio.\n\n\nA comparison of publishing features is available from RStudio.\n\n5.0.8.1 Publishing to shinyapps.io\nThe easiest path is to create an account on shinyapps.io, and then configure RStudio to use that account for publishing. Instructions for enabling your local RStudio to publish to your account are displayed when you first log into shinyapps.io:\n\nOnce your account is configured locally, you can simply use the Publish button from the application window in RStudio, and your app will be live before you know it!\n\n\n\n\n5.0.9 Summary\nShiny is a fantastic way to quickly and efficiently provide data exploration for your data and code. We highly recommend it for its interactivity, but an archival-quality repository is the best long-term home for your data and products. In this example, we used data drawn directly from the EDI repository in our Shiny app, which offers both the preservation guarantees of an archive, plus the interactive data exploration from Shiny. You can utilize the full power of R and the tidyverse for writing your interactive applications.\n\n\n5.0.10 Full source code for the final application\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n# read in the data from EDI\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName))\n\ncols &lt;- names(delta_data)\n    \n\n\n# Define UI for application that draws a two plots\nui &lt;- fluidPage(\n    \n    # Application title and data  source\n    titlePanel(\"Sacramento River floodplain fish and water quality dataa\"),\n    p(\"Data for this application are from: \"),\n    tags$ul(\n        tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n        )\n    ),\n    tags$br(),\n    tags$hr(),\n    \n    verticalLayout(\n        # Sidebar with a slider input for time axis\n        sidebarLayout(\n            sidebarPanel(\n                sliderInput(\"date\",\n                            \"Date:\",\n                            min = as.Date(\"1998-01-01\"),\n                            max = as.Date(\"2020-01-01\"),\n                            value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n            ),\n            # Show a plot of the generated timeseries\n            mainPanel(\n                plotOutput(\"distPlot\")\n            )\n        ),\n        \n        tags$hr(),\n        \n        sidebarLayout(\n            sidebarPanel(\n                selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n            ),\n            \n            # Show a plot with configurable axes\n            mainPanel(\n                plotOutput(\"varPlot\")\n            )\n        ),\n        tags$hr()\n    )\n)\n\n# Define server logic required to draw the two plots\nserver &lt;- function(input, output) {\n    \n    #  turbidity plot\n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            theme_light()\n    })\n    \n    # mix and  match plot\n    output$varPlot &lt;- renderPlot({\n      ggplot(delta_data, aes(x = .data[[input$x_variable]],\n                             y = .data[[input$y_variable]],\n                             color = .data[[input$color_variable]])) +\n        geom_point(size = 4) +\n        theme_light()\n    })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n5.0.11 A shinier app with tabs and a map!\n\nlibrary(shiny)\nlibrary(contentid)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(shinythemes)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(snakecase)\n\n# read in the data from EDI\nsha1 &lt;- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'\ndelta.file &lt;- contentid::resolve(sha1, registries=c(\"dataone\"), store = TRUE)\n\n# fix the sample date format, and filter for species of interest\ndelta_data &lt;- read.csv(delta.file) %&gt;% \n    mutate(SampleDate = mdy(SampleDate))  %&gt;% \n    filter(grepl(\"Salmon|Striped Bass|Smelt|Sturgeon\", CommonName)) %&gt;% \n    rename(DissolvedOxygen = DO,\n           Ph = pH,\n           SpecificConductivity = SpCnd)\n\ncols &lt;- names(delta_data)\n\nsites &lt;- delta_data %&gt;% \n    distinct(StationCode, Latitude, Longitude) %&gt;% \n    drop_na() %&gt;% \n    st_as_sf(coords = c('Longitude','Latitude'), crs = 4269,  remove = FALSE)\n\n\n\n# Define UI for application\nui &lt;- fluidPage(\n    navbarPage(theme = shinytheme(\"flatly\"), collapsible = TRUE,\n               HTML('&lt;a style=\"text-decoration:none;cursor:default;color:#FFFFFF;\" class=\"active\" href=\"#\"&gt;Sacramento River Floodplain Data&lt;/a&gt;'), id=\"nav\",\n               windowTitle = \"Sacramento River floodplain fish and water quality data\",\n               \n               tabPanel(\"Data Sources\",\n                        verticalLayout(\n                            # Application title and data  source\n                            titlePanel(\"Sacramento River floodplain fish and water quality data\"),\n                            p(\"Data for this application are from: \"),\n                            tags$ul(\n                                tags$li(\"Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018.\",\n                                        tags$a(\"doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\", href=\"http://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f\")\n                                )\n                            ),\n                            tags$br(),\n                            tags$hr(),\n                            p(\"Map of sampling locations\"),\n                            mainPanel(leafletOutput(\"map\"))\n                        )\n               ),\n               \n               tabPanel(\n                   \"Explore\",\n                   verticalLayout(\n                       mainPanel(\n                           plotOutput(\"distPlot\"),\n                           width =  12,\n                           absolutePanel(id = \"controls\",\n                                         class = \"panel panel-default\",\n                                         top = 175, left = 75, width = 300, fixed=TRUE,\n                                         draggable = TRUE, height = \"auto\",\n                                         sliderInput(\"date\",\n                                                     \"Date:\",\n                                                     min = as.Date(\"1998-01-01\"),\n                                                     max = as.Date(\"2020-01-01\"),\n                                                     value = c(as.Date(\"1998-01-01\"), as.Date(\"2020-01-01\")))\n                                         \n                           )\n                       ),\n                       \n                       tags$hr(),\n                       \n                       sidebarLayout(\n                           sidebarPanel(\n                               selectInput(\"x_variable\", \"X Variable\", cols, selected = \"SampleDate\"),\n                               selectInput(\"y_variable\", \"Y Variable\", cols, selected = \"Count\"),\n                               selectInput(\"color_variable\", \"Color\", cols, selected = \"CommonName\")\n                           ),\n                           \n                           # Show a plot with configurable axes\n                           mainPanel(\n                               plotOutput(\"varPlot\")\n                           )\n                       ),\n                       tags$hr()\n                   )\n               )\n    )\n)\n\n# Define server logic required to draw the two plots\nserver &lt;- function(input, output) {\n    \n    \n    output$map &lt;- renderLeaflet({leaflet(sites) %&gt;% \n            addTiles() %&gt;% \n            addCircleMarkers(data = sites,\n                             lat = ~Latitude,\n                             lng = ~Longitude,\n                             radius = 10, # arbitrary scaling\n                             fillColor = \"gray\",\n                             fillOpacity = 1,\n                             weight = 0.25,\n                             color = \"black\",\n                             label = ~StationCode)\n    })\n    \n    #  turbidity plot\n    output$distPlot &lt;- renderPlot({\n        \n        ggplot(delta_data, mapping = aes(SampleDate, Secchi)) +\n            geom_point(colour=\"red\", size=4) +\n            xlim(c(input$date[1],input$date[2])) +\n            labs(x = \"Sample Date\", y = \"Secchi Depth (m)\") +\n            theme_light()\n    })\n    \n    # mix and  match plot\n    output$varPlot &lt;- renderPlot({\n        ggplot(delta_data, mapping = aes(x = .data[[input$x_variable]],\n                                         y = .data[[input$y_variable]],\n                                         color = .data[[input$color_variable]])) +\n            labs(x = to_any_case(input$x_variable, case = \"title\"),\n                 y = to_any_case(input$y_variable, case = \"title\"),\n                 color = to_any_case(input$color_variable, case = \"title\")) +\n            geom_point(size=4) +\n            theme_light()\n    })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n5.0.12 Resources\n\nMain Shiny site\nOfficial Shiny Tutorial"
  },
  {
    "objectID": "session_09.html",
    "href": "session_09.html",
    "title": "9  Parellel Processing",
    "section": "",
    "text": "Note: foreach package missing!"
  },
  {
    "objectID": "session_12.html",
    "href": "session_12.html",
    "title": "12  Git Workflows",
    "section": "",
    "text": "12.0.1 Learning Objectives\nIn this lesson, you will learn:\n\nNew mechanisms to collaborate using Git\nWhat is a Pull Request in GitHub?\nHow to contribute code to colleague’s repository using Pull Requests\nWhat is a branch in Git?\nHow to use a branch to organize code\nWhat is a tag in Git and how is it useful for collaboration?\n\n\n\n12.0.2 Pull requests\nWe’ve shown in other chapters how to directly collaborate on a repository with colleagues by granting them write privileges as a collaborator to your repository. This is useful with close collaborators, but also grants them tremendous latitude to change files and analyses, to remove files from the working copy, and to modify all files in the repository.\nPull requests represent a mechanism to more judiciously collaborate, one in which a collaborator can suggest changes to a repository, the owner and collaborator can discuss those changes in a structured way, and the owner can then review and accept all or only some of those changes to the repository. This is useful with open source code where a community is contributing to shared analytical software, to students in a lab working on related but not identical projects, and to others who want the capability to review changes as they are submitted.\nTo use pull requests, the general procedure is as follows. The collaborator first creates a fork of the owner’s repository, which is a cloned copy of the original that is linked to the original. This cloned copy is in the collaborator’s GitHub account, which means they have the ability to make changes to it. But they don’t have the right to change the original owner’s copy. So instead, they clone their GitHub copy onto their local machine, which makes the collaborator’s GitHub copy the origin as far as they are concerned. In this scenario, we generally refer to the Collaborator’s repository as the remote origin, and the Owner’s repository as upstream.\n\nPull requests are a mechanism for someone that has a forked copy of a repository to request that the original owner review and pull in their changes. This allows them to collaborate, but keeps the owner in control of exactly what changed.\n\n\n12.0.3 Exercise: Create and merge pull requests\nIn this exercise, work in pairs. Each pair should create a fork of their partner’s training repository, and then clone that onto their local machine. Then they can make changes to that forked repository, and, from the GitHub interface, create a pull request that the owner can incorporate. We’ll walk through the process from both the owner and the collaborator’s perspectives. In the following example, mbjones will be the repository owner, and metamattj will be the collaborator.\n\nChange settings (Owner): Edit the GitHub settings file for your training-test repository, and ensure that the collaborator does not have editing permission. Also, be sure that all changes in your repository are committed and pushed to the origin server.\nFork (Collaborator): Visit the GitHub page for the owner’s GitHub repository on which you’d like to make changes, and click the Fork button. This will create a clone of that repository in your own GitHub account. You will be able to make changes to this forked copy of the repository, but you will not be able to make direct changes to the owner’s copy. After you have forked the repository, visit your GitHub page for your forked repository, copy the url, and create a new RStudio project using that repository url.\n\n\n\nEdit README.md (Collaborator): The collaborator should make one or more changes to the README.md file from their cloned copy of the repository, commit the changes, and push them to their forked copy. At this point, their local repo and GitHub copy both have the changes that they made, but the owner’s repository has not yet been changed. When you now visit your forked copy of the repository on GitHub, you will now see your change has been made, and it will say that This branch is 1 commit ahead of mbjones:main.\n\n\n\nCreate Pull Request (Collaborator): At this point, click the aptly named Pull Request button to create a pull request which will be used to ask that the owner pull in your changes to their copy.\n\n\nWhen you click Create pull request, provide a brief summary of the request, and a more detailed message to start a conversation about what you are requesting. It’s helpful to be polite and concise while providing adequate context for your request. This will start a conversation with the owner in which you can discuss your changes, they can easily review the changes, and they can ask for further changes before the accept and pull them in. The owner of the repository is in control and determines if and when the changes are merged.\n\n\nReview pull request (Owner): The owner will get an email notification that the Pull Request was created, and can see the PR listed in their Pull requests tab of their repsoitory.\n\n\nThe owner can now initiate a conversation about the change, requesting further changes. The interface indicates whether there are any conflicts with the changes, and if not, gives the owner the option to Merge pull request.\n\n\nMerge pull request (Owner): Once the owner thinks the changes look good, they can click the Merge pull request button to accept the changes and pull them into their repository copy. Edit the message, and then click Confirm merge.\n\n\nCongratulations, the PR request has now been merged into the owner’s copy, and has been closed with a note indicating that the changes have been made.\n\n\nSync with owner (Collaborator): Now that the pull request has been merged, there is a new merge commit in the Owner’s repository that is not present in either of the collaborator’s repositories. To fix that, one needs to pull changes from the upstream repository into the collaborator’s local repository, and then push those changes from that local repository to the collaborator’s origin repository.\n\nTo add a reference to the upstream remote (the repository you made your fork from), in the terminal, run:\ngit remote add upstream https://GitHub.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git\nThen to pull from the main branch of the upstream repository, in the terminal, run:\ngit pull upstream main\nAt this point, the collaborator is fully up to date.\n\n\n\n12.0.4 Branches\nBranches are a mechanism to isolate a set of changes in their own thread, allowing multiple types of work to happen in parallel on a repository at the same time. These are most often used for trying out experimental work, or for managing bug fixes for historical releases of software. Here’s an example graph showing a branch2.1 that has changes in parallel to the main branch of development:\n\nThe default branch in almost all repositories is called main, and it is the branch that is typically shown in the GitHub interface and elsewhere. There are many mechanisms to create branches. The one we will try is through RStudio, in which we use the branch dialog to create and switch between branches.\n\n12.0.4.1 Exercise:\nCreate a new branch in your training repository called exp-1, and then make changes to the RMarkdown files in the directory. Commit and push those changes to the branch. Now you can switch between branches using the GitHub interface."
  }
]