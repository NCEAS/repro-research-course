[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gulf Ecosystem Initiative:  Fisheries and Extreme Weather Working Group Training Material",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#welcome-to-nceas-learning-hub",
    "href": "index.html#welcome-to-nceas-learning-hub",
    "title": "Gulf Ecosystem Initiative:  Fisheries and Extreme Weather Working Group Training Material",
    "section": "Welcome to NCEAS Learning Hub",
    "text": "Welcome to NCEAS Learning Hub\nThe NCEAS Learning Hub’s mission is to support environmental scientists throughout their data science journey. We teach cutting-edge data science curriculum, facilitate collaborative learning, and promote best practices in open science. We empower data scientists to more efficiently answer environmental questions."
  },
  {
    "objectID": "index.html#working-group-training",
    "href": "index.html#working-group-training",
    "title": "Gulf Ecosystem Initiative:  Fisheries and Extreme Weather Working Group Training Material",
    "section": "Working Group Training",
    "text": "Working Group Training\nThis book contains NCEAS training materials on topics requested by the group. These topics range from Data Management to Team Science and Data Science tools. We will add content as your project advances and you identify training that could help you advance your work.\n\n\n\n\n\n\n\n\nWeek 1 Traning Learning Objectives:\n\nGain general understanding on data management best practices and how to apply them into the group’s work.\nGet everyone set up on NCEAS Server Aurora to use for analytical work.\nIntroduce how to work with Git and GitHub and why is it important in a synthesis context."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Gulf Ecosystem Initiative:  Fisheries and Extreme Weather Working Group Training Material",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Gulf Ecosystem Initiative:  Fisheries and Extreme Weather Working Group Training Material",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Camila Vargas Poulsen, Halina Do-Linh, Julien Brun, Amber E. Budden, Jeanette Clark, Samantha Csik, Julie Lowndes, Matthew B. Jones (2023). NCEAS Learning Hub, Working Group Training.\nAdditional contributors: Ben Bolker, Samantha Csik, Natasha Haycock-Chavez, Stephanie Hampton, Samanta Katz, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "session_01.html#learning-objectives",
    "href": "session_01.html#learning-objectives",
    "title": "1  Data Management Essentials",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the importance of data management for successfully preserving data\nReview the Data Life Cycle and how it can guide the data management in a project\nFamiliarize with what goes into a data management plans\nLearn about metadata guidelines and best practices for reproducibility\nBecome familiar with environmental data repositories for accessing and publishing data"
  },
  {
    "objectID": "session_01.html#the-big-idea",
    "href": "session_01.html#the-big-idea",
    "title": "1  Data Management Essentials",
    "section": "1.1 The Big Idea",
    "text": "1.1 The Big Idea\n\nThis lesson aims to get you thinking about how are you going to manage your data. Even though you are in the early stages of defining you synthesis project, we believe that when it comes to data management, the earlier you start thinking about it the better."
  },
  {
    "objectID": "session_01.html#the-data-problem",
    "href": "session_01.html#the-data-problem",
    "title": "1  Data Management Essentials",
    "section": "1.2 The Data Problem",
    "text": "1.2 The Data Problem\nWhen working with data, specially when your projects has lots of moving pieces (like a synthesis project) things can get easily get really messy leading to DATA CHAOS.\nWe know the situation is chaotic when we can’t find the files we need to use, when our file names are so long because of all the versions we have saved, when we are searching on our email history for that one fact that we know our colleague send us at some point. This is a problem. This is a data and management problem."
  },
  {
    "objectID": "session_01.html#introduction-to-data-management",
    "href": "session_01.html#introduction-to-data-management",
    "title": "1  Data Management Essentials",
    "section": "1.3 Introduction to Data Management",
    "text": "1.3 Introduction to Data Management\nIn its most simple form, data management is about taking care of your data better so you don’t experience small frustrations when actively working with your data. It is the process of handling, organizing, documenting, and preserving data used in a research project. This is particularly important in synthesis science given the nature of synthesis, which involves combining data and information from multiple sources to answer broader questions, generate knowledge and provide insights into a particular problem or phenomenon.\nData management, aims to provide a structure on how to organize all the data inputs and output along your research project. The goal is to avoid data chaos at all cost. The rule of thumb is that by every minute you spend managing your data, can save you 10 min of headache later (Briney (2015)).\n\n1.3.1 Benefits of managing your data\nSuccessfully managing your data throughout a research project helps ensures its preservation for future use. It also facilitates collaboration within your team, and it helps advance your scientific outcomes.\nAdvancement of science\n\nData is a valuable asset – it is expensive and time consuming to collect\nMaximize the effective use and value of data and information assets\nContinually improve the quality including: data accuracy, integrity, integration, timeliness of data capture and presentation, relevance, and usefulness\nEnsure appropriate use of data and information\nFacilitate data sharing\nEnsure sustainability and accessibility in long term for re-use in science\n\nFrom a researcher perspective\n\nKeep yourself organized – be able to find your files (data inputs, analytic scripts, outputs at various stages of the analytic process, etc.)\nTrack your science processes for reproducibility – be able to match up your outputs with exact inputs and transformations that produced them\nBetter control versions of data – easily identify versions that can be periodically purged\nQuality control your data more efficiently\nTo avoid data loss (e.g. making backups)\nFormat your data for re-use (by yourself or others)\nBe prepared to document your data for your own recollection, accountability, and re-use (by yourself or others)\nGain credibility and recognition for your science efforts through data sharing!\n\nAdvancement of science\n\nData is a valuable asset – it is expensive and time consuming to collect\nMaximize the effective use and value of data and information assets\nContinually improve the quality including: data accuracy, integrity, integration, timeliness of data capture and presentation, relevance, and usefulness\nEnsure appropriate use of data and information\nFacilitate data sharing\nEnsure sustainability and accessibility in long term for re-use in science\n\n\n\n\n\n\n\nDefining Data Management\n\n\n\n… is the compilation of many small practices that make your data easier to find, easier to understand, less likely to be lost, and more likely to be useable during a project or ten [or more] years later.”\nKristin Briney, Data Management for Researchers (Briney (2015))"
  },
  {
    "objectID": "session_01.html#the-data-life-cycle",
    "href": "session_01.html#the-data-life-cycle",
    "title": "1  Data Management Essentials",
    "section": "1.4 The Data Life Cycle",
    "text": "1.4 The Data Life Cycle\nThe Data Life Cycle gives you an overview of meaningful steps data goes through in a research project, from planning to archival. This step-by-step breakdown facilitates overseeing individual actions, operations and processes required at each stage. This is a visual tool that aims o help scientists plan and anticipate what will be the “data needs” for a specific project (Faundeen et al 2013) .\n\n\n\nSource: Adapted from Faundeen et al 2013, USGS & DataONE\n\n\n\n1.4.1 Primary Elements\n\n\n\n\n\n\n\n\nStep\nDescription\nTool\n\n\n\n\nPlan\nMap out the processes and resources for all activities related to the handling of the project’s data assets. Start with the project goals (desired outputs, outcomes, and impacts) and work backwards to build a data management plan, supporting data policies, and sustainability plans for each step.\nData Management Plan (DPM)\n\n\nAcquire & Discover\nActivities needed to collect new or existing data. You can structure the process of collecting data upfront to better implement data management. Consider data policies and best practices that maintain the provenance and integrity of the data.\nIdentifying data sources and mechanisms to access data\n\n\nProcess\nEvery step needed to prepare new or existing data to be able to use it as an input for synthesis. Consider the structure of the data, unit transformation, extrapolations, etc\nCleaning & Wrangling data skills\n\n\nIntegrate\nData from multiple sources are combined into a form that can be readily analyzed. Successful data integration depends on documentation of the integration process, clearly citing and making accessible the data you are using, and employing good data management practices throughout the Data Life Cycle.\nModeling & Interpretation\n\n\nAnalyze\nCreate analyses and visualizations to identify patterns, test hypotheses, and illustrate findings. During this process, record your methods, document data processing steps, and ensure your data are reproducible. Learn about these best practices and more.\nModeling, Interpretation & Statistics\n\n\nPreserve\nPlan on how you are going to store your data for long-term use and accessibility so others can access, interpret, and use the data in the future. Decide what data to preserve, where to preserve it, and what documentation needs to accompany the data.\nData packages & repositories\n\n\nPublish and Share\nPublication and distribution of your data through the web or in data catalogs, social media or other venues to increase the chances of your data being discovered. Data is a research product as publications are.\nDOIs and citations\n\n\n\n\n\n1.4.2 Cross-Cutting Elements\nThese elements are involved across all stages describes above. They need to constantly by addressed throughout all the Data Life Cycle, making sure effective data management is in place.\n\n\n\n\n\n\n\n\nElement\nDescription\nTool\n\n\nDescribe\nDocument data and each of the data stages by describing the why, who, what, when, where, and how of the data and processes. Metadata, or data about data, is key to data sharing and reuse. Documentation such as software code comments, data models, and workflows facilitate indexing, acquiring, understanding, and future uses of the data.\nMetadata and documentation\n\n\nManage Quality\nEmploy quality assurance and quality control procedures that enhance the quality of data, making sure the measurements or outputs within expected values.Identify potential errors and techniques to address them.\nQuality Control and Quality Assurance techniques\n\n\nBackup and Secure\nPlan to preserve data in the short term to minimize potential losses (e.g., via software failure, human error, natural disaster). This avoids risk and ensures data is accessible to collaborators. This applies to raw and process data, original science plan, data management plan, data acquisition strategy, processing procedures, versioning, analysis methods, published products, and associated metadata.\nServers, secure data sharing services"
  },
  {
    "objectID": "session_01.html#data-management-plans",
    "href": "session_01.html#data-management-plans",
    "title": "1  Data Management Essentials",
    "section": "1.5 Data Management Plans",
    "text": "1.5 Data Management Plans\nAs you can see there is a lot happening around the Data Life Cycle. This is why PLANNING is a key first step. It is advisable to initiate your data management planning at the beginning of your research process before any data has been collected or discovered.\nIn order to better plan and keep track of all the moving pieces when working with data, a good place to start is creating a Data Management Plan. However, this is not only the starting point. This is a “living” document that should be consulted and updated throughout the project.\nA Data Management Plan (DMP) is a document that describes how you will use your data during a research project, as well as what you will do with your data long after the project ends. DMPs should be updated as research plans change to ensure new data management practices are captured (Environmental Data Initiative).\nA well-thought-out plan means you are more likely to:\n\nstay organized\nwork efficiently\ntruly share data\nengage your team\nmeet funder requirements as DMPs are becoming common in the submission process for proposals\n\nA DMP is both a straightforward blueprint for how you manage your data, and provides guidelines for you and your team on policies, roles and responsibilities. While it is important to plan, it is equally important to recognize that no plan is perfect, as change is inevitable. To make your DMP as robust as possible, review it periodically with your team and adjust as the needs of the project change.\n\n\n\n\n\n\nHow to plan\n\n\n\n\nPlan early - information gets lost over time. Think about your data needs as you are starting your project.\nPlan in collaboration - engaging all the team makes your plan more resilient, including diverse expertise and perspectives.\nMake revision part of the process - adapt as needed, revising your plan helps you make sure your are on track.\nInclude a tidy data and data ethic lens. It is important to start thinking through these lenses during the planning process of your DMP, it will make it easier to include and maintain tidy and ethical principles throughout the entire project.\n\n\n\n\n1.5.1 Creating a Good Data management Plan\nThe article Ten Simple Rules for Creating a Good Data Management Plan (Michener (2015)) outlines big picture ideas to keep in mind when you start your “planning stage”. Here we summarize each point and provide useful resources or examples to help you achieve this “rules” and write an awesome DMP.\n\n1.5.1.1 Determine what are the organization and/or sponsor requirements\n\nGenerally, each organization or funding agency have specific expectations on how to manage and disseminate data. Even though a DMP is a good idea to keep you organize. It will save you significant time and effort by first understanding the specific requirements set by the organization you are working for. Each organization often provide templates on how to structure your DMP.\nResources\n\nThe DMPTool provides templates for different funding agencies plan requirements.\nUSGS has multiple resources on DMPs. Here the Data Policy and Guidance for the Climate Adaptation Science Centers\n\n\n\n\n1.5.1.2 Identify the desired/necessary data sets for the project\n\nData is the ultimate reason why we create a DMP. Identifying what data will be use is crucial to planning. Key aspects of the data to consider are:\n\nType (text, spatial, images, tabular, etc)\nSource (where does the data currently live?, is it proprietary data?)\nVolume (10 terabytes, 10 megabytes?)\nFormat (csv, xlsx, shapefiles, etc)\n\nResource\n\nHere is a template spreadsheet to collect all information about the data set you intent to use for your synthesis project. Please make a copy and adapt as needed.\n\n\n\n\n1.5.1.3 Define how the data will be organized\n\nOnce you know the data you will be using (rule #2) it is time to define how are you going to work with your data. Where will the raw data live? How are the different collaborators going to access the data? The needs vary widely from one project to another depending on the data. When drafting your DMP is helpful to focus on identifying what products and software you will be using. When collaborating with a team it is important to identify f there are any limitations to accessing any software or tool.\nResource\n\nHere is an example from the LTER Scientific Computing Support Team on working on NCEAS Server.\n\n\n\n\n1.5.1.4 Explain how the data will be documented\n\nWe know documenting data is very important. To successfully achieve this we need a plan in place. Three main steps to plan accordingly are:\n\nIdentifying the type of information you want/need to collect to document your data thoroughly\nDetermine if the is a metadata standard or schema (organized set of elements) you will follow (eg. EML, Dublin Core, ISO 19115, ect). In many cases this relates with what data repository you intend to archive your data.\nEstablish tools that can help you create and manage metadata content.\n\nResource\n\nExcel-to EML by Li Kui is a workflow that provides a spreadsheet template to collect metadata based on the Ecological Metadata Language Schema (EML)\nThe Environmental Data Repository (EDI) providees a tool called ezEML. A form-based web application that helps streamline the process of creating EML metadata.\n\n\n\n\n1.5.1.5 Describe how data quality will be assured\n\nQuality assurance and quality control (QA/QC) are the procedures taken to ensure data looks how we expect it to be. The ultimate goal is to improve the quality of the data products. Some fields of study, data types or funding organizations have specific set of guidelines for QA/QCing data. However, when writing your DMP it is important to describe what measures you plan to take to QA/QC the data (e.g: instrument calibration, verification tests, visualization approaches for error detection, etc.)\nResources\n\nEnvironmental Data Initiative (EDI) description and examples of Quality Assurance and Quality Control\n\n\n\n\n1.5.1.6 Have a data storage strategy (short and long term)\n\nPapers get lost, hardware disk crash, URLs break, different media format degrade. It’s inevitable! Plan ahead and think on where your data will live in the short and long-term to ensure the access and use of this data during and long after the project. It is important to have a backup mechanism in place during the project to avoid losing any information.\nResource\n\nRemote locations to store your data during your project are: institutional repositories or servers or commercial services such as Amazon, Dropbox, Google, Microsoft, Box, etc.\nLong-term storage: identify an appropriate and stable data repository for your research domain (See section 3.6 Data Preservation and Sharing)\n\n\n\n\n1.5.1.7 Define the project’s data policies\n\nMany organizations and institutions require to include an explicit policy statement about how data will be managed and shared. Including licensing or sharing arrangements and legal and ethical restrictions on access and use of human subject and other sensitive data. It is valuable to establish a shared agreement around handling of data when embarking on collaborative projects. Collaborative research brings together data from across research projects with different data management plans and can include publicly accessible data from repositories where no management plan is available. For these reasons, a discussion and agreement around the handling of data brought into and resulting from the collaboration is warranted, and management of this new data may benefit from going through a data management planning process.\nResource\n\nTemplate provided by the Arctic Data Center, including sections for individual data not in the public domain, individual data with public access, derived data resulting from the project.\n\n\n\n\n1.5.1.8 What data products will be made available and how?\n\nThis portion of the DMP tries to ensure that the data products of your project will be disseminated. This can be achieved by stating how, when and where these products will be available. We encourage open data practices, this means making data extensively available and with the least restrictions possible.\nExamples\n\nPublishing the data in an open repository or archive\nSubmitting the data (or subsets thereof) as appendices or supplements to journal articles\nPublishing the data, metadata, and relevant code as a “data paper”\n\n\n\n\n1.5.1.9 Assign roles and responsibilities\n\nIt is important to clearly determine the roles and responsibilities of each group member of the project. Roles may include data collection, data entry, QA/QC, metadata creation and management, backup, data preparation and submission to an archive, and systems administration.\n\n\n\n1.5.1.10 Prepare a realistic budget\n\nGenerally overlooked, but preparing a realistic budget it’s an important part when planning for your data management. Data management takes time and it may have cost associates to it for example access to software, hardware, and personnel. Make sure you plan considers budget to support people involved as well as software or data fees or other services as needed."
  },
  {
    "objectID": "session_01.html#metadata-best-practices",
    "href": "session_01.html#metadata-best-practices",
    "title": "1  Data Management Essentials",
    "section": "1.6 Metadata Best Practices",
    "text": "1.6 Metadata Best Practices\nWithin the data life cycle you can be collecting data (creating new data) or integrating data that has all ready been collected. Either way, metadata plays plays a major role to successfully spin around the cycle because it enables data reuse long after the original collection.\nImagine that you’re writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what’s inside your data files?\nThe goal is to have enough information for the researcher to understand the data, interpret the data, and then reuse the data in another study.\nHere we walk you through detail guidelines to keep track of have a well documented data.\n\n1.6.1 Overall Guidelines\nAnother way to think about metadata is to answer the following questions with the documentation:\n\nWhat was measured?\nWho measured it?\nWhen was it measured?\nWhere was it measured?\nHow was it measured?\nHow is the data structured?\nWhy was the data collected?\nWho should get credit for this data (researcher AND funding agency)?\nHow can this data be reused (licensing)?\n\n\n\n1.6.2 Bibliographic Guidelines\nThe details that will help your data be cited correctly are:\n\nGlobal identifier like a digital object identifier (DOI)\nDescriptive title that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data\nDescriptive abstract that serves as a brief overview off the specific contents and purpose of the data package\nFunding information like the award number and the sponsor\nPeople and organizations like the creator of the dataset (i.e. who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset\n\n\n\n1.6.3 Discovery Guidelines\nThe details that will help your data be discovered correctly are:\n\nGeospatial coverage of the data, including the field and laboratory sampling locations, place names and precise coordinates\nTemporal coverage of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to\nTaxonomic coverage of the data, including what species were measured and what taxonomy standards and procedures were followed\nAny other contextual information as needed\n\n\n\n1.6.4 Interpretation Guidelines\nThe details that will help your data be interpreted correctly are:\n\nCollection methods for both field and laboratory data the full experimental and project design as well as how the data in the dataset fits into the overall project\nProcessing methods for both field and laboratory samples\nAll sample quality control procedures\nProvenance information to support your analysis and modelling methods\nInformation about the hardware and software used to process your data, including the make, model, and version\nComputing quality control procedures like testing or code review\n\n\n\n1.6.5 Data Structure and Contents\n\nEverything needs a description: the data model, the data objects (like tables, images, matrices, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation.\nVariable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (i.e. 0 = not collected), and any missing values (i.e. 999 = NA).\n\nNot only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable).\nFor example, if you were to search for the character string “carbon dioxide flux” in a data repository, not all relevant results will be shown due to varying vocabulary conventions (i.e., “CO2 flux” instead of “carbon dioxide flux”) across disciplines — only datasets containing the exact words “carbon dioxide flux” are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search.\n\n\n1.6.6 Rights and Attribution\nCorrectly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like:\n\nCitation format to be used when giving credit for the data\nAttribution expectations for the dataset\nReuse rights, which describe who may use the data and for what purpose\nRedistribution rights, which describe who may copy and redistribute the metadata and the data\nLegal terms and conditions like how the data are licensed for reuse.\n\n\n\n1.6.7 Metadata Standards\nSo, how does a computer organize all this information? There are a number of metadata standards that make your metadata machine readable and therefore easier for data curators to publish your data.\n\nEcological Metadata Language (EML)\nGeospatial Metadata Standards (ISO 19115 and ISO 19139)\n\nSee NOAA’s ISO Workbook\n\nBiological Data Profile (BDP)\nDublin Core\nDarwin Core\nPREservation Metadata: Implementation Strategies (PREMIS)\nMetadata Encoding Transmission Standard (METS)\n\nNote this is not an exhaustive list.\n\n\n1.6.8 Data Identifiers\nMany journals require a DOI (a digital object identifier) be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to.\nSome data repositories assign a DOI for each dataset you publish on their repository. But, if you need to update the datasets, check the policy of the data repository. Some repositories assign a new DOI after you update the dataset. If this is the case, researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available.\n\n\n1.6.9 Data Citation\nResearchers should get in the habit of citing the data that they use (even if it’s their own data!) in each publication that uses that data.\n\n\n1.6.10 Metadata tools (EML)\n\nExcel-to EML by Li Kui is a workflow that provides a spreadsheet template to collect metadata based on the Ecological Metadata Language Schema (EML)\nThe Environmental Data Repository (EDI) provides a tool called ezEML. A form-based web application that helps streamline the process of creating EML metadata."
  },
  {
    "objectID": "session_01.html#data-preservation-sharing",
    "href": "session_01.html#data-preservation-sharing",
    "title": "1  Data Management Essentials",
    "section": "1.7 Data Preservation & Sharing",
    "text": "1.7 Data Preservation & Sharing\n\n\n1.7.1 Data Packages\n\nWe define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve.\n\nSometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\nMany data repositories assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier, often assigned to the metadata and becomes a publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. This allows to identify a digital entity within a data package.\nIn the graphic to the side, the package can be cited with the DOI doi:10.5063/F1Z1899CZ,and each of the individual files have their own identifiers as well.\n\n\n\n1.7.2 Data Repositories: Built for Data (and code)\n\nGitHub is not an archival location\nExamples of dedicated data repositories:\n\nKNB\nArctic Data Center\ntDAR\nEDI\nZenodo\n\nDedicated data repositories are:\n\nRich in metadata\nArchival in their mission\nCertified\n\nData papers, e.g., Scientific Data\nre3data is a global registry of research data repositories\nRepository Finder is a pilot project and tool to help researchers find an appropriate repository for their work\n\n\n1.7.2.1 DataOne Federation\nDataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses.\nDataONE can be searched on the web, which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the (currently 44!) repositories one at a time."
  },
  {
    "objectID": "session_01.html#data-management-summary",
    "href": "session_01.html#data-management-summary",
    "title": "1  Data Management Essentials",
    "section": "1.8 Data Management Summary",
    "text": "1.8 Data Management Summary\n\nThe Data Life Cycle help us see the big picture of our data project.\nIt is extremely helpful to develop a data management plan describing each step of the data life cycle to stay organized.\nDocument everything. Having rich metadata is a key factor to enable data reuse. Describe your data and files and use an appropriate metadata standard.\nIdentify software and tools that will help you and your team organize and document the project’s data life cycle.\nPublish your data in a stable long live repository and assign a unique identifier."
  },
  {
    "objectID": "session_01.html#resources",
    "href": "session_01.html#resources",
    "title": "1  Data Management Essentials",
    "section": "Resources",
    "text": "Resources\n\nDMP Example: Using NASA Remote Sensing Data to Reduce Uncertainty of Land-Use Transitions in Global Carbon-Climate Models\nDPM Example: USGS Coastal and Marine Science Center Data Management Plan\nUSGS Data Management Plan Checklist\nSOP for data management for Ocean Health Index assessments (2023 version)"
  },
  {
    "objectID": "session_01.html#references",
    "href": "session_01.html#references",
    "title": "1  Data Management Essentials",
    "section": "References",
    "text": "References\n\n\n\n\nBriney, Kristin. 2015. Data Management for Researchers : Organize, Maintain and Share Your Data for Research Success. Exeter, UK: Pelagic Publishing.\n\n\nMichener, William K. 2015. “Ten Simple Rules for Creating a Good Data Management Plan.” PLOS Computational Biology 11 (10): 1–9. https://doi.org/10.1371/journal.pcbi.1004525."
  },
  {
    "objectID": "session_02.html#learning-objectives",
    "href": "session_02.html#learning-objectives",
    "title": "2  Server and Git and GitHub Setup",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this module, you will be able to:\n\nSummarize the primary steps for getting set up on a server\nConnect your GitHub self with your server self"
  },
  {
    "objectID": "session_02.html#overview",
    "href": "session_02.html#overview",
    "title": "2  Server and Git and GitHub Setup",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nWorking on NCEAS’ Server is similar to working on an entirely separate computer from the laptop or desktop computer on which you typically work. This means that you need to go through the steps of connecting GitHub to your “RStudio” again for the instance of RStudio accessed through Aurora. GitHub’s Personal Access Token is referred to as “token” hereafter for simplicity."
  },
  {
    "objectID": "session_02.html#getting-started-on-the-server",
    "href": "session_02.html#getting-started-on-the-server",
    "title": "2  Server and Git and GitHub Setup",
    "section": "2.2 Getting Started on the Server",
    "text": "2.2 Getting Started on the Server\n\n\n\n\n\n\nAcknowledgement\n\n\n\nThis section was developed by the LTER Scientific Computing Team. Thank you!\n\n\n\n2.2.1 Necessary Software\nThe only software that you will need on your personal computer to get set up on NCEAS’ server is RStudio!\nYou likely have worked in the “Console” tab of RStudio (where run lines and outputs appear; see below) but RStudio has another tab to the right of the “Console” called “Terminal” which offers RStudio users access to the command line (a.k.a. the shell). If you are a veteran command line user you may prefer to use the standalone Terminal app on MacOS or PuTTY on Windows but for the sake of keeping your tool kit streamlined, we’ll walk through getting set up on NCEAS’ server using only RStudio’s Terminal tab.\n\n\n\nThe Terminal does not accept R syntax (and the Console doesn’t accept Terminal syntax) so you may notice that some of the code we’ll walk you through below is formatted differently than you would write an R script.\n\n\n\n\n\n2.2.2 Get your Invite Ready!\nAfter your group RSVP’d for this workshop, our team contacted NCEAS’ IT team to get you an invite email to create an account on the server. An example of what that email may look like is included below but there are two key pieces of information:\n\nYour username\nYour temporary password (covered by a red bar in the screenshot).\n\nIf you have not received that email, check your Spam folder for emails from Thomas Hetmank (hetmank@nceas.ucsb.edu) or Nick Outin (outin@nceas.ucsb.edu). If you have not received the email and it is not in your Spam, reach out to our team and we will work to get an invite sent to you.\n\n\n\n\n\n2.2.3 Signing into the Server\nIn the following instructions, all words that look like this should be typed into the Terminal tab and run by pressing return or enter. Note that typing these commands into an R script or R Markdown will not work because it will attempt to run in the Console. All words that look [like this] (i.e., bracketed) should also be typed into the Terminal tab but the specific text should be replaced in a user-specific way that is clarified in the nearby text.\n\nIn the Terminal pane of RStudio, you will “ssh” into the server by running the following code: ssh [your username]@aurora.nceas.ucsb.edu. It is @aurora because the name of the server is Aurora.\nIf this is the first time you’ve accessed the server you will need to enter yes to accept the server’s SSH key.\nYou will then be prompted to enter your [temporary password] (see the above email example). Note that the cursor will not advance as you type but it is registering your entries! There will be no indication that you are typing (such as: “•••••••••”) This throws off many users so hopefully the above note helps set your mind at ease.\nYou will then be prompted to change your “expired” password. We consider your temporary password to be expired as soon as you use it because sharing passwords via email is not secure and this “expiration” lets you set the password to something that only you know at the outset of your time in the server. Note again that the cursor will not advance as you type but it is working! To update your password, enter your [temporary password], then [your strong new password] and finally re-type [your strong new password] to confirm it. Note that your new password should not be “your strong new password” :)\nYou are ready to go! Run exit to log out of the server in the Terminal tab.\nNow that you have set a new password, use your favorite web browser (e.g., Firefox, Chrome, etc.) to access Aurora and click “Login to RStudio Server”\nIn the resulting page, you can sign in with the same username and password you just signed in on the Terminal tab with.\nYou should now be in something that looks very much like RStudio but is housed in a tab on your browser! We will work together from here on out so once you have reached this point, let our team know and we can gather the group before continuing.\n\nIf the above steps have not resulted in successfully accessing Aurora, consult NCEAS’ instructions on first login and/or SSH-specific instructions for Mac vs. Windows and/or email us!"
  },
  {
    "objectID": "session_02.html#configuring-git-and-github-on-the-server",
    "href": "session_02.html#configuring-git-and-github-on-the-server",
    "title": "2  Server and Git and GitHub Setup",
    "section": "2.3 Configuring Git and GitHub on the server",
    "text": "2.3 Configuring Git and GitHub on the server\n\n2.3.1 Set up global options in Git\nBefore using Git, you need to tell it who you are, also known as setting the global options. To do this, we will be setting the global options in the Terminal.\n\n\n\n\n\n\nWhat’s the Terminal?\n\n\n\nTechnically, the Terminal is an interface for the shell, a computer program. To put that simply, we use the Terminal to tell a computer what to do. This is different from the Console in RStudio, which interprets R code and returns a value.\n\n\nTo get started, let’s open a new Terminal window in RStudio. Do this by clicking Tools &gt; Terminal &gt; New Terminal.\nA Terminal tab should now be open where your Console usually is.\n\n\n\n\n\n\nDon’t be afraid to dip your toes in the Terminal\n\n\n\nMost of our Git operations will be done in RStudio, but there are some situations where you must work in the Terminal and use command line. It may be daunting to code in the Terminal, but as your comfort increases over time, you might find you prefer it. Either way, it’s beneficial to learn enough command line and to feel comfortable in the Terminal.\n\n\nLet’s start by adding your user name to the global options. Type the following into the command prompt, with your exact GitHub username, and press enter:\ngit config --global user.name \"my_user_name\"\n\n\nNote that if the code ran successfully, it will look like nothing happened. We will check at the end to make sure it worked.\nNext, enter the following line, with the email address you used when you created your account on github.com:\ngit config --global user.email \"my_email@nceas.ucsb.edu\"\n\n\n\n\n\n\nCase and spelling matters!\n\n\n\nWhen you add your username and email to the global options you must use the exact same spelling and case that you used on GitHub otherwise, Git won’t be able to sync to your account.\n\n\nNext, we will set our credentials to not time out for a very long time. This is related to how our server operating system handles credentials - not doing this will make your Personal Access Token (PAT, which we will set up in the next section) expire immediately on the system, even though it is actually valid for at least a month.\n\n\n\n\n\n\nWhen setting up Git and GitHub on your Personal Computer\n\n\n\nYou will not need to run the git config line below to set the cache. This is a specific configuration for the RStudio Server we are working on.\n\n\ngit config --global credential.helper 'cache --timeout=10000000'\nNext, we will set the default branch name to main for any new repositories that are created moving forward. Why are we doing this? Previously, the default branch name was master and this racist terminology for Git branches motivates us to update our default branch to main instead.\ngit config --global init.defaultBranch main\nFinally, check to make sure everything looks correct by entering this command, which will return the global options you have set.\ngit config --global --list\n\n\n2.3.2 GitHub Authentication\nGitHub recently deprecated password authentication for accessing repositories, so we need to set up a secure way to authenticate.\nThe book Happy Git and GitHub for the useR has a wealth of information related to working with Git in R, and these instructions are based off of Chapter 9 Personal access token for HTTPS.\nWe will be using a Personal Access Token (PAT) in this course. For better security and long term use, we recommend taking the extra steps to set up SSH keys (check out Chapter 10 Set up Keys for SSH).\n\n\n\n\n\n\nSetting up your PAT\n\n\n\n\nRun usethis::create_github_token() in the Console.\nA new browser window should open up to GitHub, showing all the scopes options. You can review the scopes, but you don’t need to worry about which ones to select this time. Using create_github_token() automatically pre-selects some recommended scopes. Go ahead and scroll to the bottom and click “Generate Token”.\nCopy the generated token.\nBack in RStudio, run gitcreds::gitcreds_set() in the Console.\nPaste your PAT when the prompt asks for it.\nLast thing, run usethis::git_sitrep() in the Console to check your Git configuration and that you’ve successful stored your PAT.\n\n\n\nCongrats! Now you’ve setup your authentication you should be able to work with GitHub in RStudio now."
  },
  {
    "objectID": "session_03.html#learning-objectives",
    "href": "session_03.html#learning-objectives",
    "title": "3  Introduction to Git and GitHub",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nApply the principles of Git to track and manage changes of a project\nUtilize the Git workflow including pulling changes, staging modified files, committing changes, pulling again to incorporate remote changes, and pushing changes to a remote repository\nCreate and configure Git repositories using different workflows"
  },
  {
    "objectID": "session_03.html#introduction-to-version-control",
    "href": "session_03.html#introduction-to-version-control",
    "title": "3  Introduction to Git and GitHub",
    "section": "3.1 Introduction to Version Control",
    "text": "3.1 Introduction to Version Control\n\n\n\n\n\nEvery file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when bugs are discovered. Sometimes those fixes lead to even more bugs, leading to more changes in the code base. Data files get combined together. Sometimes those same files are split and combined again. In just one research project, we can expect thousands of changes to occur.\nThese changes are important to track, and yet, we often use simplistic file names to do so. Many of us have experienced renaming a document or script multiple times with the ingenuine addition of “final” to the file name (like the comic above demonstrates).\nYou might think there is a better way, and you’d be right: version control. Version control provides an organized and transparent way to track changes in code and additional files. This practice was designed for software development, but is easily applicable to scientific programming.\nThere are many benefits to using a version control software including:\n\nMaintain a history of your research project’s development while keeping your workspace clean\nFacilitate collaboration and transparency when working on teams\nExplore bugs or new features without disrupting your team members’ work\nand more!\n\nThe version control system we’ll be diving into is Git, the most widely used modern version control system in the world."
  },
  {
    "objectID": "session_03.html#introduction-to-git-github",
    "href": "session_03.html#introduction-to-git-github",
    "title": "3  Introduction to Git and GitHub",
    "section": "3.2 Introduction to Git + GitHub",
    "text": "3.2 Introduction to Git + GitHub\nBefore diving into the details of Git and how to use it, let’s start with a motivating example that’s representative of the types of problems Git can help us solve.\n\n3.2.1 A Motivating Example\nSay, for example, you’re working on an analysis in R and you’ve got it into a state you’re pretty happy with. We’ll call this version 1:\n\n\n\nYou come into the office the following day and you have an email from your boss, “Hey, you know what this model needs?”\n\n\n\nYou’re not entirely sure what she means but you figure there’s only one thing she could be talking about: more cowbell. So you add it to the model in order to really explore the space.\nBut you’re worried about losing track of the old model so, instead of editing the code in place, you comment out the old code and put as serious a warning as you can muster in a comment above it.\n\n\n\nCommenting out code you don’t want to lose is something probably all of us have done at one point or another but it’s really hard to understand why you did this when you come back years later or you when you send your script to a colleague. Luckily, there’s a better way: Version control. Instead of commenting out the old code, we can change the code in place and tell Git to commit our change. So now we have two distinct versions of our analysis and we can always see what the previous version(s) look like.\n\n\n\nYou may have noticed something else in the diagram above: Not only can we save a new version of our analysis, we can also write as much text as we like about the change in the commit message. In addition to the commit message, Git also tracks who, when, and where the change was made.\n\nWith Git we can enhance our workflow:\n\nEliminate the need for cryptic filenames and comments to track our work.\nProvide detailed descriptions of our changes through commits, making it easier to understand the reasons behind code modifications.\nUse commits to access and even execute older versions of our code.\nAdditionally, Git offers a powerful distributed feature. Multiple individuals can work on the same analysis concurrently on their own computers, with the ability to merge everyone’s changes together.\n\nADVANCED: - Work on multiple branches simultaneously, allowing for parallel development, and optionally merge them together. - Assign meaningful tags to specific versions of our code.\n\n\n\n3.2.2 What exactly are Git and GitHub?\n\nGit:\n\nan open-source distributed version control software\ndesigned to manage the versioning and tracking of source code files and project history\noperates locally on your computer, allowing you to create repositories, and track changes\nprovides features such as committing changes, branching and merging code, reverting to previous versions, and managing project history\nworks directly with the files on your computer and does not require a network connection to perform most operations\nprimarily used through the command-line interface (CLI, e.g. Terminal), but also has various GUI tools available (e.g. RStudio IDE)\n\n\n\n\n\n\nGitHub:\n\nonline platform and service built around Git\nprovides a centralized hosting platform for Git repositories\nallows us to store, manage, and collaborate on their Git repositories in the cloud\noffers additional features on top of Git, such as a web-based interface, issue tracking, project management tools, pull requests, code review, and collaboration features\nenables easy sharing of code with others, facilitating collaboration and contribution to open source projects\nprovides a social aspect, allowing users to follow projects, star repositories, and discover new code\n\n\n\n\n\n\n3.2.2.1 General Picture\n\nRepositories (“repos”) are Git and GitHub main unit. A “contained” folder with permissions. A repository can be public or private.\nFiles are stored in repositories.\nRepositories are “owned” (live under) by users /organizations.\nAll repositories have the same structure. Makes is easy and familiar to navigate.\n\n\n\n\n3.2.3 Let’s take a look at a repository\nOne of the first thing to note here it that every repository will have the same structure. At the top we have the username or organization name/name-of-repository (doesn’t change, each repository under a user name or organization has a unique name). Note that the url to a repo always follows the same structure too “github.com/username/repo-name”. In the repository landing page we can find information about the most activity in this repository. This is important because it tells you how recently the work in this repo has been updated. And then you have all the files in the repository.\n Generally, a repository will also have a README. You don’t have to have a readme but it’s best practice to have a read-me document at the top level of your repository to say describe the repository. What are people looking into in this repository? Depending on the kind of repository, often there are installation instructions and how to get help. In the case of palmerpeguins, this is an R package so the readme provides all the infromation about the package and how to install it.\n\nCheck out other repositories:\n\nNCEAS Learning Hub Modules Website Repository\nPreparation for the 2023 Ocean Health Index\n\n\n\n3.2.4 Git and GitHub Workflow\nThere are different workflows for creating version-controlled repositories. Here we will describe one of them: Create a remote repo (on GitHub) first, then clone to your local computer.\n\n3.2.4.1 Go to github.com and create a repository under your user or your organization\n\n\n\n\n\n\n\n3.2.4.2 Clone the repository to your local computer\n Clone: download an identical copy - a ‘clone’ - of a repository to your local computer. Cloned repositories can still be synced with the online version(s) at your whim.\n\n\n3.2.4.3 In your local computer, you work on your code, analysis, report, etc.\n\n\n\n3.2.4.4 You stage and commit your changes to your local repository.\nStage: Indicating which of the modified files are ready to be committed. Commit: Records changes to the repository and include a descriptive message (you should always include a commit message!).\n\n\n\n3.2.4.5 You pull to make sure your local repository is up to date with the remote repository.\n Pull: Retrieves changes from a remote repository and merges them into your local working file(s).\n\n\n3.2.4.6 You push your commits into the GitHub remote repository.\nPush: Sends local commits to a remote repository.\n\n\n\nAll together: Git Workflow Vocabulary\n\n\n\n\n\n\n\n\nTerm\nAction\nDefinition\n\n\n\n\nClone\nClone the repository to your local computer\ndownload an identical copy - a ‘clone’ - of a repository to your local computer. Cloned repositories can still be synced with the online version(s) at your whim\n\n\nStage\nYou stage modified files to indicate the changes you want to commit\nIndicating which of the modified files are ready to be committed\n\n\nCommit\nYou commit your changes to your local repository\nCommit: Records changes to the repository and include a descriptive message (you should always include a commit message!)\n\n\nPull\nYou pull to make sure your local repository is up to date with the remote repository\nRetrieves changes from a remote repository and merges them into your local working file(s)\n\n\nPush\nYou push your commits into the GitHub remote repository\nSends local commits to a remote repository\n\n\n\nThe processes described in the above sections (i.e. making changes to local working files, recording “snapshots” of them to create a versioned history of changes in a local Git repository, and sending those versions from our local Git repository to a remote repository on GitHub is illustrated using islands, buildings, bunnies, and packages in the artwork, below:\nA basic git workflow represented as two islands, one with “local repo” and “working directory”, and another with “remote repo.” Bunnies move file boxes from the working directory to the staging area, then with Commit move them to the local repo. Bunnies in rowboats move changes from the local repo to the remote repo (labeled “PUSH”) and from the remote repo to the working directory (labeled “PULL”).\n\n\n\nArtwork by Allison Horst\n\n\nLet’s put this workflow on practice!"
  },
  {
    "objectID": "session_03.html#exercise-1-create-a-remote-repository-on-github",
    "href": "session_03.html#exercise-1-create-a-remote-repository-on-github",
    "title": "3  Introduction to Git and GitHub",
    "section": "3.3 Exercise 1: Create a remote repository on GitHub",
    "text": "3.3 Exercise 1: Create a remote repository on GitHub\n\n\n\n\n\n\nSetup\n\n\n\n\nLogin to GitHub\nClick the New repository button\nName it {FIRSTNAME}_test\nAdd a short description\nCheck the box to add a README.md file\nAdd a .gitignore file using the R template\nSet the LICENSE to Apache 2.0\n\n\n\nIf you were successful, it should look something like this:\n\n\n\n\n\nYou’ve now created your first repository! It has a couple of files that GitHub created for you: README.md, LICENSE, and .gitignore.\n\n\n\n\n\n\nREADME.md files are used to share important information about your repository\n\n\n\nYou should always add a README.md to the root directory of your repository – it is a markdown file that is rendered as HTML and displayed on the landing page of your repository. This is a common place to include any pertinent information about what your repository contains, how to use it, etc.\n\n\n\n\n \n\nFor simple changes to text files, such as the README.md, you can make edits directly in the GitHub web interface.\n\n\n\n\n\n\nChallenge\n\n\n\nNavigate to the README.md file in the file listing, and edit it by clicking on the pencil icon (top right of file). This is a regular Markdown file, so you can add markdown text. Add a new level-2 header called “Purpose” and add some bullet points describing the purpose of the repo. When done, add a commit message, and hit the Commit changes button.\n\n\n\n\n\n\n\nCongratulations, you’ve now authored your first versioned commit! If you navigate back to the GitHub page for the repository, you’ll see your commit listed there, as well as the rendered README.md file.\n\n\n\n\n\nThe GitHub repository landing page provides us with lots of useful information. To start, we see:\n\nall of the files in the remote repository\nwhen each file was last edited\nthe commit message that was included with each file’s most recent commit (which is why it’s important to write good, descriptive commit messages!)\n\nAdditionally, the header above the file listing shows the most recent commit, along with its commit message, and a unique ID (assigned by Git) called a SHA. The SHA (aka hash) identifies the specific changes made, when they were made, and by who. If you click on the SHA, it will display the set of changes made in that particular commit.\n\n\n\n\n\n\nWhat should I write in my commit message?\n\n\n\nWriting effective Git commit messages is essential for creating a meaningful and helpful version history in your repository. It is crucial to avoid skipping commit messages or resorting to generic phrases like “Updates.” When it comes to following best practices, there are several guidelines to enhance the readability and maintainability of the codebase.\nHere are some guidelines for writing effective Git commit messages:\n\nBe descriptive and concise: Provide a clear and concise summary of the changes made in the commit. Aim to convey the purpose and impact of the commit in a few words.\nUse imperative tense: Write commit messages in the imperative tense, as if giving a command. For example, use “Add feature” instead of “Added feature” or “Adding feature.” This convention aligns with other Git commands and makes the messages more actionable.\nSeparate subject and body: Start with a subject line, followed by a blank line, and then provide a more detailed explanation in the body if necessary. The subject line should be a short, one-line summary, while the body can provide additional context, motivation, or details about the changes.\nLimit the subject line length: Keep the subject line within 50 characters or less. This ensures that the commit messages are easily scannable and fit well in tools like Git logs.\nCapitalize and punctuate properly: Begin the subject line with a capital letter and use proper punctuation. This adds clarity and consistency to the commit messages.\nFocus on the “what” and “why”: Explain what changes were made and why they were made. Understanding the motivation behind a commit helps future researchers and collaborators (including you!) comprehend its purpose.\nUse present tense for subject, past tense for body: Write the subject line in present tense as it represents the current state of the codebase. Use past tense in the body to describe what has been done.\nReference relevant issues: If the commit is related to a specific issue or task, include a reference to it. For example, you can mention the issue number or use keywords like “Fixes,” “Closes,” or “Resolves” followed by the issue number."
  },
  {
    "objectID": "session_03.html#exercise-2-clone-your-repository-and-use-git-locally-in-rstudio",
    "href": "session_03.html#exercise-2-clone-your-repository-and-use-git-locally-in-rstudio",
    "title": "3  Introduction to Git and GitHub",
    "section": "3.4 Exercise 2: clone your repository and use Git locally in RStudio",
    "text": "3.4 Exercise 2: clone your repository and use Git locally in RStudio\nCurrently, our repository just exists on GitHub as a remote repository. It’s easy enough to make changes to things like our README.md file (as demonstrated above), from the web browser, but that becomes a lot harder (and discouraged) for scripts and other code files. In this exercise, we’ll bring a copy of this remote repository down to our local computer (aka clone this repository) so that we can work comfortably in RStudio.\n\n\n\n\n\n\nAn important distinction\n\n\n\nWe refer to the remote copy of the repository that is on GitHub as the origin repository (the one that we cloned from), and the copy on our local computer as the local repository.\n\n\nStart by clicking the green Code button (top right of your file listing) and copying the URL to your clipboard (this URL represents the repository location):\n\n\n\n\n\n\n\nRStudio makes working with Git and version controlled files easy – to do so, you’ll need to be working within an R project folder. The following steps will look similar to those you followed when first creating an R Project (see Appendix), with a slight difference. Follow the instructions in the Setup box below to clone your remote repository to your local computer in RStudio:\n\n\n\n\n\n\nSetup\n\n\n\n\nClick File &gt; New Project\nSelect Version Control and paste the remote repository URL (which should be copied to your clipboard) in the Repository ULR field\nPress Tab, which will auto-fill the Project directory name field with the same name as that of your remote repo – while you can name the local copy of the repository anything, it’s typical (and highly recommended) to use the same name as the GitHub repository to maintain the correspondence\n\n\n\n\n\n\n\n\n\nOnce you click Create Project, a new RStudio window will open with all of the files from the remote repository copied locally. Depending on how your version of RStudio is configured, the location and size of the panes may differ, but they should all be present – you should see a Git tab, as well as the Files tab, where you can view all of the files copied from the remote repo to this local repo.\n\n\n\n\nYou’ll note that there is one new file sam_test.Rproj, and three files that we created earlier on GitHub (.gitignore, LICENSE, and README.md).\nIn the Git tab, you’ll note that the one new file, sam_test.Rproj, is listed. This Git tab is the status pane that shows the current modification status of all of the files in the repository. Here, we see sam_test.Rproj is preceded by a ?? symbol to indicate that the file is currently untracked by Git. This means that we have not yet committed this file using Git (i.e. Git knows nothing about the file; hang tight, we’ll commit this file soon so that it’s tracked by Git). As you make version control decisions in RStudio, these icons will change to reflect the current version status of each of the files.\nInspect the history. Click on the History button in the Git tab to show the log of changes that have occurred – these changes will be identical to what we viewed on GitHub. By clicking on each row of the history, you can see exactly what was added and changed in each of the two commits in this repository.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nMake a change to the README.md file – this time from RStudio – then commit the README.md change\nAdd a new section to your README.md called “Creator” using a level-2 header. Under it include some information about yourself. Bonus: Add some contact information and link your email using Markdown syntax.\n\n\n\nOnce you save, you’ll immediately see the README.md file show up in the Git tab, marked as a modification. Select the file in the Git tab, and click Diff to see the changes that you saved (but which are not yet committed to your local repository). Newly made changes are highlighted in green.\n\n\n\n\nCommit the changes. To commit the changes you made to the README.md file using RStudio’s GUI (Graphical User Interface), rather than the command line:\n\nStage (aka add) README.md by clicking the check box next to the file name – this tells Git which changes you want included in the commit and is analogous to using the git command, git add README.md, in the command line\nCommit README.md by clicking the Commit button and providing a descriptive commit message in the dialog box. Press the Commit button once you’re satisfied with your message. This is analogous to using the git command, git commit -m \"my commit message\", in the command line.\n\n\nA few notes about our local repository’s state:\n\nWe still have a file, sam_test.Rproj, that is listed as untracked (denoted by ?? in the Git tab).\nYou should see a message at the top of the Git tab that says, Your branch is ahead of ‘origin/main’ by 1 commit., which tells us that we have 1 commit in the local repository, but that commit has not yet been pushed up to the origin repository (aka remote repository on GitHub).\n\nCommit the remaining project file by staging/adding and committing it with an informative commit message.\n\nWhen finished, you’ll see that no changes remain in the Git tab, and the repository is clean.\nInspect the history. Note that under Changes, the message now says:\nYour branch is ahead of ‘origin/main’ by 2 commits.\nThese are the two commits that we just made, but have not yet been pushed to GitHub.\nClick on the History button to see a total of four commits in the local repository (the two we made directly to GitHub via the web browser and the two we made in RStudio).\nPush these changes to GitHub. Now that we’ve made and committed changes locally, we can push those changes to GitHub using the Push button. This sends your changes to the remote repository (on GitHub) leaving your repository in a totally clean and synchronized state (meaning your local repository and remote repository should look the same).\n\n\n\n\n\n\nIf you are prompted to provide your GitHub username and password when Pushing…\n\n\n\nit’s a good indicator that you did not set your GitHub Personal Access Token (PAT) correctly. You can redo the steps outlined in the GitHub Authentication section of the Appendix to (re)set your PAT, then Push again.\n\n\n\n &lt;––&gt;\n\nIf you look at the History pane again, you’ll notice that the labels next to the most recent commit indicate that both the local repository (HEAD) and the remote repository (origin/HEAD) are pointing at the same version in the history. If we look at the commit history on GitHub, all the commits will be shown there as well.\n\n\n\n\n\n\n\n\nLast thing, some Git configuration to surpress warning messages\n\n\n\nGit version 2.27 includes a new feature that allows users to specify the default method for integrating changes from a remote repository into a local repository, without receiving a warning (this warning is informative, but can get annoying). To suppress this warning for this repository only we need to configure Git by running this line of code in the Terminal:\n\ngit config pull.rebase false\n\npull.rebase false is a default strategy for pulling where Git will first try to auto-merge the files. If auto-merging is not possible, it will indicate a merge conflict (more on resolving merge conflicts in Chapter 11)."
  },
  {
    "objectID": "session_03.html#exercise-3-setting-up-git-on-an-existing-project",
    "href": "session_03.html#exercise-3-setting-up-git-on-an-existing-project",
    "title": "3  Introduction to Git and GitHub",
    "section": "3.5 Exercise 3: Setting up Git on an existing project",
    "text": "3.5 Exercise 3: Setting up Git on an existing project\nThere are a number of different workflows for creating version-controlled repositories that are stored on GitHub. We started with Exercise 1 and Exercise 2 using one common approach: creating a remote repository on GitHub first, then cloning that repository to your local computer (you used your {FIRSTNAME}_test repo).\nHowever, you may find yourself in the situation where you have an existing directory (i.e. a “normal” folder) of code that you want to make a Git repository out of, and then send it to GitHub. In this last exercise, we will practice this workflow using your training_{USERNAME} project.\nFirst, switch to your training_{USERNAME} project using the RStudio project dropdown menu. The project drop down menu is in the upper right corner of your RStudio pane. Click the drop down next to your project name ({FIRSTNAME}_test), and then select the training_{USERNAME} project from the RECENT PROJECTS list.\nThere are a few approaches for turning an existing project folder into a Git repository, then sending it to GitHub – if you’re an R-user, the simplest way is to use the {usethis} package, which is built to automate tasks involved with project setup and development. However, you can also initialize a local git repository and set the remote repository from the command line (a language-agnostic workflow). Steps for both approaches are included below (demonstrated using your training_{USERNAME} project):\n::: panel-tabset ## Using R & {usethis}\n\nInstall the {usethis} package (if you haven’t done so already) by running the following in your Console:\n\n\ninstall.packages(\"usethis\")\n\n\nInitialize training_{USERNAME} as a Git repository by running usethis::use_git() in the Console. Choose yes when asked if it’s okay to commit any uncommitted files. Choose yes again if asked to restart R. Once complete, you should see the Git tab appear in your top left pane in RStudio and a .gitignore file appear in your Files tab.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.gitignore files allow you to specify which files/folders you don’t want Git to track\n\n\n\nA .gitignore file is automatically created in the root directory of your project when you initialize it as a Git repository. You’ll notice that there are already some R / R Project-specific files that have been added by default.\nWhy is this useful? For many reasons, but possibly the greatest use-case is adding large files (GitHub has a file size limit of 2 GB) or files with sensitive information (e.g. keys, tokens) that you don’t want to accidentally push to GitHub.\nHow do I do this? Let’s say I create a file with sensitive information that I don’t want to push to GitHub. I can add a line to my .gitignore file:\n\n# added by default when I initalized my RProj as a Git Repository\n.Rproj.user\n.Rhistory\n.Rdata\n.httr-oauth\n.DS_Store\n.quarto\n\n# add file so that it doesn't get pushed to the remote repo (on GitHub); \ncontains_sensitive_info.R\n\nIf this file is currently untracked by Git, it should appear in my Git tab. Once I add it to the .gitignore and save the modified .gitignore file, you should see contains_sensitive_info.R disappear from the Git tab, and a modified .gitignore (denoted by a blue M) appear. Stage/commit/push this modified .gitignore file.\n\n\n\nCreate an upstream remote repository on GitHub by running usethis::use_github() in the Console. Your web browser should open up to your new GitHub repository, with the same name as your local Git repo/R Project.\n\n\n\n\n\n\n\n\n\n\n\nEnsure that your default branch is named main rather than master by:\n\nrunning git branch in the Terminal to list all your branches (you should currently only have one, which is your default)\nif it’s named master, run the following line in the Console to update it\n\n\n\nusethis::git_default_branch_rename(from = \"master\", to = \"main\")\n\nYou can verify that your update worked by running git branch once more in the Terminal.\n\n\n\n\n\n\nWhy are we doing this?\n\n\n\nThe racist “master” terminology for git branches motivates us to update our default branch to “main” instead.\nThere is a push across platforms and software to update this historical default branch name from master to main. GitHub has already done so – you may have noticed that creating a remote repository first (like we did in Exercises 1 & 2) results in a default branch named main. Depending on your version of Git, however, you may need to set update the name manually when creating a local git repository first (as we’re doing here).\n\n\n\nYou’re now ready to edit, stage/add, commit, and push files to GitHub as practiced earlier!\n\n\n\n\n\n\n\nChallenge: add a README.md file to training_{USERNAME}\n\n\n\nGitHub provides a button on your repo’s landing page for quickly adding a README.md file. Click the Add a README button and use markdown syntax to create a README.md. Commit the changes to your repository.\nGo to your local repository (in RStudio) and pull the changes you made."
  },
  {
    "objectID": "session_03.html#go-further-with-git",
    "href": "session_03.html#go-further-with-git",
    "title": "3  Introduction to Git and GitHub",
    "section": "3.6 Go further with Git",
    "text": "3.6 Go further with Git\n\n3.6.1 Understanding how local working files, Git, and GitHub all work together\nIt can be a bit daunting to understand all the moving parts of the Git / GitHub life cycle (i.e. how file changes are tracked locally within repositories, then stored for safe-keeping and collaboration on remote repositories, then brought back down to a local machine(s) for continued development). It gets easier with practice, but we’ll explain (first in words, then with an illustration) at a high-level how things work:\n\n3.6.1.1 What is the difference between a “normal” folder vs. a Git repository\nWhether you’re a Mac or a PC user, you’ll likely have created a folder at some point in time for organizing files. Let’s pretend that we create a folder, called myFolder/, and add two files: myData.csv and myAnalysis.R. The contents of this folder are not currently version controlled – meaning, for example, that if we make some changes to myAnalysis.R that don’t quite work out, we have no way of accessing or reverting back to a previous version of myAnalysis.R (without remembering/rewriting things, of course).\nGit allows you to turn any “normal” folder, like myFolder/, into a Git repository – you’ll often see/hear this referenced as “initializing a Git repository”. When you initialize a folder on your local computer as a Git repository, a hidden .git/ folder is created within that folder (e.g. myFolder/.git/) – this .git/ folder is the Git repository. As you use Git commands to capture versions or “snapshots” of your work, those versions (and their associated metadata) get stored within the .git/ folder. This allows you to access and/or recover any previous versions of your work. If you delete .git/, you delete your project’s history.\nHere is our example folder / Git repository represented visually:\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.1.2 How do I actually tell Git to preserve versions of my local working files?\nGit was built as a command-line tool, meaning we can use Git commands in the command line (e.g. Terminal, Git Bash, etc.) to take “snapshots” of our local working files through time. Alternatively, RStudio provides buttons that help to easily execute these Git commands.\nGenerally, that workflow looks something like this:\n\nMake changes to a file(s) (e.g. myAnalysis.R) in your working directory.\nStage the file(s) using git add myAnalysis.R (or git add . to stage multiple changed files at once). This lets Git know that you’d like to include the file(s) in your next commit.\nCommit the file(s) using git commit -m \"a message describing my changes\". This records those changes (along with a descriptive message) as a “snapshot” or version in the local repository (i.e. the .git/ folder).\n\n\n\n3.6.1.3 My versioned work is on my local computer, but I want to send it to GitHub. How?\nThe last step is synchronizing the changes made to our local repository with a remote repository (oftentimes, this remote repository is stored on GitHub). The git push command is used to send local commits up to a remote repository. The git pull command is used to fetch changes from a remote repository and merge them into the local repository – pulling will become a regular part of your workflow when collaborating with others, or even when working alone but on different machines (e.g. a laptop at home and a desktop at the office).\n\n\n3.6.1.4 Git Vocabulary & Commands\nWe know the world of Git and GitHub can be daunting. Use these tables as references while you use Git and GitHub, and we encourage you to build upon this list as you become more comfortable with these tools.\nThis table contains essential terms and commands that complement intro to Git skills. They will get you far on personal and individual projects.\n\nEssential Git Commands\n\n\n\n\n\n\n\nTerm\nGit Command(s)\nDefinition\n\n\n\n\nAdd/Stage\ngit add [file]\nStaging marks a modified file in its current version to go into your next commit snapshot. You can also stage all modified files at the same time using git add .\n\n\nCommit\ngit commit\nRecords changes to the repository.\n\n\nCommit Message\ngit commit -m \"my commit message\"\nRecords changes to the repository and include a descriptive message (you should always include a commit message!).\n\n\nFetch\ngit fetch\nRetrieves changes from a remote repository but does not merge them into your local working file(s).\n\n\nPull\ngit pull\nRetrieves changes from a remote repository and merges them into your local working file(s).\n\n\nPush\ngit push\nSends local commits to a remote repository.\n\n\nStatus\ngit status\nShows the current status of the repository, including (un)staged files and branch information.\n\n\n\nThis table includes more advanced Git terms and commands that are commonly used in both individual and collaborative projects.\n\nAdvanced Git Commands\n\n\n\n\n\n\n\nTerm\nGit Command(s)\nDefinition\n\n\n\n\nBranch\ngit branch\nLists existing branches or creates a new branch.\n\n\nCheckout\ngit checkout [branch]\nSwitches to a different branch or restores files from a specific commit.\n\n\nClone\ngit clone [repository]\nCreates a local copy of a remote repository.\n\n\nDiff\ngit diff\nShows differences between files, commits, or branches.\n\n\nFork\n-\nCreates a personal copy of a repository under your GitHub account for independent development.\n\n\nLog\ngit log\nDisplays the commit history of the repository.\n\n\nMerge\ngit merge [branch]\nIntegrates changes from one branch into another branch.\n\n\nMerge Conflict\n-\nOccurs when Git cannot automatically merge changes from different branches, requiring manual resolution.\n\n\nPull Request (PR)\n-\nA request to merge changes from a branch into another branch, typically in a collaborative project.\n\n\nRebase\ngit rebase\nIntegrates changes from one branch onto another by modifying commit history.\n\n\nRemote\ngit remote\nManages remote repositories linked to the local repository.\n\n\nRepository\ngit init\nA directory where Git tracks and manages files and their versions.\n\n\nStash\ngit stash\nTemporarily saves changes that are not ready to be committed.\n\n\nTag\ngit tag\nAssigns a label or tag to a specific commit.\n\n\n\nGit has a rich set of commands and features, and there are many more terms beyond either table. Learn more by visiting the git documentation."
  },
  {
    "objectID": "session_03.html#git-resources",
    "href": "session_03.html#git-resources",
    "title": "3  Introduction to Git and GitHub",
    "section": "3.7 Git resources",
    "text": "3.7 Git resources\nThere’s a lot we haven’t covered in this brief tutorial. There are some great and much longer tutorials that cover advanced topics, such as:\n\nUsing Git on the command line\nResolving conflicts\nBranching and merging\nPull requests versus direct contributions for collaboration\nUsing .gitignore to protect sensitive data\nGitHub Issues - how to use them for project management and collaboration\n\nand much, much more.\n\nPro Git Book\nHappy Git and GitHub for the useR\nGitHub Documentation\nLearn Git Branching is an interactive tool to learn Git on the command line\nSoftware Carpentry Version Control with Git\nBitbucket’s tutorials on Git Workflows"
  }
]