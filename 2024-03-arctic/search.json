[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "About",
    "text": "About\nThis 5-day in-person workshop will provide researchers with an introduction to advanced topics in computationally reproducible research in python, including software and techniques for working with very large datasets. This includes working in cloud computing environments, docker containers, and parallel processing using tools like parsl and dask. The workshop will also cover concrete methods for documenting and uploading data to the Arctic Data Center, advanced approaches to tracking data provenance, responsible research and data management practices including data sovereignty and the CARE principles, and ethical concerns with data-intensive modeling and analysis.\n\n\n\nSchedule\n\n\n\n\n\nCode of Conduct\nPlease note that by participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#setting-up",
    "href": "index.html#setting-up",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "Setting Up",
    "text": "Setting Up\nIn this course, we will be using Python (3.12.12) as our primary language, and VS Code as our IDE. Below are instructions on how to get VS Code set up to work for the course. If you are already a regular Python user, you may already have another IDE set up. We strongly encourage you to set up VS Code with us, because we will use your local VS Code instance to write and execute code on one of the NCEAS servers.\n\nDownload VS Code and Remote - SSH Extension\nFirst, download VS Code if you do not already have it installed.\nYou’ll also need to download the Remote - SSH extension.\n\n\nLog in to the server\nTo connect to the server using VS Code follow these steps, from the VS Code window:\n\nopen the command pallette (Cmd + Shift + P)\nenter “Remote SSH: Connect to Host”\n\n\n\nselect “Add New SSH Host”\nenter the ssh command to connect to the host as if in a terminal (ssh username@included-crab.nceas.ucsb.edu)\n\nNote: you will only need to do this step once\n\n\n\n\nselect the SSH config file to update with the name of the host. You should select the one in your user directory (eg: /Users/jclark/.ssh/config)\nclick “Connect” in the popup in the lower right hand corner\n\nNote: If the dialog box does not appear, reopen the command palette (Cmd + Shift + P), type in “Remote-SSH: Connect to Host…”, choose included-crab.nceas.ucsb.edu from the options of configured SSH hosts, then enter your password into the dialog box that appears\n\nenter your password in the dialog box that pops up\n\nWhen you are connected, you will see in the lower left hand corner of the window a green bar that says “SSH: included-crab.nceas.ucsb.edu.”\n\n\n\nInstall extensions on the server\nAfter connecting to the server, in the extensions pane (View &gt; Extensions) search for, and install, the following extensions:\n- Python\n- Jupyter\n- Jupyter Keymap\nNote that these extensions will be installed on the server, and not locally.\n\n\nCreate a (free) Google Earth Engine (GEE) account\nIn order to code along during the Google Earth Engine lesson (Ch 15) on Thursday, you’ll need to sign up for an account at https://signup.earthengine.google.com. Once submitted, you’ll receive an email with some helpful links and a message that it may take a few days for your account to be up and running. Please be sure to do this a few days ahead of needing to use GEE.\n\n\n\n\n\n\nImportant\n\n\n\nGEE authentication (more on that in Lesson 15) uses Cloud Projects. Some organizations control who can create Cloud Projects, which may prevent you from completing the authentication process. To circumvent authentication issues, we recommend creating your GEE account using a non-organizational account (e.g. a personal email account). Check out GEE’s authentication troubleshooting recommendations if you continue to run into issues.\n\n\n\n\nTest your local setup (Optional)\nWe are going to be working on the server exclusively, but if you are interested in setting up VS Code to work for you locally with Python, you can follow these instructions. This local setup section summarizes the official VS Code tutorial. For more detailed instructions and screenshots, see the source material. This step is 100% optional, if you already have an IDE set up to work locally that you like, or already have VS code set up to work locally, you are welcome to skip this.\nLocally (not connected to the server), check to make sure you have Python installed if you aren’t sure you do. File &gt; New Window will open up a new VS Code window locally.\nTo check your python, from the terminal run:\npython3 --version\nIf you get an error, it means you need to install Python. Here are instructions for getting installed, depending on your operating system. Note: There are many ways to install and manage your Python installations, and advantages and drawbacks to each. If you are unsure about how to proceed, feel free to reach out to the instructor team for guidance.\n\nWindows: Download and run an installer from Python.org.\nMac: Install using homebrew. If you don’t have homebrew installed, follow the instructions from their webpage.\n\nbrew install python3\n\n\nAfter you run your install, make sure you check that the install is on your system PATH by running python3 --version again.\nNext, install the Python extension for VS Code.\nOpen a terminal window in VS Code from the Terminal drop down in the main window. Run the following commands to initialize a project workspace in a directory called training. This example will show you how to do this locally. Later, we will show you how to set it up on the remote server with only one additional step.\nmkdir training\ncd training\ncode .\nNext, select the Python interpreter for the project. Open the Command Palette using Command + Shift + P (Control + Shift + P for windows). The Command Palette is a handy tool in VS Code that allows you to quickly find commands to VS Code, like editor commands, file edit and open commands, settings, etc. In the Command Palette, type “Python: Select Interpreter.” Push return to select the command, and then select the interpreter you want to use (your Python 3.X installation).\nTo make sure you can write and execute code in your project, create a Hello World test file.\n\nFrom the File Explorer toolbar, or using the terminal, create a file called hello.py\nAdd some test code to the file, and save\n\nmsg = \"Hello World\"\nprint(msg)\n\nExecute the script using either the Play button in the upper-right hand side of your window, or by running python3 hello.py in the terminal.\n\nFor more ways to run code in VS Code, see the tutorial\n\n\nFinally, to test Jupyter, download the Jupyter extension. You’ll also need to install ipykernel. From the terminal, run pip install ipykernel.\nYou can create a test Jupyter Notebook document from the command pallete by typing “Create: New Jupyter Notebook” and selecting the command. This will open up a code editor pane with a notebook that you can test.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Scalable and Computationally Reproducible Approaches to Arctic Research",
    "section": "About this book",
    "text": "About this book\nThese written materials reflect the continuous development of learning materials at the Arctic Data Center and NCEAS to support individuals to understand, adopt, and apply ethical open science practices. In bringing these materials together we recognize that many individuals have contributed to their development. The primary authors are listed alphabetically in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: S. Jeanette Clark, Matthew B. Jones, Samantha Csik, Carmen Galaz García, Bryce Mecum, Natasha Haycock-Chavez, Daphne Virlar-Knight, Juliet Cohen, Anna Liljedahl. 2023. Scalable and Computationally Reproducible Approaches to Arctic Research. Arctic Data Center. doi:10.18739/A2QF8JM2V\nAdditional contributors: Amber E. Budden, Noor Johnson, Robyn Thiessen-Bock\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "sections/adc-intro.html",
    "href": "sections/adc-intro.html",
    "title": "1  Welcome and Overview",
    "section": "",
    "text": "1.0.1 Objectives",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome and Overview</span>"
    ]
  },
  {
    "objectID": "sections/adc-intro.html#arctic-data-center-overview",
    "href": "sections/adc-intro.html#arctic-data-center-overview",
    "title": "1  Welcome and Overview",
    "section": "1.1 Arctic Data Center Overview",
    "text": "1.1 Arctic Data Center Overview\nThe Arctic Data Center is the primary data and software repository for the Arctic section of National Science Foundation’s Office of Polar Programs (NSF OPP).\nWe’re best known in the research community as a data archive – researchers upload their data to preserve it for the future and make it available for re-use. This isn’t the end of that data’s life, though. These data can then be downloaded for different analyses or synthesis projects. In addition to being a data discovery portal, we also offer top-notch tools, support services, and training opportunities. We also provide data rescue services.\n\nNSF has long had a commitment to data reuse and sharing. Since our start in 2016, we’ve grown substantially – from that original 4 TB of data from ACADIS to now over 119.6 TB. In 2021 alone, we saw 16% growth in dataset count, and about 30% growth in data volume. This increase has come from advances in tools – both ours and of the scientific community, plus active community outreach and a strong culture of data preservation from NSF and from researchers. We plan to add more storage capacity in the coming months, as researchers are coming to us with datasets in the terabytes, and we’re excited to preserve these research products in our archive. We’re projecting our growth to be around several hundred TB this year, which has a big impact on processing time. Give us a heads up if you’re planning on having larger submissions so that we can work with you and be prepared for a large influx of data.\n\nThe data that we have in the Arctic Data Center comes from a wide variety of disciplines. These different programs within NSF all have different focuses – the Arctic Observing Network supports scientific and community-based observations of biodiversity, ecosystems, human societies, land, ice, marine and freshwater systems, and the atmosphere as well as their social, natural, and/or physical environments, so that encompasses a lot right there in just that one program. We’re also working on a way right now to classify the datasets by discipline, so keep an eye out for that coming soon.\n\nAlong with that diversity of disciplines comes a diversity of file types. The most common file type we have are image files in four different file types. Probably less than 200-300 of the datasets have the majority of those images – we have some large datasets that have image and/or audio files from drones. Most of those 6600+ datasets are tabular datasets. There’s a large diversity of data files, though, whether you want to look at remote sensing images, listen to passive acoustic audio files, or run applications – or something else entirely. We also cover a long period of time, at least by human standards. The data represented in our repository spans across centuries.\n\nWe also have data that spans the entire Arctic, as well as the sub-Arctic, regions.\n\n\n1.1.1 Data Discovery Portal\nTo browse the data catalog, navigate to arcticdata.io. Go to the top of the page and under data, go to search. Right now, you’re looking at the whole catalog. You can narrow your search down by the map area, a general search, or searching by an attribute.\n\nClicking on a dataset brings you to this page. You have the option to download all the files by clicking the green “Download All” button, which will zip together all the files in the dataset to your Downloads folder. You can also pick and choose to download just specific files.\n\nAll the raw data is in open formats to make it easily accessible and compliant with FAIR principles – for example, tabular documents are in .csv (comma separated values) rather than Excel documents.\nThe metrics at the top give info about the number of citations with this data, the number of downloads, and the number of views. This is what it looks like when you click on the Downloads tab for more information.\n\nScroll down for more info about the dataset – abstract, keywords. Then you’ll see more info about the data itself. This shows the data with a description, as well as info about the attributes (or variables or parameters) that were measured. The green check mark indicates that those attributes have been annotated, which means the measurements have a precise definition to them. Scrolling further, we also see who collected the data, where they collected it, and when they collected it, as well as any funding information like a grant number. For biological data, there is the option to add taxa.\n\n\n1.1.2 Tools and Infrastructure\nAcross all our services and partnership, we are strongly aligned with the community principles of making data FAIR (Findable, Accesible, Interoperable and Reusable).\n\nWe have a number of tools available to submitters and researchers who are there to download data. We also partner with other organizations, like Make Data Count and DataONE, and leverage those partnerships to create a better data experience.\n\nOne of those tools is provenance tracking. With provenance tracking, users of the Arctic Data Center can see exactly what datasets led to what product, using the particular script that the researcher ran.\n\nAnother tool are our Metadata Quality Checks. We know that data quality is important for researchers to find datasets and to have trust in them to use them for another analysis. For every submitted dataset, the metadata is run through a quality check to increase the completeness of submitted metadata records. These checks are seen by the submitter as well as are available to those that view the data, which helps to increase knowledge of how complete their metadata is before submission. That way, the metadata that is uploaded to the Arctic Data Center is as complete as possible, and close to following the guideline of being understandable to any reasonable scientist.\n\n\n\n1.1.3 Support Services\nMetadata quality checks are the automatic way that we ensure quality of data in the repository, but the real quality and curation support is done by our curation team. The process by which data gets into the Arctic Data Center is iterative, meaning that our team works with the submitter to ensure good quality and completeness of data. When a submitter submits data, our team gets a notification and beings to evaluate the data for upload. They then go in and format it for input into the catalog, communicating back and forth with the researcher if anything is incomplete or not communicated well. This process can take anywhere from a few days or a few weeks, depending on the size of the dataset and how quickly the researcher gets back to us. Once that process has been completed, the dataset is published with a DOI (digital object identifier).\n\n\n\n1.1.4 Training and Outreach\nIn addition to the tools and support services, we also interact with the community via trainings like this one and outreach events. We run workshops at conferences like the American Geophysical Union, Arctic Science Summit Week and others. We also run an intern and fellows program, and webinars with different organizations. We’re invested in helping the Arctic science community learn reproducible techniques, since it facilitates a more open culture of data sharing and reuse.\n\nWe strive to keep our fingers on the pulse of what researchers like yourselves are looking for in terms of support. We’re active on Twitter to share Arctic updates, data science updates, and specifically Arctic Data Center updates, but we’re also happy to feature new papers or successes that you all have had with working with the data. We can also take data science questions if you’re running into those in the course of your research, or how to make a quality data management plan. Follow us on Twitter and interact with us – we love to be involved in your research as it’s happening as well as after it’s completed.\n\n\n\n1.1.5 Data Rescue\nWe also run data rescue operations. We digitiazed Autin Post’s collection of glacier photos that were taken from 1964 to 1997. There were 100,000+ files and almost 5 TB of data to ingest, and we reconstructed flight paths, digitized the images of his notes, and documented image metadata, including the camera specifications.\n\n\n\n1.1.6 Who Must Submit\nProjects that have to submit their data include all Arctic Research Opportunities through the NSF Office of Polar Programs. That data has to be uploaded within two years of collection. The Arctic Observing Network has a shorter timeline – their data products must be uploaded within 6 months of collection. Additionally, we have social science data, though that data often has special exceptions due to sensitive human subjects data. At the very least, the metadata has to be deposited with us.\nArctic Research Opportunities (ARC)\n\nComplete metadata and all appropriate data and derived products\nWithin 2 years of collection or before the end of the award, whichever comes first\n\nARC Arctic Observation Network (AON)\n\nComplete metadata and all data\nReal-time data made public immediately\nWithin 6 months of collection\n\nArctic Social Sciences Program (ASSP)\n\nNSF policies include special exceptions for ASSP and other awards that contain sensitive data\nHuman subjects, governed by an Institutional Review Board, ethically or legally sensitive, at risk of decontextualization\nMetadata record that documents non-sensitive aspects of the project and data\n\nTitle, Contact information, Abstract, Methods\n\n\nFor more complete information see our “Who Must Submit” webpage\nRecognizing the importance of sensitive data handling and of ethical treatment of all data, the Arctic Data Center submission system provides the opportunity for researchers to document the ethical treatment of data and how collection is aligned with community principles (such as the CARE principles). Submitters may also tag the metadata according to community develop data sensitivity tags. We will go over these features in more detail shortly.\n\n\n\n1.1.7 Supporting data reuse\nAs the Arctic Data Center has grown in size, we have envisioned new capabilities to support efficient reuse of these valuable data. We are developing new tools both within the Arctic Data Center, and through collaborative projects with other researchers in the community. In house, some of the new features we plan to support include:\n\nEfficient submission and access to multi-Terabyte datasets\nData Quality assessment services\nAutomated workflows for building derived data products\n\nAs a concrete example, we are collaborating with the Permafrost Discovery Gateway to build new capacity for computing and visualization at sub-meter spatial resolution across the global Arctic. The services we are building support workflows for pre-processing massive image data collections to prepare them for modeling, as well as automating machine learning and other computations across large data on distributed compute clusters, and finally post-processing the results to create viable pathways for results mapping and visualization at global scales.\n\n\n\n1.1.8 Summary\nAll the above informtion can be found on our website or if you need help, ask our support team at support@arcticdata.io or tweet us @arcticdatactr!\n\n\n\n\n\n\n 6,958 Datasets\n\n\n \n\n\n 2,584 Creators\n\n\n\n\n 76 TB\n\n\n \n\n\n 16,162 Users",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome and Overview</span>"
    ]
  },
  {
    "objectID": "sections/adc-intro.html#scalable-computing-topics",
    "href": "sections/adc-intro.html#scalable-computing-topics",
    "title": "1  Welcome and Overview",
    "section": "1.2 Scalable Computing Topics",
    "text": "1.2 Scalable Computing Topics\nThe overall objective of this course is to facilitate effective analysis and modeling of the massive data resources that we curate for the Arctic research community. We assume a baseline proficiency, and plan to build on that to better support time-consuming computations with multi-terabyte datasets.\nActivities will include:\n\nTechnical tutorials with a lot of hands-on time in Python\nLectures and discussions on core concepts for scalable computing\nSemi-structured group projects to practice and cement key skills\n\nKey topics will include:\n\nArctic Data Center services and tools\nOverview and review of core computing concepts\nFundamentals of concurrent programming\n\nconcurrent.futures\nParsl\nDask\n\nFundamentals of working with big data and imagery\nGood practices in research software design for efficiency, reproducibility, and reuse\nFundamentals of cloud computing\n\nDocker, Zarr, containers, reproducibility, and more\n\nGeoscience applications in machine learning\n\nAs this is a lot to cover in a short survey course, we will break it down to fundamentals daily, and provide ample time for hands-on practice.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome and Overview</span>"
    ]
  },
  {
    "objectID": "sections/remote-computing.html",
    "href": "sections/remote-computing.html",
    "title": "2  Remote and Cloud Computing",
    "section": "",
    "text": "2.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote and Cloud Computing</span>"
    ]
  },
  {
    "objectID": "sections/remote-computing.html#learning-objectives",
    "href": "sections/remote-computing.html#learning-objectives",
    "title": "2  Remote and Cloud Computing",
    "section": "",
    "text": "Understand the basic architecture of computer networks\nLearn how to connect to a remote computer via a shell\nBecome familiarized with Bash Shell programming to navigate your computer’s file system, manipulate files and directories, and automate processes\nLearn different uses of the term cloud computing\nUnderstand the services of cloud providers\nGain familiarity with containerized computing",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote and Cloud Computing</span>"
    ]
  },
  {
    "objectID": "sections/remote-computing.html#introduction",
    "href": "sections/remote-computing.html#introduction",
    "title": "2  Remote and Cloud Computing",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nScientific synthesis and our ability to effectively and efficiently work with big data depends on the use of computers and the internet. Working on a personal computer may be sufficient for many tasks, but as data get larger and analyses more computationally intensive, scientists often find themselves needing more computing resources than they have available locally. Remote computing, or the process of connecting to a computer(s) in another location via a network link is becoming more and more common in overcoming big data challenges.\nIn this lesson, we’ll learn about the architecture of computer networks and explore some of the different remote computing configurations that you may encounter, we’ll learn how to securely connect to a remote computer via a shell, and we’ll become familiarized with using Bash Shell to efficiently manipulate files and directories. We will begin working in the VS Code IDE (integrated development environment), which is a versatile code editor that supports many different languages.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote and Cloud Computing</span>"
    ]
  },
  {
    "objectID": "sections/remote-computing.html#servers-networking",
    "href": "sections/remote-computing.html#servers-networking",
    "title": "2  Remote and Cloud Computing",
    "section": "2.3 Servers & Networking",
    "text": "2.3 Servers & Networking\nRemote computing typically involves communication between two or more “host” computers. Host computers connect via networking equipment and can send messages to each other over communication protocols (aka an Internet Protocol, or IP). Host computers can take the role of client or server, where servers share their resources with the client. Importantly, these client and server roles are not inherent properties of a host (i.e. the same machine can play either role).\n\nClient: the host computer intiating a request\nServer: the host computer responding to a request\n\n\n\n\nFig 1. Examples of different remote computing configurations. (a) A client uses secure shell protocol (SSH) to login/connect to a server over the internet. (b) A client uses SSH to login/connect to a computing cluster (i.e. a set of computers (nodes) that work together so that they can be viewed as a single system) over the internet. In this example, servers A - I are each nodes on this single cluster. The connection is first made through a gateway node (i.e. a computer that routes traffic from one network to another). (c) A client uses SSH to login/connect to a computing cluser where each node is a virtual machine (VM). In this example, the cluster comprises three servers (A, B, and C). VM1 (i.e. node 1) runs on server A while VM4 runs on server B, etc. The connection is first made through a gateway node.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote and Cloud Computing</span>"
    ]
  },
  {
    "objectID": "sections/remote-computing.html#ip-addressing",
    "href": "sections/remote-computing.html#ip-addressing",
    "title": "2  Remote and Cloud Computing",
    "section": "2.4 IP addressing",
    "text": "2.4 IP addressing\nHosts are assigned a unique numerical address used for all communication and routing called an Internet Protocol Address (IP Address). They look something like this: 128.111.220.7. Each IP Address can be used to communicate over various “ports”, which allow multiple applications to communicate with a host without mixing up traffic.\n\n\n\n\n\n\nPort numbers are divided into three ranges:\n\n\n\n\nwell-known ports, range from 0 through 1023 and are reserved for the most commonly used services (see table below for examples of some well-known port numbers)\nregistered ports, range from 1024 through 49151 and are not assigned or controlled, but can be registered (e.g. by a vendor for use with thier own server application) to prevent duplication\ndynamic ports, range from 49152 through 65535 and are not assigned, controlled, or registered but may instead be used as temporary or private ports\n\n\n\n\n\n\n\n\nwell-known port\nassignment\n\n\n\n\n20, 21\nFile Transfer Protocol (FTP), for transfering files between a client & server\n\n\n22\nsecure shell (SSH), to create secure network connections\n\n\n53\nDomain Name System (DNS) service, to match domain names to IP addresses\n\n\n80\nHypertext Transfer Protocol (HTTP), used in the World Wide Web\n\n\n443\nHTTP Secure (HTTPS), an encrypted version of HTTP\n\n\n\n\n\nBecause IP addresses can be difficult to remember, they are also assigned hostnames, which are handled through the global Domain Name System (DNS). Clients first look up a hostname in the DNS to find the IP address, then open a connection to the IP address.\n\n\n\n\n\n\nIn order to connect to remote servers, computing clusters, virtual machines, etc., you need to know their IP address (or hostname)\n\n\n\nA couple important ones:\n\nThroughout this course, we’ll be working on a server with the hostname, included-crab and IP address, 128.111.85.28 (in just a little bit, we’ll learn how to connect to included-crab using SSH)\n\nlocalhost is a hostname that refers to your local computer and is assigned the IP address 127.0.0.1 – the concept of localhost is important for tasks such as website testing, and is also important to understand when provisioning local execution resources (e.g. we’ll practice this during the section 6 exercise when working with Parsl.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote and Cloud Computing</span>"
    ]
  },
  {
    "objectID": "sections/remote-computing.html#bash-shell-programming",
    "href": "sections/remote-computing.html#bash-shell-programming",
    "title": "2  Remote and Cloud Computing",
    "section": "2.5 Bash Shell Programming",
    "text": "2.5 Bash Shell Programming\nWhat is a shell? From Wikipedia:\n\n“a computer program which exposes an operating system’s services to a human user or other programs. In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), depending on a computer’s role and particular operation.”\n\n\n\n\nWhat is Bash? Bash, or Bourne-again Shell, is a command line tool (language) commonly used to manipulate files and directories. Accessing and using bash is slightly different depending on what type of machine you work on:\n\nMac: bash via the Terminal, which comes ready-to-use with all Macs and Linux machines\nWindows: running bash depends on which version of Windows you have – newer versions may ship with bash or may require a separate install (e.g. Windows Subsystem for Linux (WSL) or Git Bash), however there are a number of different (non-bash) shell options as well (they all vary slightly; e.g. PowerShell, Command Prompt).\n\n\n\n\n\n\n\nNote\n\n\n\nMac users may have to switch from Z Shell, or zsh, to bash. Use the command exec bash to switch your default shell to bash (or exec zsh to switch back).\n\n\n\n2.5.1 Some commonly used (and very helpful) shell commands:\nBelow are just a few shell commands that you’re likely to use. Some may be extended with options (more on that in the next section) or even piped together (i.e. where the output of one command gets sent to the next command, using the | operator). You can also find some nice bash cheat sheets online, like this one. Alternatively, the Bash Reference Manual has all the content you need, albeit a bit dense.\n\n\n\n\n\n\n\nbash command\nwhat it does\n\n\n\n\npwd\nprint your current working directory\n\n\ncd\nchange directory\n\n\nls\nlist contents of a directory\n\n\ntree\ndisplay the contents of a directory in the form of a tree structure (not installed by default)\n\n\necho\nprint text that is passed in as an argument\n\n\nmv\nmove or rename a file\n\n\ncp\ncopy a file(s) or directory(ies)\n\n\ntouch\ncreate a new empty file\n\n\nmkdir\ncreate a new directory\n\n\nrm/rmdir\nremove a file/ empty directory (be careful – there is no “trash” folder!)\n\n\ngrep\nsearches a given file(s) for lines containing a match to a given pattern list\n\n\nawk\na text processing language that can be used in shell scripts or at a shell prompt for actions like pattern matching, printing specified fields, etc.\n\n\nsed\nstands for Stream Editor; a versatile command for editing files\n\n\ncut\nextract a specific portion of text in a file\n\n\njoin\njoin two files based on a key field present in both\n\n\ntop, htop\nview running processes in a Linux system (press Q to quit)\n\n\n\n\n\n2.5.2 General command syntax\nBash commands are typically are written as: command [options] [arguments] where the command must be an executable on your PATH and where options (settings that change the shell and/or script behavior) take one of two forms: short form (e.g. command -option-abbrev) or long form (e.g. command --option-name or command -o option-name). An example:\n# the `ls` command lists the files in a directory\nls file/path/to/directory\n\n# adding on the `-a` or `--all` option lists all files (including hidden files) in a directory\nls -a file/path/to/directory # short form\nls --all file/path/to/directory # long form\nls -o all file/path/to/directory # long form\n\n\n2.5.3 Some useful keyboard shortcuts\nIt can sometimes feel messy working on the command line. These keyboard shortcuts can make it a little easier:\n\nCtrl + L: clear your terminal window\nCtrl + U: delete the current line\nCtrl + C: abort a command\nup & down arrow keys: recall previously executed commands in chronological order\nTAB key: autocompletion",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote and Cloud Computing</span>"
    ]
  },
  {
    "objectID": "sections/remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "href": "sections/remote-computing.html#connecting-to-a-remote-computer-via-a-shell",
    "title": "2  Remote and Cloud Computing",
    "section": "2.6 Connecting to a remote computer via a shell",
    "text": "2.6 Connecting to a remote computer via a shell\nIn addition to navigating your computer/manipulating your files, you can also use a shell to gain accesss to and remotely control other computers. To do so, you’ll need the following:\n\na remote computer (e.g. server) which is turned on\nclient and server ssh clients installed/enabled\nthe IP address or name of the remote computer\nthe necessary permissions to access the remote computer\n\nSecure Shell, or SSH, is a network communication protocol that is often used for securely connecting to and running shell commands on a remote host, tremendously simplifying remote computing.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote and Cloud Computing</span>"
    ]
  },
  {
    "objectID": "sections/remote-computing.html#git-via-a-shell",
    "href": "sections/remote-computing.html#git-via-a-shell",
    "title": "2  Remote and Cloud Computing",
    "section": "2.7 Git via a shell",
    "text": "2.7 Git via a shell\nGit, a popular version control system and command line tool can be accessed via a shell. While there are lots of graphical user interfaces (GUIs) that faciliatate version control with Git, they often only implement a small subset of Git’s most-used functionality. By interacting with Git via the command line, you have access to all Git commands. While all-things Git is outside the scope of this workshop, we will use some basic Git commands in the shell to clone GitHub (remote) repositories to the server and save/store our changes to files. A few important Git commands:\n\n\n\n\n\n\n\nGit command\nwhat it does\n\n\n\n\ngit clone\ncreate a copy (clone) of repository in a new directory in a different location\n\n\ngit add\nadd a change in the working directory to the staging area\n\n\ngit commit\nrecord a snapshot of a repository; the -m option adds a commit message\n\n\ngit push\nsend commits from a local repository to a remote repository\n\n\ngit fetch\ndownloads contents (e.g. files, commits, refs) from a remote repo to a local repo\n\n\ngit pull\nfetches contents of a remote repo and merges changes into the local repo",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote and Cloud Computing</span>"
    ]
  },
  {
    "objectID": "sections/remote-computing.html#lets-practice",
    "href": "sections/remote-computing.html#lets-practice",
    "title": "2  Remote and Cloud Computing",
    "section": "2.8 Let’s practice!",
    "text": "2.8 Let’s practice!\nWe’ll now use bash commands to do the following:\n\nconnect to the server (included-crab) that we’ll be working on for the remainder of this course\nnavigate through directories on the server and add/change/manipulate files\nclone a GitHub repository to the server\nautomate some of the above processes by writing a bash script\n\n\n2.8.1 Exercise 1: Connect to a server using the ssh command (or using VS Code’s command palette)\nLet’s connect to a remote computer (included-crab) and practice using some of above commands.\n\nLaunch a terminal in VS Code\n\n\nThere are two options to open a terminal window, if a terminal isn’t already an open pane at the bottom of VS Code\n\nClick on Terminal &gt; New Terminal in top menu bar\nClick on the + (dropdown menu) &gt; bash in the bottom right corner\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou don’t need to use the VS Code terminal to ssh into a remote computer, but it’s conveniently located in the same window as your code when working in the VS Code IDE.\n\n\n\nConnect to a remote server\n\n\nYou can choose to SSH into the server (included-crab.nceas.ucsb.edu) through (a) the command line by using the ssh command, or (b) through VS Code’s command palette. If you prefer the latter, please refer back to the Log in to the server section. To do so via the command line, use the ssh command followed by yourusername@included-crab.nceas.ucsb.edu. You’ll be prompted to type/paste your password to complete the login. It should look something like this:\n\nyourusername:~$ ssh yourusername@included-crab.nceas.ucsb.edu \nyourusername@included-crab.nceas.ucsb.edu's password: \nyourusername@included-crab:~$ \n\n\n\n\n\n\nImportant\n\n\n\nYou won’t see anything appear as you type or paste your password – this is a security feature! Type or paste your password and press enter/return when done to finish connecting to the server.\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo log out of the server, type exit – it should look something like this:\nyourusername@included-crab.nceas.ucsb.edu:$ exit\nlogout\nConnection to included-crab.nceas.ucsb.edu closed.\n(base) .....\n\n\n\n\n2.8.2 Exercise 2: Practice using some common bash commands\n\nUse the pwd command to print your current location, or working directory. You should be in your home directory on the server (e.g. /home/yourusername).\nUse the ls command to list the contents (any files or subdirectories) of your home directory\nUse the mkdir command to create a new directory named bash_practice:\n\nmkdir bash_practice\n\nUse the cd command to move into your new bash_practice directory:\n\n# move from /home/yourusername to home/yourusername/bash_practice\ncd bash_practice\n\nTo move up a directory level, use two dots, .. :\n\n# move from /home/yourusername/bash_practice back to /home/yourusername\n$ cd ..\n\n\n\n\n\n\nNote\n\n\n\nTo quickly navigate back to your home directory from wherever you may be on your computer, use a tilde, ~ :\n# e.g. to move from from some subdirectory, /home/yourusername/Projects/project1/data, back to your home directory, home/yourusername\n$ cd ~\n\n# or use .. to back out three subdirectories\n$ cd ../../..\n\n\n\nAdd some .txt files (file1.txt, file2.txt, file3.txt) to your bash_practice subdirectory using the touch command (Note: be sure to cd into bash_practice if you’re not already there):\n\n# add one file at a time\ntouch file1.txt\ntouch file2.txt\ntouch file3.txt\n\n# or add all files simultanously like this:\ntouch file{1..3}.txt\n\n# or like this:\ntouch file1.txt file2.txt file3.txt\n\nYou can also add other file types (e.g. .py, .csv, etc.)\n\ntouch mypython.py mycsv.csv\n\nPrint out all the .txt files in bash_practice using a wildcard, *:\n\nls *.txt\n\nCount the number of .txt files in bash_practice by combining the ls and wc (word count) funtions using the pipe, |, operator:\n\n# `wc` returns a word count (lines, words, chrs)\n# the `-l` option only returns the number of lines\n# use a pipe, `|`, to send the output from `ls *.txt` to `wc -l`\nls *.txt | wc -l\n\nDelete mypython.py using the rm command:\n\nrm mypython.py \n\nCreate a new directory inside bash_practice called data and move mycsv.csv into it.\n\nmkdir data\nmv mycsv.csv ~/bash_practice/data\n\n# add the --interactive option (-i for short) to prevent a file from being overwritten by accident (e.g. in case there's a file with the same name in the destination location)\nmv -i mycsv.csv ~/bash_practice/data\n\nUse mv to rename mycsv.csv to mydata.csv\n\nmv mycsv.csv mydata.csv\n\nAdd column headers col1, col2, col3 to mydata.csv using echo + the &gt; operator\n\necho \"col1, col2, col3\" &gt; mydata.csv\n\n\n\n\n\n\nTip\n\n\n\nYou can check to see that mydata.csv was updated using GNU nano, a text editor for the command line that comes preinstalled on Linux machines (you can edit your file in nano as well). To do so, use the nano command followed by the file you want to open/edit:\nnano mydata.csv\nTo save and quit out of nano, use the control + X keyboard shortcut.\nYou can also create and open a file in nano in just one line of code. For example, running nano hello_world.sh is the same as creating the file first using touch hello_world.sh, then opening it with nano using nano hello_world.sh.\n\n\n\nAppend a row of data to mydata.csv using echo + the &gt;&gt; operator\n\n# using `&gt;` will overwrite the contents of an existing file; `&gt;&gt;` appends new information to an existing file\necho \"1, 2, 3\" &gt;&gt; mydata.csv\n\n\n2.8.3 Exercise 3: Clone a GitHub repository to the server\nIDEs commonly have helper buttons for cloning (i.e. creating a copy of) remote repositories to your local computer (or in this case, a server), but using git commands in a terminal can be just as easy. We can practice that now, following the steps below:\n\nGo to the scalable-computing-examples repository on GitHub at https://github.com/NCEAS/scalable-computing-examples – this repo contains example files for you to edit and practice in throughout this course. Fork (make your own copy of the repository) this repo by clicking on the Fork button (top right corner of the repository’s page).\n\n\n\n\n\n\nOnce forked, click on the green Code button (from the forked version of the GitHub repo) and copy the URL to your clipboard.\n\n\n\n\n\n\nIn the VS Code terminal, use the git clone command to create a copy of the scalable-computing-examples repository in the top level of your user directory (i.e. your home directory) on the server (Note: use pwd to check where you are; use cd ~/ to navigate back to your home directory if you find that you’re somewhere else).\n\ngit clone &lt;url-of-forked-repo&gt;\n\nYou should now have a copy of the scalable-computing-examples repository to work on on the server. Use the tree command to see the structure of the repo (you need to be in the scalable-computing-examples directory for this to work) – there should be a subdirectory called 02-bash-babynames that contains (i) a README.MD file, (ii) a KEY.sh file (this is a functioning bash script available for reference; we’ll be recreating it together in the next exercise) and (iii) a namesbystate folder containing 51 .TXT files and a StateReadMe.pdf file with some metadata.\n\n\n\n2.8.4 Bonus Exercise: Automate data processing with a Bash script\nAs we just demonstrated, we can use bash commands in the terminal to accomplish a variety of tasks like navigating our computer’s directories, manipulating/creating/adding files, and much more. However, writing a bash script allows us to gather and save our code for automated execusion.\nWe just cloned the scalable-computing-examples GitHub repository to the server in Exercise 3 above. This contains a 02-bash-babynames folder with 51 .TXT files (one for each of the 50 US states + The District of Columbia), each with the top 1000 most popular baby names in that state. We’re going to use some of the bash commands we learned in Exercise 2 to concatenate all rows of data from these 51 files into a single babynames_allstates.csv file.\nLet’s begin by creating a simple bash script that when executed, will print out the message, “Hello, World!” This simple script will help us determine whether or not things are working as expected before writing some more complex (and interesting) code.\n\nOpen a terminal window and determine where you are by using the pwd command – we want to be in scalable-computing-examples/02-bash-babynames. If necessary, navigate here using the cd command.\nNext, we’ll create a shell script called mybash.sh using the touch command:\n\n$ touch mybash.sh\n\nThere are a number of ways to edit a file or script – we’ll use Nano, a terminal-based text editor, as we did earlier. Open your mybash.sh with nano by running the following in your terminal:\n\n$ nano mybash.sh\n\nWe can now start to write our script. Some important considerations:\n\n\nAnything following a # will not be executed as code – these are useful for adding comments to your scripts\nThe first line of a Bash script starts with a shebang, #!, followed by a path to the Bash interpreter – this is used to tell the operating system which interpreter to use to parse the rest of the file. There are two ways to use the shebang to set your interpreter (read up on the pros & cons of both methods on this Stack Overflow post):\n\n# (option a): use the absolute path to the bash binary\n#!/bin/bash\n\n# (option b): use the env untility to search for the bash executable in the user's $PATH environmental variable\n#!/usr/bin/env bash\n\nWe’ll first specify our bash interpreter using the shebang, which indicates the start of our script. Then, we’ll use the echo command, which when executed, will print whatever text is passed as an argument. Type the following into your script (which should be opened with nano), then save (Use the keyboard shortcut control + X to exit, then type Y when it asks if you’d like to save your work. Press enter/return to exit nano).\n\n# specify bash as the interpreter\n#!/bin/bash\n\n# print \"Hello, World!\"\necho \"Hello, World!\"\n\nTo execute your script, use the bash command followed by the name of your bash script (be sure that you’re in the same working directory as your mybash.sh file or specify the file path to it). If successful, “Hello, World!” should be printed in your terminal window.\n\nbash mybash.sh\n\nNow let’s write our script. Re-open your script in nano by running nano mybash.sh. Using what we practiced above and the hints below, write a bash script that does the following:\n\n\nprints the number of .TXT files in the namesbystate subdirectory\nprints the first 10 rows of data from the CA.TXT file (HINT: use the head command)\nprints the last 10 rows of data from the CA.TXT file (HINT: use the tail command)\ncreates an empty babynames_allstates.csv file in the namesbystate subdirectory (this is where the concatenated data will be saved to)\nadds the column names, state, gender, year, firstname, count, in that order, to the babynames_allstates.csv file\nconcatenates data from all .TXT files in the namesbystate subdirectory and appends those data to the babynames_allstates.csv file (HINT: use the cat command to concatenate files)\n\nHere’s a script outline to fill in (Note: The echo statements below are not necessary but can be included as progress indicators for when the bash script is executed – these also make it easier to diagnose where any errors occur during execution):\n#!bin/bash\necho \"THIS IS THE START OF MY SCRIPT!\"\n\necho \"-----Verify that we have .TXT files for all 50 states + DC-----\"\n# &lt;add your code here&gt;\n\necho \"-----Printing head of CA.TXT-----\"\n# &lt;add your code here&gt;\n\necho \"-----Printing tail of CA.TXT-----\"\n# &lt;add your code here&gt;\n\necho \"-----Creating empty .csv file to concatenate all data-----\"\n# &lt;add your code here&gt;\n\necho \"-----Adding column headers to csv file-----\"\n# &lt;add your code here&gt;\n\necho \"-----Concatenating files-----\"\n# &lt;add your code here&gt;\n\necho \"DONE!\"\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n#!bin/bash\necho \"THIS IS THE START OF MY SCRIPT!\"\n\necho \"-----Verify that we have .TXT files for all 50 states + DC-----\"\nls namesbystate/*.TXT | wc -l\n\necho \"-----Printing head of CA.TXT-----\"\nhead namesbystate/CA.TXT\n\necho \"-----Printing tail of CA.TXT-----\"\ntail namesbystate/CA.TXT\n\necho \"-----Creating empty .csv file to concatenate all data-----\"\ntouch namesbystate/babynames_allstates.csv\n\necho \"-----Adding column headers to csv file-----\"\necho \"state, gender, year, firstname, count\" &gt; namesbystate/babynames_allstates.csv\n\necho \"-----Concatenating files-----\"\ncat namesbystate/*.TXT &gt;&gt; namesbystate/babynames_allstates.csv\n\necho \"DONE!\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote and Cloud Computing</span>"
    ]
  },
  {
    "objectID": "sections/remote-computing.html#what-is-cloud-computing-anyways",
    "href": "sections/remote-computing.html#what-is-cloud-computing-anyways",
    "title": "2  Remote and Cloud Computing",
    "section": "2.9  What is cloud computing anyways?",
    "text": "2.9  What is cloud computing anyways?\nThe buzzword we all hear, but maybe don’t quite understand. ..\nCloud computing is a lot of things… but generally speaking:\n\n\n\n\n\n\nCloud computing is the delivery of on-demand computer resources over the Internet. Or just “using someone else’s computer”.\n\n\n“The Cloud” is powered by a global network of data centers which house the hardware (servers), power, and backup systems, etc. These data centers and infrastructure are managed by cloud providers\n\n\n\n\nCloud computing services are typically offered using a “pay-as-you-go” pricing model, which in some scenarios may reduce capital expenses.\n\n\nCloud computing is a technology approach to using lightweight virtualization services to share large physical computing clusters across many users\n\n\n\nCheck out this article by Ingrid Burrington in The Atlantic, Why Amazon’s Data Centers are Hidden in Spy Country, for some interesting insight into one of the largest cloud provider’s data centers.\n\n2.9.1 Commercial clouds\nThere are a lots of different cloud computing platforms, but the big ones are:\n Amazon Web Services (AWS)  Google Cloud Platform (GCP)  Microsoft Azure\n\nThere are many other cloud service providers that offer varying degrees of infrastructure, ease of use, and cost. Check out DigitalOcean, Kamatera, and Vultr to start.\n\n\n2.9.2 Academic clouds\n\n\n\n\n\n\n   \n\n\nFederal agencies in the US and other institutions also support massive computing facilities supporting cloud computing. While there are too many to fully list, programs such as the National Science Foundation’s ACCESS program, the Department of Energy’s National Energy Research Scientific Computing Center (NERSC), and the CyVerse platform provide massive cloud computing resources to academic and agency researchers. The huge advantage is that these resources are generally free-for-use for affiliated researchers. When you need access to massive CPU and GPU hours, an application for access to these facilities can be extremely effective.\n\n\n\nAnd the Pangeo project is creating an open community focused on maintaining, supporting, and deploying open infrastructure for cloud computing. They support key scientific software packages used throughout the cloud community, including xarray and dask, and generally are broadening capacity for large-scale, impactful research.\n\n\n2.9.3 Cloud deployment options\nDifferent cloud service and deployment models offer a suite of options to fit client needs\n Service Models: When you work in “the cloud” you’re using resources – including servers, storage, networks, applications, services, (and more!) – from a very large resource pool that is managed by you or the cloud service provider. Three cloud service models describe to what extent your resources are managed by yourself or by your cloud service providers.\n\n\n\n\n\n\n Infrastructure as a Service (IaaS)\n\n\n\n\n Platform as a Service (PaaS)\n\n\n\n\n Software as a Service (SaaS)\n\n\n\n Deployment Models: Cloud deployment models describe the type of cloud environment based on ownership, scale, and access.\n\n\n\n\n\n\n Private Cloud\n\n\n\n\n Public Cloud\n\n\n\n\n Hybrid Cloud\n\n\n\n\n\n2.9.4 Service Models\n Infrastructure as a Service (IaaS) provides users with computing resources like processing power, data storage capacity, and networking. IaaS platforms offer an alternative to on-premise physical infrastructure, which can be costly and labor-intensive. In comparison, IaaS platforms are more cost-effective (pay-as-you-go), flexible, and scalable.\nOne example of IaaS is Amazon EC2, which allows users to rent virtual computers on which to run their own computer applications (e.g. R/RStudio).\n Platform as a Service (PaaS) provides developers with a framework and tools for creating unique applications and software. A benefit of SaaS is that developers don’t need to worry about managing servers and underlying infrastructure (e.g. managing software updates or security patches). Rather, they can focus on the development, testing, and deploying of their application/software.\nOne example of SaaS is AWS Lambda, a serverless, event-driven compute service that lets you run code for virtually any type of application or backend service without provisioning or managing servers.\n Software as a Service (SaaS) makes software available to users via the internet. With SaaS, users don’t need to install and run software on their computers. Rather, they can access everything they need over the internet by logging into their account(s). The software/application owner does not have any control over the backend except for application-related management tasks.\nSome examples of SaaS applications include Dropbox, Slack, and DocuSign.\n\n\n2.9.5  An Analogy: Pizza as a Service\n\nImage Source: David Ng, Oursky",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote and Cloud Computing</span>"
    ]
  },
  {
    "objectID": "sections/remote-computing.html#virtual-machines-and-containers",
    "href": "sections/remote-computing.html#virtual-machines-and-containers",
    "title": "2  Remote and Cloud Computing",
    "section": "2.10 Virtual Machines and Containers",
    "text": "2.10 Virtual Machines and Containers\nAs servers grow in size, we have increasing amounts of power and resources, but also a larger space to manage. Traditional operating systems use a common memory and process management model that is shared by all users and applications, which can cause some issues if one of the users consumes all of the memory, fills the disk, or causes a kernel panic. When running on a bare server, all of the processes from all users are mixed together and are not isolated, so the actions of one process can have large consequences for all of the others.\n\n\n\n\n\n\n\nVirtual machines Virtualization is an approach to isolate the environments of the various users and services of a system so that we can make better use of the resource, and protect processes. In a virtualized environment, the host server still runs a host operating system, which includes a hypervisor process that can mediate between guest hosts on the machine and the underlying host operating system and hardware. This is effective at creating multiple virtual machines (VMs) running side-by-side on the same hardware. From the outside, and to most users, virtual machines appear to be a regular host on the network, when in fact they are virtual hosts that share the same underlying physical resources. But it also results in a fair amount of redundancy, in that each virtual machine must have its own operating system, libraries, and other resources. And calls pass through the guest operating system through the hypervisor to the physical layer, which can impose a performance penalty.\n\n\n\n\nContainers A further step down the isolation road is to use a Container Runtime such as containerd or Docker Engine. Like virtual machines, containers provide mechanisms to create images that can be executed by a container runtime, and which provide stronger isolation among deployments. But they are also more lightweight, as the container only contains the libraries and executables needed to execute a target application, and not an entire guest operating system. They also are built using a layered file system, which allows multiple images to be layered together to create a composite that provides rich services withot as much duplication. This means that applications run with fewer resources, start up and shut down more quickly, and can be migrated easily to other hosts in a network.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote and Cloud Computing</span>"
    ]
  },
  {
    "objectID": "sections/python-intro.html",
    "href": "sections/python-intro.html",
    "title": "3  Python Programming on Clusters",
    "section": "",
    "text": "3.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Programming on Clusters</span>"
    ]
  },
  {
    "objectID": "sections/python-intro.html#learning-objectives",
    "href": "sections/python-intro.html#learning-objectives",
    "title": "3  Python Programming on Clusters",
    "section": "",
    "text": "Basic Python review\nUsing virtual environments\nWriting in Jupyter notebooks\nWriting functions in Python",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Programming on Clusters</span>"
    ]
  },
  {
    "objectID": "sections/python-intro.html#introduction",
    "href": "sections/python-intro.html#introduction",
    "title": "3  Python Programming on Clusters",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nWe’ve chosen to use VS Code in this training, in part, because it has great support for developing on remote machines. Hopefully, your VS Code setup went easily, and you were able to connect to our server included-crab. Once connected, the VS Code interface looks just like you were working locally, and connection to the server is seamless.\nOther aspects of VS Code that we like: it supports all languages thanks to the extensive free extension library, it has built in version control integration, and it is highly flexible/configurable.\nWe will also be working quite a bit in Jupyter notebooks in this course. Notebooks are great ways to interleave rich text (markdown formatted text, equations, images, links) and code in a way that a ‘literate analysis’ is generated. Although Jupyter notebooks are not subsitutes for python scripts, they can be great communication tools, and can also be convenient for code development.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Programming on Clusters</span>"
    ]
  },
  {
    "objectID": "sections/python-intro.html#connect-to-the-server-if-you-arent-already-connected",
    "href": "sections/python-intro.html#connect-to-the-server-if-you-arent-already-connected",
    "title": "3  Python Programming on Clusters",
    "section": "3.3 Connect to the server (if you aren’t already connected)",
    "text": "3.3 Connect to the server (if you aren’t already connected)\nTo get set up for the course, let’s connect to the server again. If you were able to work through the setup for the lesson without difficulty, follow these steps to connect:\n\nopen VS Code\nopen the command pallette (Cmd + Shift + P)\nenter “Remote SSH: Connect to Host”\nselect included-crab\nenter your password in the dialog box that pops up\n\nAlternatively, you may see a popup window that says “Cannot reconnect. Please reload the window”. Choose the blue Reload Window button and enter your password, if prompted.\nTo open the scalable-computing-examples workspace, select File &gt; Open Workspace from File. Then navigate to and select ~/scalable-computing-examples/scalable-computing-examples.code-workspace. If you cannot find the scalable-computing-examples directory, in a terminal you can use the ls command to list out the contents of your home directory and ensure that you have a clone of the respository. If not, follow the instructions in Exercise 3 in the Remote Computing session).\nOnce connected and in the workslace, use the pwd command in the terminal to make sure you’re in your project directory (/home/yourusername/scalable-computing-examples).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Programming on Clusters</span>"
    ]
  },
  {
    "objectID": "sections/python-intro.html#virtual-environments",
    "href": "sections/python-intro.html#virtual-environments",
    "title": "3  Python Programming on Clusters",
    "section": "3.4 Virtual Environments",
    "text": "3.4 Virtual Environments\n\nWhen you install a python library, let’s say pandas, via pip, unless you specify otherwise, pip will go out and grab the most recent version of the library, and install it somewhere on your system path (where, exactly, depends highly on how you install python originally, and other factors). That library is going to sit alongside every other python library you have ever installed (probably a lot of them) in your site-packages directory. This might work okay, until you start a new project, have no idea what version of pandas you had installed, realize that version conflicts with a new library that you need, and then instead of writing code you are spending days trying to untangle the sphaghetti mess of your python install, libraries and their versions. Sound familiar? Virtual environments help to solve this issue without making the all to common situation in the comic above even more complicated.\nA virtual environment is a folder structure which creates a symlink (pointer) to all of the libraries that you specify into the folder. The three main components will be: the python distribution itself, its configuration, and a site-packages directory (where your libraries like pandas live). So the folder is a self contained directory of all the version-specific python software you need for your project.\nVirtual environments are very helpful to create reproducible workflows, and we’ll talk more about this concept of reproducible environements later in the course. Perhaps most importantly though, virtual environments also help you maintain your sanity when python programming. Because they are just folders, you can create and delete new ones at will, without worrying about bungling your underlying python setup.\nIn this course, we are going to use virtualenv as our tool to create and manage virtual environments. Other virtual environment tools used commonly are conda and pipenv. One reason we like using virtualenv is there is an extension to it called virtualenvwrapper, which provides easy to remember wrappers around common virtualenv operations that make creating, activating, and deactivating a virtual environment very easy.\nFirst we will create a .bash_profile file to create variables that point to the install locations of python and virtualenvwrapper. .bash_profile is just a text file that contains bash commands that are run every time you start up a new terminal. Although setting up this file is not required to use virtualenvwrapper, it is convenient because it allows you to set up some reasonable defaults to the commands (meaning less typing, overall), and it makes sure that the package is available every time you start a new terminal.\n\n3.4.0.1 Setup\n\nIn VS Code, select ‘File &gt; New Text File’\nPaste this text into the file:\n\nexport VIRTUALENVWRAPPER_VIRTUAWORKON_HOME=$HOME/.virtualenvs\nsource /usr/share/virtualenvwrapper/virtualenvwrapper.sh\nThe first line points virtualenvwrapper to the directory where your virtual environments will be stored. We point it to a hidden directory (.virtualenvs) in your home directory. The last line sources a bash script that ships with virtualenvwrapper, which makes all of virtualenvwrapper commands available in your terminal session.\n\nSave the file in the top of your home directory as .bash_profile.\nRestart your terminal (Terminal &gt; New Terminal)\nCheck to make sure it was installed and configured correctly by running this in the terminal:\n\nmkvirtualenv --version\nIt should return some content that looks like this (with more output, potentially).\nvirtualenv 20.13.0+ds from /usr/lib/python3/dist-packages/virtualenv/__init__.py\n\n\n3.4.0.2 Course environment\nNow we can create the virtual environment we will use for the course. In the terminal run:\nmkvirtualenv -p python3.10 scomp\nHere, we’ve specified explicitly which python version to use by using the -p flag, and the path to the python 3.10 installation on the server. After making a virtual environment, it will automatically be activated. You’ll see the name of the env you are working in on the left side of your terminal prompt in parentheses. To deactivate your environment (like if you want to work on a different project), just run deactivate. To activate it again, run:\nworkon scomp\nYou can get a list of all available environments by just running:\nworkon\nNow let’s install the dependencies for this course into that environment. To install our libraries we’ll use pip. As of Python 3.4, pip is automatically included with your python installation. pip is a package manager for python, and you might have used it already to install common python libraries like pandas or numpy. pip goes out to PyPI, the Python Package Index, to download the code and put it in your site-packages directory. Note that on this shared server, your user directory will ahve a site-packages directory, in addition to one that our systems administrator manages as the root of the system.\npip install -r requirements.txt\n\n\n3.4.0.3 Installing locally (optional)\nvirtualenvwrapper was already installed on the server we are working on. To install on your local computer, run:\npip3 install virtualenvwrapper\nAnd then follow the instructions as described above, making sure that you have the correct paths set when you edit your .bash_profile.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Programming on Clusters</span>"
    ]
  },
  {
    "objectID": "sections/python-intro.html#brief-overview-of-python-syntax",
    "href": "sections/python-intro.html#brief-overview-of-python-syntax",
    "title": "3  Python Programming on Clusters",
    "section": "3.5 Brief overview of python syntax",
    "text": "3.5 Brief overview of python syntax\nWe’ll very briefly go over some basic python syntax and the base variable types. First, open a python script. From the File menu, select New File, type “python”, then save it as ‘python-intro.py’ in the top level of your directory.\nIn your file, assign a value to a variable using = and print the result.\n\nx = 4\nprint(x)\n\n4\n\n\nTo run this code in python we can:\n\nexecute python python-intro.py in the terminal\nclick the Play button in the upper right hand corner of the file editor\nright click any line and select: “Run to line in interactive terminal”\n\nIn that interactive window you can then run python code interactively, which is what we’ll use for the next bit of exploring data types.\nThere are 5 standard data types in python\n\nNumber (int, long, float, complex)\nString\nList\nTuple\nDictionary\n\nWe already saw a number type, here is a string:\n\nstr = 'Hello World!'\nprint(str)\n\nHello World!\n\n\nLists in python are very versatile, and are created using square brackets []. Items in a list can be of different data types.\n\nlist = [100, 50, -20, 'text']\nprint(list)\n\n[100, 50, -20, 'text']\n\n\nYou can access items in a list by index using the square brackets. Note indexing starts with 0 in python. The slice operator enables you to easily access a portion of the list without needing to specify every index.\n\nlist[0] # print first element\nlist[1:3] # print 2nd until 4th elements\nlist[:2] # print first until the 3rd\nlist[2:] # print last elements from 3rd\n\n100\n\n\n[50, -20]\n\n\n[100, 50]\n\n\n[-20, 'text']\n\n\nThe + and * operators work on lists by creating a new list using either concatenation (+) or repetition (*).\n\nlist2 = ['more', 'things']\n\nlist + list2\nlist * 3\n\n[100, 50, -20, 'text', 'more', 'things']\n\n\n[100, 50, -20, 'text', 100, 50, -20, 'text', 100, 50, -20, 'text']\n\n\nTuples are similar to lists, except the values cannot be changed in place. They are constructed with parentheses.\n\ntuple = ('a', 'b', 'c', 'd')\ntuple[0]\ntuple * 3\ntuple + tuple\n\n'a'\n\n\n('a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'a', 'b', 'c', 'd')\n\n\n('a', 'b', 'c', 'd', 'a', 'b', 'c', 'd')\n\n\nObserve the difference when we try to change the first value. It works for a list:\n\nlist[0] = 'new value'\nlist\n\n['new value', 50, -20, 'text']\n\n\n…and errors for a tuple.\n\ntuple[0] = 'new value'\n\nTypeError: 'tuple' object does not support item assignment\nDictionaries consist of key-value pairs, and are created using the syntax {key: value}. Keys are usually numbers or strings, and values can be any data type.\n\ndict = {'name': ['Jeanette', 'Matt'],\n        'location': ['Tucson', 'Juneau']}\n\ndict['name']\ndict.keys()\n\n['Jeanette', 'Matt']\n\n\ndict_keys(['name', 'location'])\n\n\nTo determine the type of an object, you can use the type() method.\n\ntype(list)\ntype(tuple)\ntype(dict)\n\nlist\n\n\ntuple\n\n\ndict",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Programming on Clusters</span>"
    ]
  },
  {
    "objectID": "sections/python-intro.html#jupyter-notebooks",
    "href": "sections/python-intro.html#jupyter-notebooks",
    "title": "3  Python Programming on Clusters",
    "section": "3.6 Jupyter notebooks",
    "text": "3.6 Jupyter notebooks\nTo create a new notebook, from the file menu select File &gt; New File &gt; Jupyter Notebook. Go ahead and save this notebook at the top level of your scalable-computing-examples directory.\nAt the top of your notebook, add a first level header using a single hash. Practice some markdown text by creating:\n\na list\nbold text\na link\n\nUse the Markdown cheat sheet if needed.\nYou can click the plus button below any chunk to add a chunk of either markdown or python.\n\n3.6.1 Load libraries\nIn your first code chunk, lets load in some modules. We’ll use pandas, numpy, matplotlib.pyplot, requests, skimpy, and exists from os.path.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport urllib\nimport skimpy\nimport os\n\nA note on style: There are a few ways to construct import statements. The above code uses three of the most common:\nimport module\nimport module as m\nfrom module import function\nThe first way of importing will make the module a function comes from more explicitly clear, and is the simplest. However for very long module names, or ones that are used very frequently (like pandas, numpy, and matplotlib.plot), the code in the notebook will be more cluttered with constant calls to longer module names. So module.function() instead is written as m.function()\nThe second way of importing a module is a good style to use in cases where modules are used frequently, or have extremely long names. If you import every single module with a short name, however, you might have a hard time remembering which modules are named what, and it might be more confusing for others trying to read your code. Many of the most commonly used libraries for python data science have community-driven styling for how they are abbreviated in import statements, and these community norms are generally best followed.\nFinally, the last way to import a single object from a module can be helpful if you only need that one piece from a larger module, but again, like the first case, results in less explicit code and therefore runs the risk of your or someone else misremembering the usage and source.\n\n\n3.6.2 Read in a csv\nCreate a new code chunk that will download the csv that we are going to use for this tutorial.\n\nNavigate to Rohi Muthyala, Åsa Rennermalm, Sasha Leidman, Matthew Cooper, Sarah Cooley, et al. 2022. 62 days of Supraglacial streamflow from June-August, 2016 over southwest Greenland. Arctic Data Center. doi:10.18739/A2XW47X5F.\nRight click the download button for ‘Discharge_timeseries.csv’\nClick ‘copy link address’\n\nCreate a variable called URL and assign it the link copied to your clipboard. Then use urllib.request.urlretrieve to download the file, and open to write it to disk, to a directory called data/. We’ll write this bundled in an if statement so that we only download the file if it doesn’t yet exist. First, we create the directory if it doesn’t exist:\n\nif not os.path.exists ('data/'):\n        os.mkdir('data/')\n\n\nif not os.path.exists('data/discharge_timeseries.csv'):\n\n        url = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3Ae248467d-e1f9-4a32-9e38-a9b4fb17cefb'\n\n        msg = urllib.request.urlretrieve(url, 'data/discharge_timeseries.csv')\n\nNow we can read in the data from the file.\n\ndf = pd.read_csv('data/discharge_timeseries.csv')\ndf.head()\n\n\n\n\n\n\n\n\n\nDate\nTotal Pressure [m]\nAir Pressure [m]\nStage [m]\nDischarge [m3/s]\ntemperature [degrees C]\n\n\n\n\n0\n6/13/2016 0:00\n9.816\n9.609775\n0.206225\n0.083531\n-0.1\n\n\n1\n6/13/2016 0:05\n9.810\n9.609715\n0.200285\n0.077785\n-0.1\n\n\n2\n6/13/2016 0:10\n9.804\n9.609656\n0.194344\n0.072278\n-0.1\n\n\n3\n6/13/2016 0:15\n9.800\n9.609596\n0.190404\n0.068756\n-0.1\n\n\n4\n6/13/2016 0:20\n9.793\n9.609537\n0.183463\n0.062804\n-0.1\n\n\n\n\n\n\n\n\nThe column names are a bit messy so we can use clean_columns from skimpy to make them cleaner for programming very quickly. We can also use the skim function to get a quick summary of the data.\n\n\nclean_df = skimpy.clean_columns(df)\nskimpy.skim(clean_df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 17856  │ │ float64     │ 5     │                                                          │\n│ │ Number of columns │ 6      │ │ string      │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name             ┃ NA  ┃ NA %  ┃ mean    ┃ sd     ┃ p0       ┃ p25    ┃ p50  ┃ p75  ┃ p100 ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n│ │ total_pressure_m        │   0 │     0 │     9.9 │   0.12 │      9.6 │    9.8 │  9.9 │   10 │   10 │ ▁▅▇▇▅▂ │  │\n│ │ air_pressure_m          │   0 │     0 │     9.6 │   0.06 │      9.5 │    9.6 │  9.6 │  9.7 │  9.7 │ ▂▅▅▆▇▃ │  │\n│ │ stage_m                 │   0 │     0 │    0.28 │   0.12 │  0.00056 │   0.17 │ 0.27 │ 0.37 │ 0.56 │ ▁▇▇▇▆▁ │  │\n│ │ discharge_m_3_s         │   0 │     0 │    0.22 │   0.19 │  4.7e-08 │  0.055 │ 0.16 │ 0.35 │ 0.96 │  ▇▃▃▁  │  │\n│ │ temperature_degrees_c   │   8 │  0.04 │  -0.034 │  0.053 │     -0.1 │   -0.1 │    0 │    0 │  0.2 │   ▅▇   │  │\n│ └─────────────────────────┴─────┴───────┴─────────┴────────┴──────────┴────────┴──────┴──────┴──────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name               ┃ NA      ┃ NA %       ┃ words per row                ┃ total words              ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ date                      │       0 │          0 │                            2 │                    35712 │  │\n│ └───────────────────────────┴─────────┴────────────┴──────────────────────────────┴──────────────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nWe can see that the date column is classed as a string, and not a date, so let’s fix that.\n\n\nclean_df['date'] = pd.to_datetime(clean_df['date'])\nskimpy.skim(clean_df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 17856  │ │ float64     │ 5     │                                                          │\n│ │ Number of columns │ 6      │ │ datetime64  │ 1     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name             ┃ NA  ┃ NA %  ┃ mean    ┃ sd     ┃ p0       ┃ p25    ┃ p50  ┃ p75  ┃ p100 ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n│ │ total_pressure_m        │   0 │     0 │     9.9 │   0.12 │      9.6 │    9.8 │  9.9 │   10 │   10 │ ▁▅▇▇▅▂ │  │\n│ │ air_pressure_m          │   0 │     0 │     9.6 │   0.06 │      9.5 │    9.6 │  9.6 │  9.7 │  9.7 │ ▂▅▅▆▇▃ │  │\n│ │ stage_m                 │   0 │     0 │    0.28 │   0.12 │  0.00056 │   0.17 │ 0.27 │ 0.37 │ 0.56 │ ▁▇▇▇▆▁ │  │\n│ │ discharge_m_3_s         │   0 │     0 │    0.22 │   0.19 │  4.7e-08 │  0.055 │ 0.16 │ 0.35 │ 0.96 │  ▇▃▃▁  │  │\n│ │ temperature_degrees_c   │   8 │  0.04 │  -0.034 │  0.053 │     -0.1 │   -0.1 │    0 │    0 │  0.2 │   ▅▇   │  │\n│ └─────────────────────────┴─────┴───────┴─────────┴────────┴──────────┴────────┴──────┴──────┴──────┴────────┘  │\n│                                                    datetime                                                     │\n│ ┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name        ┃ NA    ┃ NA %     ┃ first            ┃ last                           ┃ frequency      ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩  │\n│ │ date               │     0 │        0 │    2016-06-13    │      2016-08-13 23:55:00       │ 5min           │  │\n│ └────────────────────┴───────┴──────────┴──────────────────┴────────────────────────────────┴────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nIf we wanted to calculate the daily mean flow (as opposed to the flow every 5 minutes), we need to:\n\ncreate a new column with only the date\ngroup by that variable\nsummarize over it by taking the mean\n\nFirst we should probably rename our existing date/time column to prevent from getting confused.\n\nclean_df = clean_df.rename(columns = {'date': 'datetime'})\n\nNow create the new date column\n\nclean_df['date'] = clean_df['datetime'].dt.date\n\nFinally, we use group by to split the data into groups according to the date. We can then apply the mean method to calculate the mean value across all of the columns. Note that there are other methods you can use to calculate different statistics across different columns (eg: clean_df.groupby('date').agg({'discharge_m_3_s': 'max'})).\n\ndaily_flow = clean_df.groupby('date', as_index = False).mean(numeric_only = True)\n\n\ncreate a simple plot\n\n\nvar = 'discharge_m_3_s'\nvar_labs = {'discharge_m_3_s': 'Total Discharge'}\n\nfig, ax = plt.subplots(figsize=(7, 7))\nplt.plot(daily_flow['date'], daily_flow[var])\nplt.xticks(rotation = 45)\nax.set_ylabel(var_labs.get('discharge_m_3_s'))\n\n(array([16967., 16974., 16983., 16990., 16997., 17004., 17014., 17021.,\n        17028.]),\n [Text(16967.0, 0, '2016-06-15'),\n  Text(16974.0, 0, '2016-06-22'),\n  Text(16983.0, 0, '2016-07-01'),\n  Text(16990.0, 0, '2016-07-08'),\n  Text(16997.0, 0, '2016-07-15'),\n  Text(17004.0, 0, '2016-07-22'),\n  Text(17014.0, 0, '2016-08-01'),\n  Text(17021.0, 0, '2016-08-08'),\n  Text(17028.0, 0, '2016-08-15')])\n\n\nText(0, 0.5, 'Total Discharge')",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Programming on Clusters</span>"
    ]
  },
  {
    "objectID": "sections/python-intro.html#functions",
    "href": "sections/python-intro.html#functions",
    "title": "3  Python Programming on Clusters",
    "section": "3.7 Functions",
    "text": "3.7 Functions\nThe plot we made above is great, but what if we wanted to make it for each variable? We could copy paste it and replace some things, but this violates a core tenet of programming: Don’t Repeat Yourself! Instead, we’ll create a function called myplot that accepts the data frame and variable as arguments.\n\ncreate myplot.py\n\n\nimport matplotlib.pyplot as plt\n\ndef myplot(df, var):\n\n        var_labs = {'discharge_m_3_s': 'Total Discharge (m^3/s)',\n                    'total_pressure_m': 'Total Pressure (m)',\n                    'air_pressure_m': 'Air Pressure (m)',\n                    'stage_m': 'Stage (m)',\n                    'temperature_degrees_c': 'Temperature (C)'}\n\n        fig, ax = plt.subplots(figsize=(7, 7))\n        plt.plot(df['date'], df[var])\n        plt.xticks(rotation = 45)\n        ax.set_ylabel(var_labs.get(var))\n\n\nload myplot into jupyter notebook (from myplot import myplot)\nadd libraries\nreplace old plot method with new function\n\n\nmyplot(daily_flow, 'temperature_degrees_c')\n\n\n\n\n\n\n\n\nWe’ll have more on functions in the software design sections.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Programming on Clusters</span>"
    ]
  },
  {
    "objectID": "sections/python-intro.html#summary",
    "href": "sections/python-intro.html#summary",
    "title": "3  Python Programming on Clusters",
    "section": "3.8 Summary",
    "text": "3.8 Summary\nIn this lesson we learned all about virtual environments, how to use them, and why. We got our environments set up for the course, did a brief python syntax review, and then jumped into Jupyter notebooks, pandas, and writing functions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Programming on Clusters</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html",
    "href": "sections/parallel-programming.html",
    "title": "4  Pleasingly Parallel Programming",
    "section": "",
    "text": "4.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#learning-objectives",
    "href": "sections/parallel-programming.html#learning-objectives",
    "title": "4  Pleasingly Parallel Programming",
    "section": "",
    "text": "Understand what parallel computing is and when it may be useful\nUnderstand how parallelism can work\nReview sequential loops and map functions\nBuild a parallel program using concurrent.futures\nBuild a parallel program using parsl\nUnderstand Thread Pools and Process pools",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#introduction",
    "href": "sections/parallel-programming.html#introduction",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nProcessing large amounts of data with complex models can be time consuming. New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well. For example, here’s a 2 TB (that’s Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America:\n\n\n\nLevy et al. 2016. doi:10.5063/F1Z899CZ\n\n\nThere are over 400,000 individual netCDF files in the Levy et al. microclimate data set. Processing them would benefit massively from parallelization.\nAlternatively, think of remote sensing data. Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years.\n\n\n\nNEON Data Cube",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#why-parallelism",
    "href": "sections/parallel-programming.html#why-parallelism",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.3 Why parallelism?",
    "text": "4.3 Why parallelism?\nMuch R code runs fast and fine on a single processor. But at times, computations can be:\n\ncpu-bound: Take too much cpu time\nmemory-bound: Take too much memory\nI/O-bound: Take too much time to read/write from disk\nnetwork-bound: Take too much time to transfer\n\nTo help with cpu-bound computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire set of those processors. Plus, these machines also have large amounts of memory to avoid memory-bound computing jobs.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#processors-cpus-cores-and-threads",
    "href": "sections/parallel-programming.html#processors-cpus-cores-and-threads",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.4 Processors (CPUs), Cores, and Threads",
    "text": "4.4 Processors (CPUs), Cores, and Threads\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with mutliple processors, each of which in turn can contain multiple cores. These processors and cores are available to perform computations. But, just what’s the difference between processors and cores? A computer with one processor may still have 4 cores (quad-core), allowing 4 (or possibly more) computations to be executed at the same time.\n\n\nMicroprocessor: an integrated circuit that contains the data processing logic and control for a computer.\nMulti-core processor: a microprocessor containing multiple processing units (cores) on a single integrated circuit. Each core in a multi-core processor can execute program instructions at the same time.\nProcess: an instance of a computer program (including instructions, memory, and other resources) that is executed on a microprocessor.\nThread: a thread of execution is the smallest sequence of program instructions that can be executed independently, and is typically a component of a process. The threads in a process can be executed concurrently and typically share the same memory space. They are faster to create than a process.\nCluster: a set of multiple, physically distinct computing systems, each with its own microprocessors, memory, and storage resources, connected together by a (fast) network that allows the nodes to be viewed as a single system.\n\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here we show four quad-core processors for a total of 16 cores in this machine.\n\nYou can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later).\nHistorically, many languages only utilized one processor, which makes them single-threaded. Which is a shame, because the 2019 MacBook Pro that I am writing this on is much more powerful than that, and has mutliple cores that would support concurrent execution of multiple threads:\njones@powder:~$ sysctl hw.ncpu hw.physicalcpu\nhw.ncpu: 12\nhw.physicalcpu: 6\nTo interpret that output, this machine powder has 6 physical CPUs, each of which has two processing cores, for a total of 12 cores for computation. I’d sure like my computations to use all of that processing power. Because its all on one machine, we can easily use multicore processing tools to make use of those cores. Now let’s look at the computational server included-crab at NCEAS:\njones@included-crab:~$ lscpu | egrep 'CPU\\(s\\)|per core|per socket' \nCPU(s):                          88\nOn-line CPU(s) list:             0-87\nThread(s) per core:              1\nCore(s) per socket:              1\nNUMA node0 CPU(s):               0-87\nNow that’s more compute power! included-crab has 384 GB of RAM, and ample storage. All still under the control of a single operating system.\nFinally, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now:\n\n\n\nStampede2 at TACC\n\n4200 KNL nodes: 285,600 cores\n1736 SKX nodes: 83,328 cores\n224 ICX nodes: 17,920 cores\nTOTAL: 386,848 cores\n\nDelta at NCSA\n\n124 CPU Milan nodes (15,872 cores)\n100 quad A100 GPU nodes (6400 cores + 400 GPUs)\n100 quad A40 GPU nodes (6400 cores + 400 GPUs)\n5 eight-way A100 GPU nodes (640 cores + 40 GPUs):\n1 MI100 GPU node (128 cores + 8 GPUs)\n7 PB of disk-based Lustre storage\n3 PB of flash based storage\nTOTAL: 29,440 cores, 848 gpus\n\n\n\n\n\n\n\nDelta Supercomputer\n\n\n\n\nNote that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster. One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine. But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#parallel-processing-in-the-shell",
    "href": "sections/parallel-programming.html#parallel-processing-in-the-shell",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.5 Parallel processing in the shell",
    "text": "4.5 Parallel processing in the shell\nShell programming helps massively speed up data management tasks, and even more so with simple use of the GNU parallel utility to execute bash commands in parallel. In its simplest form, this can be used to speed up common file operations, such as renaming, compression, decompression, and file transfer. Let’s look at a common example – calculating checksums to verify file integrity for data files. Calculating a hash checksum using the shasum command can be time consuming, especially when you have a lot of large files to work on. But it is a classic processor-limited task, and one that can be massively faster using parallel.\n$ for fn in `ls *.gpkg`; do shasum -a 256 ${fn}; done\n\nreal    35.081s\nuser    32.745s\nsystem  2.336s\n\n$ ls *.gpkg | parallel \"shasum -a 256 {}\"\n\nreal    2.97s \nuser    37.16s \nsystem  2.70s\nThe first invocation takes 35 seconds to execute the tasks one at a time serially, while the second version only takes 🎉 3 seconds 🎉 to do the same tasks. Note that the computational time spent in user and system processing is about the same, with the major difference being that the user-space tasks were conducted on multiple cores in parallel, resulting in more than 10x faster performance, Using htop, you can see processor cores spiking in usage when the command is run:\n\n\n\nProcessor usage for parallel tasks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#modes-of-parallelization",
    "href": "sections/parallel-programming.html#modes-of-parallelization",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.6 Modes of parallelization",
    "text": "4.6 Modes of parallelization\nSeveral different approaches can be taken to structuring a computer program to take advantage of the hardware capabilities of multi-core processors. In the typical, and simplest, case, each task in a computation is executed serially in order of first to last. The total computation time is the sum of the time of all of the subtasks that are executed. In the next figure, a single core of the processor is used to sequentially execute each of the five tasks, with time flowing from left to right.\n\n\n\nSerial and parallel execution of tasks using threads and processes.\n\n\nIn comparison, the middle panel shows two approaches to parallelization on a single computer: Parallel Threads and Parallel Processes. With multi-threaded execution, a separate thread of execution is created for each of the 5 tasks, and these are executed concurrently on 5 of the cores of the processor. All of the threads are in the same process and share the same memory and resources, so one must take care that they do not interfere with each other.\nWith multi-process execution, a separate process is created for each of the 5 tasks, and these are executed concurrently on the cores of the processor. The difference is that each process has it’s own copy of the program memory, and changes are merged when each child process completes. Because each child process must be created and resources for that process must be marshalled and unmarshalled, there is more overhead in creating a process than a thread. “Marshalling” is the process of transforming the memory representation of an object into another format, which allows communication between remote objects by converting an object into serialized form.\nFinally, cluster parallel execution is shown in the last panel, in which a cluster with multiple computers is used to execute multiple processes for each task. Again, there is a setup task associated with creating and mashaling resources for the task, which now includes the overhead of moving data from one machine to the others in the cluster over the network. This further increases the cost of creating and executing multiple processes, but can be highly advantageous when accessing exceedingly large numbers of processing cores on clusters.\nThe key to performance gains is to ensure that the overhead associated with creating new threads or processes is small relative to the time it takes to perform a task. Somewhat unintuitively, when the setup overhead time exceeds the task time, parallel execution will likely be slower than serial.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#task-parallelization-in-python",
    "href": "sections/parallel-programming.html#task-parallelization-in-python",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.7 Task parallelization in Python",
    "text": "4.7 Task parallelization in Python\nPython also provides a number of easy to use packages for concurrent processing. We will review two of these, concurrent.futures and parsl, to show just how easy it can be to parallelize your programs. concurrent.futures is built right into the python3 release, and is a great starting point for learning concurrency.\nWe’re going to start with a task that is a little expensive to compute, and define it in a function. All this task(x) function does is to use numpy to create a fairly large range of numbers, and then sum them.\n\ndef task(x):\n    import numpy as np\n    result = np.arange(x*10**8).sum()\n    return result\n\nWe can start by executing this task function serially ten times with varying inputs. In this case, we create a function run_serial that takes a list of inputs to be run, and it calls the task function for each of those inputs. The @timethis decorator is a simple way to wrap the function with timing code so that we can see how long it takes to execute.\n\nimport numpy as np\n\n@timethis\ndef run_serial(task_list):\n    return [task(x) for x in task_list]\n\nrun_serial(np.arange(10))\n\nrun_serial: 7914.26157951355 ms\n\n\n[0,\n 4999999950000000,\n 19999999900000000,\n 44999999850000000,\n 79999999800000000,\n 124999999750000000,\n 179999999700000000,\n 244999999650000000,\n 319999999600000000,\n 404999999550000000]\n\n\nIn this case, it takes around 25 seconds to execute 10 tasks, depending on what else is happening on the machine and network.\nSo, can we make this faster using a multi-threaded parallel process? Let’s try with concurrent.futures. The main concept in this package is one of a future, which is a structure which represents the value that will be created in a computation in the future when the function completes execution. With concurrent.futures, tasks are scheduled and do not block while they await their turn to be executed. Instead, threads are created and executed asynchronously, meaning that the function returns it’s future potentially before the thread has actually been executed. Using this approach, the user schedules a series of tasks to be executed asynchronously, and keeps track of the futures for each task. When the future indicates that the execution has been completed, we can then retrieve the result of the computation.\nIn practice this is a simple change from our serial implementation. We will use the ThreadPoolExecutor to create a pool of workers that are available to process tasks. Each worker is set up in its own thread, so it can execute in parallel with other workers. After setting up the pool of workers, we use concurrent.futures map() to schedule each task from our task_list (in this case, an input value from 1 to 10) to run on one of the workers. As for all map() implementations, we are asking for each value in task_list to be executed in the task function we defined above, but in this case it will be executed using one of the workers from the executor that we created.\n\nfrom concurrent.futures import ThreadPoolExecutor\n\n@timethis\ndef run_threaded(task_list):\n    with ThreadPoolExecutor(max_workers=20) as executor:\n        return executor.map(task, task_list)\n\nresults = run_threaded(np.arange(10))\n[x for x in results]\n\nrun_threaded: 4440.109491348267 ms\n[0,\n 4999999950000000,\n 19999999900000000,\n 44999999850000000,\n 79999999800000000,\n 124999999750000000,\n 179999999700000000,\n 244999999650000000,\n 319999999600000000,\n 404999999550000000]\nThis execution took about 🎉 4 seconds 🎉, which is about 6.25x faster than serial. Congratulations, you wrote your a multi-threaded python program!\nBut you may have also seen the thread pool wasn’t much faster at all. In that case, you may be encountering the python Global Interpreter Lock (GIL), and would be better off with using parallel processes. Let’s do that with ProcessPoolExecutor.\n\nfrom concurrent.futures import ProcessPoolExecutor\n\n@timethis\ndef run_processes(task_list):\n    with ProcessPoolExecutor(max_workers=10) as executor:\n        return executor.map(task, task_list)\n\nresults = run_processes(np.arange(10))\n[x for x in results]\n\nFor me, that took about 8 seconds, about half the time of the serial case, but results can vary tremendously depending on what others are doing on the machine.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#exercise-parallel-downloads",
    "href": "sections/parallel-programming.html#exercise-parallel-downloads",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.8 Exercise: Parallel downloads",
    "text": "4.8 Exercise: Parallel downloads\nIn this exercise, we’re going to parallelize a simple task that is often very time consuming – downloading data. And we’ll compare performance of simple downloads using first a serial loop, and then using two parallel execution libraries: concurrent.futures and parsl. We’re going to see an example here where parallel execution won’t always speed up this task, as this is likely an I/O bound task if you’re downloading a lot of data. But we still should be able to speed things up a lot until we hit the limits of our disk arrays.\nThe data we are downloading is a pan-Arctic time series of TIF images containing rasterized Arctic surface water indices from:\n\nElizabeth Webb. 2022. Pan-Arctic surface water (yearly and trend over time) 2000-2022. Arctic Data Center doi:10.18739/A2NK3665N.\n\n\n\n\nWebb surface water index data\n\n\nFirst, let’s download the data serially to set a benchmark. The data files are listed in a table with their filename and identifier, and can be downloaded directly from the Arctic Data Center using their identifier. To make things easier, we’ve already provided a data frame with the names and identifiers of each file that could be downloaded.\n\n\n\n\n\n\n\n\n\n\nfilename\nidentifier\n\n\n\n\n0\nSWI_2007.tif\nurn:uuid:5ee72c9c-789d-4a1c-95d8-cb2b24a20662\n\n\n1\nSWI_2019.tif\nurn:uuid:9cd1cdc3-0792-4e61-afff-c11f86d3a9be\n\n\n2\nSWI_2021.tif\nurn:uuid:14e1e509-77c0-4646-9cc3-d05f8d84977c\n\n\n3\nSWI_2020.tif\nurn:uuid:1ba473ff-8f03-470b-90d1-7be667995ea1\n\n\n4\nSWI_2001.tif\nurn:uuid:85150557-05fd-4f52-8bbd-ec5a2c27e23d\n\n\n\n\n\n\n\n\n\n4.8.1 Serial\nWhen you have a list of repetitive tasks, you may be able to speed it up by adding more computing power. If each task is completely independent of the others, then it is pleasingly parallel and a prime candidate for executing those tasks in parallel, each on its own core. For example, let’s build a simple loop that downloads the data files that we need for an analysis. First, we start with the serial implementation.\n\nimport urllib\n\ndef download_file(row):\n    service = \"https://arcticdata.io/metacat/d1/mn/v2/object/\"\n    pid = row[1]['identifier']\n    filename = row[1]['filename']\n    url = service + pid\n    print(\"Downloading: \" + filename)\n    msg = urllib.request.urlretrieve(url, filename)\n    return filename\n\n@timethis\ndef download_serial(table):\n    results = [download_file(row) for row in table.iterrows()]\n    return results\n    \nresults = download_serial(file_table[0:5])\nprint(results)\n\nDownloading: SWI_2007.tif\nDownloading: SWI_2019.tif\nDownloading: SWI_2021.tif\nDownloading: SWI_2020.tif\nDownloading: SWI_2001.tif\ndownload_serial: 62585.78824996948 ms\n['SWI_2007.tif', 'SWI_2019.tif', 'SWI_2021.tif', 'SWI_2020.tif', 'SWI_2001.tif']\n\n\nIn this code, we have one function (download_file) that downloads a single data file and saves it to disk. It is called iteratively from the function download_serial. The serial execution takes about 20-25 seconds, but can vary considerably based on network traffic and other factors.\nThe issue with this loop is that we execute each download task sequentially, which means that only one of our processors on this machine is in use. In order to exploit parallelism, we need to be able to dispatch our tasks and allow each to run at the same time, with one task going to each core. To do that, we can use one of the many parallelization libraries in python to help us out.\n\n\n4.8.2 Multi-threaded with concurrent.futures\nIn this case, we’ll use the same download_file function from before, but let’s switch up and create a download_threaded() function to use concurrent.futures with a ThreadPoolExecutor just as we did earlier.\n\nfrom concurrent.futures import ThreadPoolExecutor\n\n@timethis\ndef download_threaded(table):\n    with ThreadPoolExecutor(max_workers=15) as executor:\n        results = executor.map(download_file, table.iterrows(), timeout=60)\n        return results\n\nresults = download_threaded(file_table[0:5])\nfor result in results:\n    print(result)\n\nDownloading: SWI_2007.tif\nDownloading: SWI_2019.tif\nDownloading: SWI_2021.tif\nDownloading: SWI_2020.tif\nDownloading: SWI_2001.tif\ndownload_threaded: 18338.350772857666 ms\nSWI_2007.tif\nSWI_2019.tif\nSWI_2021.tif\nSWI_2020.tif\nSWI_2001.tif\n\n\nNote how the “Downloading…” messages were printed immediately, but then it still took over 20 seconds to download the 5 files. This could be for several reasons, including that one of the files alone took that long (e.g., due to network congestion), or that there was a bottleneck in writing the files to disk (i.e., we could have been disk I/O limited). Or maybe the multithreaded executor pool didn’t do a good job parallelizing the tasks. The trick is figuring out why you did or didn’t get a speedup when parallelizing. So, let’s try this another way, using a multi-processing approach, rather than multi-threading.\n\n\n4.8.3 Multi-process with concurrent.futures\nYou’ll remember from earlier that you can execute tasks concurrently by creating multiple threads within one process (multi-threaded), or by creating and executing muliple processes. The latter creates more independence, as each of the executing tasks has their own memory and process space, but it also takes longer to set up. With concurrent.futures, we can switch to a multi-process pool by using a ProcessPoolExecutor, analogously to how we used ThreadPoolExecutor previously. So, some simple changes, and we’re running multiple processes.\n\nfrom concurrent.futures import ProcessPoolExecutor\n\n@timethis\ndef download_process(table):\n    with ProcessPoolExecutor(max_workers=15) as executor:\n        results = executor.map(download_file, table.iterrows(), timeout=60)\n        return results\n\nresults = download_process(file_table[0:5])\nfor result in results:\n    print(result)\n\nDownloading: SWI_2007.tifDownloading: SWI_2019.tifDownloading: SWI_2021.tif\nDownloading: SWI_2020.tif\n\nDownloading: SWI_2001.tif\n\ndownload_process: 9896.435499191284 ms\nSWI_2007.tif\nSWI_2019.tif\nSWI_2021.tif\nSWI_2020.tif\nSWI_2001.tif\n\n\nAgain, the output messages print almost immediately, but then later the processes finish and report that it took between 10 to 15 seconds to run. Your mileage may vary. When I increase the number of files being downloaded to 10 or even to 20, I notice it is actually about the same, around 10-15 seconds. So, part of our time now is the overhead of setting up multiple processes. But once we have that infrastructure in place, we can make effective euse of the pool of processes to handle many downloads.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#parallel-processing-with-parsl",
    "href": "sections/parallel-programming.html#parallel-processing-with-parsl",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.9 Parallel processing with parsl",
    "text": "4.9 Parallel processing with parsl\nconcurrent.futures is great and powerful, but it has its limits. Particularly as you try to scale up into the thousands of concurrent tasks, other libraries like Parsl (docs), Dask, Ray, and others come into play. They all have their strengths, but Parsl makes it particularly easy to build parallel workflows out of existing python code through it’s use of decorators on existing python functions.\nIn addition, Parsl supports a lot of different kinds of providers, allowing the same python code to be easily run multi-threaded using a ThreadPoolExecutor and via multi-processing on many different cluster computing platforms using the HighThroughputExecutor. For example, Parsl includes providers supporting local execution, and on Slurm, Condor, Kubernetes, AWS, and other platforms. And Parsl handles data staging as well across these varied environments, making sure the data is in the right place when it’s needed for computations.\nSimilarly to before, we start by configuring an executor in parsl, and loading it. We’ll use multiprocessing by configuring the HighThroughputExecutor to use our local resources as a cluster, and we’ll activate our virtual environment to be sure we’re executing in a consistent environment.\n\n# Required packages\nimport parsl\nfrom parsl import python_app\nfrom parsl.config import Config\nfrom parsl.executors import HighThroughputExecutor\nfrom parsl.providers import LocalProvider\n\n# Configure the parsl executor\nactivate_env = 'workon scomp'\nhtex_local = Config(\n    executors=[\n        HighThroughputExecutor(\n            max_workers=15,\n            provider=LocalProvider(\n                worker_init=activate_env\n            )\n        )\n    ],\n)\nparsl.clear()\nparsl.load(htex_local)\n\n&lt;parsl.dataflow.dflow.DataFlowKernel at 0x7ff40cb1cdc0&gt;\n\n\nWe now have a live parsl executor (htex_local) that is waiting to execute processes. We tell it to execute processes by annotating functions with decorators that indicate which tasks should be parallelized. Parsl then handles the scheduling and execution of those tasks based on the dependencies between them. In the simplest case, we’ll decorate our previous function for downloading a file with the @python_app decorator, which tells parsl that any function calls with this function should be run on the default executor (in this case, htex_local).\n\n# Decorators seem to be ignored as the first line of a cell, so print something first\nprint(\"Create decorated function\")\n\n@python_app\ndef download_file_parsl(row):\n    import urllib\n    service = \"https://arcticdata.io/metacat/d1/mn/v2/object/\"\n    pid = row[1]['identifier']\n    filename = row[1]['filename']\n    url = service + pid\n    print(\"Downloading: \" + filename)\n    msg = urllib.request.urlretrieve(url, filename)\n    return filename\n\nCreate decorated function\n\n\nNow we just write regular python code that calls that function, and parsl handles the scheduling. Parsl app executors return an AppFuture, and we can call the AppFuture.done() function to determine when the future result is ready without blocking. Or, we can just block on AppFuture.result() which waits for each of the executions to complete and then returns the result.\n\n#! eval: true\nprint(\"Define our download_futures function\")\n\n@timethis\ndef download_futures(table):\n    results = []\n    for row in table.iterrows():\n        result = download_file_parsl(row)\n        print(result)\n        results.append(result)\n    return(results)\n\n@timethis\ndef wait_for_futures(table):\n    results = download_futures(table)\n    done = [app_future.result() for app_future in results]\n    print(done)\n\nwait_for_futures(file_table[0:5])\n\nDefine our download_futures function\n&lt;AppFuture at 0x7ff3d87299f0 state=pending&gt;\n&lt;AppFuture at 0x7ff3d8728e80 state=pending&gt;\n&lt;AppFuture at 0x7ff3d872b0a0 state=pending&gt;\n&lt;AppFuture at 0x7ff3d87cb730 state=pending&gt;\n&lt;AppFuture at 0x7ff3d87cb970 state=pending&gt;\ndownload_futures: 10.04648208618164 ms\n['SWI_2007.tif', 'SWI_2019.tif', 'SWI_2021.tif', 'SWI_2020.tif', 'SWI_2001.tif']\nwait_for_futures: 25056.697607040405 ms\n\n\nWhen we’re done, be sure to clean up and shutdown the htex_local executor, or it will continue to persist in your environment and utilize resources. Generally, an executor should be created when setting up your environment, and then it can be used repeatedly for many different tasks.\n\nhtex_local.executors[0].shutdown()\nparsl.clear()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#are-we-done-yet",
    "href": "sections/parallel-programming.html#are-we-done-yet",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.10 Are we done yet?",
    "text": "4.10 Are we done yet?\nParsl and other concurrency libraries generally provide both blocking and non-blocking methods for accessing the results of asycnchronous method calls. By blocking, we are referring to methods that wait for the asynchronous opearation to complete before returning results, which blocks execution of the rest of a program. By non-blocking, we are referring to methods that return the result if it is available, but not if the async method hasn’t completed yet. In that case, it lets the rest of the program to continue execution.\nIn practice this means that we can either 1) wait for all async calls to complete, and then process them using the blocking methods, or 2) query with a non-blocking method to see if each async call is complete, and only then retrieve the results for that method. We illustrate this approach below with parsl.\n\n# Required packages\nimport parsl\nfrom parsl import python_app\nfrom parsl.config import Config\nfrom parsl.executors import HighThroughputExecutor\nfrom parsl.providers import LocalProvider\n\n# Configure the parsl executor\nactivate_env = 'workon scomp'\nhtex_local = Config(\n    executors=[\n        HighThroughputExecutor(\n            max_workers=5,\n            provider=LocalProvider(\n                worker_init=activate_env\n            )\n        )\n    ],\n)\nparsl.clear()\nparsl.load(htex_local)\n\n&lt;parsl.dataflow.dflow.DataFlowKernel at 0x7ff40cb1dbd0&gt;\n\n\nDefine the task we want to run.\n\n@python_app\ndef do_stuff(x):\n    import time\n    time.sleep(1)\n    return x**2\n\nAnd now execute the tasks with some sleeps to see which are blocking and which are completed.\n\nimport time\nall_futures = []\nfor x in range(0,10):\n    future = do_stuff(x)\n    all_futures.append(future)\n    print(future)\n\ntime.sleep(3)\n\nfor future in all_futures:\n    print(\"Checking: \", future)\n    if (future.done()):\n        print(\"Do more with result: \", future.result())\n    else:\n        print(\"Sorry, come back later.\")\n\n&lt;AppFuture at 0x7ff3d87c8640 state=pending&gt;\n&lt;AppFuture at 0x7ff3d8729d80 state=pending&gt;\n&lt;AppFuture at 0x7ff40cb1ce80 state=pending&gt;\n&lt;AppFuture at 0x7ff3d87cafe0 state=pending&gt;\n&lt;AppFuture at 0x7ff3d87cb6d0 state=pending&gt;\n&lt;AppFuture at 0x7ff3d87c8a90 state=pending&gt;\n&lt;AppFuture at 0x7ff3d87cbf40 state=pending&gt;\n&lt;AppFuture at 0x7ff3d87c8130 state=pending&gt;\n&lt;AppFuture at 0x7ff3d8728dc0 state=pending&gt;\n&lt;AppFuture at 0x7ff3d8729e70 state=pending&gt;\nChecking:  &lt;AppFuture at 0x7ff3d87c8640 state=finished returned int&gt;\nDo more with result:  0\nChecking:  &lt;AppFuture at 0x7ff3d8729d80 state=finished returned int&gt;\nDo more with result:  1\nChecking:  &lt;AppFuture at 0x7ff40cb1ce80 state=pending&gt;\nSorry, come back later.\nChecking:  &lt;AppFuture at 0x7ff3d87cafe0 state=pending&gt;\nSorry, come back later.\nChecking:  &lt;AppFuture at 0x7ff3d87cb6d0 state=pending&gt;\nSorry, come back later.\nChecking:  &lt;AppFuture at 0x7ff3d87c8a90 state=pending&gt;\nSorry, come back later.\nChecking:  &lt;AppFuture at 0x7ff3d87cbf40 state=pending&gt;\nSorry, come back later.\nChecking:  &lt;AppFuture at 0x7ff3d87c8130 state=pending&gt;\nSorry, come back later.\nChecking:  &lt;AppFuture at 0x7ff3d8728dc0 state=pending&gt;\nSorry, come back later.\nChecking:  &lt;AppFuture at 0x7ff3d8729e70 state=pending&gt;\nSorry, come back later.\n\n\nNotice in particular that about half of the jobs are not done yet.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#when-to-parallelize",
    "href": "sections/parallel-programming.html#when-to-parallelize",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.11 When to parallelize",
    "text": "4.11 When to parallelize\nIt’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!\nIn addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced. Some propose that this may follow Amdahl’s Law, where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines):\n\n\n\nAmdahl’s Law\n\n\nSo, it is important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#parallel-pitfalls",
    "href": "sections/parallel-programming.html#parallel-pitfalls",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.12 Parallel Pitfalls",
    "text": "4.12 Parallel Pitfalls\nA set of tasks is considered ‘pleasingly parallel’ when large portions of the code can be executed indpendently of the other portions and have few or no dependencies on other parts of the execution. This situation is common, and we can frequently execute parallel tasks on independent subsets of our data. Nevertheless, dependencies among different parts of your computation can definitely create bottlenecks and slow down computations. Some of the challenges you may need to work around include:\n\nTask dependencies: occur when one task in the code depends on the results of another task or computation in the code.\nRace conditions: occur when two tasks execute in parallel, but produce different results based on which task finishes first. Ensuring that results are correct under different timing situations requires careful testing.\nDeadlocks: occur when two concurrent tasks block on the output of the other. Deadlocks cause parallel programs to lock up indefinitely, and can be difficult to track down.\n\nEven when tasks exhibit strong dependencies, it is frequently possible to still optimize that code by parallelizing explicit code sections, sometimes bringing other concurrency tools into the mix, such as the Message Passing Interface (MPI). But simply improving the efficiency of pleasingly parallel tasks can be liberating.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#summary",
    "href": "sections/parallel-programming.html#summary",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.13 Summary",
    "text": "4.13 Summary\nIn this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores. Next, we reviewed the way in which traditional sequential loops can be rewritten as functions that are applied to a list of inputs both serially and in parallel to utilize multiple cores to speed up computations. We reviewed the challenges of optimizing code, where one must constantly examine the bottlenecks that arise as we improve cpu-bound, I/O bound,and memory bound computations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/parallel-programming.html#further-reading",
    "href": "sections/parallel-programming.html#further-reading",
    "title": "4  Pleasingly Parallel Programming",
    "section": "4.14 Further Reading",
    "text": "4.14 Further Reading\nRyan Abernathey & Joe Hamman. 2020. Closed Platforms vs. Open Architectures for Cloud-Native Earth System Analytics. Medium.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pleasingly Parallel Programming</span>"
    ]
  },
  {
    "objectID": "sections/data-structures-netcdf.html",
    "href": "sections/data-structures-netcdf.html",
    "title": "5  Data Structures and Formats for Large Data",
    "section": "",
    "text": "5.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures and Formats for Large Data</span>"
    ]
  },
  {
    "objectID": "sections/data-structures-netcdf.html#learning-objectives",
    "href": "sections/data-structures-netcdf.html#learning-objectives",
    "title": "5  Data Structures and Formats for Large Data",
    "section": "",
    "text": "Learn about the NetCDF data format:\n\nCharacteristics: self-describing, scalable, portable, appendable, shareable, and archivable\nUnderstand the NetCDF data model: what are dimensions, variables, and attributes\nAdvantages and differences between NetCDF and tabular data formats\n\nLearn how to use the xarray Python package to work with NetCDF files:\n\nDescribe the core xarray data structures, the xarray.DataArray and the xarray.Dataset, and their components, including data variables, dimensions, coordinates, and attributes\nCreate xarray.DataArrays and xarra.DataSets out of raw numpy arrays and save them as netCDF files\nLoad xarray.DataSets from netCDF files and understand the attributes view\nPerform basic indexing, processing, and reduction of xarray.DataArrays\nConvert pandas.DataFrames into xarray.DataSets",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures and Formats for Large Data</span>"
    ]
  },
  {
    "objectID": "sections/data-structures-netcdf.html#introduction",
    "href": "sections/data-structures-netcdf.html#introduction",
    "title": "5  Data Structures and Formats for Large Data",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nEfficient and reproducible data analysis begins with choosing a proper format to store our data, particularly when working with large, complex, multi-dimensional datasets. Consider, for example, the following Earth System Data Cube from Mahecha et al. 2020, which measures nine environmental variables at high resolution across space and time. We can consider this dataset large (high-resolution means we have a big file), complex (multiple variables), and multi-dimensional (each variable is measured along three dimensions: latitude, longitude, and time). Additionally, necessary metadata must accompany the dataset to make it functional, such as units of measurement for variables, information about the authors, and processing software used.\n\n\n\nMahecha et al. 2020 . Visualization of the implemented Earth system data cube. The figure shows from the top left to bottom right the variables sensible heat (H), latent heat (LE), gross primary production (GPP), surface moisture (SM), land surface temperature (LST), air temperature (Tair), cloudiness (C), precipitation (P), and water vapour (V). The resolution in space is 0.25° and 8 d in time, and we are inspecting the time from May 2008 to May 2010; the spatial range is from 15° S to 60° N, and 10° E to 65° W.\n\n\nKeeping complex datasets in a format that facilitates access, processing, sharing, and archiving can be at least as important as how we parallelize the code we use to analyze them. In practice, it is common to convert our data from less efficient formats into more efficient ones before we parallelize any processing. In this lesson, we will\n\nfamiliarize ourselves with the NetCDF data format, which enables us to store large, complex, multi-dimensional data efficiently, and\nlearn to use the xarray Python package to read, process, and create NetCDF files.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures and Formats for Large Data</span>"
    ]
  },
  {
    "objectID": "sections/data-structures-netcdf.html#netcdf-data-format",
    "href": "sections/data-structures-netcdf.html#netcdf-data-format",
    "title": "5  Data Structures and Formats for Large Data",
    "section": "5.3 NetCDF Data Format",
    "text": "5.3 NetCDF Data Format\nNetCDF (Network Common Data Form) is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. NetCDF was initially developed at the Unidata Program Center, is supported on almost all platforms, and parsers exist for most scientific programming languages.\nThe NetCDF documentation outlines that this data format is desgined to be:\n\n\nSelf-describing: Information describing the data contents of the file is embedded within the data file itself. This means that there is a header describing the layout of the rest of the file and arbitrary file metadata.\nScalable: Small subsets of large datasets may be accessed efficiently through netCDF interfaces, even from remote servers.\nPortable: A NetCDF file is machine-independent i.e. it can be accessed by computers with different ways of storing integers, characters, and floating-point numbers.\nAppendable: Data may be appended to a properly structured NetCDF file without copying the dataset or redefining its structure.\nSharable: One writer and multiple readers may simultaneously access the same NetCDF file.\nArchivable: Access to all earlier forms of NetCDF data will be supported by current and future versions of the software.\n\n\n\n5.3.1 Data Model\nThe NetCDF data model is the way that NetCDF organizes data. This lesson will follow the Classic NetCDF Data Model, which is at the core of all netCDF files. \nThe model consists of three key components: variables, dimensions, and attributes.\n\nVariables are N-dimensional arrays of data. We can think of these as varying/measured/dependent quantities.\nDimensions describe the axes of the data arrays. A dimension has a name and a length. We can think of these as the constant/fixed/independent quantities at which we measure the variables.\nAttributes are small notes or supplementary metadata to annotate a variable or the file as a whole.\n\n\n\n\nClassic NetCDF Data Model (NetCDF documentation)\n\n\n\n\n5.3.2 Metadata Standards\nThe most commonly used metadata standard for geospatial data is the Climate and Forecast metadata standard, also called the CF conventions.\n\nThe CF conventions are specifically designed to promote the processing and sharing of files created with the NetCDF API. Principles of CF include self-describing data (no external tables needed for understanding), metadata equally readable by humans and software, minimum redundancy, and maximum simplicity. (CF conventions FAQ)\n\nThe CF conventions provide a unique standardized name and precise description of over 1,000 physical variables. To maximize the reusability of our data, it is best to include a variable’s standardized name as an attribute called standard_name. Variables should also include a units attribute. This attribute should be a string that can be recognized by UNIDATA’s UDUNITS package. In these links you can find:\n\na table with all of the CF convention’s standard names, and\na list of the units found in the UDUNITS database maintained by the North Carolina Institute for Climate Studies.\n\n\n\n5.3.3 Exercise\nLet’s do a short practice now that we have reviewed the classic NetCDF model and know a bit about metadata best practices.\n\nPart 1\n\nImagine the following scenario: we have a network of 25 weather stations. They are located in a square grid: starting at 30°0′N 60°0′E, there is a station every 10° North and every 10° East. Each station measures the air temperature at a set time for three days, starting on September 1st, 2022. On the first day, all stations record a temperature of 0°C. On the second day, all temperatures are 1°C, and on the third day, all temperatures are 2°C. What are the variables, dimensions and attributes for this data?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nVariables: There is a single variable being measured: temperature. The variable values can be represented as a 5x5x3 array, with constant values for each day.\nDimensions: This dataset has three dimensions: time, latitude, and longitude. Time indicates when the measurement happened, we can encode it as the dates 2022-09-01, 2022-09-02, and 2022-09-03. The pairs of latitude and longitude values indicate the positions of the weather stations. Latitude has values 30, 40, 50, 60, and 70, measured in degrees North. Longitude has values 60, 70, 80, 90, and 100, measured in degrees East.\n\nAttributes: Let’s divide these into attributes for the variable, the dimensions, and the whole dataset:\n\nVariable attributes:\n\nTemperature attributes:\n\nstandard_name: air_temperature\nunits: degree_C\n\n\nDimension attributes:\n\nTime attributes:\n\ndescription: date of measurement\n\nLatitude attributes:\n\nstandard_name: grid_latitude\nunits: degrees_N\n\nLongitude attributes:\n\nstandard_name: grid_longitude\nunits: degree_E\n\n\nDataset attributes:\n\ntitle: Temperature Measurements at Weather Stations\nsummary: an example of NetCDF data format\n\n\n\n\n\n\nPart 2\n\nNow imagine we calculate the average temperature over time at each weather station, and we wish to incorporate this data into the same dataset. How will adding the average temperature data change the dataset’s variables, attributes, and dimensions?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nVariables: Now we are measuring two variables: temperature and average temperature. The temperature data stays the same. We can represent the average temperature as a single 5x5 array with value 1 at each cell.\nDimensions: This dataset still has three dimensions: time, latitude, and longitude. The temperature variable uses all three dimensions, and the average temperature variable only uses two (latitude and longitude). This is ok! The dataset’s dimensions are the union of the dimensions of all the variables in the dataset. Variables in the same dataset may have all, some, or no dimensions in common.\n\nAttributes: To begin with, we need to keep all the previous attributes. Notice that the dataset’s title is general enough that we don’t need to update it. The only update we need to do is add the attributes for our new average temperature variable:\n\nAverage temperature attributes:\n\nstandard_name: average_air_temperature\ndescription: average temperature over three days\n\n\n\n\n\nOur next step is to see how we can translate all this information into something we can store and handle on our computers.\n\n\n\n\n\n\nGoing further with zarr\n\n\n\n\n\nZarr is a file storage format for chunked, compressed N-dimensional arrays based on an open source specification. It is very similar to NetCDF. The key is in the “chunked.” Zarr stores chunks of data in separate files, so that parallel operations (like reading and writing data) can be done much, much faster than working with NetCDF. This format is gaining lots of popularity for working with big data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures and Formats for Large Data</span>"
    ]
  },
  {
    "objectID": "sections/data-structures-netcdf.html#xarray",
    "href": "sections/data-structures-netcdf.html#xarray",
    "title": "5  Data Structures and Formats for Large Data",
    "section": "5.4 xarray",
    "text": "5.4 xarray\nxarray is an open source project and Python package that augments NumPy arrays by adding labeled dimensions, coordinates and attributes. xarray is based on the netCDF data model, making it the appropriate tool to open, process, and create datasets in netCDF format.\n\n\n\n\nxarray’s development portal\n\n\n\n5.4.1 xarray.DataArray\nThe xarray.DataArray is the primary data structure of the xarray package. It is an n-dimensional array with labeled dimensions. We can think of it as representing a single variable in the NetCDF data format: it holds the variable’s values, dimensions, and attributes.\nApart from variables, dimensions, and attributes, xarray introduces one more piece of information to keep track of a dataset’s content: in xarray each dimension has at least one set of coordinates. A dimension’s coordinates indicate the dimension’s values. We can think of the coordinate’s values as the tick labels along a dimension. For example, in our previous exercise about temperature measured in weather stations, latitude is a dimension, and the latitude’s coordinates are 30, 40, 50, 60, and 70 because those are the latitude values at which we are collecting temperature data. In that same exercise, time is a dimension, and its coordinates are 2022-09-1, 2022-09-02, and 2022-09-03.\nHere you can read more about the xarray terminology.\n\n5.4.1.1 Create an xarray.DataArray\nLet’s suppose we want to make an xarray.DataArray that includes the information from our previous exercise about measuring temperature across three days. First, we import all the necessary libraries.\n\nimport os              \nimport urllib \nimport pandas as pd\nimport numpy as np\n\nimport xarray as xr   # This is the package we'll explore\n\nVariable Values\nThe underlying data in the xarray.DataArray is a numpy.ndarray that holds the variable values. So we can start by making a numpy.ndarray with our mock temperature data:\n\n# values of a single variable at each point of the coords \ntemp_data = np.array([np.zeros((5,5)), \n                      np.ones((5,5)), \n                      np.ones((5,5))*2]).astype(int)\ntemp_data\n\narray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])\n\n\nWe could think this is “all” we need to represent our data. But if we stopped at this point, we would need to\n\nremember that the numbers in this array represent the temperature in degrees Celsius (doesn’t seem too bad),\nremember that the first dimension of the array represents time, the second latitude and the third longitude (maybe ok), and\nkeep track of the range of values that time, latitude, and longitude take (not so good).\n\nKeeping track of all this information separately could quickly get messy and could make it challenging to share our data and analyses with others. This is what the netCDF data model and xarray aim to simplify. We can get data and its descriptors together in an xarray.DataArray by adding the dimensions over which the variable is being measured and including attributes that appropriately describe dimensions and variables.\nDimensions and Coordinates\nTo specify the dimensions of our upcoming xarray.DataArray, we must examine how we’ve constructed the numpy.ndarray holding the temperature data. The diagram below shows how the dimensions of temp_data are ordered: the first dimension is time, the second is latitude, and the third is longitude.\n\nRemember that indexing in 2-dimensional numpy.ndarrays starts at the top-left corner of the array, and it is done by rows first and columns second (like matrices). This is why latitude is the second dimension and longitude the third. From the diagram, we can also see that the coordinates (values of each dimension) are as follow:\n\ndate coordinates are 2022-09-01, 2022-09-02, 2022-09-03\nlatitude coordinates are 70, 60, 50, 40, 30 (notice decreasing order)\nlongitude coordinates are 60, 70, 80, 90, 100 (notice increasing order)\n\nWe add the dimensions as a tuple of strings and coordinates as a dictionary:\n\n# names of the dimensions in the required order\ndims = ('time', 'lat', 'lon')\n\n# create coordinates to use for indexing along each dimension \ncoords = {'time' : pd.date_range(\"2022-09-01\", \"2022-09-03\"),\n          'lat' : np.arange(70, 20, -10),\n          'lon' : np.arange(60, 110, 10)}  \n\nAttributes\nNext, we add the attributes (metadata) for our temperature data as a dictionary:\n\n# attributes (metadata) of the data array \nattrs = { 'title' : 'temperature across weather stations',\n          'standard_name' : 'air_temperature',\n          'units' : 'degree_c'}\n\nPutting It All Together\nFinally, we put all these pieces together (data, dimensions, coordinates, and attributes) to create an xarray.DataArray:\n\n# initialize xarray.DataArray\ntemp = xr.DataArray(data = temp_data, \n                    dims = dims,\n                    coords = coords,\n                    attrs = attrs)\ntemp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (time: 3, lat: 5, lon: 5)&gt; Size: 600B\narray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])\nCoordinates:\n  * time     (time) datetime64[ns] 24B 2022-09-01 2022-09-02 2022-09-03\n  * lat      (lat) int64 40B 70 60 50 40 30\n  * lon      (lon) int64 40B 60 70 80 90 100\nAttributes:\n    title:          temperature across weather stations\n    standard_name:  air_temperature\n    units:          degree_cxarray.DataArraytime: 3lat: 5lon: 50 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2array([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Coordinates: (3)time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03array(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')lat(lat)int6470 60 50 40 30array([70, 60, 50, 40, 30])lon(lon)int6460 70 80 90 100array([ 60,  70,  80,  90, 100])Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2022-09-01', '2022-09-02', '2022-09-03'], dtype='datetime64[ns]', name='time', freq='D'))latPandasIndexPandasIndex(Index([70, 60, 50, 40, 30], dtype='int64', name='lat'))lonPandasIndexPandasIndex(Index([60, 70, 80, 90, 100], dtype='int64', name='lon'))Attributes: (3)title :temperature across weather stationsstandard_name :air_temperatureunits :degree_c\n\n\nWe can also update the variable’s attributes after creating the object. Notice that each of the coordinates is also an xarray.DataArray, so we can add attributes to them.\n\n# update attributes\ntemp.attrs['description'] = 'simple example of an xarray.DataArray'\n\n# add attributes to coordinates \ntemp.time.attrs = {'description':'date of measurement'}\n\ntemp.lat.attrs['standard_name']= 'grid_latitude'\ntemp.lat.attrs['units'] = 'degree_N'\n\ntemp.lon.attrs['standard_name']= 'grid_longitude'\ntemp.lon.attrs['units'] = 'degree_E'\ntemp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (time: 3, lat: 5, lon: 5)&gt; Size: 600B\narray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])\nCoordinates:\n  * time     (time) datetime64[ns] 24B 2022-09-01 2022-09-02 2022-09-03\n  * lat      (lat) int64 40B 70 60 50 40 30\n  * lon      (lon) int64 40B 60 70 80 90 100\nAttributes:\n    title:          temperature across weather stations\n    standard_name:  air_temperature\n    units:          degree_c\n    description:    simple example of an xarray.DataArrayxarray.DataArraytime: 3lat: 5lon: 50 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2array([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Coordinates: (3)time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03description :date of measurementarray(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')lat(lat)int6470 60 50 40 30standard_name :grid_latitudeunits :degree_Narray([70, 60, 50, 40, 30])lon(lon)int6460 70 80 90 100standard_name :grid_longitudeunits :degree_Earray([ 60,  70,  80,  90, 100])Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2022-09-01', '2022-09-02', '2022-09-03'], dtype='datetime64[ns]', name='time', freq='D'))latPandasIndexPandasIndex(Index([70, 60, 50, 40, 30], dtype='int64', name='lat'))lonPandasIndexPandasIndex(Index([60, 70, 80, 90, 100], dtype='int64', name='lon'))Attributes: (4)title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArray\n\n\nAt this point, since we have a single variable, the dataset attributes and the variable attributes are the same.\n\n\n5.4.1.2 Indexing\nAn xarray.DataArray allows both positional indexing (like numpy) and label-based indexing (like pandas). Positional indexing is the most basic, and it’s done using Python’s [] syntax, as in array[i,j] with i and j both integers. Label-based indexing takes advantage of dimensions in the array having names and coordinate values that we can use to access data instead of remembering the positional order of each dimension.\nAs an example, suppose we want to know what was the temperature recorded by the weather station located at 40°0′N 80°0′E on September 1st, 2022. By recalling all the information about how the array is setup with respect to the dimensions and coordinates, we can access this data positionally:\n\ntemp[0,1,2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray ()&gt; Size: 8B\narray(0)\nCoordinates:\n    time     datetime64[ns] 8B 2022-09-01\n    lat      int64 8B 60\n    lon      int64 8B 80\nAttributes:\n    title:          temperature across weather stations\n    standard_name:  air_temperature\n    units:          degree_c\n    description:    simple example of an xarray.DataArrayxarray.DataArray0array(0)Coordinates: (3)time()datetime64[ns]2022-09-01description :date of measurementarray('2022-09-01T00:00:00.000000000', dtype='datetime64[ns]')lat()int6460standard_name :grid_latitudeunits :degree_Narray(60)lon()int6480standard_name :grid_longitudeunits :degree_Earray(80)Indexes: (0)Attributes: (4)title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArray\n\n\nOr, we can use the dimensions names and their coordinates to access the same value:\n\ntemp.sel(time='2022-09-01', lat=40, lon=80)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray ()&gt; Size: 8B\narray(0)\nCoordinates:\n    time     datetime64[ns] 8B 2022-09-01\n    lat      int64 8B 40\n    lon      int64 8B 80\nAttributes:\n    title:          temperature across weather stations\n    standard_name:  air_temperature\n    units:          degree_c\n    description:    simple example of an xarray.DataArrayxarray.DataArray0array(0)Coordinates: (3)time()datetime64[ns]2022-09-01description :date of measurementarray('2022-09-01T00:00:00.000000000', dtype='datetime64[ns]')lat()int6440standard_name :grid_latitudeunits :degree_Narray(40)lon()int6480standard_name :grid_longitudeunits :degree_Earray(80)Indexes: (0)Attributes: (4)title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArray\n\n\nNotice that the result of this indexing is a 1x1 xarray.DataArray. This is because operations on an xarray.DataArray (resp. xarray.DataSet) always return another xarray.DataArray (resp. xarray.DataSet). In particular, operations returning scalar values will also produce xarray objects, so we need to cast them as numbers manually. See xarray.DataArray.item.\nMore about xarray indexing.\n\n\n5.4.1.3 Reduction\nxarray has implemented several methods to reduce an xarray.DataArray along any number of dimensions. One of the advantages of xarray.DataArray is that, if we choose to, it can carry over attributes when doing calculations. For example, we can calculate the average temperature at each weather station over time and obtain a new xarray.DataArray.\n\navg_temp = temp.mean(dim = 'time') \n# to keep attributes add keep_attrs = True\n\navg_temp.attrs = {'title':'average temperature over three days'}\navg_temp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (lat: 5, lon: 5)&gt; Size: 200B\narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])\nCoordinates:\n  * lat      (lat) int64 40B 70 60 50 40 30\n  * lon      (lon) int64 40B 60 70 80 90 100\nAttributes:\n    title:    average temperature over three daysxarray.DataArraylat: 5lon: 51.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0array([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])Coordinates: (2)lat(lat)int6470 60 50 40 30standard_name :grid_latitudeunits :degree_Narray([70, 60, 50, 40, 30])lon(lon)int6460 70 80 90 100standard_name :grid_longitudeunits :degree_Earray([ 60,  70,  80,  90, 100])Indexes: (2)latPandasIndexPandasIndex(Index([70, 60, 50, 40, 30], dtype='int64', name='lat'))lonPandasIndexPandasIndex(Index([60, 70, 80, 90, 100], dtype='int64', name='lon'))Attributes: (1)title :average temperature over three days\n\n\nMore about xarray computations.\n\n\n\n5.4.2 xarray.DataSet\nAn xarray.DataSet resembles an in-memory representation of a NetCDF file and consists of multiple variables (each being an xarray.DataArray), with dimensions, coordinates, and attributes, forming a self-describing dataset. Attributes can be specific to each variable, each dimension, or they can describe the whole dataset. The variables in an xarray.DataSet can have the same dimensions, share some dimensions, or have no dimensions in common. Let’s see an example of this.\n\n5.4.2.1 Create an xarray.DataSet\nFollowing our previous example, we can create an xarray.DataSet by combining the temperature data with the average temperature data. We also add some attributes that now describe the whole dataset, not only each variable.\n\n# make dictionaries with variables and attributes\ndata_vars = {'avg_temp': avg_temp,\n            'temp': temp}\n\nattrs = {'title':'temperature data at weather stations: daily and and average',\n        'description':'simple example of an xarray.Dataset'}\n\n# create xarray.Dataset\ntemp_dataset = xr.Dataset( data_vars = data_vars,\n                        attrs = attrs)\n\nTake some time to click through the data viewer and read through the variables and metadata in the dataset. Notice the following:\n\ntemp_dataset is a dataset with three dimensions (time, latitude, and longitude),\ntemp is a variable that uses all three dimensions in the dataset, and\naveg_temp is a variable that only uses two dimensions (latitude and longitude).\n\n\ntemp_dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 904B\nDimensions:   (lat: 5, lon: 5, time: 3)\nCoordinates:\n  * lat       (lat) int64 40B 70 60 50 40 30\n  * lon       (lon) int64 40B 60 70 80 90 100\n  * time      (time) datetime64[ns] 24B 2022-09-01 2022-09-02 2022-09-03\nData variables:\n    avg_temp  (lat, lon) float64 200B 1.0 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0\n    temp      (time, lat, lon) int64 600B 0 0 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2\nAttributes:\n    title:        temperature data at weather stations: daily and and average\n    description:  simple example of an xarray.Datasetxarray.DatasetDimensions:lat: 5lon: 5time: 3Coordinates: (3)lat(lat)int6470 60 50 40 30standard_name :grid_latitudeunits :degree_Narray([70, 60, 50, 40, 30])lon(lon)int6460 70 80 90 100standard_name :grid_longitudeunits :degree_Earray([ 60,  70,  80,  90, 100])time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03description :date of measurementarray(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (2)avg_temp(lat, lon)float641.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0title :average temperature over three daysarray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])temp(time, lat, lon)int640 0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2 2title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArrayarray([[[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n\n       [[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]],\n\n       [[2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2]]])Indexes: (3)latPandasIndexPandasIndex(Index([70, 60, 50, 40, 30], dtype='int64', name='lat'))lonPandasIndexPandasIndex(Index([60, 70, 80, 90, 100], dtype='int64', name='lon'))timePandasIndexPandasIndex(DatetimeIndex(['2022-09-01', '2022-09-02', '2022-09-03'], dtype='datetime64[ns]', name='time', freq='D'))Attributes: (2)title :temperature data at weather stations: daily and and averagedescription :simple example of an xarray.Dataset\n\n\n\n\n5.4.2.2 Save and Reopen\nFinally, we want to save our dataset as a NetCDF file. To do this, specify the file path and use the .nc extension for the file name. Then save the dataset using the to_netcdf method with your file path. Opening NetCDF is similarly straightforward using xarray.open_dataset().\n\n# specify file path: don't forget the .nc extension!\nfp = os.path.join(os.getcwd(),'temp_dataset.nc') \n# save file\ntemp_dataset.to_netcdf(fp)\n\n# open to check:\ncheck = xr.open_dataset(fp)\ncheck\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 904B\nDimensions:   (lat: 5, lon: 5, time: 3)\nCoordinates:\n  * lat       (lat) int64 40B 70 60 50 40 30\n  * lon       (lon) int64 40B 60 70 80 90 100\n  * time      (time) datetime64[ns] 24B 2022-09-01 2022-09-02 2022-09-03\nData variables:\n    avg_temp  (lat, lon) float64 200B ...\n    temp      (time, lat, lon) int64 600B ...\nAttributes:\n    title:        temperature data at weather stations: daily and and average\n    description:  simple example of an xarray.Datasetxarray.DatasetDimensions:lat: 5lon: 5time: 3Coordinates: (3)lat(lat)int6470 60 50 40 30standard_name :grid_latitudeunits :degree_Narray([70, 60, 50, 40, 30])lon(lon)int6460 70 80 90 100standard_name :grid_longitudeunits :degree_Earray([ 60,  70,  80,  90, 100])time(time)datetime64[ns]2022-09-01 2022-09-02 2022-09-03description :date of measurementarray(['2022-09-01T00:00:00.000000000', '2022-09-02T00:00:00.000000000',\n       '2022-09-03T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (2)avg_temp(lat, lon)float64...title :average temperature over three days[25 values with dtype=float64]temp(time, lat, lon)int64...title :temperature across weather stationsstandard_name :air_temperatureunits :degree_cdescription :simple example of an xarray.DataArray[75 values with dtype=int64]Indexes: (3)latPandasIndexPandasIndex(Index([70, 60, 50, 40, 30], dtype='int64', name='lat'))lonPandasIndexPandasIndex(Index([60, 70, 80, 90, 100], dtype='int64', name='lon'))timePandasIndexPandasIndex(DatetimeIndex(['2022-09-01', '2022-09-02', '2022-09-03'], dtype='datetime64[ns]', name='time', freq=None))Attributes: (2)title :temperature data at weather stations: daily and and averagedescription :simple example of an xarray.Dataset\n\n\n\n\n\n5.4.3 Exercise\nFor this exercise, we will use a dataset including time series of annual Arctic freshwater fluxes and storage terms. The data was produced for the publication Jahn and Laiho, 2020 about changes in the Arctic freshwater budget and is archived at the Arctic Data Center doi:10.18739/A2280504J\n\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A792bfc37-416e-409e-80b1-fdef8ab60033'\n\nmsg = urllib.request.urlretrieve(url, \"FW_data_CESM_LW_2006_2100.nc\")\n\n\nfp = os.path.join(os.getcwd(),'FW_data_CESM_LW_2006_2100.nc')\nfw_data = xr.open_dataset(fp)\nfw_data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 135kB\nDimensions:                          (time: 95, member: 11)\nCoordinates:\n  * time                             (time) float64 760B 2.006e+03 ... 2.1e+03\n  * member                           (member) float64 88B 1.0 2.0 ... 10.0 11.0\nData variables: (12/16)\n    FW_flux_Fram_annual_net          (time, member) float64 8kB ...\n    FW_flux_Barrow_annual_net        (time, member) float64 8kB ...\n    FW_flux_Nares_annual_net         (time, member) float64 8kB ...\n    FW_flux_Davis_annual_net         (time, member) float64 8kB ...\n    FW_flux_BSO_annual_net           (time, member) float64 8kB ...\n    FW_flux_Bering_annual_net        (time, member) float64 8kB ...\n    ...                               ...\n    Solid_FW_flux_BSO_annual_net     (time, member) float64 8kB ...\n    Solid_FW_flux_Bering_annual_net  (time, member) float64 8kB ...\n    runoff_annual                    (time, member) float64 8kB ...\n    netPrec_annual                   (time, member) float64 8kB ...\n    Liquid_FW_storage_Arctic_annual  (time, member) float64 8kB ...\n    Solid_FW_storage_Arctic_annual   (time, member) float64 8kB ...\nAttributes:\n    creation_date:   02-Jun-2020 15:38:31\n    author:          Alexandra Jahn, CU Boulder, alexandra.jahn@colorado.edu\n    title:           Annual timeseries of freshwater data from the CESM Low W...\n    description:     Annual mean Freshwater (FW) fluxes and storage relative ...\n    data_structure:  The data structure is |Ensemble member | Time (in years)...xarray.DatasetDimensions:time: 95member: 11Coordinates: (2)time(time)float642.006e+03 2.007e+03 ... 2.1e+03long_name :time in years, 1920-2100array([2006., 2007., 2008., 2009., 2010., 2011., 2012., 2013., 2014., 2015.,\n       2016., 2017., 2018., 2019., 2020., 2021., 2022., 2023., 2024., 2025.,\n       2026., 2027., 2028., 2029., 2030., 2031., 2032., 2033., 2034., 2035.,\n       2036., 2037., 2038., 2039., 2040., 2041., 2042., 2043., 2044., 2045.,\n       2046., 2047., 2048., 2049., 2050., 2051., 2052., 2053., 2054., 2055.,\n       2056., 2057., 2058., 2059., 2060., 2061., 2062., 2063., 2064., 2065.,\n       2066., 2067., 2068., 2069., 2070., 2071., 2072., 2073., 2074., 2075.,\n       2076., 2077., 2078., 2079., 2080., 2081., 2082., 2083., 2084., 2085.,\n       2086., 2087., 2088., 2089., 2090., 2091., 2092., 2093., 2094., 2095.,\n       2096., 2097., 2098., 2099., 2100.])member(member)float641.0 2.0 3.0 4.0 ... 9.0 10.0 11.0long_name :Ensemble member, 1-11array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])Data variables: (16)FW_flux_Fram_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Fram Strait, relative to 34.8[1045 values with dtype=float64]FW_flux_Barrow_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Barrow Strait, relative to 34.8[1045 values with dtype=float64]FW_flux_Nares_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Nares Strait, relative to 34.8[1045 values with dtype=float64]FW_flux_Davis_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Davis Strait, relative to 34.8[1045 values with dtype=float64]FW_flux_BSO_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through the Barents Sea Opening, relative to 34.8[1045 values with dtype=float64]FW_flux_Bering_annual_net(time, member)float64...units :km3/yrlong_name :Net liquid FW flux through Bering Strait, relative to 34.8[1045 values with dtype=float64]Solid_FW_flux_Fram_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Fram Strait, relative to 34.8[1045 values with dtype=float64]Solid_FW_flux_Barrow_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Barrow Strait, relative to 34.8[1045 values with dtype=float64]Solid_FW_flux_Nares_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Nares Strait, relative to 34.8[1045 values with dtype=float64]Solid_FW_flux_Davis_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Davis Strait, relative to 34.8[1045 values with dtype=float64]Solid_FW_flux_BSO_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through the Barents Sea Opening, relative to 34.8[1045 values with dtype=float64]Solid_FW_flux_Bering_annual_net(time, member)float64...units :km3/yrlong_name :Net Solid (ice+snow) FW flux through Bering Strait, relative to 34.8[1045 values with dtype=float64]runoff_annual(time, member)float64...units :km3/yrlong_name :FW flux from river runoff into the Arctic Ocean domain, relative to 34.8[1045 values with dtype=float64]netPrec_annual(time, member)float64...units :km3/yrlong_name :Net FW flux from precipitation minus evaporation over the Arctic Ocean domain, relative to 34.8[1045 values with dtype=float64]Liquid_FW_storage_Arctic_annual(time, member)float64...units :km3long_name :FW storage in the Arctic Ocean domain, , relative to 34.8. Ignoring any negative FW (water above reference salinity)[1045 values with dtype=float64]Solid_FW_storage_Arctic_annual(time, member)float64...units :km3long_name :FW storage in sea ice and snow in the Arctic Ocean domain, relative to 34.8[1045 values with dtype=float64]Indexes: (2)timePandasIndexPandasIndex(Index([2006.0, 2007.0, 2008.0, 2009.0, 2010.0, 2011.0, 2012.0, 2013.0, 2014.0,\n       2015.0, 2016.0, 2017.0, 2018.0, 2019.0, 2020.0, 2021.0, 2022.0, 2023.0,\n       2024.0, 2025.0, 2026.0, 2027.0, 2028.0, 2029.0, 2030.0, 2031.0, 2032.0,\n       2033.0, 2034.0, 2035.0, 2036.0, 2037.0, 2038.0, 2039.0, 2040.0, 2041.0,\n       2042.0, 2043.0, 2044.0, 2045.0, 2046.0, 2047.0, 2048.0, 2049.0, 2050.0,\n       2051.0, 2052.0, 2053.0, 2054.0, 2055.0, 2056.0, 2057.0, 2058.0, 2059.0,\n       2060.0, 2061.0, 2062.0, 2063.0, 2064.0, 2065.0, 2066.0, 2067.0, 2068.0,\n       2069.0, 2070.0, 2071.0, 2072.0, 2073.0, 2074.0, 2075.0, 2076.0, 2077.0,\n       2078.0, 2079.0, 2080.0, 2081.0, 2082.0, 2083.0, 2084.0, 2085.0, 2086.0,\n       2087.0, 2088.0, 2089.0, 2090.0, 2091.0, 2092.0, 2093.0, 2094.0, 2095.0,\n       2096.0, 2097.0, 2098.0, 2099.0, 2100.0],\n      dtype='float64', name='time'))memberPandasIndexPandasIndex(Index([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0], dtype='float64', name='member'))Attributes: (5)creation_date :02-Jun-2020 15:38:31author :Alexandra Jahn, CU Boulder, alexandra.jahn@colorado.edutitle :Annual timeseries of freshwater data from the CESM Low Warming Ensembledescription :Annual mean Freshwater (FW) fluxes and storage relative to 34.8 shown in Jahn and Laiho, GRL, 2020, calculated from the 11-member Community Earth System Model (CESM) Low Warming Ensemble output (Sanderson et al., 2017, Earth Syst. Dynam., 8, 827-847. doi: 10.5194/esd-8-827-2017). These 11 ensemble members were branched from the first 11 ensemble members of the CESM Large Ensemble (companion data file) at the end of 2005. Convention for the fluxes is that positive fluxes signify a source of FW to the Arctic and negative fluxes are a sink/export of FW for the Arctic. FW fluxes are the net fluxes through a strait over the full ocean depth, adding up any positive and negative fluxes. Liquid FW storage is calculated over the full depth of the ocean but ignoring any negative FW (resulting from salinties over 34.8). Solid FW storage includes FW stored in sea ice and FW stored in snow on sea ice. Surface fluxes and FW storage is calculated over the Arctic domain bounded by Barrow Strait, Nares Strait, Bering Strait, Fram Strait, and the Barents Sea Opeing (BSO). Davis Strait fluxes are included for reference only and are outside of the Arctic domain. A map showing the domain and the location of the straits can be found in Jahn and Laiho, GRL, 2020.data_structure :The data structure is |Ensemble member | Time (in years)|. All data are annual means. The data covers 2006-2100. There are 11 ensemble members.\n\n\n\nHow many dimensions does the runoff_annual variable have? What are the coordinates for the second dimension of this variable?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe can see in the object viewer that the runoff_annual variable has two dimensions: time and member, in that order. We can also access the dimensions by calling:\n\nfw_data.runoff_annual.dims\n\nThe second dimensions is member. Near the top of the object viewer, under coordinates, we can see that that member’s coordinates is an array from 1 to 11. We can directly see this array by calling:\n\nfw_data.member\n\n\n\n\n\nSelect the values for the second member of the netPrec_annual variable.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmember2 = fw_data.netPrec_annual.sel(member=2)\n\n\n\n\n\nWhat is the maximum value of the second member of the netPrec_annual variable in the time period 2022 to 2100? Hint.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBased on our previous answer, this maximum is:\n\nx_max = member2.loc[2022:2100].max()\nx_max.item()\n\nNotice we had to use item to transform the array into a number.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures and Formats for Large Data</span>"
    ]
  },
  {
    "objectID": "sections/data-structures-netcdf.html#tabular-data-and-netcdf",
    "href": "sections/data-structures-netcdf.html#tabular-data-and-netcdf",
    "title": "5  Data Structures and Formats for Large Data",
    "section": "5.5 Tabular Data and NetCDF",
    "text": "5.5 Tabular Data and NetCDF\nUndoubtedly, tabular data is one of the most popular data formats. In this last section, we will discuss the relation between tabular data and the NetCDF data format and how to transform a pandas.DataFrame into an xarray.DataSet.\n\n5.5.1 Tabular to NetCDF\nWe assume our starting point is tabular data that meets the criteria for tidy data, which means:\n\nEach column holds a different variable.\nEach row holds a different observation.\n\nTake, for example, this tidy data subset from our exercise about weather stations measuring temperature: \nTo understand how this will transform into NetCDF format, we first need to identify which columns will act as dimensions and which as variables. We can also think of the values of the dimension columns as the coordinates in the xarray.DataSet. The diagram below shows how these columns transform into variables, dimensions, and coordinates.\n\nTabular formats like csv do not offer an intrinsic way to encode attributes for the dimensions or variables, this is why we don’t see any attributes in the resulting NetCDF data. One of the most significant advantages of NetCDF is its self-describing properties.\n\n\n5.5.2 pandas to xarray\nWhat does the previous example look like when working with pandas and xarray?\nLet’s work with a csv file containing the previous temperature measurements. Essentially, we need to read this file as a pandas.DataFrame and then use the pandas.DataFrame.to_xarray() method, taking into account that the dimensions of the resulting xarray.DataSet will be formed using the index column(s) of the pandas.DataFrame. In this case, we know the first three columns will be our dimension columns, so we need to group them as a multindex for the pandas.DataFrame. We can do this by using the index_col argument directly when we read in the csv file.\n\nfp = os.path.join(os.getcwd(),'netcdf_temp_data.csv') \n\n\n# specify columns representing dimensions\ndimension_columns = [0,1,2]\n\n# read file\ntemp = pd.read_csv(fp, index_col=dimension_columns)\ntemp\n\n\n\n\n\n\n\n\n\n\n\ntemperature\n\n\ntime\nlongitude\nlatitude\n\n\n\n\n\n2022-09-01\n30\n60\n0\n\n\n70\n0\n\n\n40\n60\n0\n\n\n70\n0\n\n\n2022-09-02\n30\n60\n1\n\n\n70\n1\n\n\n40\n60\n1\n\n\n70\n1\n\n\n\n\n\n\n\n\nAnd this is our resulting xarray.DataSet:\n\ntemp.to_xarray()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 112B\nDimensions:      (time: 2, longitude: 2, latitude: 2)\nCoordinates:\n  * time         (time) object 16B '2022-09-01' '2022-09-02'\n  * longitude    (longitude) int64 16B 30 40\n  * latitude     (latitude) int64 16B 60 70\nData variables:\n    temperature  (time, longitude, latitude) int64 64B 0 0 0 0 1 1 1 1xarray.DatasetDimensions:time: 2longitude: 2latitude: 2Coordinates: (3)time(time)object'2022-09-01' '2022-09-02'array(['2022-09-01', '2022-09-02'], dtype=object)longitude(longitude)int6430 40array([30, 40])latitude(latitude)int6460 70array([60, 70])Data variables: (1)temperature(time, longitude, latitude)int640 0 0 0 1 1 1 1array([[[0, 0],\n        [0, 0]],\n\n       [[1, 1],\n        [1, 1]]])Indexes: (3)timePandasIndexPandasIndex(Index(['2022-09-01', '2022-09-02'], dtype='object', name='time'))longitudePandasIndexPandasIndex(Index([30, 40], dtype='int64', name='longitude'))latitudePandasIndexPandasIndex(Index([60, 70], dtype='int64', name='latitude'))Attributes: (0)\n\n\nFor further reading and examples about switching between pandas and xarray you can visit the following:\n\nxarray’s Frequently Asked Questions\nxarray’s documentation about working with pandas\npandas.DataFrame.to_xarray documentation",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures and Formats for Large Data</span>"
    ]
  },
  {
    "objectID": "sections/parallel-with-dask.html",
    "href": "sections/parallel-with-dask.html",
    "title": "6  Parallelization with Dask",
    "section": "",
    "text": "6.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parallelization with Dask</span>"
    ]
  },
  {
    "objectID": "sections/parallel-with-dask.html#learning-objectives",
    "href": "sections/parallel-with-dask.html#learning-objectives",
    "title": "6  Parallelization with Dask",
    "section": "",
    "text": "Become familiar with the Dask processing workflow:\n\nWhat are the client, scheduler, workers, and cluster\nUnderstand delayed computations and “lazy” evaluation\nObtain information about computations via the Dask dashboard\n\nLearn to load data and specify partition/chunk sizes of dask.arrays/dask.dataframes \nIntegrate xarray and rioxarray with Dask for geospatial computations\nShare best practices and resources for further reading",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parallelization with Dask</span>"
    ]
  },
  {
    "objectID": "sections/parallel-with-dask.html#introduction",
    "href": "sections/parallel-with-dask.html#introduction",
    "title": "6  Parallelization with Dask",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nDask is a library for parallel computing in Python. It can scale up code to use your personal computer’s full capacity or distribute work in a cloud cluster. By mirroring APIs of other commonly used Python libraries, such as Pandas and NumPy, Dask provides a familiar interface that makes it easier to parallelize your code. In this lesson, we will get acquainted with Dask’s way of distributing and evaluating computations and some of its most commonly used objects.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parallelization with Dask</span>"
    ]
  },
  {
    "objectID": "sections/parallel-with-dask.html#dask-cluster",
    "href": "sections/parallel-with-dask.html#dask-cluster",
    "title": "6  Parallelization with Dask",
    "section": "6.3 Dask Cluster",
    "text": "6.3 Dask Cluster\nWe can deploy a Dask cluster on a single machine or an actual cluster with multiple machines. The cluster has three main components for processing computations in parallel. These are the client, the scheduler and the workers.\n\nWhen we code, we communicate directly with the client, which is responsible for submitting tasks to be executed to the scheduler.\nAfter receiving the tasks from the client, the scheduler determines how tasks will be distributed among the workers and coordinates them to process tasks in parallel.\nFinally, the workers compute tasks and store and return computations results. Workers can be threads, processes, or separate machines in a cluster. Here you can read more about what are threads and processes and some best practices for selecting one or the other.\n\nTo interact with the client and generate tasks that can be processed in parallel we need to use Dask objects to read and process our data. \n\n\n\nM. Schmitt, Understanding Dask Architecture: The Client, Scheduler and Workers",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parallelization with Dask</span>"
    ]
  },
  {
    "objectID": "sections/parallel-with-dask.html#connect-to-an-existing-cluster",
    "href": "sections/parallel-with-dask.html#connect-to-an-existing-cluster",
    "title": "6  Parallelization with Dask",
    "section": "6.4 Connect to an existing cluster",
    "text": "6.4 Connect to an existing cluster\nThe instructor is going to start a local cluster on the server that everyone can connect to. This scenario is a realistic simulation of what it would look like for you to connect to a shared Dask resource. You can create your own local cluster running on your laptop using code at the end of this lesson.\nFirst, the instructor (and only the instructor!) will run:\n\nfrom dask.distributed import LocalCluster\ncluster = LocalCluster(n_workers=70, memory_limit='auto', processes=True)\n\nTo connect to it, we will use the address that the scheduler is listening on. The port is generated randomly, so first the instructor needs to get address for you to use in your code. In a ‘real world’ scenario, this address would be given to you by the administrator of the Dask cluster.\n\ncluster.scheduler_address\n\nNow, you can pass the address to the Client function, which sets up your session as a client of the Dask cluster,\n\nfrom dask.distributed import LocalCluster, Client\n\n# if you are copy pasting this from the book, make sure \n# the address is the same that the instructor gave in the lesson!\naddress = 'tcp://127.0.0.1:40869'\nclient = Client(address)\nclient\n\n\n6.4.1 Dask Dashboard\nWe chose to use the Dask cluster in this lesson instead of the default Dask scheduler to take advantage of the cluster dashboard, which offers live monitoring of the performance and progress of our computations. You can learn more about different Dask clusters here.\nAs seen in the images above, when we set up a cluster we can see the cluster dashboard address by looking at either the client or the cluster. In this example the dashboard address is http://128.111.85.28:8787/status.\nIn order to get access to the dashboard, click the “ports” tab net to terminal. Add the port 8787 so that it is forwarded to your localhost.\nWhen we go that address in a web browser we can see the dashboard’s main page. This page shows diagnostics about:\n\nthe cluster’s and individual worker’s memory usage,\nnumber of tasks being processed by each worker,\nindividual tasks being processed across workers, and\nprogress towards completion of individual tasks.\n\nThere’s much to say about interpreting the Dask dashboard’s diagnostics. We recommend this documentation to understand the basics of the dashboard diagnostics and this video as a deeper dive into the dashboard’s functions.\n\n\n\nA Dask dashboard.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parallelization with Dask</span>"
    ]
  },
  {
    "objectID": "sections/parallel-with-dask.html#dask.dataframes",
    "href": "sections/parallel-with-dask.html#dask.dataframes",
    "title": "6  Parallelization with Dask",
    "section": "6.5 dask.dataframes",
    "text": "6.5 dask.dataframes\nWhen we analyze tabular data, we usually start our analysis by loading it into memory as a Pandas DataFrame. But what if this data does not fit in memory? Or maybe our analyzes crash because we run out of memory. These scenarios are typical entry points into parallel computing. In such cases, Dask’s scalable alternative to a Pandas DataFrame is the dask.dataframe. A dask.dataframe comprises many pd.DataFrames, each containing a subset of rows of the original dataset. We call each of these pandas pieces a partition of the dask.dataframe.\n\n\n\nDask Array design (dask documentation)\n\n\n\n6.5.1 Reading a csv\nTo get familiar with dask.dataframes, we will use tabular data of soil moisture measurements at six forest stands in northeastern Siberia. The data has been collected since 2014 and is archived at the Arctic Data Center (Loranty & Alexander, doi:10.18739/A24B2X59C). Just as we did in the previous lesson, we will download the data using the requests package and the data’s URL obtained from the Arctic Data Center.\n\nimport os              \nimport urllib \n\nimport dask.dataframe as dd\n\n\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A27e4043d-75eb-4c4f-9427-0d442526c154'\n\nmsg = urllib.request.urlretrieve(url, \"dg_soil_moisture.csv\")\n\nIn the Arctic Data Center metadata we can see this file is 115 MB. To import this file as a dask.dataframe with more than one partition, we need to specify the size of each partition with the blocksize parameter. In this example, we will split the data frame into six partitions, meaning a block size of approximately 20 MB.\n\nfp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')\ndf = dd.read_csv(fp, blocksize = '20MB' , encoding='ISO-8859-1')\ndf\n\n\n\n\n\n\n\nEncoding?\n\n\n\n\n\nAbout the encoding parameter: If we try to import the file directly, we will receive an UnicodeDecodeError. We can run the following code to find the file’s encoding and add the appropriate encoding to dask.dataframe.read_csv.\n\nimport chardet\nfp = os.path.join(os.getcwd(),'dg_soil_moisture.csv')\nwith open(fp, 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(100000))\nresult\n\n\n\n\nNotice that we cannot see any values in the data frame. This is because Dask has not really loaded the data. It will wait until we explicitly ask it to print or compute something to do so.\nHowever, we can still do df.head(). It’s not costly for memory to access a few data frame rows.\n\ndf.head(3)\n\n\n\n6.5.2 Lazy Computations\nThe application programming interface (API) of a dask.dataframe is a subset of the pandas.DataFrame API. So if you are familiar with pandas, many of the core pandas.DataFrame methods directly translate to dask.dataframes. For example:\n\naverages = df.groupby('year').mean(numeric_only=True)\naverages\n\nNotice that we cannot see any values in the resulting data frame. A major difference between pandas.DataFrames and dask.dataframes is that dask.dataframes are “lazy”. This means an object will queue transformations and calculations without executing them until we explicitly ask for the result of that chain of computations using the compute method. Once we run compute, the scheduler can allocate memory and workers to execute the computations in parallel. This kind of lazy evaluation (or delayed computation) is how most Dask workloads work. This varies from eager evaluation methods and functions, which start computing results right when they are executed.\nBefore calling compute on an object, open the Dask dashboard to see how the parallel computation is happening.\n\naverages.compute()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parallelization with Dask</span>"
    ]
  },
  {
    "objectID": "sections/parallel-with-dask.html#dask.arrays",
    "href": "sections/parallel-with-dask.html#dask.arrays",
    "title": "6  Parallelization with Dask",
    "section": "6.6 dask.arrays",
    "text": "6.6 dask.arrays\nAnother common object we might want to parallelize is a NumPy array. The equivalent Dask object is the dask.array, which coordinates many NumPy arrays that may live on disk or other machines. Each of these NumPy arrays within the dask.array is called a chunk. Choosing how these chunks are arranged within the dask.array and their size can significantly affect the performance of our code. Here you can find more information about chunks.\n\n\n\nDask Array design (dask documentation)\n\n\nIn this short example we will create a 200x500 dask.array by specifying chunk sizes of 100x100.\n\nimport numpy as np\n\nimport dask.array as da\n\n\ndata = np.arange(100_000).reshape(200, 500)\na = da.from_array(data, chunks=(100, 100))\n\nComputations for dask.arrays also work lazily. We need to call compute to trigger computations and bring the result to memory.\n\na.mean()\n\n\na.mean().compute()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parallelization with Dask</span>"
    ]
  },
  {
    "objectID": "sections/parallel-with-dask.html#dask-and-xarray",
    "href": "sections/parallel-with-dask.html#dask-and-xarray",
    "title": "6  Parallelization with Dask",
    "section": "6.7 Dask and xarray",
    "text": "6.7 Dask and xarray\nIn the future, it might be more common having to read some big array-like dataset (like a high-resolution multiband raster) than creating one from scratch using NumPy. In this case, it can be useful to use the xarray module and its extender rioxarray together with Dask. In the previous lesson, Data Structures and Formats for Large Data, we explore how to use the xarray package to work with labelled arrays. rioxarray extends xarray with the rio accessor, which stands for “raster input and output”.\nIt is simple to wrap Dask around xarray objects. We only need to specify the number of chunks as an argument when we are reading in a dataset (see also [1]).\n\n6.7.1 Open .tif file\nAs an example, let’s do a Normalized Difference Vegetation Index (NDVI) calculation using remote sensing imagery collected by aerial vehicles over northeastern Siberia (Loranty, Forbath, Talucci, Alexander, DeMarco, et al. 2020. doi:10.18739/A2ZC7RV6H.). The NDVI is an index commonly used to check if an area has live green vegetation or not. It can also show the difference between water, plants, bare soil, and human-made structures, among other things.\nThe NDVI is calculated using the near-infrared and red bands of the satellite image. The formula is\n\\[NDVI = \\frac{NIR - Red}{NIR + Red}.\\]\nFirst, we download the data for the near-infrared (NIR) and red bands from the Arctic Data Center:\n\n# download red band\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3Aac25a399-b174-41c1-b6d3-09974b161e5a'\nmsg = urllib.request.urlretrieve(url, \"RU_ANS_TR2_FL005M_red.tif\")\n\n\n# download nir band\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A1762205e-c505-450d-90ed-d4f3e4c302a7'\n\nmsg = urllib.request.urlretrieve(url, \"RU_ANS_TR2_FL005M_nir.tif\")\n\nBecause these are .tif files and have geospatial metadata, we will use rioxarray to read them. You can find more information about rioxarray here.\nTo indicate we will open these .tif files with dask.arrays as the underlying object to the xarray.DataArray (instead of a numpy.array), we need to specify either a shape or the size in bytes for each chunk. Both files are 76 MB, so let’s have chunks of 15 MB to have roughly six chunks.\n\nimport rioxarray as rioxr\n\n\n# read in the file\nfp_red = os.path.join(os.getcwd(),\"RU_ANS_TR2_FL005M_red.tif\")\nred = rioxr.open_rasterio(fp_red, chunks = '15MB')\n\nWe can see a lot of useful information here:\n\nThere are eight chunks in the array. We were aiming for six, but this often happens with how Dask distributes the memory (76MB is not divisible by 6).\nThere is geospatial information (transformation, CRS, resolution) and no-data values.\nThere is an unnecessary dimension: a constant value for the band. So our next step is to squeeze the array to flatten it.\n\n\n# getting rid of unnecessary dimension\nred = red.squeeze()\n\nNext, we read in the NIR band and do the same pre-processing:\n\n# open data\nfp_nir = os.path.join(os.getcwd(),\"RU_ANS_TR2_FL005M_nir.tif\")\nnir = rioxr.open_rasterio(fp_nir, chunks = '15MB')\n\n#squeeze\nnir = nir.squeeze()\n\n\n\n6.7.2 Calculating NDVI\nNow we set up the NDVI calculation. This step is easy because we can handle xarrays and Dask arrays as NumPy arrays for arithmetic operations. Also, both bands have values of type float32, so we won’t have trouble with the division.\n\nndvi = (nir - red) / (nir + red)\n\nWhen we look at the NDVI we can see the result is another dask.array, nothing has been computed yet. Remember, Dask computations are lazy, so we need to call compute() to bring the results to memory.\n\nndvi_values = ndvi.compute()\n\nAnd finally, we can see what these look like. Notice that xarray uses the value of the dimensions as labels along the x and y axes. We use robust=True to ignore the no-data values when plotting.\n\nndvi_values.plot(robust=True)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parallelization with Dask</span>"
    ]
  },
  {
    "objectID": "sections/parallel-with-dask.html#setting-up-your-own-local-cluster",
    "href": "sections/parallel-with-dask.html#setting-up-your-own-local-cluster",
    "title": "6  Parallelization with Dask",
    "section": "6.8 Setting up your own local cluster",
    "text": "6.8 Setting up your own local cluster\n\n6.8.1 Setting up a Local Cluster\nWe can create a local cluster as follows:\n\nfrom dask.distributed import LocalCluster, Client\n\n\ncluster = LocalCluster(n_workers=4, memory_limit=0.1, processes=True)\ncluster\n\n\nAnd then we create a client to connect to our cluster, passing the Client function the cluster object.\n\nclient = Client(cluster)\nclient\n\n\nFrom here, you can continue to run Dask commands as normal.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parallelization with Dask</span>"
    ]
  },
  {
    "objectID": "sections/parallel-with-dask.html#best-practices",
    "href": "sections/parallel-with-dask.html#best-practices",
    "title": "6  Parallelization with Dask",
    "section": "6.9 Best Practices",
    "text": "6.9 Best Practices\nDask is an exciting tool for parallel computing, but it may take a while to understand its nuances to make the most of it. There are many best practices and recommendations. These are some of the basic ones to take into consideration:\n\nFor data that fits into RAM, pandas, and NumPy can often be faster and easier to use than Dask workflows. The simplest solution can often be the best.\nWhile Dask may have similar APIs to pandas and NumPy, there are differences, and not all the methods for the pandas.DataFrames and numpy.arrays translate in the same way (or with the same efficiency) to Dask objects. When in doubt, always read the documentation.\nChoose appropriate chunk and partition sizes and layouts. This is crucial to best use how the scheduler distributes work. You can read here about best practices for chunking.\nAvoid calling compute repeatedly. It is best to group similar computations together and then compute once.\n\nFurther reading:\n\nA friendly article about common dask mistakes\nGeneral Dask best practices\ndask.dataframe best practices\ndask.array best practices",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parallelization with Dask</span>"
    ]
  },
  {
    "objectID": "sections/group-project-1.html",
    "href": "sections/group-project-1.html",
    "title": "7  Group Project: Preprocessing and Rasterizing",
    "section": "",
    "text": "7.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group Project: Preprocessing and Rasterizing</span>"
    ]
  },
  {
    "objectID": "sections/group-project-1.html#learning-objectives",
    "href": "sections/group-project-1.html#learning-objectives",
    "title": "7  Group Project: Preprocessing and Rasterizing",
    "section": "",
    "text": "Get familiarized with the overall group project workflow\nWrite a parsl app that will stage and tile the IWP example data in parallel",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group Project: Preprocessing and Rasterizing</span>"
    ]
  },
  {
    "objectID": "sections/group-project-1.html#introduction",
    "href": "sections/group-project-1.html#introduction",
    "title": "7  Group Project: Preprocessing and Rasterizing",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\nThe Permafrost Discovery Gateway is an online platform for archiving, processing, analysis, and visualization of permafrost big imagery products to enable discovery and knowledge-generation. The PDG utilizes and makes available products derived from high resolution satellite imagery from the Polar Geospatial Center, Planet (3 meter resolution), Sentinel (10 meter resolution), Landsat (30 meter resolution), and MODIS (250 meter resolution). One of these products is a dataset showing Ice Wedge Polygons (IWP) that form in melting permafrost. Explore the Imagery Viewer, where you can visualize different data layers submitted by Arctic researchers.\nIce wedges form as a result of long-term melting and freezing cycles of permafrost. Very cold winters cause the frozen ground to crack, and these cracks fill with melted snow in the spring. The water in the cracks freezes and expands, and this repeats over thousands of years, until the ice wedges are several meters thick. Ice wedges can form very distinctive enclosed geometries that are clearly visible in high-resolution satellite images. Long-term gradual warming of the soil combined with extreme summer warmth causes the ice wedges to melt from the top, and eventually the ground subsides.\n\nIdentifying where these ice wedge features exist is important to understanding how the Arctic landscape is changing. Time series analysis will help researchers, community members, and policy-makers address threats to the biota, architecture, and communities throughout the region. To learn more about how this data was produced, see this publication.\nThe PDG team is using advanced analysis and computational tools to process high-resolution satellite imagery and automatically dectect where ice wedge polygons form. Below is an example of a satellite image (left) and the detected ice wedge polygons in geospatial vector format (right) of that same image.\n\nIn the group project, we are going to use a subset of the high-resolution dataset of these detected ice wedge polygons in order to learn some of the reproducible, scalable techniques that will allow us to process it. Our workflow will start with a set of GeoPackage files that contain the detected ice wedge polygons. These files all have irregular and sometimes overlapping extents due to the variation in satellite coverage, clouds, etc. Our first processing step will take these files and “tile” them into smaller files which have regular extents.\n\nIn step two of the workflow, we will take those regularly tiled GeoPackage files and rasterize them. The files will be regularly gridded, and a summary statistic will be calculated for each grid cell (such as the proportion of pixel area covered by polygons).\n\nIn the final step of the workflow, we will take the raster files and resample them to create a set of raster tiles at different resolutions. This last step is what will enable us to visualize our raster data dynamically, such that we see the lower resolution rasters when very zoomed out (and high resolution data would take too long to load), and higher resolution data when zoomed in and the extent is smaller. Then we convert each of these .tif files into PNG files that we can visualize on a basemap.\n\nToday we will undertake the first two steps of the workflow, staging and tiling the data, and then rasterizing it.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group Project: Preprocessing and Rasterizing</span>"
    ]
  },
  {
    "objectID": "sections/group-project-1.html#staging-and-tiling",
    "href": "sections/group-project-1.html#staging-and-tiling",
    "title": "7  Group Project: Preprocessing and Rasterizing",
    "section": "7.3 Staging and Tiling",
    "text": "7.3 Staging and Tiling\n\nIn your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-07.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills.\nOnly one person in the group should run the parallel code.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group Project: Preprocessing and Rasterizing</span>"
    ]
  },
  {
    "objectID": "sections/group-project-1.html#rasterizing-geopackages",
    "href": "sections/group-project-1.html#rasterizing-geopackages",
    "title": "7  Group Project: Preprocessing and Rasterizing",
    "section": "7.4 Rasterizing GeoPackages",
    "text": "7.4 Rasterizing GeoPackages\nIn the last step, we staged the input files into GeoPackages. Now we will import those .gpkg files and write each as a raster (.tif). The resulting rasters will have 2 bands, one for each statistic we calculate based on the vector geometries in each pixel.\nWe need to create the highest zoom level rasters before we create the lower zoom levels in the next lesson. This brings us one step closer to visualizing the ice wedge polygons on a basemap, with the ability to zoom in and out!\n\n7.4.1 Packages, Libraries, and Modules\n\nos 3.11.2\nparsl 2023.3.20\npdgstaging\n\npackage developed by Permafrost Discovery Gateway software developer and designer Robyn Thiessen-Bock\n\npdgraster\n\npackage developed by Permafrost Discovery Gateway software developer and designer Robyn Thiessen-Bock\n\ngeopandas0.11\nrandom\nmatplotlib 3.5",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group Project: Preprocessing and Rasterizing</span>"
    ]
  },
  {
    "objectID": "sections/data-ethics.html",
    "href": "sections/data-ethics.html",
    "title": "8  Data Ethics for Scalable Computing",
    "section": "",
    "text": "8.1 Learning objectives",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Ethics for Scalable Computing</span>"
    ]
  },
  {
    "objectID": "sections/data-ethics.html#learning-objectives",
    "href": "sections/data-ethics.html#learning-objectives",
    "title": "8  Data Ethics for Scalable Computing",
    "section": "",
    "text": "Review FAIR and CARE Principles, and their relevance to data ethics\nExamine how ethical considerations are shared and considered at the Arctic Data Center\nDiscuss ethical considerations in machine learning",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Ethics for Scalable Computing</span>"
    ]
  },
  {
    "objectID": "sections/data-ethics.html#intro-to-data-ethics",
    "href": "sections/data-ethics.html#intro-to-data-ethics",
    "title": "8  Data Ethics for Scalable Computing",
    "section": "8.2 Intro to Data Ethics",
    "text": "8.2 Intro to Data Ethics\n\nThe Arctic Data Center is an openly-accessible data repository. The data published through it is open for anyone to reuse, subject to one of two license: CC-0 Public Domain and CC-By Attribution 4.0. As an open access repository, we prioritize long-term preservation and embrace principles from the data stewardship community, which established a set of best practices for open data management. n adherence, two principles adopted by the Arctic Data Center are FAIR Principles (Findable, Accessible, Interoperable, and Reproducible) and CARE Principles for Indigenous Governance (Collective Benefit, Authority to Control, Responsibility, Ethics). Both of which serve as frameworks in how to consider data ethics.\nThe FAIR Principles\nFAIR speaks to how metadata is managed, stored, and shared.\n\nWhat is the difference between FAIR principles and open science?\nFAIR principles and open science are overlapping concepts, but are distinctive from one another. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. The FAIR principles place emphasis on machine readability, “distinct from peer initiatives that focus on the human scholar” (Wilkinson et al 2016) and as such, do not fully engage with sensitive data considerations and with Indigenous rights and interests (Research Data Alliance International Indigenous Data Sovereignty Interest Group, 2019). Metadata can be FAIR but not open. For example, sensitive data (data that contains personal information) may not be appropriate to share, however sharing the anonymized metadata that is easily understandable will reduce research redundancy.\n\nResearch has historically perpetuated colonialism and represented extractive practices, meaning that the research results were not mutually beneficial. These issues also related to how data was owned, shared, and used. To address issues like these, the Global Indigenous Data Alliance (GIDA) introduced CARE Principles for Indigenous Data Governance to support Indigenous data sovereignty. CARE Principles speak directly to how the data is stored and shared in the context of Indigenous data sovereignty. CARE Principles (Carroll et al. 2020) stand for:\n\nCollective Benefit - Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data\nAuthority to Control - Indigenous Peoples’ rights and interests in Indigenous data must be recognized and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data.\nResponsibility - Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples.\nEthics - Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n\nTo many, the FAIR and CARE principles are viewed by many as complementary: CARE aligns with FAIR by outlining guidelines for publishing data that contributes to open-science and at the same time, accounts for Indigenous’ Peoples rights and interests (Carroll et al. 2020).\nIn Arctic-based research, there is a paradigm shift to include more local Indigenous Peoples, their concerns, and knowledge throughout the research process (Loseto 2020). At the 2019 ArcticNet Annual Scientific Meeting (ASM), a 4-hour workshop was held between Indigenous and non-Indigenous participants to address the challenge of peer-reviewed publications arising when there is a lack of co-production and co-management in the research process between both groups. In the context of peer review, involving Indigenous People and Indigenous Knowledge (IK) not only can increase the validity of research findings, but also ensure the research is meaningful to those most impacted by it. Moreover, it gives power back to the appropriate people to decide who can be knowledge holders of Indigenous knowledge (Loseto et al. 2020). This example underscores the advocacy CARE framework for Indigenous sovereignty, emphasizing the essential integration of people and purpose in the peer review publication stage. Failure to do so perpetuates power imbalances between science institutions and Indigenous communities. Hence, an equitable framework would adhere to the idea ‘Not about us without us’. As an Arctic research community, it is important to reflect on ways we can continue to engage and incorporate Indigenous communities and if there are gaps to address. However, it is important there is no ‘one size fits all’ approach.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Ethics for Scalable Computing</span>"
    ]
  },
  {
    "objectID": "sections/data-ethics.html#ethics-at-the-arctic-data-center",
    "href": "sections/data-ethics.html#ethics-at-the-arctic-data-center",
    "title": "8  Data Ethics for Scalable Computing",
    "section": "8.3 Ethics at the Arctic Data Center",
    "text": "8.3 Ethics at the Arctic Data Center\nTransparency in data ethics is a vital part of open science. Regardless of discipline, various ethical concerns are always present, including professional ethics such as plagiarism, false authorship, or falsification of data, to ethics regarding the handling of animals, to concerns relevant to human subjects research. As the primary repository for the Arctic program of the National Science Foundation, the Arctic Data Center accepts Arctic data from all disciplines. Recently, a new submission feature was released which asks researchers to describe the ethical considerations that are apparent in their research. This question is asked to all researchers, regardless of disciplines.\nSharing ethical practices openly, similar in the way that data is shared, enables deeper discussion about data management practices, data reuse, sensitivity, sovereignty and other considerations. Further, such transparency promotes awareness and adoption of ethical practices.\nInspired by CARE Principles for Indigenous Data Governance (Collective Benefit, Authority to Control, Responsibility, Ethics) and FAIR Principles (Findable, Accessible, Interoperable, Reproducible), we include a space in the data submission process for researchers to describe their ethical research practices. These statements are published with each dataset, and the purpose of these statements is to promote greater transparency in data collection and to guide other researchers. For more information about the ethical research practices statement, check out this blog.\nTo help guide researchers as they write their ethical research statements, we have listed the following ethical considerations that are available on our website. The concerns are organized first by concerns that should be addressed by all researchers, and then by discipline.\nConsider the following ethical considerations that are relevant for your field of research.\n\n8.3.1 Ethical Considerations for all Arctic Researchers\n\nResearch Planning\n\nWere any permits required for your research?\nWas there a code of conduct for the research team decided upon prior to beginning data collection?\nWas institutional or local permission required for sampling?\nWhat impact will your research have on local communities or nearby communities (meaning the nearest community within a 100 mile radius)?\n\nData Collection\n\nWere any local community members involved at any point of the research process, including study site identification, sampling, camp setup, consultation or synthesis?\nWere the sample sites near or on Indigenous land or communities?\n\nData Sharing and Publication\n\nHow were the following concerns accounted for: misrepresentation of results, misrepresentation of experience, plagiarism, improper authorship, or the falsification or data?\nIf this data is intended for publication, are authorship expectations clear for everyone involved? Other professional ethics can be found here\n\n\n\n8.3.2 Archaeological and Paleontological Research\n\nResearch Planning\n\nWere there any cultural practices relevant to the study site? If yes, how were these practices accounted for by the research methodologies.\n\nData Collection\n\nDid your research include the removal or artifacts?\nWere there any contingencies made for the excavation and return of samples after cleaning, processing, and analysis?\n\nData Sharing and Publication\n\nWere the samples deposited to a physical repository?\nWere there any steps taken to account for looting threats? Please explain why or why not?\n\n\n\n8.3.3 Human Participation and Sensitive Data\n\nResearch Planning\n\nPlease describe the institutional IRB approval that was required for this research.\nWas any knowledge provided by community members?\n\nData Collection\n\nDid participants receive compensation for their participation?\nWere decolonization methods used?\n\nData Sharing and Publication\n\nHave you shared this data with the community or participants involved?\n\n\n\n8.3.4 Marines Sciences (e.g. Marine Biology Research)\n\nResearch Planning\n\nWere any of the study sites or species under federal or local protection?\n\nData Collection\n\nWere endangered, threatened, or otherwise special-status species collected?\nHow were samples collected? Please describe any handling practices used to collect data.\nWhat safety measures were in place to keep researchers, research assistants, technicians, etc., out of harms way during research?\nHow were animal care procedures evaluated, and do they follow community norms for organismal care?\n\nData Sharing and Publication\n\nDid the species or study area represent any cultural importance to local communities, or include culturally sensitive information? Please explain how you came to this conclusion and how any cultural sensitivity was accounted for.\n\n\n\n8.3.5 Physical Sciences (e.g. Geology, Glaciology, and Ice Research)\n\nResearch Planning\n\nWas any knowledge provided by community members, including information regarding the study site?\n\nData Collection\n\nWhat safety measures were in place to keep researchers, research assistants, technicians, etc., out of harms way during research?\nWere there any impacts to the environment/habitat before, during or after data collection?\n\nData Sharing and Publication\n\nIs there any sensitive information including information on sensitive sites, valuable samples, or culturally sensitive information?\n\n\n\n8.3.6 Plant and Soil Research\n\nResearch Planning\n\nWere any of the study sites protected under local or federal regulation?\nWas any knowledge provided by nearby community members, including information regarding the study site?\n\nData Collection\n\nDid sample collection result in erosion of soil or other physical damage? If so, how was this addressed?\nWhat safety measures were in place to keep researchers, research assistants, technicians, etc., out of harms way during research?\n\nData Sharing and Publication\n\nDo any of the study sites or specimens represent culturally sensitive areas or species? Please explain how you came to this conclusion, and if yes, how was this accounted for?\n\n\n\n8.3.7 Spatial Data\n\nResearch Planning\n\nWere any land permits required for this research?\n\nData Collection\n\nWere any data collected using citizen science or community participation?\nIf yes, were community members compensated for their time and made aware of their data being used and for what purpose?\n\nData Sharing and Publication\n\nIf data were ground-truthed, was institutional or local permissions required and/or obtained for land/property access?\nHave you shared this data with the community or participants involved?\nIf location sensitive data was obtained (endangered/threatened flora & fauna location, archaeological and historical sites, identifiable ships, sensitive spatial information), how were the data desensitized?\n\n\n\n8.3.8 Wildlife Sciences (e.g. Ecology and Biology Research)\n\nResearch Planning\n\nWere any permits required for data sampling?\n\nData Collection\n\nWere endangered, threatened, or otherwise special-status plants or animal species collected?\nHow were samples collected? Please describe any handling practices used to collect data.\nWhat safety measures were in place to keep researchers, research assistants, technicians, etc., out of harms way during research?\nHow were animal care procedures evaluated, and do they follow community norms for organism care?\n\nData Sharing and Publication\n\nDo any of the study sites or specimens represent culturally sensitive areas or species? Please explain how you came to this conclusion, and if yes, how was this accounted for?\n\nMenti question:\n\nHave you thought about any of the ethical considerations listed above before?\nWere any of the considerations new or surprising?\nAre there any for your relevant discipline that are missing?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Ethics for Scalable Computing</span>"
    ]
  },
  {
    "objectID": "sections/data-ethics.html#ethics-in-artificial-intelligence",
    "href": "sections/data-ethics.html#ethics-in-artificial-intelligence",
    "title": "8  Data Ethics for Scalable Computing",
    "section": "8.4 Ethics in Artificial Intelligence",
    "text": "8.4 Ethics in Artificial Intelligence\nMenti poll\n\nWhat is your level of familiarity with machine learning\nHave you thought about ethics in machine learning prior to this lesson?\nCan anyone list potential ethical considerations in machine learning?\n\n\n8.4.1 Introduction\nArtificial Intelligence (AI) can be thought of as the development of computer systems that can perform tasks we usually think require human intelligence, such as image recognition, language translation, or autonomous movement. The rapid development and adoption of AI tools in the past years, particularly machine learning algorithms, has revolutionized how big datasets are analyzed, transforming decision-making in all sectors of society. However, frameworks to examine the ethical considerations of AI are just emerging, and careful consideration of how to best develop and apply AI systems is essential to the responsible use of these new, rapidly changing tools. In this section, we will give an overview of the FAST Principles put forward by the Alan Turing Institute in their guide for the responsible design and implementation of AI systems (Leslie, 2019).\n\n\n8.4.2 The FAST Principles\nFAST stands for Fairness, Accountability, Sustainability, and Transparency. The FAST principles aim to guide the ethical development of AI projects from their inception to deployment. The continuous involvement and commitment of software developers, domain experts, technical leads, project managers, rightsholders, and collaborators involved in the AI project is crucial to implement these principles successfully. The following is a brief overview of each of the FAST principles, we greatly encourage you to read through the Alan Turing Institute guide to learn more!\n\n\n\n8.4.3 Fairness\nBias can enter at any point of a research project, from data collection and preprocessing, to model design and implementation. This is because AI projects, as any other, are created by human beings who (even with the best of intentions) can introduce error, prejudice, or misjudgement into a system. Fairness refers to the active minimization of bias and commitment to not harm others through the outcomes of an AI system. FAST principles (Leslie, 2019) suggest the following baseline for fairness:\n\n“The designers and users of AI systems ensure that the decisions and behaviours of their models do not generate discriminatory or inequitable impacts on affected individuals and communities. This entails that these designers and users ensure that the AI systems.” (Leslie, 2019)\n\nThey are developing and deploying:\n\nAre trained and tested on properly representative, relevant, accurate, and generalisable datasets (Data Fairness)\nHave model architectures that do not include target variables, features, processes, or analytical structures (correlations, interactions, and inferences) which are unreasonable, morally objectionable, or unjustifiable (Design Fairness)\nDo not have discriminatory or inequitable impacts on the lives of the people they affect (Outcome Fairness)\nAre deployed by users sufficiently trained to implement them responsibly and without bias (Implementation Fairness)\n\nReal-life Example : Insufficient Radar Network\nThe following figure (McGovern et al., 2022) shows coverage of the national Doppler weather network (green and yellow circles) over a demographic map of the Black population in the southeast US. This would be an example of an issue in data fairness, since radar coverage does not represent the population uniformly, leaving out areas with higher Black population. Problems with outcome fairness could ensue if this non-representative biases an AI model to under-predict weather impacts in such populations.\n\n\n\n8.4.4 Accountability\nAccountability in AI projects stems from the shared view that isolated AI models used to automate decisions are not morally responsible in the same way as a decision-making human. As outputs from AI models are increasingly used to make decisions that affect the environment and human lives, there is a critical need for competent human authorities to offer explanations and justifications for the development process, outputs, and ensuing decisions made by AI systems. Such answerability assignments can be challenging, as AI implementations are often the product of big development teams where the responsibility to answer for a project’s outcome may not be delineated, creating an issue known as “the problem of many hands.” The FAST principles encourage the following accountability implementation:\nAccountability by Design: All AI systems must be designed to facilitate end-to-end answerability and auditability. This requires both responsible humans-in-the-loop across the entire design and implementation chain as well as activity monitoring protocols that enable end-to-end oversight and review.\nReal-life Example: AI for natural disasters response\nAccountability and the ability to audit AI methods can be crucial when model outputs support critical decision-making, such as in natural disasters. In 2021, a New York Times investigation (Fink, 2021) covered a private company’s premature release of outputs about neighborhoods most affected by potential earthquakes in Seattle. While the initial release erroneously did not show threats for non-residential areas, ensuing updated versions showed non-compatible predictions again. Although the company acknowledged that its AI models would not replace the first responder’s judgment, the lack of audibility and opacity in the model development hindered accountability for any party, ultimately eroding the public confidence in the tools and leading to a loss of public resources.\n\n\n\n8.4.5 Sustainability\nSustainability in the FAST principles includes continuous assessment of the social impacts of an AI system and technical sustainability of the AI model. In the first consideration, the FAST principles advocate for performing a Stakeholder Impact Assessment (SIA) at different stages to help build confidence in the project and uncover unexpected risks or biases, among other benefits. The Alan Turing Institute guide shares a prototype of an SIA (Leslie, 2019). The core of technical sustainability is creating safe, accurate, reliable, secure, and robust AI systems. To achieve these technical goals, teams must implement thorough testing, performance metrics, uncertainty quantification, and be aware of changes to the underlying distribution of data, among other essential practices.\nReal-life Example: SpaceCows\nThe SpaceCows project (Shepherd, 2021; ABC Australia, 2024) in northern Australia is a collaboration between scientists, industry leaders, and local indigenous communities developing AI centered platforms to analyze GPS tracking data collected from feral cows alongside satellite imagery and weather data. Indigenous knowledge and traditional land owners have been at the center of the development, providing guidance and ultimately benefiting from the AI tools to protect their land and cultural sites.\n\nImportant indigenous cultural sites can be damaged by feral cattle. Image from CSIRO, SpaceCows: Using AI to tackle feral herds in the Top End.\n\n\n8.4.6 Transparency\nUnder the FAST principles, transparency in AI projects refers to transparency about how an AI project was designed and implemented and the content and justification of the outcome produced by the AI model. To ensure process transparency, the project should show how the design and implementation included ethical, safety, and fairness considerations throughout the project. To clarify the content and explain the outcomes of an AI system, the project should offer plain language, non-technical explanations accessible to non-specialists that convey how and why a model performed the way it did. In this direction, it is essential to avoid a ‘mathematical glass box’ where the code and mathematics behind the algorithm are openly available, but there is a lack of rationale about how or why the model goes from input to output. Finally, the explanations about how the outcomes were produced should become the basis to justify the outcomes in terms of ethical permissibility, fairness, and trustworthiness. A careful consideration of the balance between the sustainability and transparency principles is necessary when dealing with protected or private data.\nReal-life Example: France’s Digital Republic Act\nThe concern for transparency in using personal data is an active space for debate. In 2018, the French government passed a law to protect citizens’ privacy, establishing the citizen’s “right to an explanation” regarding, among other things, how an algorithm contributed to decisions on their persona and which data was processed (Edwards and Veale, 2018; Lo Piano, 2020). Overall, this legislation aims to create a fairer and more transparent digital environment where everyone can enjoy equal opportunities.\n\n\n\n8.4.7 Conclusion\nAs new AI developments and applications rapidly emerge and transform everyday life, we need to pause and ensure these technologies are fair, sustainable, and transparent. We must acknowledge human responsibility in designing and implementing AI systems to use these novel tools fairly and with accountability. Finally, we acknowledge that the information covered here is a lightning introduction to AI’s ethical considerations and implications. Whether you are a researcher interested in using AI for the first time or a seasoned ML practitioner, we urge you to dive into the necessary and ever-expanding AI ethics work to learn how to best incorporate these concepts into your work.\n\n\n8.4.8 Bonus - ImageNet: A case study of ethics and bias in machine learning\nThe stories that hit the news are often of privacy breaches or biases seeping into the training data. Bias can enter at any point of the research project, from preparing the training data, designing the algorithms, to collecting and interpreting the data. When working with sensitive data, a question to also consider is how to deanonymize, anonymized data. A unique aspect to machine learning is how personal bias can influence the analysis and outcomes. A great example of this is the case of ImageNet.\n Image source: Kate Crawford and Trevor Paglen, “Excavating AI: The Politics of Training Sets for Machine Learning” (September 19, 2019).\nImageNet is a great example of how personal bias can enter machine learning through the training data. ImageNet was a training data set of photos that was used to train image classifiers. The data set was initially created as a large collection of pictures, which were mainly used to identify objects, but some included images of people. The creators of the data set created labels to categorize the images, and through crowdsourcing, people from the internet labeled these images. (This example is from Kate Crawford and Trevor Paglen, “Excavating AI: The Politics of Training Sets for Machine Learning”, September 19, 2019).\nDiscussion:\n\nWhere are the two areas bias could enter this scenario?\nAre there any ways that this bias could be avoided?\nWhile this example is specific to images, can you think of any room for bias in your research?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Ethics for Scalable Computing</span>"
    ]
  },
  {
    "objectID": "sections/data-ethics.html#references-and-further-reading",
    "href": "sections/data-ethics.html#references-and-further-reading",
    "title": "8  Data Ethics for Scalable Computing",
    "section": "8.5 References and Further Reading",
    "text": "8.5 References and Further Reading\nCarroll, S.R., Herczog, E., Hudson, M. et al. (2021) Operationalizing the CARE and FAIR Principles for Indigenous data futures. Sci Data 8, 108 https://doi.org/10.1038/s41597-021-00892-0\nChen, W., & Quan-Haase, A. (2020) Big Data Ethics and Politics: Towards New Understandings. Social Science Computer Review. https://journals.sagepub.com/doi/10.1177/0894439318810734\nCrawford, K., & Paglen, T. (2019) Excavating AI: The Politics of Training Sets for Machine Learning. https://excavating.ai/\nEdwards, Lilian, and Michael Veale. (2017). Enslaving the algorithm: From a  right to an explanationn to a  right to better decisionss?. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3052831.\nFink, Sheri. (2019). “This High-Tech Solution to Disaster Response May Be Too Good to Be True.” The New York Times. www.nytimes.com/2019/08/09/us/emergency-response-disaster-technology.html.\nGray, J., & Witt, A. (2021) A feminist data ethics of care framework for machine learning: The what, why, who and how. First Monday, 26(12), Article number: 11833\nLeslie, D. (2019). Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector. The Alan Turing Institute. https://doi.org/10.5281/zenodo.3240529\nL.L.Loseto, K.Breton-Honeyman, D.N.Etiendem, N.Johnson, T.Pearce, J.Allen, A.Amos, J.Arqviq, J.E.Baak, É.Bélanger, M.P.T.Bourdages, J.R.Brammer, D.Fawcett, J.Gérin-Lajoie, G.Gilbert, K.Hansen-Craik, E.Loring, A.Perrin, and M.Slavitch. 2020. Indigenous participation in peer review publications and the editorial process: reflections from a workshop. Arctic Science. 6(3): 352-360. https://doi.org/10.1139/as-2020-0023\nLo Piano, S. (2020). Ethical principles in machine learning and artificial intelligence: cases from the field and possible ways forward. Humanit Soc Sci Commun 7, 9. https://doi.org/10.1057/s41599-020-0501-9\nMcGovern, A., Ebert-Uphoff, I., Gagne, D. J., & Bostrom, A. (2022). Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science. Environmental Data Science, 1, e6. doi:10.1017/eds.2022.5\nO’Brien, Kristy. (2023). Indigenous Rangers Team up with Scientists for Project SpaceCows, the next Frontier in Feral Pest Control. SpaceCows Surveillance Project Unites Indigenous Rangers, CSIRO in Huge Bovine Mustering Effort. www.abc.net.au/news/rural/2023-11-19/spacecows-csiro-indigenous-rangers-buffalo-surveillance/103103624.\nPuebla, I., & Lowenberg, D. (2021) Recommendations for the Handling for Ethical Concerns Relating to the Publication of Research Data. FORCE 11. https://force11.org/post/recommendations-for-the-handling-of-ethical-concerns-relating-to-the-publication-of-research-data/\nResearch Data Alliance International Indigenous Data Sovereignty Interest Group. (2019). “CARE Principles for Indigenous Data Governance.” The Global Indigenous Data Alliance. GIDA-global.org\nShepard, Tory. (2021) Indigenous Rangers to Use SpaceCows Program to Protect Sacred Sites and Rock Art from Feral Herds.” The Guardian, Guardian News and Media. www.theguardian.com/australia-news/2021/sep/15/indigenous-rangers-to-use-spacecows-program-to-protect-sacred-sites-and-rock-art-from-feral-herds.\nWilkinson, M., Dumontier, M., Aalbersberg, I. et al. (2016) The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018. https://doi.org/10.1038/sdata.2016.18\nZwitter, A., Big Data ethics. (2014) Big Data and Society. DOI: 10.1177/2053951714559253\n(2021). SpaceCows: Using AI to Tackle Feral Herds in the Top End. CSIRO. www.csiro.au/en/news/all/news/2021/september/spacecows-using-ai-to-tackle-feral-herds-in-the-top-end.\nVideos with more information on SpaceCows:\nCSIRO rolls out world’s largest remote ‘space cows’ herd management system\nSpaceCows: Using AI to tackle feral herds in the Top End",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Ethics for Scalable Computing</span>"
    ]
  },
  {
    "objectID": "sections/geopandas.html",
    "href": "sections/geopandas.html",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "",
    "text": "9.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial and Image Data Using GeoPandas</span>"
    ]
  },
  {
    "objectID": "sections/geopandas.html#learning-objectives",
    "href": "sections/geopandas.html#learning-objectives",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "",
    "text": "Manipulating raster data with rasterio\nManipulating vector data with geopandas\nWorking with raster and vector data together",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial and Image Data Using GeoPandas</span>"
    ]
  },
  {
    "objectID": "sections/geopandas.html#introduction",
    "href": "sections/geopandas.html#introduction",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.2 Introduction",
    "text": "9.2 Introduction\nIn this lesson, we’ll be working with geospatial raster and vector data to do an analysis on vessel traffic in south central Alaska. If you aren’t already familiar, geospatial vector data consists of points, lines, and/or polygons, which represent locations on the Earth. Geospatial vector data can have differing geometries, depending on what it is representing (eg: points for cities, lines for rivers, polygons for states.) Raster data uses a set of regularly gridded cells (or pixels) to represent geographic features.\nBoth geospatial vector and raster data have a coordinate reference system, which describes how the points in the dataset relate to the 3-dimensional spheroid of Earth. A coordinate reference system contains both a datum and a projection. The datum is how you georeference your points (in 3 dimensions!) onto a spheroid. The projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. All coordinate reference systems require a datum. However, some coordinate reference systems are “unprojected” (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984.\nCoordinate reference systems are often referenced using a shorthand 4 digit code called an EPSG code. We’ll be working with two coordinate reference systems in this lesson with the following codes:\n\n3338: Alaska Albers\n4326: WGS84 (World Geodetic System 1984), used in GPS\n\nIn this lesson, we are going to take two datasets:\n\nAlaskan commercial salmon fishing statisical areas\nNorth Pacific and Arctic Marine Vessel Traffic Dataset\n\nand use them to calculate the total distance travelled by ships within each fishing area.\nThe high level steps will be\n\nread in the datasets\nreproject them so they are in the same projection\nextract a subset of the raster and vector data using a bounding box\nturn each polygon in the vector data into a raster mask\nuse the masks to calculate the total distance travelled (sum of pixels) for each fishing area",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial and Image Data Using GeoPandas</span>"
    ]
  },
  {
    "objectID": "sections/geopandas.html#pre-processing-raster-data",
    "href": "sections/geopandas.html#pre-processing-raster-data",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.3 Pre-processing raster data",
    "text": "9.3 Pre-processing raster data\nFirst we need to load in our libraries. We’ll use geopandas for vector manipulation, rasterio for raster maniupulation.\nFirst, we’ll use requests to download the ship traffic raster from Kapsar et al.. We grab a one month slice from August, 2020 of a coastal subset of data with 1km resolution. To get the URL in the code chunk below, you can right click the download button for the file of interest and select “copy link address.”\n\nimport urllib\n\nurl = 'https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A6b847ab0-9a3d-4534-bf28-3a96c5fa8d72'\n\nmsg = urllib.request.urlretrieve(url, \"Coastal_2020_08.tif\")\n\nUsing rasterio, open the raster file, plot it, and look at the metadata. We use the with here as a context manager. This ensures that the connection to the raster file is closed and cleaned up when we are done with it.\n\nimport rasterio\nimport matplotlib.pyplot as plt\n\nwith rasterio.open(\"Coastal_2020_08.tif\") as ship_con:\n    # read in raster (1st band)\n    ships = ship_con.read(1)\n    ships_meta = ship_con.profile\n\nplt.imshow(ships)\nprint(ships_meta)\n\n{'driver': 'GTiff', 'dtype': 'float32', 'nodata': -3.3999999521443642e+38, 'width': 3087, 'height': 2308, 'count': 1, 'crs': CRS.from_epsg(3338), 'transform': Affine(999.7994153462766, 0.0, -2550153.29233849,\n       0.0, -999.9687691991521, 2711703.104608573), 'blockysize': 1, 'tiled': False, 'compress': 'lzw', 'interleave': 'band'}\n\n\n\n\n\n\n\n\n\nYou’ll notice that we are saving two objects here, ships and ships_meta. Looking at the types of these two objects is useful to understand what rasterio is doing.\n\ntype(ships)\n\nnumpy.ndarray\n\n\n\ntype(ships_meta)\n\nrasterio.profiles.Profile\n\n\nThe ships object is a numpy array, while the ships_meta is a special rasterio class called Profile. To understand why the raster data is represented as an array, and what that profile object is, let’s look into what raster data are, exactly.\n\nSource: Leah A. Wasser, Megan A. Jones, Zack Brym, Kristina Riemer, Jason Williams, Jeff Hollister, Mike Smorul. Raster 00: Intro to Raster Data in R. National Evologial Observatory Network (NEON).\nThe upper left panel of the figure above shows some satellite imagery data. These data are in raster format, which when you zoom in, you can see consist of regularly gridded pixels, each of which contains a value. When we plot these data, we can assign a color map to the pixel values, which generates the image we see. The data themselves, though, are just an n-dimensional grid of numbers. Another way we might describe this is…an array! So, this is why raster data is represented in python using a numpy array.\nThis is all great, and the array of values is a lot of information, but there are some key items that are missing. This array isn’t imaginary, it represents a physical space on this earth, so where is all of that contextual information? The answer is in the rasterio profile object. This object contains all of the metadata needed to interpret the raster array. Here is what our ships_meta contains:\n'driver': 'GTiff',\n'dtype': 'float32',\n'nodata': -3.3999999521443642e+38,\n'width': 3087,\n'height': 2308,\n'count': 1,\n'crs': CRS.from_epsg(3338),\n'transform': Affine(999.7994153462766, 0.0, -2550153.29233849, 0.0, -999.9687691991521, 2711703.104608573),\n'tiled': False,\n'compress': 'lzw',\n'interleave': 'band'}\nThis object gives us critical information, like the CRS of the data, the no data value, and the transform. The transform is what allows us to move from image pixel (row, column) coordinates to and from geographic/projected (x, y) coordinates. The transform and the CRS are critically important, and related. If the CRS are instructions for how the coordinates can be represented in space and on a flat surface (in the case of projected coordinate systems), then the transform describes how to locate the raster array positions in the correct coordinates given by the CRS.\nNote that since the array and the profile are in separate objects it is easy to lose track of one of them, accidentally overwrite it, etc. Try to adopt a naming convention that works for you because they usually need to work together in geospatial operations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial and Image Data Using GeoPandas</span>"
    ]
  },
  {
    "objectID": "sections/geopandas.html#pre-processing-vector-data",
    "href": "sections/geopandas.html#pre-processing-vector-data",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.4 Pre-processing vector data",
    "text": "9.4 Pre-processing vector data\nNow download a vector dataset of commercial fishing districts in Alaska.\n\nurl = 'https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7c942c45-1539-4d47-b429-205499f0f3e4'\n\nmsg = urllib.request.urlretrieve(url, \"Alaska_Commercial_Salmon_Boundaries.gpkg\")\n\nRead in the data using geopandas.\n\nimport geopandas as gpd\n\ncomm = gpd.read_file(\"Alaska_Commercial_Salmon_Boundaries.gpkg\")\n\nNote the “pandas” in the library name “geopandas.” Our comm object is really just a special type of pandas data frame called a geodataframe. This means that in addition to any geospatial stuff we need to do, we can also just do regular pandas things on this data frame.\nFor example, we can get a list of column names (there are a lot!)\n\ncomm.columns.values\n\narray(['OBJECTID', 'GEOMETRY_START_DATE', 'GEOMETRY_END_DATE',\n       'STAT_AREA', 'STAT_AREA_NAME', 'FISHERY_GROUP_CODE',\n       'GIS_SERIES_NAME', 'GIS_SERIES_CODE', 'REGION_CODE',\n       'REGISTRATION_AREA_NAME', 'REGISTRATION_AREA_CODE',\n       'REGISTRATION_AREA_ID', 'REGISTRATION_LOCATION_ABBR',\n       'MANAGEMENT_AREA_NAME', 'MANAGEMENT_AREA_CODE', 'DISTRICT_NAME',\n       'DISTRICT_CODE', 'DISTRICT_ID', 'SUBDISTRICT_NAME',\n       'SUBDISTRICT_CODE', 'SUBDISTRICT_ID', 'SECTION_NAME',\n       'SECTION_CODE', 'SECTION_ID', 'SUBSECTION_NAME', 'SUBSECTION_CODE',\n       'SUBSECTION_ID', 'COAR_AREA_CODE', 'CREATOR', 'CREATE_DATE',\n       'EDITOR', 'EDIT_DATE', 'COMMENTS', 'STAT_AREA_VERSION_ID',\n       'Shape_Length', 'Shape_Area', 'geometry'], dtype=object)\n\n\nWe can also look at the head of the data frame:\n\ncomm.head()\n\n\n\n\n\n\n\n\n\nOBJECTID\nGEOMETRY_START_DATE\nGEOMETRY_END_DATE\nSTAT_AREA\nSTAT_AREA_NAME\nFISHERY_GROUP_CODE\nGIS_SERIES_NAME\nGIS_SERIES_CODE\nREGION_CODE\nREGISTRATION_AREA_NAME\n...\nCOAR_AREA_CODE\nCREATOR\nCREATE_DATE\nEDITOR\nEDIT_DATE\nCOMMENTS\nSTAT_AREA_VERSION_ID\nShape_Length\nShape_Area\ngeometry\n\n\n\n\n0\n12\n1975-01-01 00:00:00+00:00\nNaT\n33461\nTanana River mouth to Kantishna River\nB\nSalmon\nB\n3\nYukon Area\n...\nYU\nEvelyn Russel\n2006-03-26 00:00:00+00:00\nSabrina Larsen\n2017-02-02 00:00:00+00:00\nYukon District, 6 Subdistrict and 6-A Section ...\nNone\n4.610183\n0.381977\nMULTIPOLYGON (((-151.32805 64.96913, -151.3150...\n\n\n1\n13\n1975-01-01 00:00:00+00:00\nNaT\n33462\nKantishna River to Wood River\nB\nSalmon\nB\n3\nYukon Area\n...\nYU\nEvelyn Russel\n2006-03-26 00:00:00+00:00\nSabrina Larsen\n2017-02-02 00:00:00+00:00\nYukon District, 6 Subdistrict and 6-B Section ...\nNone\n3.682421\n0.321943\nMULTIPOLYGON (((-149.96255 64.70518, -149.9666...\n\n\n2\n18\n1978-01-01 00:00:00+00:00\nNaT\n33431\nToklik to Cottonwood Point\nB\nSalmon\nB\n3\nYukon Area\n...\nYL\nEvelyn Russel\n2006-03-26 00:00:00+00:00\nSabrina Larsen\n2017-02-02 00:00:00+00:00\nYukon District and 3 Subdistrict until 1/1/1980\nNone\n2.215641\n0.198740\nMULTIPOLYGON (((-161.39853 61.55463, -161.4171...\n\n\n3\n19\n1980-01-01 00:00:00+00:00\nNaT\n33442\nRight Bank, Bishop Rock to Illinois Creek\nB\nSalmon\nB\n3\nYukon Area\n...\nYU\nEvelyn Russel\n2006-03-26 00:00:00+00:00\nSabrina Larsen\n2017-02-02 00:00:00+00:00\nNone\nNone\n9.179852\n0.382788\nMULTIPOLYGON (((-153.15234 65.24944, -153.0761...\n\n\n4\n20\n1980-01-01 00:00:00+00:00\nNaT\n33443\nLeft Bank, Cone Point to Illinois Creek\nB\nSalmon\nB\n3\nYukon Area\n...\nYU\nEvelyn Russel\n2006-03-26 00:00:00+00:00\nSabrina Larsen\n2017-02-02 00:00:00+00:00\nNone\nNone\n9.500826\n0.378262\nMULTIPOLYGON (((-152.99905 65.17027, -152.9897...\n\n\n\n\n5 rows × 37 columns\n\n\n\n\nNote the existence of the geometry column. This is where the actual geospatial points that comprise the vector data are stored, and this brings up the important difference between raster and vector data - while raster data is regularly gridded at a specific resolution, vector data are just points in space.\n\nSource: Leah A. Wasser, Megan A. Jones, Zack Brym, Kristina Riemer, Jason Williams, Jeff Hollister, Mike Smorul. Raster 00: Intro to Raster Data in R. National Evologial Observatory Network (NEON).\nThe diagram above shows the three different types of geometries that geospatial vector data can take, points, lines or polygons. Whatever the geometry type, the geometry information (the x,y points) is stored in the column named geometry in the geopandas data frame. In this example, we have a dataset containing polygons of fishing districts. Each row in the dataset corresponds to a district, with unique attributes (the other columns in the dataset), and its own set of points defining the boundaries of the district, contained in the geometry column.\n\ncomm['geometry'][:5]\n\n0    MULTIPOLYGON (((-151.32805 64.96913, -151.3150...\n1    MULTIPOLYGON (((-149.96255 64.70518, -149.9666...\n2    MULTIPOLYGON (((-161.39853 61.55463, -161.4171...\n3    MULTIPOLYGON (((-153.15234 65.24944, -153.0761...\n4    MULTIPOLYGON (((-152.99905 65.17027, -152.9897...\nName: geometry, dtype: geometry\n\n\nSo, now we know where our x,y points are, where is all of the other information like the crs? With vector data, all of this information is contained within the geodataframe. We can access the crs attribute on the data frame and print it like so:\n\ncomm.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nWe can also plot one geometry cell by simply calling it:\n\ncomm['geometry'][8]\n\n\n\n\n\n\n\n\nNow that we know a little about what we are working with, let’s get this data ready to analyze. First, we can make a plot of it just to see what we have.\n\ncomm.plot(figsize=(9,9))\n\n\n\n\n\n\n\n\nThis plot doesn’t look so good. Turns out, these data are in WGS 84 (EPSG 4326), as opposed to Alaska Albers (EPSG 3338), which is what our raster data are in. To make pretty plots, and allow our raster data and vector data to be analyzed together, we’ll need to reproject the vector data into 3338. To to this, we’ll use the to_crs method on our comm object, and specify as an argument the projection we want to transform to.\n\ncomm_3338 = comm.to_crs(\"EPSG:3338\")\n\ncomm_3338.plot()\n\n\n\n\n\n\n\n\nMuch better!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial and Image Data Using GeoPandas</span>"
    ]
  },
  {
    "objectID": "sections/geopandas.html#crop-data-to-area-of-interest",
    "href": "sections/geopandas.html#crop-data-to-area-of-interest",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.5 Crop data to area of interest",
    "text": "9.5 Crop data to area of interest\nFor this example, we are only interested in south central Alaska, encompassing Prince William Sound, Cook Inlet, and Kodiak. Our raster data is significantly larger than that, and the vector data is statewide. So, as a first step we might want to crop our data to the area of interest.\nFirst, we’ll need to create a bounding box. We use the box function from shapely to create the bounding box, then create a geoDataFrame from the points, and finally convert the WGS 84 coordinates to the Alaska Albers projection.\n\nfrom shapely.geometry import box\n\ncoord_box = box(-159.5, 55, -144.5, 62)\n\ncoord_box_df = gpd.GeoDataFrame(\n    crs = 'EPSG:4326',\n    geometry = [coord_box]).to_crs(\"EPSG:3338\")\n\nNow, we can read in raster again cropped to bounding box. We use the mask function from rasterio.mask. Note that we apply this to the connection to the raster file (with rasterio.open(...)), then update the metadata associated with the raster, because the mask function requires as its first dataset argument a dataset object opened in r mode.\n\nimport rasterio.mask\nimport numpy as np\n\nwith rasterio.open(\"Coastal_2020_08.tif\") as ship_con:\n    shipc_arr, shipc_transform = rasterio.mask.mask(ship_con,\n                                                    coord_box_df[\"geometry\"],\n                                                    crop=True)\n    shipc_meta = ship_con.meta\n    # select just the 2-D array (by default a 3-D array is returned even though we only have one band)\n    shipc_arr = shipc_arr[0,:,:]\n    # turn the no-data values into NaNs.\n    shipc_arr[shipc_arr == ship_con.nodata] = np.nan\n\n\nshipc_meta.update({\"driver\": \"GTiff\",\n                 \"height\": shipc_arr.shape[0],\n                 \"width\": shipc_arr.shape[1],\n                 \"transform\": shipc_transform,\n                 \"compress\": \"lzw\"})\n\nNow we’ll do a similar task with the vector data. Tin this case, we use a spatial join. The join will be an inner join, and select only rows from the left side (our fishing districts) that are within the right side (our bounding box). I chose this method as opposed to a clipping type operation because it ensures that we don’t end up with any open polygons at the boundaries of our box, which could cause problems for us down the road.\n\ncomm_clip = gpd.sjoin(comm_3338,\n                      coord_box_df,\n                      how='inner',\n                      predicate='within')\n\n\n9.5.1 Check extents\nNow let’s look at the two cropped datasets overlaid on each other to ensure that the extents look right.\n\nimport rasterio.plot\n\n# set up plot\nfig, ax = plt.subplots(figsize=(7, 7))\n# plot the raster\nrasterio.plot.show(shipc_arr,\n                   ax=ax,\n                   vmin = 0,\n                   vmax = 50000,\n                   transform = shipc_transform)\n# plot the vector\ncomm_clip.plot(ax=ax, facecolor='none', edgecolor='red')",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial and Image Data Using GeoPandas</span>"
    ]
  },
  {
    "objectID": "sections/geopandas.html#calculate-total-distance-per-fishing-area",
    "href": "sections/geopandas.html#calculate-total-distance-per-fishing-area",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.6 Calculate total distance per fishing area",
    "text": "9.6 Calculate total distance per fishing area\nIn this step, we rasterize each polygon in the vector data, such that pixels in or touching the polygon get a value of 1, and pixels not touching it get a value of 0. Then, for each polygon, we extract the indices of the raster array that are equal to 1. We then extract the values of these indices from the original ship traffic raster data, and calculate the sum of the values over all of those pixels.\nHere is a simplified diagram of the process:\n\n\n9.6.0.1 Zonal statistics over one polygon\nLet’s look at how this works over just one fishing area first. We use the rasterize method from the features module in rasterio. This takes as arguments the data to rasterize (in this case the 40th row of our dataset), and the shape and transform the output raster will take. We also set the all_touched argument to true, which means any pixel that touches a boundary of our vector will be burned into the mask.\n\nfrom rasterio import features\n\nr40 = features.rasterize(comm_clip['geometry'][40].geoms,\n                                    out_shape=shipc_arr.shape,\n                                    transform=shipc_meta['transform'],\n                                    all_touched=True)\n\nIf we have a look at a plot of our rasterized version of the single fishing district, we can see that instead of a vector, we now have a raster showing the rasterized district (with pixel values of 1) and any area not in the district has a pixel value of 0.\n\n# set up plot\nfig, ax = plt.subplots(figsize=(7, 7))\n# plot the raster\nrasterio.plot.show(r40,\n                   ax=ax,\n                   vmin = 0,\n                   vmax = 1,\n                   transform = shipc_meta['transform'])\n# plot the vector\ncomm_clip.plot(ax=ax, facecolor='none', edgecolor='red')\n\n\n\n\n\n\n\n\nA quick call to np.unique shows our unique values are 0 or 1, which is what we expect.\n\nnp.unique(r40)\n\narray([0, 1], dtype=uint8)\n\n\nFinally, we need to know the indices where the fishing district is. We can use np.where to extract this information\n\nr40_index = np.where(r40 == 1)\nprint(r40_index)\n\n(array([108, 108, 108, 108, 108, 109, 109, 109, 109, 109, 109, 109, 109,\n       109, 109, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110,\n       110, 110, 110, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111,\n       111, 111, 111, 111, 112, 112, 112, 112, 112, 112, 112, 112, 112,\n       112, 112, 112, 112, 112, 112, 113, 113, 113, 113, 113, 113, 113,\n       113, 113, 113, 113, 113, 113, 113, 113, 114, 114, 114, 114, 114,\n       114, 114, 114, 114, 114, 114, 115, 115, 115, 115, 115, 115, 116]), array([759, 760, 762, 763, 764, 755, 756, 757, 758, 759, 761, 762, 763,\n       764, 765, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763,\n       764, 765, 766, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762,\n       763, 764, 765, 766, 753, 754, 755, 756, 757, 758, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 753, 754, 755, 756, 757, 758, 759,\n       760, 761, 762, 763, 764, 765, 766, 767, 753, 754, 755, 756, 757,\n       758, 759, 760, 761, 762, 763, 753, 754, 755, 756, 757, 758, 754]))\n\n\nIn the last step, we’ll use these indices to extract the values of the data from the ship traffic raster, and sum them to get a total distance travelled.\n\nnp.nansum(shipc_arr[r40_index])\n\n14369028.0\n\n\nNow that we know the individual steps, let’s run this over all of the districts. First we’ll create an id column in the vector data frame. This will help us track unique fishing districts later.\n\n\n9.6.0.2 Zonal statistics over all polygons\n\ncomm_clip['id'] = range(0,len(comm_clip))\n\nFor each district (with geometry and id), we run the features.rasterize function. Then we calculate the sum of the values of the shipping raster based on the indices in the raster where the district is located.\n\ndistance_dict = {}\nfor geom, idx in zip(comm_clip['geometry'], comm_clip['id']):\n    rasterized = features.rasterize(geom.geoms,\n                                    out_shape=shipc_arr.shape,\n                                    transform=shipc_meta['transform'],\n                                    all_touched=True)\n\n    r_index = np.where(rasterized == 1)\n    distance_dict[idx] = np.nansum(shipc_arr[r_index])\n\nNow we just create a data frame from that dictionary, and join it to the vector data using pandas operations.\n\nimport pandas as pd\n\n# create a data frame from the result\ndistance_df = pd.DataFrame.from_dict(distance_dict,\n                                     orient='index',\n                                     columns=['distance'])\n\n# extract the index of the data frame as a column to use in a join and convert distance to kilometers\ndistance_df['id'] = distance_df.index\ndistance_df['distance'] = distance_df['distance']/1000\n\nNow we join the result to the original geodataframe.\n\n# join the sums to the original data frame\nres_full = comm_clip.merge(distance_df,\n                           on = \"id\",\n                           how = 'inner')\n\nFinally, we can plot our result!\n\nimport matplotlib.ticker\nfig, ax = plt.subplots(figsize=(7, 7))\n\nax = res_full.plot(column = \"distance\", legend = True, ax = ax)\nfig = ax.figure\nlabel_format = '{:,.0f}'\ncb_ax = fig.axes[1]\nticks_loc = cb_ax.get_yticks().tolist()\ncb_ax.yaxis.set_major_locator(matplotlib.ticker.FixedLocator(ticks_loc))\ncb_ax.set_yticklabels([label_format.format(x) for x in ticks_loc])\nax.set_axis_off()\nax.set_title(\"Distance Traveled by Ships in Kilometers\")\nplt.show()\n\n\n\n\n\n\n\n\nFrom here we can do any additional geopandas operations we might be interested in. For example, what if we want to calculate the total distance by registration area (a superset of fishing district). We can do that using dissolve from geopandas.\n\nreg_area = res_full.dissolve(by = \"REGISTRATION_AREA_NAME\", \n                             aggfunc = 'sum',\n                             numeric_only = True)\n\nLet’s have a look at the same plot as before, but this time over our aggregated data.\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\nax = reg_area.plot(column = \"distance\", legend = True, ax = ax)\nfig = ax.figure\nlabel_format = '{:,.0f}'\ncb_ax = fig.axes[1]\nticks_loc = cb_ax.get_yticks().tolist()\ncb_ax.yaxis.set_major_locator(matplotlib.ticker.FixedLocator(ticks_loc))\ncb_ax.set_yticklabels([label_format.format(x) for x in ticks_loc])\nax.set_axis_off()\nax.set_title(\"Distance Traveled by Ships in Kilometers\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial and Image Data Using GeoPandas</span>"
    ]
  },
  {
    "objectID": "sections/geopandas.html#summary",
    "href": "sections/geopandas.html#summary",
    "title": "9  Spatial and Image Data Using GeoPandas",
    "section": "9.7 Summary",
    "text": "9.7 Summary\nWe covered a lot of ground here, so let’s recap some of the high level points:\n\nRaster data consist of regularly gridded values, and can be represented in python as an array\nVector data consist of any number of points, that might be connected, and is represented in python as a geodataframe\nWe can do geospatial operations like changing the projection or cropping the data to a particular extent on both raster and vector data\nYou can use vector data to help analyze raster data (and vice versa!) by rasterizing the vector data and using numpy operations on the resulting array.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial and Image Data Using GeoPandas</span>"
    ]
  },
  {
    "objectID": "sections/geoscience-and-ai.html",
    "href": "sections/geoscience-and-ai.html",
    "title": "10  AI in Arctic Science",
    "section": "",
    "text": "10.1 Cyberinfrastructure and AI in Arctic Science: Tracking and Mapping Arctic Permafrost Thaw",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/geoscience-and-ai.html#cyberinfrastructure-and-ai-in-arctic-science-tracking-and-mapping-arctic-permafrost-thaw",
    "href": "sections/geoscience-and-ai.html#cyberinfrastructure-and-ai-in-arctic-science-tracking-and-mapping-arctic-permafrost-thaw",
    "title": "10  AI in Arctic Science",
    "section": "",
    "text": "Cyber2A is a project focused on increasing the relevance and utility of machine learning and deep learning for Arctic research. This will be accomplished bby designing a training curriculum on machine learning that focuses on Arctic researc problems, and that can introduce new epople to this field. The Cyber2A network will serve as an important venue for collecting community input on training needs, to engage and recruit potential trainees, and to cultivate new connections with existing Arctic research communities to achieve collective and broader impacts of the Cyber2A training and education.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/geoscience-and-ai.html#resources",
    "href": "sections/geoscience-and-ai.html#resources",
    "title": "10  AI in Arctic Science",
    "section": "10.2 Resources",
    "text": "10.2 Resources\nDr. Wenwen Li, 2024, Cyberinfrastructure and AI in Arctic Science: Tracking and Mapping Arctic Permafrost Thaw\n\nDownload GeoAI: Where machine leanrning and big data converge in GIScience\nDownload Capturing the dance of the earth: PolarGlobe; Real-time scientific visualization of vector field data to support climate science\nDownload Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model’s Generalizability in Permafrost Mapping\nDownload Lowering the Barriers for Accesing Distributed Geospatial Big Data to Advance Spatial Data Science: The PolarHub Solution",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/group-project-2.html",
    "href": "sections/group-project-2.html",
    "title": "11  Group Project: Visualization",
    "section": "",
    "text": "11.1 Setup\nIn your fork of the scalable-computing-examples repository, open the Jupyter notebook in the group-project directory called session-15.ipynb. This workbook will serve as a skeleton for you to work in. It will load in all the libraries you need, including a few helper functions we wrote for the course, show an example for how to use the method on one file, and then lays out blocks for you and your group to fill in with code that will run that method in parallel.\nIn your small groups, work together to write the solution, but everyone should aim to have a working solution on their own fork of the repository. In other words, everyone should type out the solution themselves as part of the group effort. Writing the code out yourself (even if others are contributing to the content) is a great way to get “mileage” as you develop these skills.\nOnly one person in the group should run the parallel code.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Group Project: Visualization</span>"
    ]
  },
  {
    "objectID": "sections/group-project-2.html#resampling-rasters",
    "href": "sections/group-project-2.html#resampling-rasters",
    "title": "11  Group Project: Visualization",
    "section": "11.2 Resampling rasters",
    "text": "11.2 Resampling rasters\nIn this portion of the group project, we will further process the raster files by resampling them to lower zoom levels. This means we will take the average of clusters of pixels, and that value will be assigned to the single pixel that encompasses the entire area that is represented by the original cluster of pixels. Here is an example of creating a zoom level 10 pixel from 4 zoom level 11 pixels:\n\n\n\nDrawing of downsampling a raster, created by Robyn Thiessen-Bock.\n\n\nAs we aggregate cells to produce rasters at lower zoom levels, the cell size increases, but the extent of the raster remains the same. This is called “downsampling”.\nAfter we do this for each zoom level, you can imagine they are organzied as a pyramid, with the lowest zoom levels (and lowest resolution rasters) at the top, like so:\n\n\n\nA pyramid of rasters at different resolutions (zoom levels). Souce: maptiler\n\n\nThe highest zoom level has the most rasters. As we create the lower zoom levels, we aggregate the rasters only from the zoom level directly above. As the zoom level decreases, our computation is faster.\nCheck out the Permafrost Discovery Gateway Imagery Viewer and zoom in and out. You can see for yourself how the resolution increases and the extent that you are trying to view decreases.\n\n\n\nIce wedge polygons at high resolution, visible when zoomed-in to the Permafrost Discovery Gateway Imagery Viewer.\n\n\nThe higher resolution tiles are the “child” tiles, and the lower resolution tiles are the “parent” tiles.\n\n11.2.1 Packages, Libraries, and Modules\n\nos 3.11.2\nparsl 2023.3.20\npdgstaging\n\ndeveloped by Permafrost Discovery Gateway software developer and designer Robyn Thiessen-Bock\n\npdgraster\n\ndeveloped by Permafrost Discovery Gateway software developer and designer Robyn Thiessen-Bock\n\ngeopandas0.11\nrandom\nmatplotlib 3.5\nipyleaflet 0.17",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Group Project: Visualization</span>"
    ]
  },
  {
    "objectID": "sections/software-design-1.html",
    "href": "sections/software-design-1.html",
    "title": "12  Software Design I: Functions and Concurrency",
    "section": "",
    "text": "12.1 Learning Objectives\nThe goal of this module is to gain some useful approaches to software design and modularity that will help with building scalable, portable, and reusable code. We will cover several key aspects of software design for concurrency:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Software Design I: Functions and Concurrency</span>"
    ]
  },
  {
    "objectID": "sections/software-design-1.html#learning-objectives",
    "href": "sections/software-design-1.html#learning-objectives",
    "title": "12  Software Design I: Functions and Concurrency",
    "section": "",
    "text": "Functions as objects\nGlobal variables\nPure functions\nTask dependencies\nLocks, deadlocks, and race conditions",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Software Design I: Functions and Concurrency</span>"
    ]
  },
  {
    "objectID": "sections/software-design-1.html#why-functions",
    "href": "sections/software-design-1.html#why-functions",
    "title": "12  Software Design I: Functions and Concurrency",
    "section": "12.2 Why functions?",
    "text": "12.2 Why functions?\n\n\n\n\n\n\n DRY: Don’t Repeat Yourself\n\n\n\n\n\n\nBy creating small functions that handle only one logical task and do it well, we quickly gain:\n\nImproved understanding\nReuse via decomposing tasks into bite-sized chunks\nImproved error testing\nImproved concurrency\n\nWhen writing functions that are to be used in concurrent programming, it is best to keep them short and focused on a single, well-defined task. This enables you test the function thoroughly, and reuse it in multiple contexts. It also enables you to more easily debug what is happening in the function when trying to understands parallel execution.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Software Design I: Functions and Concurrency</span>"
    ]
  },
  {
    "objectID": "sections/software-design-1.html#functions-as-objects",
    "href": "sections/software-design-1.html#functions-as-objects",
    "title": "12  Software Design I: Functions and Concurrency",
    "section": "12.3 Functions as objects",
    "text": "12.3 Functions as objects\nFunctions are first-class objects in Python (and many other languages). This has some real benefits for and implications for parallel programming. Because a function is an object, it means that it can be 1) stored in a variable, and 2) passed as an argument to another function. We saw that in the module on pleasingly parallel codes when we used ThreadPoolExecutor.map(), which takes a function and an iterable object as arguments. Let’s check out how you can use and manipulate functions as objects. First, let’s define a simple function, assign it to another variable, and then use both:\n\ndef double(x):\n    return 2*x\n\n# also assign the function to the `twotimes` variable\ntwotimes = double\ntype(twotimes)\n\nfunction\n\n\nNote that when we print it to screen, we see that prod is of type function, and when we use the two instances, we get identical results:\n\nprint(double(7))\nprint(twotimes(7))\nprint(double(5) == twotimes(5))\n\n14\n14\nTrue\n\n\nThis representation of a function as an object comes in handy when we want to invoke a function in multiple different contexts, such as in a parallel execution environment via a map() function.\n\nlist(map(twotimes, [2,3,4]))\n\n[4, 6, 8]\n\n\nThis works because the function twotimes can be passed to map and executed from within map. When you execute a function that is passed in via an argument, it is called function composition. We can easily illustrate this by creating some function and passing it to a wrapper function to be executed:\n\ndef some_function():\n    print(\"Ran some_function\")\n\ndef wrapper(func_to_run):\n    print(\"Ran wrapper\")\n    func_to_run()\n    print(\"Finished wrapper\")\n\nwrapper(some_function)\n\nRan wrapper\nRan some_function\nFinished wrapper\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote how we passed the some_function as a variable name without the parentheses.\n\n\n\n\n\n\n\n\nDecorators\n\n\n\nThis approach to function composition is exactly what is used by Python decorator functions.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Software Design I: Functions and Concurrency</span>"
    ]
  },
  {
    "objectID": "sections/software-design-1.html#global-variables",
    "href": "sections/software-design-1.html#global-variables",
    "title": "12  Software Design I: Functions and Concurrency",
    "section": "12.4 Global variables",
    "text": "12.4 Global variables\nWhen executing a function, the variables that are in scope to that function are local, meaning that the presence of another variable with the same name in another scope will not affect a calculation. For example:\n\ndef do_task():\n    x = 10\n\nx = 5\ndo_task()\nprint(x)\n\n5\n\n\nHowever, if that same variable is declared as global inside the function, then the assignment will have global impact on the value of the parent scope:\n\ndef do_task():\n    global x\n    x = 10\n\nx = 5\ndo_task()\nprint(x)\n\n10\n\n\nSo, you can see that writing a function that uses global variables can have effects outside of the scope of the function call. This can have drastic consequences on concurrent code, as the order in which function calls are made when operating concurrently are not deterministic, and so the impact of global variables will also not be deterministic.\nA related issue arises when code in a function depends on its enclosing namespace, such as when a function is defined inside of another function. When resolving a variable, python first looks in the Local namespace, and then in the Enclosing namespace, Global namespace, and Built-in namespace. So, even if a variable is not defined locally, it might still be resolved by one of the other namespaces in surprising ways.\n\na = 3\ndef do_stuff(b):\n    return a*b\n\ndo_stuff(6)\n\n18",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Software Design I: Functions and Concurrency</span>"
    ]
  },
  {
    "objectID": "sections/software-design-1.html#pure-functions",
    "href": "sections/software-design-1.html#pure-functions",
    "title": "12  Software Design I: Functions and Concurrency",
    "section": "12.5 Pure functions",
    "text": "12.5 Pure functions\nA pure function is a function that depends only on its input arguments, and it has no side effects. In other words, a pure function returns the same value if called repeatedly with the same arguments. Pure functions are particularly amenable to concurrency. For example, the double(x) function above is a pure function, because in all cases calling double(2) will always return 4.\nIn contrast, a non-pure function is a function in which the return value may change if the function is called repeatedly, typically because it depends on some particular state that affects the outcome but is not part of the input arguments. For example, the time.time() function returns different values based on the current state of the system clock.\nUsing a global variable in a function creates a side-effect that makes it an impure function, as would other operations that modify an external state variable.\n\n\n\n\n\n\nImportant\n\n\n\nPure functions make writing concurrent code much easier.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Software Design I: Functions and Concurrency</span>"
    ]
  },
  {
    "objectID": "sections/software-design-1.html#task-dependencies",
    "href": "sections/software-design-1.html#task-dependencies",
    "title": "12  Software Design I: Functions and Concurrency",
    "section": "12.6 Task dependencies",
    "text": "12.6 Task dependencies\nTask dependencies occur when one task in the code depends on the results of another task or computation in the code.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Software Design I: Functions and Concurrency</span>"
    ]
  },
  {
    "objectID": "sections/software-design-1.html#locks",
    "href": "sections/software-design-1.html#locks",
    "title": "12  Software Design I: Functions and Concurrency",
    "section": "12.7 Locks",
    "text": "12.7 Locks\nLocks are a mechanism to manage access to a resource so that multiple threads can access the resource. By adding locks to an otherwise parallel process, we introduce a degree of serial execution to the locked portion of the process. Basically, each thread can only access the resource when it has the lock, and only one lock is given out at a time. Take this example of what happens without locking:\n\nfrom concurrent.futures import ProcessPoolExecutor\nimport time\n\ndef hello(i):\n    print(i, 'Hello')\n    print(i, 'world')\n\nexecutor = ProcessPoolExecutor()\nfutures = [executor.submit(hello, i) for i in range(3)]\nfor future in futures:\n    future.result()\n\n102  Hello HelloHello\n\n\n102   worldworldworld\n\n\n\n\nYou can see that the results come back in a semi-random order, and the call to sleep creates a delay between printing the two words, which means that the three messages get jumbled when printed. To fix this, we can introduce a lock from the multiprocessing package.\n\nfrom concurrent.futures import ProcessPoolExecutor\nimport time\nimport multiprocessing\n\ndef hello(i, lock):\n    with lock:\n        print(i, 'Hello')\n        print(i, 'world')\n\nlock = multiprocessing.Manager().Lock()\nexecutor = ProcessPoolExecutor()\nfutures = [executor.submit(hello, i, lock) for i in range(3)]\nfor future in futures:\n    future.result()\n\n0 Hello\n0 world\n1 Hello\n1 world\n2 Hello\n2 world\n\n\nThe lock is generated and then passed to each invocation of the hello function. Using with triggers the use of the context manager for the lock, which allows the manager to synchronize use of the lock. This ensures that only one process can be printing at the same time, ensuring that the outputs are properly ordered.\n\n\n\n\n\n\nWarning\n\n\n\nSynchronizing with a Lock turns a parallel process back into a serial process, at least while the lock is in use. So use with care lest you lose all benefits of concurrency.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Software Design I: Functions and Concurrency</span>"
    ]
  },
  {
    "objectID": "sections/software-design-1.html#race-conditions",
    "href": "sections/software-design-1.html#race-conditions",
    "title": "12  Software Design I: Functions and Concurrency",
    "section": "12.8 Race conditions",
    "text": "12.8 Race conditions\nRace conditions occur when two tasks execute in parallel, but produce different results based on which task finishes first. Ensuring that results are correct under different timing situations requires careful testing.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Software Design I: Functions and Concurrency</span>"
    ]
  },
  {
    "objectID": "sections/software-design-1.html#deadlocks",
    "href": "sections/software-design-1.html#deadlocks",
    "title": "12  Software Design I: Functions and Concurrency",
    "section": "12.9 Deadlocks",
    "text": "12.9 Deadlocks\nDeadlocks occur when two concurrent tasks block on the output of the other. Deadlocks cause parallel programs to lock up indefinitely, can be difficult to track down, and will often require the program to be killed.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Software Design I: Functions and Concurrency</span>"
    ]
  },
  {
    "objectID": "sections/software-design-1.html#further-reading",
    "href": "sections/software-design-1.html#further-reading",
    "title": "12  Software Design I: Functions and Concurrency",
    "section": "12.10 Further reading",
    "text": "12.10 Further reading\n\nWith Statement Context Managers",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Software Design I: Functions and Concurrency</span>"
    ]
  },
  {
    "objectID": "sections/adc-data-publishing.html",
    "href": "sections/adc-data-publishing.html",
    "title": "13  Documenting and Publishing Data",
    "section": "",
    "text": "13.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Documenting and Publishing Data</span>"
    ]
  },
  {
    "objectID": "sections/adc-data-publishing.html#learning-objectives",
    "href": "sections/adc-data-publishing.html#learning-objectives",
    "title": "13  Documenting and Publishing Data",
    "section": "",
    "text": "Become familiar with the submission process\nUnderstand what constitutes as “large data”\n\nKnow when to reach out to support team for help\n\nLean how data & code can be documented and published in open data archives",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Documenting and Publishing Data</span>"
    ]
  },
  {
    "objectID": "sections/adc-data-publishing.html#introduction",
    "href": "sections/adc-data-publishing.html#introduction",
    "title": "13  Documenting and Publishing Data",
    "section": "13.2 Introduction",
    "text": "13.2 Introduction\nA data repository is a database infrastructure that collects, manages, and stores data. In addition to the Arctic Data Center, there are many other repositories dedicated to archiving data, code, and creating rich metadata. The Knowledge Network for Biocomplexity (KNB), the Digital Archaeological Record (tDAR), Environmental Data Initiative (EDI), and Zenodo are all examples of dedicated data repositories.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Documenting and Publishing Data</span>"
    ]
  },
  {
    "objectID": "sections/adc-data-publishing.html#metadata",
    "href": "sections/adc-data-publishing.html#metadata",
    "title": "13  Documenting and Publishing Data",
    "section": "13.3 Metadata",
    "text": "13.3 Metadata\nMetadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where they were collected, and why they were collected.\nFor consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the machine-readable version of the metadata for a sockeye salmon dataset:\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;eml:eml packageId=\"df35d.442.6\" system=\"knb\" \n    xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.1\"&gt;\n    &lt;dataset&gt;\n        &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through \n            Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt;\n        &lt;creator id=\"1385594069457\"&gt;\n            &lt;individualName&gt;\n                &lt;givenName&gt;Mark&lt;/givenName&gt;\n                &lt;surName&gt;Willette&lt;/surName&gt;\n            &lt;/individualName&gt;\n            &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt;\n            &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt;\n            &lt;address&gt;\n                &lt;city&gt;Soldotna&lt;/city&gt;\n                &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt;\n                &lt;country&gt;USA&lt;/country&gt;\n            &lt;/address&gt;\n            &lt;phone phonetype=\"voice\"&gt;(907)260-2911&lt;/phone&gt;\n            &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt;\n        &lt;/creator&gt;\n        ...\n    &lt;/dataset&gt;\n&lt;/eml:eml&gt;\nAlternatively, the same metadata document can be converted to HTML format and displayed in a more readable form on the web:\n\nAs you can see from the picture above, users can download either the whole dataset or its individual components. This makes the dataset and its associated data resuable.\nAdditionally, the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Documenting and Publishing Data</span>"
    ]
  },
  {
    "objectID": "sections/adc-data-publishing.html#data-package-structure",
    "href": "sections/adc-data-publishing.html#data-package-structure",
    "title": "13  Documenting and Publishing Data",
    "section": "13.4 Data Package Structure",
    "text": "13.4 Data Package Structure\nNote that the dataset above lists a collection of files that are contained within the dataset. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.\n\nThese data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI (digital object identifier) identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID (universally unique identifier) that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4, and each of the individual files have their own identifiers as well.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Documenting and Publishing Data</span>"
    ]
  },
  {
    "objectID": "sections/adc-data-publishing.html#archiving-data-the-large-data-perspective",
    "href": "sections/adc-data-publishing.html#archiving-data-the-large-data-perspective",
    "title": "13  Documenting and Publishing Data",
    "section": "13.5 Archiving Data: The Large Data Perspective",
    "text": "13.5 Archiving Data: The Large Data Perspective\nThere are two components to any data package archived with the Arctic Data Center: the metadata & the data themselves. Data can be images, plain text documents, tabular data, spatial data, scripts used to analyze the data, a readme file, and more. To the best of your ability, please make sure that the data uploaded are in an open format, rather than proprietary format. We strongly recommend using open, self-documenting binary formats for large data archival. NetCDF, HDF, .mat (v7.3) and Parquet files are all examples of “self-documenting” files. In the case of a NetCDF file, users can input the attribute name, attribute description, missing value codes, units, and more into the file itself. When these data are well-documented within themselves, it can save the time when users submit their data to us, since the documentation for variable level information is already mostly complete. We’ll discuss NetCDF and metadata more in Session 8. For geospatial data, we recommend using geotiff for raster files, and geopackage files for vector files.\nThis section provides an overview of some highlights within the data submission process, and will specifically address issues related to datasets with large amounts of data, whether that be in number of files or cumulative file size.\nFirst we’ll go over the metadata submission; then learn how to upload the data using a secure File Transfer Protocol; and finally how to add attribute information to the data.\n\n13.5.1 Step 1: The Narrative Metadata Submission\n\n13.5.1.1 ORCiDs\nIn order to archive data with the Arctic Data Center, you must log in with your ORCID account. If you do not have one, you can create at https://orcid.org/. ORCID is a non-profit organization made up of research institutions, funders, publishers and other stakeholders in the research space. ORCID stands for Open Researcher and Contributor ID. The purpose of ORCID is to give researchers a unique identifier which then helps highlight and give credit to researchers for their work. If you click on someone’s ORCID, their work and research contributions will show up (as long as the researcher used ORCID to publish or post their work).\nOnce you’re logged into the Arctic Data Center with your ORCID, you can access the data submission form by clicking “Submit Data” in the navigation bar. For most dataset submissions, you would submit your data and metadata at the same using the “Add Files” buttons seen in the image below. However, when you know you have a large quantity of files or large cumulative file size, you should focus only on submitting metadata through the web form. We’ll discuss how to submit large quantities of data in the next section.\n\n\n\n13.5.1.2 Overview Section\nIn the overview section, you will include a descriptive title of your data set, select the appropriate data sensitivity tag, an abstract of the data set, keywords, funding information, and a license.\nIn general, if your data has been anonymized or de-identified in any way, your submission is no longer considered to have “Non-sensitive data”. If you have not had to de-identify your data or through an Instituional Review Board process, you should select the “Non-sensitive data” tag. You can find a more in-depth review of the data sensitivity tag in Chapter 12 of our Fundamentals in Data Management coursebook.\n\nYou also must enter a funding award number and choose a license. The funding field will search for an NSF award identifier based on words in its title or the number itself. When including funding information not from an NSF award, please make sure to add an award number, title, and organization if possible.\nThe licensing options are CC-0 and CC-BY, both of which allow your data to be downloaded and re-used by other researchers.\n\nCC-0 Public Domain Dedication: “…can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.”\nCC-BY: Attribution 4.0 International License: “…free to…copy,…redistribute,…remix, transform, and build upon the material for any purpose, even commercially,…[but] must give appropriate credit, provide a link to the license, and indicate if changes were made.”\n\n\n\n\n13.5.1.3 People Information\nInformation about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people:\n\nCreators - all the people who should be in the citation for the dataset\nContacts - one is required, but defaults to the dataset submitter if omitted\nPrincipal Investigators\nAny others that are relevant\n\nFor each, please provide their ORCID identifier, which helps link this dataset to their other scholarly works.\n\n\n\n13.5.1.4 Temporal Information\nAdd the temporal coverage of the data, which represents the time period when data was collected. If your sampling over time was discontinuous and you require multiple date ranges to represent your data, please email us the date ranges and we will add that to the submission on the back-end.\n\n\n\n\n13.5.1.5 Location Information\nThe geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that spatial search will be more precise.\n\nNote that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global:\n\nMission Canyon, Santa Barbara, California, USA\n\n\n\n\n13.5.1.6 Methods\nMethods are critical to the accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, so that you can include details of sampling methods, experimental design, quality assurance procedures, and/or computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data. Ideally, enough detail should be provided such that a reasonable scientist could interpret the study and data for reuse without needing to consult the researchers or any other resources.\nIncluded in the methods section is a question that asks users about the ethical research practices that may or may not have been considered throughout the research process. You will learn more about this in Chapter 14, and can find more in-depth information on our website’s data ethics page.\nThe ethical research practices response box must be filled out in order to save your dataset. If users feel as though this question is not applicable to their research, we encourage them to discuss why that is rather than simply stating “Not Applicable,” or some variation thereof. For example, say a researcher has compiled satellite imagery of weather patterns over the open ocean. Rather than respond with “N/A”, a user should instead include something to the effect of “Dataset contains satellite imagery of weather patterns over the Atlantic Ocean. Imagery was downloaded from NASA and NOAA, and does not contain any individual’s identifiable information.” When in doubt, you can always email the support team at support@arcticdata.io.\n\n\n\n13.5.1.7 Save Metadata Submission\nWhen you’re finished editing the narrative metadata, click the Save Dataset button at the bottom right of your screen.\nIf there are errors or missing fields, they will be highlighted with a red banner as seen earlier. Correct those, and then try submitting again. If the save button disappears after making corrections, add a space in the abstract and the save button should reappear. If not, please reach out to the support team for assistance.\nWhen you are successful, you should see a large green banner with a link to the current dataset view. Click the X to close that banner if you want to continue editing metadata.\n\n\n\n\n13.5.2 Step 2: Adding File & Variable Level Metadata\nThe final major section of metadata concerns the structure and content of your data files. Assuming there are many files (and not a few very large ones), it would be unreasonable for users to input file and variable level metadata for each file. When this situation occurs, we encourage users to fill out as much information as possible for each unique type of file. Once that is completed, usually with some assistance from the Data Team, we will then programmatically carry over the information to other relevant files.\nWhen you’re data are associated with your metadata submission, they will appear in the data section at the top of the page when you go to edit your dataset. Choose which file you would like to begin editing by selecting the “Describe” button to the right of the file name.\n\nOnce there, you will see the following screen. In the Overview section, we recommend not editing the file name, and instead add a descriptive overview of the file. Once done, click the Attributes tab.\n\nThe Attributes tab is where you enter variable (aka attribute) information, including:\n\nattribute name (for programs)\nattribute label (for display)\n\n\n\nvariable definition (be specific)\ntype of measurement\n\n\n\nunits & code definitions\n\n\nUsers will need to add these definitions for every variable (column) in the file. When done, click Done. Now, the list of data files will show a green checkbox indicating that you have fully described that file’s internal structure. Proceed with documenting other unique file types, and then click Save Dataset to save all of these changes.\n\n\n\n\n\n\nTip\n\n\n\nWhen dealing with large datasets, the Data Team can help users fill out metadata for similar file types with identical internal structures. An example of this will be discussed in the Best Practices section.\n\n\n\nAfter you get the big green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes.\n\n\n13.5.3 Step 3: Uploading Large Data\nIn order to submit your large data files to the Arctic Data Center repository, we encourage users to directly upload their data to the Data Team’s servers using a secure file transfer protocol (SFTP). There are a number of GUI driven and command line programs out there that all work well. For a GUI program, our team uses and recommends the free program Cyberduck. We will discuss command line programs in Session 18 in more detail, including rsync and Globus.\nBefore we begin, let’s answer the following question: Why would a user want to upload their data through a separate process, rather than the web form when they submit their metadata?\nDepending on your internet connection, the number of files you have, and the cumulative size of the data, users may experience difficulty uploading their data through the submission form. These difficulties are most often significantly reduced upload speeds and submission malfunctions. As such, it is best for all parties for large quantities of data to be uploaded directly to our server through an FTP.\nThe second question is, under what circumstances should I consider uploading data directly to the server, versus the webform? Although server uploads are more efficient for large datasets, for smaller datasets it is much more efficient to go through the webform. So, what constitutes a large dataset and what tools should you consider? Here is a helpful diagram:\n\n\nWeb Editor: The web editor is most often used by those with less than a hundred files and a small cumulative file size (0-5 GBs). Overall, this option is best for those who have less than 250 files with a small cumulative file size.\nSFTP: For users that expect to upload more than 250 files and have a medium cumulative file size (10-100 GBs), uploading data to our servers via SFTP is the recommended method. This can be done through the command line, or a program like Cyberduck. If you find yourself considering uploading a zip file through the web editor, you should instead upload your files using this method.\nGridFTP: For users that expect to upload hundreds or thousands of files, with a cumulative file size of hundreds of GB to TBs, you will likely want to make use of GridFTP through Globus. Jeanette will be talking about data transfers in more depth on Thursday.\n\nBefore you can upload your data to the Data Team’s server, make sure to email us at support@arcticdata.io to retrieve the login password. Once you have that, you can proceed through the following steps.\n\n\n\n\n\n\nTip\n\n\n\nIf you know that you will need to use this process for more than one dataset, we suggest creating folders with the same name as the associated dataset’s title. This way, it will be clear as to which submission the data should be associated with.\n\n\nOnce you have finished uploading your data to our servers, please let the Data Team know via email so that we can continue associate your uploaded data with your metadata submission.\nAs mentioned in Step 1: The Narrative Metadata Submission section above, when the data package is finalized and made public, there will be a sentence in the abstract that directs users to a separate page where your data will live. The following image is an example of where the data from this dataset live.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Documenting and Publishing Data</span>"
    ]
  },
  {
    "objectID": "sections/adc-data-publishing.html#best-practices-for-submitting-large-data",
    "href": "sections/adc-data-publishing.html#best-practices-for-submitting-large-data",
    "title": "13  Documenting and Publishing Data",
    "section": "13.6 Best Practices for Submitting Large Data",
    "text": "13.6 Best Practices for Submitting Large Data\n\n13.6.1 Data Organization\nThe data archival process is much smoother when users already have a well structured and organized file management system. We strongly recommend starting to work with the Arctic Data Center towards the beginning of your project, instead of the very end. Our curation team can help you publish your data faster, and more smoothly, the earlier you talk to us. Regardless of where you are in your project, however, considering your organization strategy is best done before data upload.\nTake a look at this recently made data portal. Originally, this user had only submitted one dataset with all of her files (over 700, at varying resolutions). The data contain rasters and shapefiles depicting vessel paths in the North Pacific at many different resolutions. Upon review, the submitter and curation team together determined that it would be best if other users could download the data at specific resolution scales. As a solution, we created separate datasets according to resolution and data type. With the addition of the data portal, the user was able to house their related datasets at a single URL, which allows for search and discovery within only the datasets for that project.\n\n\n\n\n\n\nNote\n\n\n\nTo find out more about portals, visit https://arcticdata.io/data-portals/. You can view our catalog of portals here. We recommend reviewing the Distributed Biological Observatory’s portal to see what a more fleshed out portal can look like.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis example also illustrates that users can create multiple datasets for a single NSF award. It’s understandable that you may think you have to submit all your data for a given award under one dataset. However, we think breaking up data into separate datasets (in logical, structured ways) increases the discoverability and usability of the data. This is especially true when users also create a data portal for their project.\nWhen uploading your data files using SFTP, either through the command line or a GUI, we recommend creating folders with the same name as your dataset’s title so that it is easier to associate the data with their associated metadata submission.\nFor more complex folder structures, it is wise to include a README file that explicitly walks users through the structure and provides a basic understanding of what is available. Generally speaking, we recommend structuring your data in an easy to understand way such that a README isn’t completely necessary.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Documenting and Publishing Data</span>"
    ]
  },
  {
    "objectID": "sections/adc-data-publishing.html#data-transfer-tools",
    "href": "sections/adc-data-publishing.html#data-transfer-tools",
    "title": "13  Documenting and Publishing Data",
    "section": "13.7 Data transfer tools",
    "text": "13.7 Data transfer tools\nNow that we’ve talked about what types of large datasets you might have that need to get published on the Arctic Data Center, let’s discuss how to actually get the data there. If you have even on the order of only 50GB, or more than 500 files, it will likely be more expedient for you to transfer your files via a command line tool than uploading them via our webform. So you know that you need to move a lot of data, how are you going to do it? More importantly, how can you do it in an efficient way?\nThere are three key elements to data transfer efficiency:\n\nendpoints\nnetwork\ntransfer tool\n\n\nEndpoints\nThe from and to locations of the transfer, an endpoint is a remote computing device that can communicate back and forth with the network to which it is connected. The speed with which an endpoint can communicate with the network varies depending on how it is configured. Performance depends on the CPU, RAM, OS, and disk configuration. One key factor that affects data transfer speed is how quickly that machine can write data to disk. Slow write speeds will throttle a data transfer on even the fastest internet connection with the most streamlined transfer tool. Examples of endpoints could be:\n\nNCEAS included-crab server\nYour standard laptop\nA cloud service like AWS\n\n\n\nNetwork speed\nNetwork speed determines how quickly information can be sent between endpoints. It is largely, but not enitrely, dependent on what you pay for. Importantly, not all networks are created equal, even if they nominally have the same speed capability. Wired networks get significantly more speed than wireless. Networks with lots of “stuff” along the pipe (like switches or firewalls) can perform worse than those that don’t. Even the length and type of network cabling used can matter.\n\n\nTransfer tools\nPoll: what data transfer tools do you use regularly?\nFinally, the tool or software that you use to transfer data can also significantly affect your transfer speed. There are a lot of tools out there that can move data around, both GUI driven and command line. We’ll discuss a few here, and their pros and cons.\n\nscp\nscp or secure copy uses ssh for authentication and transfer, and it is included with both unix and linux. It requires no setup (unless you are on a Windows machine and need to install), and if you can ssh to a server, you can probably use scp to move files without any other setup. scp copies all files linearly and simply. If a transfer fails in the middle, it is difficult to know exactly what files didn’t make it, so you might have to start the whole thing over and re-transfer all the files. This, obviously, would not be ideal for large data transfers. For a file or two, scp is a fine tool to use.\n\n\nrsync\nrsync is similar to scp, but syncs files/directories as opposed to copying. This means that rsync checks the destination to see if that file (with the same size and modified date) already exists. If it does, rsync will skip the file. This means that if an rsync transfer fails, it can be restarted again and will pick up where it left off, essentially. Neat!\n\n\nGlobus\nGlobus is a software that uses multiple network sockets simultaneously on endpoints, such that data transfers can run in parallel. As you can imagine, that parallelization can dramatically speed up data transfers. Globus, like rsync can also fail gracefully, and even restart itself. Globus does require that each endpoint be configured as a Globus node, which is more setup than is required of either scp or rsync. Many instituions computing resources may have endpoints already configured as Globus endpoints, so it is always worth checking in with any existing resources that might already be set up before setting up your own. Although Globus is a free software, there are paid options which provide support for configuring your local workstation as a Globus node. Globus is a fantastic tool, but remember the other two factors controlling data transfer, it can only help so much in overcoming slow network or write speeds.\n\n\n\n13.7.0.1 AWS sync\nAmazon Web Services (AWS) has a Command Line Interface (CLI) that includes a sync utility. This works much like rsync does in that it only copies new or updated files to the destination. The difference, of course, is that AWS sync is specifically built to work with interacting with the AWS cloud, and is compatible with S3 buckets.\n\n\n13.7.0.2 nc\nnc (or netcat) is a low level file transfer utility that is extremely efficient when moving files around on nodes in a cluster. It is not the easiest of these tools to use, however, in certain situations it might be the best option because it has the least overhead, and therefore can run extremely efficiently.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Documenting and Publishing Data</span>"
    ]
  },
  {
    "objectID": "sections/adc-data-publishing.html#summary",
    "href": "sections/adc-data-publishing.html#summary",
    "title": "13  Documenting and Publishing Data",
    "section": "13.8 Summary",
    "text": "13.8 Summary\nIn this lesson we learned about metadata and the structure of a data package; how to submit narrative metadata; how to upload data and when to use different services; how to document file and attribute-level metadata; and best practices for data organization and management. As mentioned earlier, more in-depth information on uploading data to our servers using Globus and RSync can be found in Session 18.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Documenting and Publishing Data</span>"
    ]
  },
  {
    "objectID": "sections/zarr.html",
    "href": "sections/zarr.html",
    "title": "14  Zarr for Cloud Data Storage",
    "section": "",
    "text": "14.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zarr for Cloud Data Storage</span>"
    ]
  },
  {
    "objectID": "sections/zarr.html#learning-objectives",
    "href": "sections/zarr.html#learning-objectives",
    "title": "14  Zarr for Cloud Data Storage",
    "section": "",
    "text": "Understand the principles of the Zarr file format\nRecognize the primary use case for Zarr\nLearn to access an example Zarr dataset from Google Cloud Storage",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zarr for Cloud Data Storage</span>"
    ]
  },
  {
    "objectID": "sections/zarr.html#introduction",
    "href": "sections/zarr.html#introduction",
    "title": "14  Zarr for Cloud Data Storage",
    "section": "14.2 Introduction",
    "text": "14.2 Introduction\nIn working with Earth systems data, it is common to have datasets that have multiple dimensions. Remember from the lesson on data structures for large data the diagram below, showing an Earth system data cube with 9 variables and 3 measurement dimensions. When we work with data like this, we need to make sure that we store it in a format that allows us to store the data efficiently, access it easily, and understand it correctly. The NetCDF format achieves all of these goals, and is widely used for multi-dimensional datasets. The NetCDF format does have some limitations though, and the increasing use of distributed computing in Earth science research makes the utility of a cloud-optimized NetCDF-like format clear.\n\n\n\nMahecha et al. 2020 . Visualization of the implemented Earth system data cube. The figure shows from the top left to bottom right the variables sensible heat (H), latent heat (LE), gross primary production (GPP), surface moisture (SM), land surface temperature (LST), air temperature (Tair), cloudiness (C), precipitation (P), and water vapour (V). The resolution in space is 0.25° and 8 d in time, and we are inspecting the time from May 2008 to May 2010; the spatial range is from 15° S to 60° N, and 10° E to 65° W.\n\n\nCloud computing is distributed - meaning that files and processes are spread around different pieces of hardware that may or may not be located in the same place. There is great power in this - it enables flexibility and scalability of systems - but there are challenges associated with it too. One of those challenges is data storage and access. Medium sized datasets, like we have seen so far, work great in a NetCDF file accessed by tools like xarray. What happens though, when datasets get extremely large, in the range of multiple terabytes or even petabytes. Storing a couple of TB of data in a single netCDF file is not practical in most cases - especially when you are trying to leverage distributed computing, where there are multiple places where files are stored. Of course, one could (and many have) artificially split that terabyte scale dataset into multiple files, such as daily observations. This is fine - but what if even the daily data is too large for a single file? What if the total volume of files is too large for one hard drive? How would you keep track of what file is where? The Zarr file format is meant to solve these problems and more, making it easier to access data on distributed systems.\n“Zarr is a file storage format for chunked, compressed, N-dimensional arrays based on an open-source specification.” If this sounds familiar - it should! Zarr shares many characteristics in terms of functionality and design with NetCDF. Below is a mapping of NetCDF and Zarr terms from the NASA earthdata wiki\n\n\n\nNetCDF Model\nZarr Model\n\n\n\n\nFile\nStore\n\n\nGroup\nGroup\n\n\nVariable\nArray\n\n\nAttribute\nUser Attribute\n\n\nDimension\n(Not supported natively)\n\n\n\nThe first thing to notice about this list is that the terms are very similar. NetCDF and Zarr share many characteristics, and when working with the data from those file types in python, the workflows are nearly identical. The first of the above listed terms, however, highlights the biggest difference between NetCDF and Zarr. While NetCDF is a file, the Zarr model instead is a “store.” Rather than storing all of the data in one file, Zarr stores data in a directory where each file is a chunk of data. Below is a diagram, also from the earthdata wiki, showing an example layout.\n\nIn this layout diagram, key elements of the Zarr specification are introduced.\n\nArray: a multi-dimensional, chunked dataset\nAttribute: ancillary data or metadata in key-value pairs\nGroup: a container for organizing multiple arrays or sub-groups within a hierarchical structure\nMetadata: key information enabling correct interpretation of stored data format, eg shape, chunk dimensions, compression, and fill value.\n\nThe chunked arrays and groups allow for easier distributed computing, and enable a high degree of parallelism. Because the chunks are in seperate files, you can run concurrent operations on different parts of the same dataset, and the dataset itself can exist on multiple nodes in a cluster or cloud computing configuration. Because Zarr is laid out as a store and not a series of self contained files (like NetCDF), it means the entire dataset can be accessed via a single remote URI. This not only can make analysis more streamlined, but it also means that you don’t have to store any part of the dataset locally. The Zarr format, when used with xarray, also allows for working with datasets too large to fit into local memory. A high degree of flexibility in compression and filtering schemes also means that chunks can be stored extremely efficiently.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zarr for Cloud Data Storage</span>"
    ]
  },
  {
    "objectID": "sections/zarr.html#using-zarr",
    "href": "sections/zarr.html#using-zarr",
    "title": "14  Zarr for Cloud Data Storage",
    "section": "14.3 Using Zarr",
    "text": "14.3 Using Zarr\nZarr is a file format, which has implementations in several languages. The primary implementation is in Python, but there is also support in Java and R. Here, we will look at an example of how to use the Zarr format by looking at some features of the zarr library and how Zarr files can be opened with xarray.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zarr for Cloud Data Storage</span>"
    ]
  },
  {
    "objectID": "sections/zarr.html#retrieving-cmip6-data-from-google-cloud",
    "href": "sections/zarr.html#retrieving-cmip6-data-from-google-cloud",
    "title": "14  Zarr for Cloud Data Storage",
    "section": "14.4 Retrieving CMIP6 Data from Google Cloud",
    "text": "14.4 Retrieving CMIP6 Data from Google Cloud\nHere, we will show how to uze Zarr and xarray to read in the CMIP6 climate model data from the World Climate Research Program, hosted by Googe Cloud. CMIP6 is a huge set of climate models, with around 100 models produced across 49 different groups. There is an enormous amount of data produced by these models, and in this demonstration we will load a tiny fraction of it. This demonstration is based off of an example put together by Pangeo\nFirst we’ll load some libraries. pandas, numpy, and xarray should all be familiar by now. We’ll also load in zarr, of course, and gcsfs, the Google Cloud Storage File System library, which enables access to Google Cloud datasets, including our CMIP6 example.\n\nimport pandas as pd\nimport numpy as np\nimport zarr\nimport xarray as xr\nimport gcsfs\n\nNext, we’ll read a csv file that contains information about all of the CMIP6 stores available on Google Cloud. You can find this URL to this csv file by navigating to the Google Cloud CMIP6 home page, clicking on “View Dataset,” and pressing the copy link button for the file in the table.\n\ndf = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\ndf.head()\n\n\n\n\n\n\n\n\n\nactivity_id\ninstitution_id\nsource_id\nexperiment_id\nmember_id\ntable_id\nvariable_id\ngrid_label\nzstore\ndcpp_init_year\nversion\n\n\n\n\n0\nHighResMIP\nCMCC\nCMCC-CM2-HR4\nhighresSST-present\nr1i1p1f1\nAmon\nps\ngn\ngs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...\nNaN\n20170706\n\n\n1\nHighResMIP\nCMCC\nCMCC-CM2-HR4\nhighresSST-present\nr1i1p1f1\nAmon\nrsds\ngn\ngs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...\nNaN\n20170706\n\n\n2\nHighResMIP\nCMCC\nCMCC-CM2-HR4\nhighresSST-present\nr1i1p1f1\nAmon\nrlus\ngn\ngs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...\nNaN\n20170706\n\n\n3\nHighResMIP\nCMCC\nCMCC-CM2-HR4\nhighresSST-present\nr1i1p1f1\nAmon\nrlds\ngn\ngs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...\nNaN\n20170706\n\n\n4\nHighResMIP\nCMCC\nCMCC-CM2-HR4\nhighresSST-present\nr1i1p1f1\nAmon\npsl\ngn\ngs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...\nNaN\n20170706\n\n\n\n\n\n\n\n\nCMIP6 is an extremely large collection of datasets, with their own terminology. We’ll be making a request based on the experiment id (scenario), table id (tables are organized roughly by themes), and variable id.\nFor this example, we’ll select data from a simulation of the recent past (historical) from the ocean daily (Oday) table, and select the sea surface height (tos) variable. We’ll also only select results from NOAA Geophysical Fluid Dynamics Laboratory (NOAA-GFDL) runs. We’ll do this by passing a logical statment with these criteria to the query method of our pandas data.frame.\n\ndf_ta = df.query(\"activity_id=='CMIP' & table_id == 'Oday' & variable_id == 'tos' & experiment_id == 'historical' & institution_id == 'NOAA-GFDL'\")\ndf_ta\n\n\n\n\n\n\n\n\n\nactivity_id\ninstitution_id\nsource_id\nexperiment_id\nmember_id\ntable_id\nvariable_id\ngrid_label\nzstore\ndcpp_init_year\nversion\n\n\n\n\n9445\nCMIP\nNOAA-GFDL\nGFDL-CM4\nhistorical\nr1i1p1f1\nOday\ntos\ngr\ngs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-CM4/histo...\nNaN\n20180701\n\n\n9446\nCMIP\nNOAA-GFDL\nGFDL-CM4\nhistorical\nr1i1p1f1\nOday\ntos\ngn\ngs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-CM4/histo...\nNaN\n20180701\n\n\n244828\nCMIP\nNOAA-GFDL\nGFDL-ESM4\nhistorical\nr1i1p1f1\nOday\ntos\ngn\ngs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/hist...\nNaN\n20190726\n\n\n244829\nCMIP\nNOAA-GFDL\nGFDL-ESM4\nhistorical\nr1i1p1f1\nOday\ntos\ngr\ngs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/hist...\nNaN\n20190726\n\n\n\n\n\n\n\n\nNote in the table above there is a column zstore. This gives the URI for the store described in each row of the dataset. Note there is also a version column indicating the date of that particular version. We will use both of these columns to select a single store of data.\nFirst, though, we need to set up a connection to the Google Cloud Storage file system (GCSFS). We do this using the gcsfs library to call the GCSFileSystem method using an anonymous token. Because this dataset is completely open, we do not have to authenticate at all to access it.\n\ngcs = gcsfs.GCSFileSystem(token='anon')\n\nNow, we’ll create a variable with the path to the most recent store from the table above (note the table is sorted by version, so we grab the store URL from the last row), and create a mapping interface to the store using the connection to the Google Cloud Storage system. This interface is what allows us to access the Zarr store with all of the data we are interested in.\n\nzstore = df_ta.zstore.values[-1]\nmapper = gcs.get_mapper(zstore)\n\nNex, we open the store using the xarray method open_zarr and examine its metadata. Note that the first argument to open_zarr can be a “MutableMapping where a Zarr Group has been stored” (what we are using here), or a path to a directory in file system (if a local Zarr store is being used).\n\nds = xr.open_zarr(mapper)\nds\n\nThis piece of data is around 30 GB total, and we made it accessible to our environment nearly instantly. Consider also that the total volume of CMIP6 data available on Google Cloud is in the neighborhood of a few petabytes, and it is all available at your fingertips using Zarr.\nFrom here, using the xarray indexing, we can easily grab a time slice and make a plot of the data.\n\nds.tos.sel(time='1999-01-01').squeeze().plot()\n\nWe can also get a timeseries slice, here on the equator in the Eastern Pacific.\n\nts = ds.tos.sel(lat = 0, lon = 272, method = \"nearest\")\n\nxarray allows us to do some pretty cool things, like calculate a rolling annual mean on our daily data. Again, because none of the data are loaded into memory, it is nearly instant.\n\nts_ann = ts.rolling(time = 365).mean()\n\nNote that if we explicitly want to load the data into memory, we can by using the load method.\n\nts.load()\n\n\nts_ann.load()\n\nFinally, we can make a simple plot of the daily temperatures, along with the rolling annual mean.\n\nts.plot(label = \"daily\")\nts_ann.plot(label = \"rolling annual mean\")\n\nAs an additional example, let’s look at some ocean nitrate and phosphorous data. These tasks take a little longer since the data have an additional dimension - ocean depth.\nFirst, let’s get the nitrate data by looking at the no3 variable.\n\nno3 = df.query(\"activity_id=='CMIP' & table_id == 'Omon' & experiment_id == 'historical' & institution_id == 'NOAA-GFDL' & variable_id == 'no3'\")\nds_no3 = xr.open_zarr(gcs.get_mapper(no3.zstore.values[-1]), consolidated=True)\n\nWe’ll also get the po4 variable.\n\npo4 = df.query(\"activity_id=='CMIP' & table_id == 'Omon' & experiment_id == 'historical' & institution_id == 'NOAA-GFDL' & variable_id == 'po4'\")\nds_po4 = xr.open_zarr(gcs.get_mapper(po4.zstore.values[-1]), consolidated=True)\n\nNow, use xarray methods to select data from the 2.5 level, and calculate the mean over time.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nno3_mean = ds_no3.no3.sel(lev = 2.5).mean('time').squeeze().load()po4_mean = ds_po4.po4.sel(lev = 2.5).mean('time').squeeze().load()\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(z_mean, t_mean)\n\nIn this simple demonstration we used less than 20 lines of code to establish access to a multi-petabyte climate dataset, extract a relevant variable, calculate a rolling average, and make two plots. The advent of cloud computing and cloud-native formats like Zarr are completely changing how we can do science. In their abstract for a talk on the process of storing these data on Google Cloud, Henderson and Abernathy (2020) give the motivation:\n\nAha! You have an awe-inspiring insight and can’t wait to share your results. Then an advisor/colleague/reviewer asks “But what do the CMIPx models say?”. In 2008, with the 35Tb of CMIP3 data, you could, perhaps, come up with an answer in a few days, collecting needed data for all available models, making the time and space uniform, checking units and running your analysis. (Henderson and Abernathey 2020)\n\nA few days seems optimistic, even, for sifting through 35TB of data. Imagine the process of finding, manually downloading, harmonizing, etc. many petabytes of CMIP6 data. The availability of these commonly used datasets via seamless tooling with the Pangeo universe of packages, has, to paraphrase the above abstract, has the potential to seriously accelerate earth and environmental science research.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zarr for Cloud Data Storage</span>"
    ]
  },
  {
    "objectID": "sections/zarr.html#additional-reference-creating-a-zarr-dataset",
    "href": "sections/zarr.html#additional-reference-creating-a-zarr-dataset",
    "title": "14  Zarr for Cloud Data Storage",
    "section": "14.5 Additional Reference: Creating a Zarr Dataset",
    "text": "14.5 Additional Reference: Creating a Zarr Dataset\nThis is a simplified version of the official Zarr tutorial.\nFirst, create an array of 10,000 rows and 10,000 columns, filled with zeros, divided into chunks where each chunk is 1,000 x 1,000.\n\nimport zarr\nimport numpy as np\nz = zarr.zeros((10000, 10000), chunks=(1000, 1000), dtype='i4')\nz\n\n&lt;zarr.core.Array (10000, 10000) int32&gt;\n\n\nThe array can be treated just like a numpy array, where you can assign values using indexing.\n\nz[0, :] = np.arange(10000)\nz[:]\n\narray([[   0,    1,    2, ..., 9997, 9998, 9999],\n       [   0,    0,    0, ...,    0,    0,    0],\n       [   0,    0,    0, ...,    0,    0,    0],\n       ...,\n       [   0,    0,    0, ...,    0,    0,    0],\n       [   0,    0,    0, ...,    0,    0,    0],\n       [   0,    0,    0, ...,    0,    0,    0]], dtype=int32)\n\n\nTo save the file, just use the save method.\n\nzarr.save('data/example.zarr', z)\n\nAnd open to open it again.\n\narr = zarr.open('data/example.zarr')\narr\n\n&lt;zarr.core.Array (10000, 10000) int32&gt;\n\n\nTo create groups in your store, use the create_group method after creating a root group. Here, we’ll create two groups, temp and precip.\n\nroot = zarr.group()\ntemp = root.create_group('temp')\nprecip = root.create_group('precip')\n\nYou can then create a dataset (array) within the group, again specifying the shape and chunks.\n\nt100 = temp.create_dataset('t100', shape=(10000, 10000), chunks=(1000, 1000), dtype='i4')\nt100\n\n&lt;zarr.core.Array '/temp/t100' (10000, 10000) int32&gt;\n\n\nGroups can easily be accessed by name and index.\n\nroot['temp']\nroot['temp/t100'][:, 3]\n\narray([0, 0, 0, ..., 0, 0, 0], dtype=int32)\n\n\nTo get a look at your overall dataset, the tree and info methods are helpful.\n\nroot.tree()\n\n\n\n\n\nroot.info\n\n\n\n\n\nName\n/\n\n\nType\nzarr.hierarchy.Group\n\n\nRead-only\nFalse\n\n\nStore type\nzarr.storage.MemoryStore\n\n\nNo. members\n2\n\n\nNo. arrays\n0\n\n\nNo. groups\n2\n\n\nGroups\nprecip, temp\n\n\n\n\n\n\n\n\n\n\nHenderson, N., and R. Abernathey. 2020. “CMIP6 in the Cloud - Why?”",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zarr for Cloud Data Storage</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html",
    "href": "sections/docker-containers.html",
    "title": "15  Docker Containers",
    "section": "",
    "text": "15.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#learning-objectives",
    "href": "sections/docker-containers.html#learning-objectives",
    "title": "15  Docker Containers",
    "section": "",
    "text": "What is docker anyway?\nThink about dependency management, reproducibility, and software\nBecome familiar with containers as a tool to improve computational reproducibility\nBuild and run docker containers to create reproducible python environments",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#just-what-is-a-container",
    "href": "sections/docker-containers.html#just-what-is-a-container",
    "title": "15  Docker Containers",
    "section": "15.2 Just what is a container?",
    "text": "15.2 Just what is a container?\nA container represents an instance of an image that can be run.\nAnd why might I want one? In short, as a very lightweight way to create a reproducible computing environment. Containers have some desirable properties:\n\nDeclarative: Containers are defined by a clear procedure making it possible to reliably rebuild an identical container in a variety of environments, promototing reproducibility\nPortable: Containers are designed to run on a container runtime which works identically across systems, so you can launch a container from a Mac, Linux, or Windows and get the same output every time\nLightweight: Containers contain only the exact files and data needed for a specific application, without all of the extra operating system baggage found in virtual machines\nScalable: Containers can be launched multiple times from their source image, and thus can be horizontally scaled across a compute cluster to parallelize operations\n\nFrom the following figure, one can see that a container is much more lightweight than both a bare metal server and a virtual machine, in that it contains only the files needed for a specific application.\n\nImages An image is a snapshot of a computing environment. It contains all of the files and data needed to execute a particular application or service, along with the instructions on which service should be run. But it is not executed per se. As a snapshot, an image represents a template that can be used to create one or more containers (each of which is an instantiation of the contents of the image). Images are also built using a layered file system, which allows multiple images to be layered together to create a composite that provides rich services without as much duplication.\nContainers A container represents an instance of an image that can be run. Containers are executed in a Container Runtime such as containerd or Docker Engine. Like virtual machines, containers provide mechanisms to create images that can be executed by a container runtime, and which provide stronger isolation among deployments. But they are also more lightweight, as the container only contains the libraries and executables needed to execute a target application, and not an entire guest operating system. This means that applications run with fewer resources, start up and shut down more quickly, and can be migrated easily to other hosts in a network.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\nThe Docker system was one of the first widespread implementations of containers, and its popularity drove much of the terminology around containers. After it’s inception, the software ecosystem around contaianers has been standardized and maintained by the Open Container Intitiative (OCI), which defines the image specification, runtime specification, and distribution specification followed by most vendors. So, we might often use the ‘docker’ terminology, but we are referring to OCI-compliant images and containers.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#installing-rancher-desktop",
    "href": "sections/docker-containers.html#installing-rancher-desktop",
    "title": "15  Docker Containers",
    "section": "15.3 Installing Rancher Desktop",
    "text": "15.3 Installing Rancher Desktop\n\n\n\n\n\n\nWorking with docker or containers requires a container runtime. Whether you’re on a Mac, Windows, or Linux machine, you can run containers through a variety of different runtime systems. One of the nicest lately is Rancher Desktop. Install the binary for your platform, and then after it starts, open the Preferences dialog to configure it.\n\n\n\n\n\n\nFirst, under the Virtual Machine tab, configure it to use a reasonable amount of your local machine’s resources, say about 50% of the CPUs and memory. Second, deselect “Kubernetes” to disable the kubernetes distribution, which takes up a lot of resources if you’re not using it.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are many different tools you can use with docker, including the docker client tool, and the containerd ecosystem using nerdctl as a client tool. Both the docker client command and the nerdctl command share the same command syntax.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#container-and-image-portability",
    "href": "sections/docker-containers.html#container-and-image-portability",
    "title": "15  Docker Containers",
    "section": "15.4 Container and Image portability",
    "text": "15.4 Container and Image portability\nImages are portable across container runtimes that share the same architecture (e.g., all ARM processors, or x86 processors). This makes it easy to launch and run a container by downloading an image for your architecture, and executing it in a container runtime like containerd or dockerd. While we will be working on your local Rancher Desktop instance, images you build could also be run on other hosts, such as a linux server, or on a cluster of servers, if they were built for that architecture and have containerd installed. Because the container runtime is typically run as the root or administrative user, many high-performance computing centers will only expose container runtimes such as Apptainer, which supports a more sophisticated security model, or Kubernetes, which creates a fully-featured orchestration system.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#hands-on-with-containers-and-docker",
    "href": "sections/docker-containers.html#hands-on-with-containers-and-docker",
    "title": "15  Docker Containers",
    "section": "15.5 Hands-on with Containers and Docker",
    "text": "15.5 Hands-on with Containers and Docker\nLet’s get started. At it’s simplest, you can use docker to run an image that somebody else created. Let’s do that with a simple Hello World app. Open a terminal (on a machine with docker installed), and run:\n$ docker run hello-world\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (arm64v8)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\nIn a nutshell, the docker run command will take an image from your local machine, and execute it as a container instance.\n\nYou can manage your local containers and images using the docker client. Start by listing your local images:\n\n$ docker image ls\nREPOSITORY                                          TAG       IMAGE ID       CREATED         SIZE\nhello-world                                         latest    ee301c921b8a   10 months ago   9.14kB\n\nAnd you can list the containers that are running. Because containers are somewhat analogous to processes, list them with:\n\n$ docker ps\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n\nThere is no output there, because no containers are currently running. You can see the containers that were run previously by adding the -a (all) option. Only long-running containers will generally be visible with docker ps without the -a flag.\n\n$ docker ps -a\nCONTAINER ID   IMAGE         COMMAND    CREATED          STATUS                      PORTS     NAMES\nf0a2c6c27af7   hello-world   \"/hello\"   12 minutes ago   Exited (0) 12 minutes ago             nervous_vaughan\n\nSo from that listing, we can see the hello-world image (with id f0a2c6c27af7) was run 12 minutes ago. We could run it again with either the container identifier (using docker start -i f0a2c6c27af7), or with the container name, which was assigned automatically (docker start -i nervous_vaughan). Using start, we are running the same container instance (f0a2c6c27af7) as previously, rather than creating a new one (which we could do by calling docker run again).\nOnce you are finished with a container you can remove the container, but note that the image will still be present locally.\n\n$ docker ps -a\nCONTAINER ID   IMAGE         COMMAND    CREATED          STATUS                     PORTS     NAMES\nf0a2c6c27af7   hello-world   \"/hello\"   22 minutes ago   Exited (0) 6 minutes ago             nervous_vaughan\n\n$ docker rm f0a2c6c27af7\nf0a2c6c27af7\n\n$ docker ps -a\nCONTAINER ID   IMAGE         COMMAND    CREATED      STATUS                  PORTS     NAMES\n\n$ docker image ls\nREPOSITORY                                          TAG       IMAGE ID       CREATED         SIZE\nhello-world                                         latest    ee301c921b8a   10 months ago   9.14kB\n\nSo the image is still on your local machine and could be run again to create a new container anytime.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#how-about-python",
    "href": "sections/docker-containers.html#how-about-python",
    "title": "15  Docker Containers",
    "section": "15.6 How about python?",
    "text": "15.6 How about python?\n\n\n\n\n\n\nWe’ve been using python 3.10, which is installed on the server. What if you wanted to run a newer version of python? It’s a command away with docker:\n\n\n\n\n\n\n\n$ docker run -it python:3.12\nUnable to find image 'python:3.12' locally\n3.12: Pulling from library/python\n6ee0baa58a3d: Pull complete \n992a857ef575: Pull complete \n3861a6536e4e: Pull complete \ne5e6faea05ea: Pull complete \n91c9495e7b5a: Pull complete \n9001688a971d: Pull complete \nad27ab4515af: Pull complete \nb152d3b07485: Pull complete \nDigest: sha256:336461f63f4eb1100e178d5acbfea3d1a5b2a53dea88aa0f9b8482d4d02e981c\nStatus: Downloaded newer image for python:3.12\n\nPython 3.12.2 (main, Mar 12 2024, 08:01:18) [GCC 12.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; print(\"Hello from python!\")\nHello from python!\n&gt;&gt;&gt; \n\nYou can even use this image to run commands at the command prompt. The -i interactive and -t terminal options allow you to run a command interactively with terminal output. Let’s run a quick command to print the python version:\n\n$ docker run -it --rm python:3.12 python -V\nPython 3.12.2\n\nand we could run a little python command-line calculation from the image:\n\n$ docker run -it --rm python:3.12 python -c 'print(f\"Four plus one is {4+1}\")'\nFour plus one is 5\n\nIf you were to run commands like this frequently, you’d build up a lot of container instances from your image, so instead we passed the --rm option to tell docker to delete the container (but not the image) when it is done executing each time. So, at times you’ll need to clean up. I’ve run the commands a few times interactively without remembering the --rm option, so now I can list and remove the containers that I don’t need hanging around any more.\n\n$ docker ps -a\nCONTAINER ID   IMAGE         COMMAND     CREATED             STATUS                           PORTS     NAMES\nd3f717354ed5   python:3.12   \"python3\"   About an hour ago   Exited (0) 59 minutes ago                  laughing_borg\ne882d6a89f2c   python:3.12   \"python3\"   About an hour ago   Exited (130) About an hour ago             interesting_elgamal\n9d984d596e4e   python:3.12   \"python3\"   About an hour ago   Exited (0) About an hour ago               adoring_jang\nb015e761fbab   hello-world   \"/hello\"    7 days ago          Exited (0) 7 days ago                      compassionate_wozniak\n1bf3a36a924c   hello-world   \"/hello\"    7 days ago          Exited (0) 7 days ago                      jolly_goldwasser\n\n$ docker rm laughing_borg interesting_elgamal adoring_jang compassionate_wozniak\nlaughing_borg\ninteresting_elgamal\nadoring_jang\ncompassionate_wozniak\n\n$ docker ps -a\nCONTAINER ID   IMAGE         COMMAND    CREATED      STATUS                  PORTS     NAMES\n1bf3a36a924c   hello-world   \"/hello\"   7 days ago   Exited (0) 7 days ago             jolly_goldwasser",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#exploring-image-registries",
    "href": "sections/docker-containers.html#exploring-image-registries",
    "title": "15  Docker Containers",
    "section": "15.7 Exploring image registries",
    "text": "15.7 Exploring image registries\nNow that version of python is available on your machine. When you list the image, you’ll see that the python version is provided as the tag.\n\n$ docker image ls\nREPOSITORY                                          TAG       IMAGE ID       CREATED         SIZE\npython                                              3.12      ff82813aa87b   6 weeks ago     1.02GB\nhello-world                                         latest    ee301c921b8a   10 months ago   9.14kB\n\nThe TAG associated with an image is typically used to indicate the version of the image. We downloaded this image from the DockerHub image registry. If you inspect the DockerHub python page, you’ll see that this is an ‘official’ image from the python project, and that it is one of many different images that can be downloaded. So, images are a fantastic way to be able to reliably return to a specific version of software that you might want to use.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nBe careful with images that you download from the internet – they might contain anything, including malware. Because you are running the code locally, you never know what mischief might be managed, and you probably don’t want to find out. So it is best to only run images from trusted sources. Generally, the ‘official’ images for projects are marked in image registries, and there are procedures to ensure they are from those trusted sources. But, it’s also easy to spoof something that looks official, so put on your tinfoil hat when running unfamiliar images.\n\n\n\nOf course, python is just the start. You can find images for just about any software you need, including machine learning libraries like TensorFlow (tensorflow/tensorflow), databases like Postresql (bitnami/postgresql), search systems like Solr (solr), and web servers like nginx (nginx). Take a few mintues to explore the thousands of applications available at these image registries:",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#custom-images-with-dockerfiles",
    "href": "sections/docker-containers.html#custom-images-with-dockerfiles",
    "title": "15  Docker Containers",
    "section": "15.8 Custom images with Dockerfiles",
    "text": "15.8 Custom images with Dockerfiles\nWhile pre-built images are highly useful, even more importantly, you can build your own custom images with the precise collection of software needed to run an analysis, a model, or visualize your data. This use of images to build custom containers creates portable and highly repeatible analyses because it allows us to declare the exact set of software needed in your computing environment. Basically, it allows you to take your pre-defined computing environment anywhere needed for execution, from your local laptop to the largest supercomputers in the world.\n\n\n\n\n\n\n\n\n\nImages can be built by declaring their contents in a Dockerfile, which is a simple text file containing instructions on how to build the image and what it contains. In general, you can customize an existing image by adding folders and files contianing software and data that you need. To create an image, you can start by creating a Dockerfile that contains a FROM directive listing an existing image you want to build from. In our case, let’s start by building an image based on the stock Ubuntu operating system.\n\n\n\n# Dockerfile for ADC Scalable Computing Course\nFROM ubuntu:22.04\nIn that file, the FROM command is the first directive, and it indicates that we want to build an image that extends the existing ubuntu:22.04 image. So our image will start with everything that is present in Ubuntu. Technically, that is all we need to get started. Let’s build and run the image to see what’s there. We pass docker build the -t tag we’d like to use in the form of name:version, and the . to indicate that it should look for a Dockerfile in the current directory.\n\n$ docker build -t adccourse:0.1 .\n[+] Building 0.0s (5/5) FINISHED                                                                                                                                    docker:default\n =&gt; [internal] load .dockerignore                                                                                                                                             0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                                               0.0s\n =&gt; [internal] load build definition from Dockerfile                                                                                                                          0.0s\n =&gt; =&gt; transferring dockerfile: 191B                                                                                                                                          0.0s\n =&gt; [internal] load metadata for docker.io/library/ubuntu:22.04                                                                                                               0.0s\n =&gt; CACHED [1/1] FROM docker.io/library/ubuntu:22.04                                                                                                                          0.0s\n =&gt; exporting to image                                                                                                                                                        0.0s\n =&gt; =&gt; exporting layers                                                                                                                                                       0.0s\n =&gt; =&gt; writing image sha256:7dbc68b81268657c977e24cfe712e6907f1cb76c08aaf6397f94a3149db11069                                                                                  0.0s\n =&gt; =&gt; naming to docker.io/library/adccourse:0.1                                                                                                                              0.0s\n\n$ docker run -it adccourse:0.1\nroot@76529c3b3358:/# pwd\n/\nroot@76529c3b3358:/# ls\nbin  boot  dev  etc  home  lib  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\nroot@76529c3b3358:/# exit\n\n🎉 We have a working image, declared from our Dockerfile! 🎉\n\n15.8.1 Adding software\nNow let’s extend it to add some software we might need. For example, let’s add python and some utilities. First, we’ll use the SHELL directive to indicate that we want all future commmands from the Dockerfile to be run using the Bash shell, and the WORKDIR directive to set up our working directory to a better location than /. In this case, we will be building an application for the scalable computing (scomp) course, so we’ll put our working files in a typical linux HOME directory location, /home/scomp.\nThe RUN directive can be used to run any shell command that is needed to modify the image. In this example, we will use it to run the apt update and apt install commands to install the python package and some standard utilities on the system. Note how we use the && operator to combine two bash commands into a single RUN invocation. When using apt in an image, you typically need to run apt update first to get the full list of software package sources that can be installed.\n# Dockerfile for ADC Scalable Computing Course\nFROM ubuntu:22.04\nSHELL [\"/bin/bash\", \"-c\"]\nWORKDIR /home/scomp\nRUN apt update && apt -y install python3 pip virtualenvwrapper vim nano iproute2 tree\nBuild it again with docker build -t adccourse:0.2 ., and then let’s inspect it by running docker run -it adccourse:0.2. Note now that our working directory is /home/scomp, and you can now run the python3 command, which shows we have python 3.10 installed:\n$ docker run -it adccourse:0.2\nroot@e9566a19e24b:/home/scomp# pwd\n/home/scomp\nroot@e9566a19e24b:/home/scomp# python3 -V\nPython 3.10.12\nroot@e9566a19e24b:/home/scomp# exit\nWhen we’’ve been running these containers, we have not been using the --rm flag from before. So, you’ll note that they are building up in our container list. We can clean them up with docker rm as we don’t really intend to reuse these containers as we work through building up our image. And let’s use --rm for future runs to prevent this from happening.\n\n$ docker ps -a\nCONTAINER ID   IMAGE           COMMAND       CREATED          STATUS                            PORTS     NAMES\ne9566a19e24b   adccourse:0.2   \"/bin/bash\"   3 minutes ago    Exited (127) About a minute ago             affectionate_swanson\nd493605cfb60   adccourse:0.2   \"/bin/bash\"   5 minutes ago    Exited (0) 4 minutes ago                    sharp_elbakyan\na9ab223669ac   adccourse:0.2   \"/bin/bash\"   5 minutes ago    Exited (127) 5 minutes ago                  wonderful_chaum\n76529c3b3358   adccourse:0.1   \"/bin/bash\"   22 minutes ago   Exited (0) 8 minutes ago                    heuristic_nash\n1bf3a36a924c   hello-world     \"/hello\"      8 days ago       Exited (0) 8 days ago                       jolly_goldwasser\n\n$ docker rm e9566a19e24b d493605cfb60 a9ab223669ac 76529c3b3358\ne9566a19e24b\nd493605cfb60\na9ab223669ac\n76529c3b3358\n\n\n\n15.8.2 Add a user account\nWhen we ran our images previously, we noted that the image was running as the root user, and the default working directory was /. By setting WORKDIR, we are now working within the /home/scomp directory, but still as the user root. Best practice would be to create a dedicated user account that doesn’t have the administrative priveledges of root. So, we’ll create an account scomp and group scomp that will own all of the files we create and will run our processes.\n# Dockerfile for ADC Scalable Computing Course\nFROM ubuntu:22.04\nSHELL [\"/bin/bash\", \"-c\"]\nWORKDIR /home/scomp\nRUN apt update && apt -y install python3 pip virtualenvwrapper vim nano iproute2 tree\nRUN groupadd -r scomp && useradd -r -g scomp scomp\nRUN mkdir -p /home/scomp && chown scomp.scomp /home/scomp\nRUN mkdir -p /var/data && chown scomp.scomp /var/data\nUSER scomp:scomp\nRebuild the image (docker build -t adccourse:0.3 .) and run it, and you’ll see that we are now running as the scomp user in the /home/scomp directory. This time, when we run it, we’ll also use the -h hostname option to create a bit more readable hostname, rather than the container identifier.\n$ docker run -it --rm -h adccourse adccourse:0.3\nscomp@adccourse:~$ pwd\n/home/scomp\nscomp@adccourse:~$ whoami\nscomp\nscomp@adccourse:~$ exit\n\n\n15.8.3 Add a python venv\nNow that we have a working image with python installed, lets configure the image to create a standardized python virtual environment with the python packages that we’ll need in our application. Start by creating a requirements.txt file in the same directory as your Dockerfile, with the list of packages needed. We’ll start with just two, xarray and matplotlib.\n❯ cat requirements.txt\nxarray==2024.2.0\nmatplotlib==3.8.3\nTo create the virtualenv, we will need to first configure virtualenvwrapper, and then COPY the requirements.txt file into the image, and then finally make the virtual environment inside the image using mkvirtualenv, pip, and workon. We’ll go through these in more detail after we build the image. Let’s build it, this time tagging it as version 1.0.\n# Dockerfile for ADC Scalable Computing Course\n# Build with:\n#     docker build -t adccourse:1.0 .\nFROM ubuntu:22.04\nSHELL [\"/bin/bash\", \"-c\"]\nWORKDIR /home/scomp\nRUN apt update && apt -y install python3 pip virtualenvwrapper vim nano iproute2 tree\nRUN groupadd -r scomp && useradd -r -g scomp scomp\nRUN mkdir -p /home/scomp && chown scomp.scomp /home/scomp\nUSER scomp:scomp\nRUN echo \"source /usr/share/virtualenvwrapper/virtualenvwrapper.sh\" &gt;&gt; /home/scomp/.bashrc\nCOPY ./requirements.txt .\nRUN source /usr/share/virtualenvwrapper/virtualenvwrapper.sh && \\\n        mkvirtualenv scomp && \\\n        pip install --no-cache-dir --upgrade -r requirements.txt && \\\n        echo \"workon scomp\" &gt;&gt; /home/scomp/.bashrc\nThe COPY directive is used to copy a file from the local machine into our image. In this case, we are copying the requirements.txt file that we created into the current directory (.), which in this case refers to the WORKDIR in the image (i.e., /home/scomp). We’ll use this in the next step.\nThe final RUN directive sets up oour virtual environment by doing the following:\n\nruns source to load the virtualenvwrapper commands for use during this build\nruns mkvirtualenv to create a new virtual environment titled scomp\nruns pip to install all of the packages from the requirements file\nappends workon scomp to our bash startup scripts so that the venv loads whenever the container is launched\n\n\n\n\n\n\n\nNote\n\n\n\nLayers. Each of the directives in the Dockerfile builds a single image layer, and they are run in order. Each layer is registered using the sha256 identifier for the layer, which enables layers to be cached. Thus, if you already have a layer with a given hash built or pulled into your local registry, then subsequent docker build commands can reuse that layer, rather than building them from scratch. As a result, its best practice to put the layers that change infrequently at the top of the Dockerfile, and layers that might change more frequently (such as application-specfiic commands) near the bottom. This will speed things up by maximizing the use of CACHED layers, which can be seen in the output of docker build.\n❯ docker build -t adccourse:1.0 .\n[+] Building 24.7s (13/13) FINISHED                                                                                                                docker:default\n =&gt; [internal] load build definition from Dockerfile                                                                                                         0.0s\n =&gt; =&gt; transferring dockerfile: 787B                                                                                                                         0.0s\n =&gt; [internal] load .dockerignore                                                                                                                            0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                              0.0s\n =&gt; [internal] load metadata for docker.io/library/ubuntu:22.04                                                                                              0.0s\n =&gt; [1/8] FROM docker.io/library/ubuntu:22.04                                                                                                                0.0s\n =&gt; [internal] load build context                                                                                                                            0.0s\n =&gt; =&gt; transferring context: 115B                                                                                                                            0.0s\n =&gt; CACHED [2/8] WORKDIR /home/scomp                                                                                                                         0.0s\n =&gt; CACHED [3/8] RUN apt update && apt -y install python3 pip virtualenvwrapper vim nano iproute2 tree                                                       0.0s\n =&gt; CACHED [4/8] RUN groupadd -r scomp && useradd -r -g scomp scomp                                                                                          0.0s\n =&gt; CACHED [5/8] RUN mkdir -p /home/scomp && chown scomp.scomp /home/scomp                                                                                   0.0s\n =&gt; CACHED [6/8] RUN echo \"source /usr/share/virtualenvwrapper/virtualenvwrapper.sh\" &gt;&gt; /home/scomp/.bashrc                                                  0.0s\n =&gt; [7/8] COPY ./requirements.txt .                                                                                                                          0.0s\n =&gt; [8/8] RUN source /usr/share/virtualenvwrapper/virtualenvwrapper.sh &&         mkvirtualenv scomp &&         pip install --no-cache-dir --upgrade -r re  23.7s\n =&gt; exporting to image                                                                                                                                       0.9s\n =&gt; =&gt; exporting layers                                                                                                                                      0.9s\n =&gt; =&gt; writing image sha256:a45927dfe598cb5397195ada7168743f80417faf04f14745f6d66ac352614501                                                                 0.0s\n =&gt; =&gt; naming to docker.io/library/adccourse:1.0                                                                                                             0.0s                                                                                           0.0s\n\n\nWhen we run this image, we’ll now see that the scomp virtualenvironment was activated, and that xarray can be imported in the python3 environment. By being extremely explicit about the software being installed in the Dockerfile image, we can ensure that the environment we’ve built is highly portable and reproducible.\n$ docker run -it --rm -h adccourse adccourse:1.0\n(scomp) scomp@adccourse:~$ python3\nPython 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import xarray\n&gt;&gt;&gt;\n(scomp) scomp@adccourse:~$ exit\nIn summary, a Dockerfile is a mechanism to declare a complete computing environment in a single text file. Using a Dockerfile makes it straightforward to reliably rebuild the specified environment, installing the precise software and data needed in a lightweight image. You can base images on a huge variety of existing software in the open source ecosystem. Docker images are immutable, and can be run in parallel in multiple containers. In the next section, we start to explore how to get data in and out of these immutable containers.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#anatomy-of-image-layers",
    "href": "sections/docker-containers.html#anatomy-of-image-layers",
    "title": "15  Docker Containers",
    "section": "15.9 Anatomy of image layers",
    "text": "15.9 Anatomy of image layers\nContainers are ephemeral. Any data they create is lost when the container is deleted, unless you go to extra steps to preserve it. This is because the container is actually composed of a set of layers, each of which contains a filesystem with a set of files that gets merged with the files from the other layers to produce a union filesystem that contains all of the files. Each of the layers in an image is layered on top of the one before it, and so higher layers in the stack take precedence if they contain files at identical paths. Each of these layers from the image is read-only, but a top-most read/write layer is set up to allow container-specific changes, giving the appearance that the whole filesytem is writable.\n\n\n\nOverlay images, from https://docs.docker.com/storage/storagedriver/\n\n\nOne of the benefits of this layering system is that each layer is immutable and cached, so running many copies of a container each uses the same set of cached layers, adding only the thin read/write layers at the top.\nSo, how do we get data out of our containers?",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#volume-mounts",
    "href": "sections/docker-containers.html#volume-mounts",
    "title": "15  Docker Containers",
    "section": "15.10 Volume mounts",
    "text": "15.10 Volume mounts\nOne approach to getting data out of a container is to mount one or more storage volumes from either the local host or from an accessible network into the container. You can think of a volume like a USB flash drive – when I plug it into one machine, it gets mounted there at a particular path, but if I plug it into a different machine, it might get mounted in a different location. But it still contains the same data and files.\nWhile there are many types of volumes that can be mounted, the simplest is a folder on the local machine that is mounted to be accessible within the container. Docker makes it easy to mount a folder by specifying which folder to mount and the path that you want it mounted at in the container. For example, from a server I might take a folder from my local home directory (e.g., /home/jones/adc-course) and mount it in another location in a docker container at (e.g., /var/data).\nYou control the permissions on the files and folders that you mount, which determines whether the processes you run in a script can read from or write to the volume. Permission issues can easily arise when you first start using mounted volumes, but just a small amount of tweaking of your Dockerfile can set the permissions straight.\n\n15.10.1 Setup a local directory to preserve data\nThe first step is to create a host directory on the local machine that contains files and data that we want to mount. To start, create a directory ~/adc-course in your HOME directory, and create input, output, and scripts as children.\nWe’ll be using an sample river flow discharge data from the Arcctic Data Center:\n\nThe Arctic Great Rivers Observatory. (2024). Arctic Great Rivers Observatory Biogeochemistry and Discharge Data, North America and Siberia, 2003 - 2024. Arctic Data Center. doi:10.18739/A2BC3SZ84.\n\nCopy the discharge-data.csv file into the input directory you created, and copy the sample analysis script max-discharge.py into the scripts directory.\n$ mkdir ~/adc-course\n$ mkdir ~/adc-course/input\n$ mkdir ~/adc-course/output\n$ mkdir ~/adc-course/scripts\n$ cp discharge-data.csv ~/adc-course/input/    # From doi:10.18739/A2BC3SZ84\n$ cp max-discharge.py ~/adc-course/scripts/\n$ chmod -R go+rwX ~/adc-course\n$ tree ~/adc-course\n/home/jones/adc-course\n|-- input\n|   └-- discharge-data.csv\n|-- output\n└-- scripts\n    └-- max-discharge.py\n\n\n15.10.2 Run the container and mount the volume\nWhen running a docker container, use the -v option to mount a volume. For a host folder, this takes the form -v /path/to/local/dir:/path/in/container. For example, we might want to mount the host directory that we created above from ~/adc-course/ into a directory in the container at /var/data, as follows:\n❯ docker run -it --rm -h adccourse -v ~/adc-course:/var/data adccourse:1.0\n\n(scomp) scomp@adccourse:~$ tree /var/data\n/var/data\n|-- input\n|   └-- discharge-data.csv\n|-- output\n└-- scripts\n    └-- max-discharge.py\nNote how the directory has the same structure and contents, but is located in a different path from the root (/var/data). Notice also that, because we set up our docker image with our scomp python virtual environment, it is active when we log in interactively. So we can easily launch our python script using that venv, which writes its output plot to /var/data/output.\n(scomp) scomp@adccourse:~$ python /var/data/scripts/max-discharge.py\n/var/data/scripts/max-discharge.py:6: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n  discharge = pd.read_csv(\"/var/data/input/discharge-data.csv\")\n(scomp) scomp@adccourse:~$ tree /var/data\n/var/data\n|-- input\n|   └-- discharge-data.csv\n|-- output\n|   └-- max-discharge.png\n└-- scripts\n    └-- max-discharge.py\n\n3 directories, 3 files\n(scomp) scomp@adccourse:~$ exit\nexit\n\n\n\n\n\n\n\n\n\nOur analytical script created a summary plot of maximum discharge volume for three rivers from the Arctic Rivers dataset. Because we built a completely repeatable image from a Dockerfile, creating this analysis environment is highly repeatable. Because the image that is produced contains a deterministic virtual environment that can be run on many hosts, it is both portable and scalable. And because the images are built from immutable, cacheable layers, the whole system is lightweight, even when running many instances of the container. We can wrap up by showing that the analysis is completely reproducible by running the whole thing from a single command:\n\n\n\n$ docker run -it --rm -h adccourse -v ~/adc-course:/var/data adccourse:1.0 bash -i -c \"workon scomp && python /var/data/scripts/max-discharge.py\"\n/var/data/scripts/max-discharge.py:6: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n  discharge = pd.read_csv(\"/var/data/input/discharge-data.csv\")\n$ tree ~/adc-course\n/home/jones/adc-course\n|-- input\n|   └-- discharge-data.csv\n|-- output\n|   └-- max-discharge.png\n└-- scripts\n    └-- max-discharge.py\nFinally, you can publish your Dockerfile along with your input data, output data, and scripts to create a fully reproducible data package. We encourage this type of fully reproducible package at the Arctic Data Center, and can help with the details of adding such artifacts to your packages.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#sharing-images-to-registries",
    "href": "sections/docker-containers.html#sharing-images-to-registries",
    "title": "15  Docker Containers",
    "section": "15.11 Sharing images to registries",
    "text": "15.11 Sharing images to registries\n\n\n\n\n\n\nIn addition to archiving your source code and Dockerfile at an archival repository, you may also find it helpful to publish your built images to an image registry such as Dockerhub, ArtifactHub, or GitHub Packages (formerly GitHub Container Registry (ghcr.io)). You might want to do this to publicly share the image so that others can run your image, or so that you can deploy the image on many nodes of a cluster, or simply so that you can access it without rebuilding it for future analytical runs.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#container-lifecycle",
    "href": "sections/docker-containers.html#container-lifecycle",
    "title": "15  Docker Containers",
    "section": "15.12 Container lifecycle",
    "text": "15.12 Container lifecycle\nPutting it all together, you can use the docker client tool to build an image from a Dockerfile, then run it as a container, and you can push that image to an image registry, or pull one down that you want to run on a particular machine.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/docker-containers.html#references",
    "href": "sections/docker-containers.html#references",
    "title": "15  Docker Containers",
    "section": "15.13 References",
    "text": "15.13 References\n\nDockerfile reference\nOpen Container Initiative Image Specification",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Docker Containers</span>"
    ]
  },
  {
    "objectID": "sections/software-design-2.html",
    "href": "sections/software-design-2.html",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "",
    "text": "16.1 Learning objectives",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Software Design II: Modules, Packages, and Testing</span>"
    ]
  },
  {
    "objectID": "sections/software-design-2.html#learning-objectives",
    "href": "sections/software-design-2.html#learning-objectives",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "",
    "text": "Python modules and packages\nManaging packages with poetry\nTesting: why, how, and when",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Software Design II: Modules, Packages, and Testing</span>"
    ]
  },
  {
    "objectID": "sections/software-design-2.html#python-modules-and-packages",
    "href": "sections/software-design-2.html#python-modules-and-packages",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.2 Python modules and packages",
    "text": "16.2 Python modules and packages\nThe Python Package Index currently houses over 400,000 software projects that you can install, reuse, and extend to accelerate your work. If you’ve worked in python for long at all, you have certainly used the import statement to to load modules that you need in your code into a namespace for use.\n\nimport numpy as np\nnp.pi*np.power(3, 2)\n\n28.274333882308138\n\n\nThis lesson is a first primer on building your own modules and packages in python, and provides some core tools to get started building your own packages. For a deeper dive, check out the Python Packages book, which is freely available online. First, a few definitions:\n\nModule\n\nA python module is a file containing commands that create functions, variables, and other definitions that can be reused in other code that you write. Each module in python creates names in its own namespace. Using import in python allows you to take a name from one module and be able to use it in another namespace.\n\nPackage\n\nA python package represents a way to bundle python modules, documentation, and related resources as reusable and sharable code, both for your own use and to collaborate with others.\n\n\nBuilding reusable code though modules and packages is key to the scientific reproducibility theme that weaves throughout this course. Creating your own modules and bundling them into packages will help you (and future you) in so many ways:\n\nto keep you organized, making it easier for you and others to understand and use your code\nto provide consistent approaches for documentation\nto easily import your code into your own projects in a portable way that promotes code reuse\nto formally document and manage software dependencies\nto share your software with colleagues aand the community",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Software Design II: Modules, Packages, and Testing</span>"
    ]
  },
  {
    "objectID": "sections/software-design-2.html#anatomy-of-modules-and-packages",
    "href": "sections/software-design-2.html#anatomy-of-modules-and-packages",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.3 Anatomy of modules and packages",
    "text": "16.3 Anatomy of modules and packages\nModules in python are loaded from the definitions stored in files of the same name with a .py file extension. Within each file are definitions of objects which can be accessed when the module is imported. For example, imagine you have a file named hello.py in your current directory that contains a function called helloworld with the following contents:\n\ndef helloworld():\n  \"\"\"\n  Print a hello message\n  \"\"\"\n  print(\"Hello world!\")\n\nWith that in place, you can then import and use that function in other code you write:\n\nimport helloworld\nhelloworld()\n\nPackages help organize these modules into a directory structure so that they can be predictably named and utilized. Packages can contain subpackages to keep modules organized into logical groups. Packages also provide structured metadata so that we can document the package and its dependencies so that it can be reliably installed and correctly used. The structure of a package is a simple directory tree containing the package modules and subpackages. Two common structures are supported. First, with modules in the root directory:\nadcmodel\n  ├── README.md\n  ├── adcmodel\n  │   ├── __init__.py\n  │   ├── affine.py\n  │   ├── data.py\n  │   └── reports\n  │       ├── __init__.py\n  │       └── pdf.py\n  └── tests\nAlternatively, the module code is often put in it’s own src directory, like:\nadcmodel\n  ├── README.md\n  ├── src\n  │   └── adcmodel\n  │       ├── __init__.py\n  │       ├── affine.py\n  │       ├── data.py\n  │       └── reports\n  │           ├── __init__.py\n  │           └── pdf.py\n  └── tests\nThe presence of the file __init__.py in each module directory indicates that that directory is a module. We won’t get into the details of this special file, but it is often used to load code to initialize a package.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Software Design II: Modules, Packages, and Testing</span>"
    ]
  },
  {
    "objectID": "sections/software-design-2.html#managing-packages-with-poetry",
    "href": "sections/software-design-2.html#managing-packages-with-poetry",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.4 Managing packages with poetry",
    "text": "16.4 Managing packages with poetry\n\n\n\n\n\nWhile this structure and organization can be created manually, tools like Python Poetry provide a simple way to create and manage packages and their dependencies. Poetry is but one of many tools for managing packages in the python ecosystem, but we think it is useful and straightforward to use, and it tames some of the complexity of the python packaging ecosystem.\nIn the rest of this tutorial we will:\n\nCreate a new virutal environment for our package\nSet up a new VS Code workspace to develop the package\nGet Poetry installed and create a skeleton package\nAdd some code to the package and run it\nTalk through testing strategies for the package\n\n\n\n\n\n\n\nSidebar on other package managers\n\n\n\n\n\nPoetry is a great packaging tool, especially for pure python packages that don’t include libraries in other languages or require more complex build configurations. In those situations, there are many other tools in the python ecosystem that are useful, including tools like Hatch and PDM. The pyOpenSci initiative has created a great flow chart to help guide the choice of packaging tools, especially if you need to create more complex artifacts like wheels.\n\n\n\nImage credit: pyOpenSci\n\n\n\n\n\n\n16.4.1 Get poetry in a new virtual environment\nTo get started, we’re using virtualenvwrapper to manage virtual environments in the course, so let’s be sure it is installed in your environment. We’re starting from the scalable-computing-examples workspace, so you should be in the scomp virtual environment in your terminal. So, make sure virtualenvwrapper is installed, and install it if it is not in scomp, using:\n$ pip install virtualenvwrapper\nThe example package we will create is called adcmodel, and we will create a virtual environment of the same name to manage its dependencies. We’ll also install poetry into that adcmodel virtual environment.\n$ cd ~\n$ mkvirtualenv -p python3.9 adcmodel\n$ pip install poetry\nOnce that is done, we’re going to return our current workspace to the scomp virtual environment (so that future lessons continue to work), and create a new workspace for our project work. To reset the venv, run the following from your terminal:\n$ workon scomp\n$ cd ~/scalable-computing-examples\nNow we have a virtual environment. Next we’ll use it to create our new package.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Software Design II: Modules, Packages, and Testing</span>"
    ]
  },
  {
    "objectID": "sections/software-design-2.html#setup-package-workspace",
    "href": "sections/software-design-2.html#setup-package-workspace",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.5 Setup package workspace",
    "text": "16.5 Setup package workspace\n\n16.5.1 Open VS Code window\nNow that we havea virtual environment set up, let’s create a new VS Code window, connect to `included-crab, and get our workspace set up.\n\nStart by opening a new VS Code window by connecting to included-crab using Command-Shift-P and then select “Connect to Host…” and choose included-crab.nceas.ucsb.edu\nOnce the window opens, open a new Terminal if one doesn’t open, and switch to our new adcmodel venv using:\n\nworkon adcmodel\nFinally, choose the Python version to use with Command-Shift-P, then choose Python: Select Interpreter, and choose the python version that lists your adcmodel virtual environment.\nWhen that is complete, your VS Code window should look like:\n\n\n\nVS Code remote with a virtual environment enabled.\n\n\n\n\n16.5.2 Create a new package\nNow we will create a skeleton for our package with poetry new, and take a look at what it produces:\n(adcmodel) jones@included-crab:~$ poetry new adcmodel\nCreated package adcmodel in adcmodel\n(adcmodel) jones@included-crab:~$ cd adcmodel/\n(adcmodel) jones@included-crab:~/adcmodel$ tree\n.\n├── README.md\n├── adcmodel\n│   └── __init__.py\n├── pyproject.toml\n└── tests\n    └── __init__.py\n\n2 directories, 4 files\nYou’ll see it created a README, an adcmodel directory for our source code (with the accompanying __init__.py to indicate this is a module), a metadata file defining our project metadata and dependencies, and a tests directory that we’ll come back to later when we explore testing.\n\n\n16.5.3 Setup the workspace\nTo make it easy to open up the project in VS Code, it is helpful to setup a workspace file, and configure VS Code to automatically activate our adcmodel virtual environment. First, let’s set up the workspace by opening the project folder and saving a workspace file:\n\nClick “Open Folder” to open the adcmodel folder and display it in the VS Code file exporer\nSelect “File | Save Workspace As…” to save the workspace configuration file\nNext, open the Terminal window and type workon adcmodel if it hasn’t already been done\nFinally, save a new settings file for our virtual environment:\n\ncreate a directory called .vscode using mkdir .vscode\nSave a new settings file as .vscode/settings.json with the following content\n\n\n{\n    \"python.terminal.activateEnvInCurrentTerminal\": true,\n    \"python.defaultInterpreterPath\": \"~/.virtualenvs/adcmodel/bin/python\"\n}\n\nClose the window, and open “New Window” under VS Code, then using Ctrl-R shortcut, open the recent adcmodel workspace on included-crab. If this all goes well, your session should be set to open up this adcmodel virtualenv each time that you open the workspace, and you should be able to see the contents of the skeleton package that you created.\n\nLet’s do some coding!",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Software Design II: Modules, Packages, and Testing</span>"
    ]
  },
  {
    "objectID": "sections/software-design-2.html#the-adcmodel-example-package",
    "href": "sections/software-design-2.html#the-adcmodel-example-package",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.6 The adcmodel example package",
    "text": "16.6 The adcmodel example package\nSo, imagine this adcmodel package is intended to hold the code for one of our research projects in which we need to load data from a variety of sources, handle quality analysis and data cleaning, and then integrate, analyze and visualize those data. While there are many useful and general functions you might want to build into a package such as this, we’re going to keep it simple and show how to build a single function that consistenly loads and caches CSV-formatted data from the Arctic Data Center based on the identifier of the data object.\nFor this, we’ll create a module called adcmodel.data that contains our data handling functions, one of which will be a load_adc_csv() function that takes an Arctic Data Center Identifier as input.\n\n16.6.1 Load software dependencies\nTo create this functionality, we’ll make use of existing data loading functions and data retrieval functions from the pandas package, and operating system utilities from the os package. Almost all packages and functions you write will make use of the existing ecosystem of python packages, and we need to keep track of those dependencies in our package.\nLuckily, Poetry provides simple command line utilities for adding, removing, and installing dependent packages into your virtual environment. For each package that you need, use poetry add to register that package in your dependencies list and install it in your adcmodel virtual environment. We’ll do this for pandas, but don’t need to register urllib or os because these base packages ship with python.\n\n\n\n\n\n\n\nThe pyproject.toml configuration file\n\n\n\nDependency metadata that is added to the project and other information about the project is found in the pyproject.toml configuration file that is managed by poetry. In this file you’ll find key metadata about the project like it’s name and description and authors, as well as the list of specific dependencies that the package needs installed to work correctly. You should open this file and edit the key metadata, but poetry add and poetry remove are probably better for adding and removing dependencies.\n[tool.poetry]\nname = \"adcmodel\"\nversion = \"0.1.0\"\ndescription = \"Arctic Data Center example package for scalable computing course\"\nauthors = [\"Matt Jones &lt;gitcode@magisa.org&gt;\"]\nreadme = \"README.md\"\n\n[tool.poetry.dependencies]\npython = \"^3.9\"\npandas = \"^1.5.0\"\n\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n\n\n\n16.6.2 Adding code to modules\nLet’s write some module code! In this case, we want to implement the adcmodel.data.load_adc_csv(identifier) function that we’ll use for data loading and caching. We do this by creating a new file called data.py in the adcmodel directory, and then implementing the function. Create a new python file called data.py and save it to the adcmodel module directory, with the following function implementation:\n\nimport pandas as pd\nimport urllib.request\nimport hashlib\nimport os\n\n\ndef load_adc_csv(identifier=None):\n    \"\"\"\n    Load and cache a CSV data file from the Arctic Data Center as a pandas.DataFrame\n    \"\"\"\n    hashval = hashlib.sha256(bytes(identifier, 'utf-8')).hexdigest()\n    path = \"data/\" + hashval\n    if not os.path.exists(\"data/\"):\n        os.mkdir(\"data/\")\n    if not os.path.exists(path):\n        service = \"https://arcticdata.io/metacat/d1/mn/v2/object/\"\n        url = service + identifier\n        msg = urllib.request.urlretrieve(url, path)\n    df = pd.read_csv(path)\n    return df\n\nOnce you save the file, we can use poetry install to install the package and try running the function in an interactive session. Check it out by opening a new Jupyter notebook and running the following in the first cell (after you install the needed jupyter packages):\n\nfrom adcmodel.data import load_adc_csv\n\ndf = load_adc_csv(\"urn:uuid:e248467d-e1f9-4a32-9e38-a9b4fb17cefb\")\ndf.head()\n\n🎉 Congratulations, you’ve created your first package! 🎉\n\n\n16.6.3 Add development packages\nUtilities like python linters and code formatters can make coding faster, more consistent, and more reliable. Poetry supports the use of development packages that are available in the virtual environment during the development cycle but are not required for runtime use of the package. Like regular dependencies, development packages can be added with poetry add, but we also pass in a qualifier to add it to --group dev, which marks it as only for use during development.\npoetry add --group dev pytest black\nYou can now use black to format your code (via Command-Shift-P Format Document) and pytest to run tests. Next, let’s get into testing.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Software Design II: Modules, Packages, and Testing</span>"
    ]
  },
  {
    "objectID": "sections/software-design-2.html#testing",
    "href": "sections/software-design-2.html#testing",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.7 Testing",
    "text": "16.7 Testing\nA major advantage of writing packages is that you can thoroughly test each of the functions that you write, thereby ensuring that they operate correctly both with good data and bad data. While it’s always helpful to confirm that a function returns the correct value within the bounds that it is normally meant to operate, it is also important to pass it data that it wouldn’t normally see. For example, does your linear model produce the correct results when it is passed a normal dataset? How about if it is passed a dataset with only one observation, or with all the same value, or with 0 or negative numbers? Does it produce an understandable error if some of the values are character values when they should be numeric? Does it continue operating in the face of bad data? Should it, or should it not produce an exception? These and others are the questions you need to ask when testing your packages.\nPython provides a number of robust testing frameworks, but we will work with pytest, one of the common ones used for unit testing. The poetry new command already set up our package to be ready for testing, so now all we need to do is to add some test files in the tests directory.\nIn pytest and other frameworks, the common pattern is to use the python assert function to determine whether a value produced from your code matches expectations. For example, given a function that produces a product of two numbers, we could assert that product(5, 6) == 30 and that product(-3, -2) == 6.\nGenerally, it is good practice to produce at least one test file for each of your package source files, and their names are prefixed with test_. So, for our data.py source file, we might expect to build tests in a file called test_data.py. Let’s create that file in the tests directory and walk through populating it with some simple tests. The functions in the test file are also named with test_ as the prefix on the function name, and generally each test function atomically tests one aspect of the code. That way, when a test function fails, it should be quickly clear what type of an error would produce that kind of failure, and how to locate it in the code.\nTo get started, create tests/test_data.py and populate it with our first test function:\n\nfrom adcmodel.data import load_adc_csv\n\ndef test_load_adc_csv():\n    df = load_adc_csv(\"urn:uuid:e248467d-e1f9-4a32-9e38-a9b4fb17cefb\")\n    assert df is not None\n\nYou can run the tests after saving the file by running pytest on the commandline, which will produce output indicating which tests passed and failed.\nWe can add other tests that test multiple conditions, and we can use any of the features of python to creatively test that our code will produce correct results. Here’s another test to check that the data file we loaded has the right number of rows and columns:\n\ndef test_load_adc_csv_shape():\n    df = load_adc_csv(\"urn:uuid:e248467d-e1f9-4a32-9e38-a9b4fb17cefb\")\n    assert df is not None\n    assert (df.shape[0] == 17856) & (df.shape[1] == 6)\n\n\nFinally, what happens when tests are run that fail? Try adding the following test, and determine if the failure is due to the code, the test, or the data? What would you change to make this test reliably pass, or fail more gracefully?\n\ndef test_load_adc_csv_inputs():\n    df = load_adc_csv()\n    assert df is not None\n\nThe answer to this and other more detailed questions on testing are covered in the excellent overview of testing provided by the “Chapter 5: Testing” of “Python Packages”.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Software Design II: Modules, Packages, and Testing</span>"
    ]
  },
  {
    "objectID": "sections/software-design-2.html#summary-and-next-steps",
    "href": "sections/software-design-2.html#summary-and-next-steps",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.8 Summary and next steps",
    "text": "16.8 Summary and next steps\nPython packages are an excellent mechanism to organize your modules for reuse both within your local code projects and with your colleagues and the broader science community. Adopting a structured approach to organizing, documenting, and testing your code can ultimately save you a lot of time and also increase the quality of your work. Scientific software is also a first-class product of the research enterprise, and is critical for fully understanding the lineage of derived data and research results. The emerging consensus is that publishing open source software packages to GitHub or other source code control systems, and then archiving them in repositories like Software Heritage, the Arctic Data Center, Zenodo, or the Python Package Index increases the robustness, transparency, and reproducibility of research results. And it also provides a robust starting point for research teams to build upon and amplify each other’s work. Using tools like Poetry and pytest to organize and build packages makes the process quick and efficient.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Software Design II: Modules, Packages, and Testing</span>"
    ]
  },
  {
    "objectID": "sections/software-design-2.html#further-reading",
    "href": "sections/software-design-2.html#further-reading",
    "title": "16  Software Design II: Modules, Packages, and Testing",
    "section": "16.9 Further reading",
    "text": "16.9 Further reading\n\nNamespacing with Python\npyOpenSci Python Package Guide\nPython Packages by Tomas Beuzen & Tiffany Timbers\nPython Poetry",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Software Design II: Modules, Packages, and Testing</span>"
    ]
  },
  {
    "objectID": "sections/docker-hpc-cloud.html",
    "href": "sections/docker-hpc-cloud.html",
    "title": "17  Container orchestration",
    "section": "",
    "text": "17.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Container orchestration</span>"
    ]
  },
  {
    "objectID": "sections/docker-hpc-cloud.html#learning-objectives",
    "href": "sections/docker-hpc-cloud.html#learning-objectives",
    "title": "17  Container orchestration",
    "section": "",
    "text": "Discuss containers in high performace computing and cloud computing\nExplore orchestration approaches\nLearn how to use docker compose to build a workflow\nExplore a real world Kubernetes service",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Container orchestration</span>"
    ]
  },
  {
    "objectID": "sections/docker-hpc-cloud.html#container-orchestration-systems",
    "href": "sections/docker-hpc-cloud.html#container-orchestration-systems",
    "title": "17  Container orchestration",
    "section": "17.2 Container orchestration systems",
    "text": "17.2 Container orchestration systems\nContainer orchestration is the process of linking multiple containers into an integrated application. These applications can represent a computational workflow, a backend service or data store, a frontend application, or some combination of these and others. For example, a simple application might consist of a backend database system in a container, which is accessed by containerized data processing code, and which is served by a containerized web application.\n\nAll of these types of integrated systems can be built by orchestrating the deployment of multiple containers, whether creating scientific applications for running big data jobs on high performance computers or building and deploying scientific web applications and services.\nLet’s explore a few container orchestration systems.\n\n17.2.1 docker compose\nDocker Compose is “a tool for defining and running multi-container applications”. This is the essence of orchestration. With just the compose application, you can link multiple containers into an integrated application to run it on demand. This is particularly useful to quickly bring up a backend and frontend application, say one that uses a database (like postgresql) or a caching server (like redis) and combine it with a web-based frontend.\nCreating a docker compose application is done by editing a compose.yaml configuration document that describes each of the containers to be run in the system, and how they are linked together. In the simplest “Hello world” case, you simply create the service you want to run by identifying the image to be executed. For example, given this compose.yaml, file:\nservices:\n  hello_world:\n    image: hello-world\nRun it with docker-compose up, and it produces the familiar hello world output.\n$ docker-compose up\n[+] Running 2/0\n ✔ Network 15-containers_default          Created                                                                                                            0.0s\n ✔ Container 15-containers-hello_world-1  Created                                                                                                            0.0s\nAttaching to hello_world-1\nhello_world-1  |\nhello_world-1  | Hello from Docker!\nhello_world-1  | This message shows that your installation appears to be working correctly.\nhello_world-1  |\nhello_world-1  | To generate this message, Docker took the following steps:\nhello_world-1  |  1. The Docker client contacted the Docker daemon.\nhello_world-1  |  2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\nhello_world-1  |     (arm64v8)\nhello_world-1  |  3. The Docker daemon created a new container from that image which runs the\nhello_world-1  |     executable that produces the output you are currently reading.\nhello_world-1  |  4. The Docker daemon streamed that output to the Docker client, which sent it\nhello_world-1  |     to your terminal.\nhello_world-1  |\nhello_world-1  | To try something more ambitious, you can run an Ubuntu container with:\nhello_world-1  |  $ docker run -it ubuntu bash\nhello_world-1  |\nhello_world-1  | Share images, automate workflows, and more with a free Docker ID:\nhello_world-1  |  https://hub.docker.com/\nhello_world-1  |\nhello_world-1  | For more examples and ideas, visit:\nhello_world-1  |  https://docs.docker.com/get-started/\nhello_world-1  |\nhello_world-1 exited with code 0\nThe big difference is that you can add multiple services that run containers in a compose file, and they will be launched and can communicate with one another. compose also handles some housekeeping for you, in that it cleans up containers when the application shuts down.\nLet’s build a simple web application with docker compose. First, create a directory to hold our website files, and initialize it with a simple web page.\n$ mkdir -p ~/adc-course/web\n$ echo '&lt;h1&gt;Scalable Computing Course&lt;/h1&gt;' &gt; ~/adc-course/web/index.html\nNext, create the compose.yaml, and use the stock nginx image from DockerHub to serve the file. We’ll throw the hello-world container in as well just to show two services can be run from the same application:\nservices:\n  web:\n    image: nginx\n    ports:\n      - 8000:80\n    volumes:\n      - ~/adc-course/web:/usr/share/nginx/html\n  hello_world:\n    image: hello-world\nFinally, run it with docker-compose up --detach:\n$ docker-compose up --detach\n[+] Running 8/8\n ✔ web 7 layers [⣿⣿⣿⣿⣿⣿⣿]      0B/0B      Pulled                                                                                                           304.3s\n   ✔ 59f5764b1f6d Pull complete                                                                                                                            251.0s\n   ✔ f7bd43626fa7 Pull complete                                                                                                                            288.1s\n   ✔ 2df415630b2f Pull complete                                                                                                                              5.3s\n   ✔ 059f9f6918db Pull complete                                                                                                                             10.5s\n   ✔ df91ff398a83 Pull complete                                                                                                                             15.9s\n   ✔ e75b854d63f1 Pull complete                                                                                                                             21.4s\n   ✔ 4b88df8a13cd Pull complete                                                                                                                             26.5s\n[+] Running 2/2\n ✔ Container 15-containers-web-1          Started                                                                                                            0.1s\n ✔ Container 15-containers-hello_world-1  Started                                                                                                            0.0s\nYour website should now be live on your local machine at https://localhost:8000. And while it is running, you can now see the executing containers on your host. Port 8000 on the local host is mapped to port 80 in the nginx container.\n\n$ docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                                   NAMES\n5d634a37ba09   nginx     \"/docker-entrypoint.…\"   5 seconds ago   Up 5 seconds   0.0.0.0:8000-&gt;80/tcp, :::8000-&gt;80/tcp   15-containers-web-1\n\nNote that only the nginx container is running. The hello-world container started up, printed its content, and immediately exited. Whereas the nginx image was designed to run the web server continuously until told to shut down. You can shut down the containers with docker-compose down:\n\n$ docker-compose down\n[+] Running 3/3\n ✔ Container 15-containers-web-1          Removed                                                                                                            0.5s\n ✔ Container 15-containers-hello_world-1  Removed                                                                                                            0.0s\n ✔ Network 15-containers_default          Removed                                                                                                            0.1s\n\n\n\n\n\n\n\nChallenge exercise\n\n\n\nUsing the adccourse:1.0 image that we created earlier, write a docker-compose application that:\n\nruns the max-discharge.py script to generate the max-discharge.png image showing discharge data\ncopies the png file to the web directory that you just set up\nmodifies the index.html file to display the plot on the web page\nstarts up your web application with docker-compose up --detach\n\nHint: You will likely want to use the depends_on keyword and the command keyword in your compose.yaml service description to modify the command that is run by the adccourse:1.0 container.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, modify our base index.html to display the dynamically generated image:\necho '&lt;img src=\"max-discharge.png\"&gt;' &gt;&gt; ~/adc-course/web/index.html\nSecond, add adccourse to the compose.yaml, and change the command to also copy the produced png file into the web directory.\nservices:\n  web:\n    image: nginx\n    ports:\n      - 8000:80\n    volumes:\n      - ~/adc-course/web:/usr/share/nginx/html\n    depends_on:\n      - adccourse\n  adccourse:\n    image: adccourse:1.0\n    volumes:\n      - ~/adc-course:/var/data\n    command: [ \"bash\", \"-i\", \"-c\", \"workon scomp && python /var/data/scripts/max-discharge.py && cp /var/data/output/max-discharge.png /var/data/web/max-discharge.png\" ]\nNote how we used depends_on to ensure the adccourse service produces the png file before the nginx service is started. Our web page now inlcudes the dynamically generated plot:\n\n\n\n\n\n\n17.2.2 Kubernetes\n\n\n\n\n\n\nKubernetes is fundamentally an orchestration system for executing containerized applications on the nodes of distributed computing clusters. Kubernetes creates fault tolerance, high availability, and scalability for applications deployed on a cluster.\n\n\n \n\n\n\n\n\n\nIt does so by creating a high availability control plane that manages all aspects of the definition, creation, and execution of containers on nodes of the cluster. Thus, if a container running on one node of a cluster fails, or loses network connectivity, the Kubernetes control plane brings the container back up on another node in the cluster.\nWhile the core concepts in Kubernetes are extensive and have been reviewed in some more comprehensive tutorials, a quick overview of the core concepts will show the parallels between Kubernetes and Docker Compose.\n\nSimilarly to compose, Kubernetes executes application containers based on OCI images, but these are bundled into Pods. A Pod in Kubernetes is the smallest unit of a workload that can be deployed on a node, and it consists of one or more containers that will be executed within the Pod. Containers within a Pod are tichtly linked and share network and other resources, making it easy for them to coordinate. Pods are deployed onto one or more Nodes in a Kubernetes cluster, where a Node corresponds to a physical or virtual server that is managed by Kubernetes. The Kubernetes control plane includes a scheduler that determines how to schedule the execution of Pods onto Nodes within a deployment, how to ensure that unhealthy Pods get terminated and restarted on other Nodes, and when the number of Pods needs to be increased or decreased to scale an application.\nLike other container systems, Kubernetes is configured through a set of YAML configuration files. While that is too complex for this bried introduction we can show how Kubernetes can be used by other orchestration frameworks.\n\n\n17.2.3 Parsl on Kubernetes\n\n\n\n\n\n\nParsl provides a simple mechanism to decorate python functions to be executed concurrently on a variety of platforms and under different execution models, inlcuding the ThreadPoolExecutor and the HighThroughputExecutor, which we used previously.\n\n\n\n\n\n\nRemember the basic layout of a parsl app, in which the @python_app decorator is used to wrap task functions that should be executed by parsl.\n# Define the square task.\nimport parsl\n@python_app\ndef square(x):\n    return x * x\nThis works because parsl is configured ahead of time to use a particular type of execution environment on the nodes of a cluster. The HighThroughPutExector that we used previously with a LocalProvider can instead be easily configured to work using a KubernetesProvider. Here’s a modification to our previous Config to use Kubernetes:\nactivate_env = 'workon scomp'\nhtex_kube = Config(\n    executors=[\n        HighThroughputExecutor(\n            label='kube-htex',\n            cores_per_worker=cores_per_worker,\n            max_workers=5,\n            worker_logdir_root='/',\n            # Address for the pod worker to connect back\n            address=address_by_route(),\n            provider=KubernetesProvider(\n\n                # Namespace in K8S to use for the run\n                namespace='adccourse',\n\n                # Docker image url to use for pods\n                image='ghcr.io/mbjones/k8sparsl:0.3',\n\n                # Command to be run upon pod start\n                 worker_init=activate_env,\n\n                # Should follow the Kubernetes naming rules\n                pod_name='parsl-worker',\n\n                nodes_per_block=1,\n                init_blocks=1,\n                min_blocks=1,\n                # Maximum number of pods to scale up\n                max_blocks=1,\n                # persistent_volumes (list[(str, str)]) – List of tuples\n                # describing persistent volumes to be mounted in the pod.\n                # The tuples consist of (PVC Name, Mount Directory).\n                # persistent_volumes=[('mypvc','/var/data')]\n            ),\n        ),\n    ]\n)\nWith that change, Parsl will send tasks to Kubernetes worker pods. Otherwise, the remaining parsl code is the smae as previously.\n\n\n17.2.4 Ray.io\n\n\n\n\n\n\nRay is structured similarly to Parsl, and also uses decorators to wrap task functions that are to be executed. Ray Core is the part that is fairly analogous to Parsl, and provides the core functionality for distributed execution for any kind of compute jobs.\n\n\n\n\n\n\n# Define the square task.\n@ray.remote\ndef square(x):\n    return x * x\n\n# Launch four parallel square tasks.\nfutures = [square.remote(i) for i in range(4)]\n\n# Retrieve results.\nprint(ray.get(futures))\n# -&gt; [0, 1, 4, 9]\nThe execution model also returns a Future-like object that can be queried to get the function results when it is complete.\nRay Core defines Tasks, Actors, and Objects, all of which can be used on ditributed clusters such as Kubernetes. And like Parsl, Ray can be configured to use a wide variety of execution backends such as Kubernetes. Ray also provides a mature framework for training, tuning, and serving machine learning models and the associated data.\n\n\n\n17.2.5 Kubeflow\n\n\n\n\n\n\nKubeflow is yet another orchestration package designed to asynchronously execute tasks from containers, but Kubeflow is specific to Kubernetes clusters.\n\n\n\n\n\n\nHere’s the syntax for defining a Kubeflow component and pipeline to be executed on worker pods of a Kubernetes cluster. The similarities with the previous packages are striking. But unlike Parsl and Ray, a workflow built in Kubeflow can’t be run on the rich variety of high performace computing clusters supported by the other libraries.\n# Kubeflow pipeline example\nfrom kfp import dsl\n\n@dsl.component\ndef say_hello(name: str) -&gt; str:\n    hello_text = f'Hello, {name}!'\n    print(hello_text)\n    return hello_text\n\n@dsl.pipeline\ndef hello_pipeline(recipient: str) -&gt; str:\n    hello_task = say_hello(name=recipient)\n    return hello_task.output",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Container orchestration</span>"
    ]
  },
  {
    "objectID": "sections/docker-hpc-cloud.html#how-weird-is-the-weather",
    "href": "sections/docker-hpc-cloud.html#how-weird-is-the-weather",
    "title": "17  Container orchestration",
    "section": "17.3 How weird is the weather?",
    "text": "17.3 How weird is the weather?\nBig data are hard to visualize, and sometimes we need to come up with clever ways to pack a lot of data into a small space. This is exactly what the How weird is the weather? project did (hwitw github), by summarizing the ERA5 climate reanalysis data and providing a means to visualize over 70 years of climate data interactively. This project, developed by Brentwood Higman, John McInnes, and Katmai McKittrick, shows where climate extremes have occurred over the ERA5 dataset. First, pick any location on the globe via the work-in-progress sandcastle demo:\n\nNext, configure the application to show the climate variables of interest. Here, I show the extremes of the average daily temperature where I live in Juneau, AK. The cold temperatures from the 1950s and 1960s haven’t been seen in decades, and now the hottest temperatures are constant throughout the summer.\n\n\n17.3.1 Kubernetes deployment\nLet’s examine the Kubernetes deployment. There are three major working in concert. Two CronJobs are configured, one to download data from Copernicus, and the other to transform that data into weekly summaries in a compact HDF format.\n\nEach of these CronJobs spawns a series of Pods that run the containers with the cdstool and tiletool processes. Once the data has been output by tiletool, it can be accessed on the web. Web client sessions are handled by the Ingress, and dispatched to the HWITW Service, which in turn is implemented by a Pod running the nginx web server with the hwitw frontend HTML and Javascript code.\nWe can inspect these components more carefully in Kubernetes using the kubectl commandline utility.\n\n$ kubectl get services\nNAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nservice/hwitw   ClusterIP   10.109.0.227   &lt;none&gt;        5000/TCP   407d\nThat service is deployed in a pod running the hwitw container as a web service. In the pod listing below, you can also see that there are pods that have completed running for periodic cronjobs for cdstool, which downloads data from the Copernicus Data Service, and tiletool, which processes that data into weekly summaries and reorganizes it by time region in a large HDF5 file.\n$ kubectl get pods\nNAME                                   READY   STATUS      RESTARTS   AGE\npod/hwitw-67ccd577-rltrp               1/1     Running     0          160d\npod/hwitw-cdstool-28478538--1-99m2n    0/1     Completed   0          34d\npod/hwitw-cdstool-28508778--1-l5twt    0/1     Completed   0          13d\npod/hwitw-cdstool-28518858--1-xzpkn    0/1     Completed   0          6d10h\npod/hwitw-tiletool-28478543--1-28d2n   0/1     Completed   0          34d\npod/hwitw-tiletool-28508783--1-ldzdm   0/1     Completed   0          13d\npod/hwitw-tiletool-28518863--1-js8mj   0/1     Completed   0          6d10h\nThe CronJobs show the details of the schedule for these episodic processes, and the container implementation that was used for each run.\n$ kubectl get cronjobs\nNAME                           SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE\ncronjob.batch/hwitw-cdstool    18 18 * * 5   False     0        6d10h           170d\ncronjob.batch/hwitw-tiletool   23 18 * * 5   False     0        6d10h           170d\n\n$ kubectl get jobs -o wide\nNAME                      COMPLETIONS   DURATION   AGE     CONTAINERS          IMAGES\nhwitw-cdstool-28478538    1/1           13s        34d     hwitw-cdstool-job   ghcr.io/nceas/hwitw:0.9.6\nhwitw-cdstool-28508778    1/1           9s         13d     hwitw-cdstool-job   ghcr.io/nceas/hwitw:0.9.6\nhwitw-cdstool-28518858    1/1           10s        6d11h   hwitw-cdstool-job   ghcr.io/nceas/hwitw:0.9.6\nhwitw-tiletool-28478543   1/1           13s        34d     hwitw-cdstool-job   ghcr.io/nceas/hwitw:0.9.6\nhwitw-tiletool-28508783   1/1           11s        13d     hwitw-cdstool-job   ghcr.io/nceas/hwitw:0.9.6\nhwitw-tiletool-28518863   1/1           6s         6d10h   hwitw-cdstool-job   ghcr.io/nceas/hwitw:0.9.6\n\nTogether, these Kubernetes resources, and a few others including an Ingress Controller to glue the application together, constitue a complete data access, data processing, and web visualization service for scientific data. The entire application can be deployed with a single command.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Container orchestration</span>"
    ]
  },
  {
    "objectID": "sections/docker-hpc-cloud.html#resources",
    "href": "sections/docker-hpc-cloud.html#resources",
    "title": "17  Container orchestration",
    "section": "17.4 Resources",
    "text": "17.4 Resources\n\nDocker compose tutorial\nKubernetes documentation\nDigital Ocean introduction to kubernetes",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Container orchestration</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility-containers.html",
    "href": "sections/reproducibility-containers.html",
    "title": "18  Reproducibility and Containers",
    "section": "",
    "text": "18.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility and Containers</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility-containers.html#learning-objectives",
    "href": "sections/reproducibility-containers.html#learning-objectives",
    "title": "18  Reproducibility and Containers",
    "section": "",
    "text": "Think about dependency management, reproducibility and software management\nDiscuss how the techniques from this class can improve reproducibility\nSlides: Accelerating synthesis science through reproducible science",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility and Containers</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility-containers.html#summary",
    "href": "sections/reproducibility-containers.html#summary",
    "title": "18  Reproducibility and Containers",
    "section": "18.2 Summary",
    "text": "18.2 Summary\nIn this course we reviewed many tools and techniques for how to make research more reproducible and scalable. These tools focus around three main areas: the environment, the data, and the code.\n\n18.2.1 Reproducible Environments\n\nVirtual environments with venv and virtualenvwrapper\nPython dependencies with requirements.txt\nContainers with Docker\n\n\n\n18.2.2 Accessible Data\n\nPublishing with the Arctic Data Center\nFormats for large datasets: NetCDF and Zarr\n\n\n\n18.2.3 Scalable Python\n\nParallel with concurrent.futures, parsl, dask\nN-dimensional data access with xarray\nGeospatial analysis with geopandas and rasterio\nSoftware design and python packages",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility and Containers</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility-containers.html#software-collapse",
    "href": "sections/reproducibility-containers.html#software-collapse",
    "title": "18  Reproducibility and Containers",
    "section": "18.3 Software collapse",
    "text": "18.3 Software collapse\n\n\n\n\n\n\nIn his blog post on software collapse, Konrad Hinsen outlines the fragile state of scientific software today. While we like to think that our software is robust, in reality it is being constantly revised, expanded, broken, and abandoned, such that our use of any software today is almost certainly doomed to disfunction shortly. Hinsen describes the science software ecosystem as having 4 layers, from totally general operating systems at the bottom, to scientific libraries, discipline-specific libraries, and finally project-specific software at the top. Hinsen states:\n\n\n\n\n\nAdapted from Hinsen’s software stack\n\n\n\n\n\n\nThere are essentially two tasks that need to be organized and financed: preventing collapse due to changes in layers 1 and 2, and implementing new models and methods as the scientific state of the art advances. These two tasks go on in parallel and are often executed by the same people, but in principle they are separate and one could concentrate on just one or the other.\nThe common problem of both communities is collapse, and the common enemy is changes in the foundations that scientists and developers build on.\n\nWhile most reproducibility work has focused at the project-specific software level, Hinsen argues that there is tremendous risk deep down in the software ecosystem. There are many examples of compatibility decisions in scientific software that create problems for long-term stability, even if they are the right decision at the time. For example, see the compatbility dicussions from numpy and pandas to get a sense fo the challenges faced when maintaining science software. This issue with abandoned software and lonely maintainers has become a prevalent meme in the open source software world.\n\n\n\nDependency",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility and Containers</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility-containers.html#delaying-the-inevitable",
    "href": "sections/reproducibility-containers.html#delaying-the-inevitable",
    "title": "18  Reproducibility and Containers",
    "section": "18.4 Delaying the inevitable",
    "text": "18.4 Delaying the inevitable\nWhile software is destined to continue to rot, projects like WholeTale, repo2docker, and Binder are working towards tools that can help capture more of the software stack needed to reproduce computations, thereby enabling us to freeze older versions of software and replay them in compatible combinations. Or, barring full re-execution, at least to understand them. The idea is to take advantage of containerized computing environments, which can be described as ‘software as code’ because containers can be declaratively described in a configuration file. While project-specific software and possibly disciplinary software dependencies might be captured in a requirements.txt file for python, we also need to know all of the details of lower layers, including system libraries and kernel versions. Plus data dependencies, processing workflows, and provenance. And it turns out that these very details can be captured quite effectively in a virtualized container. For example, WholeTale uses container images to record not just project-specific software dependencies, but also operating system details, data dependencies, and runtime provenance.\n\nThis is accompished by first configuring a base image that contains core software from layers 1 and 2, and then researchers work within common tools like Jupyter and RStudio to create their scientific products at layers 3 and 4. These can then be captured in virtual environments, which get added to the container configuration, along with key details such as external data dependencies and process metadata. All of this can be serialized as a “Tale”, which is basically an archival package bundled around a container image definition.\n\n\n\n\n\n\nThis approach combines the evolving approach to using a Dockerfile to precisely configure a container environment with the use of structured metadata and provenance information to create an archival-quality research artifact. These WholeTale tales can be downloaded to any machine with a container runtime and executed under the same conditions that were used when the original tale was archived.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility and Containers</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility-containers.html#discussion",
    "href": "sections/reproducibility-containers.html#discussion",
    "title": "18  Reproducibility and Containers",
    "section": "18.5 Discussion",
    "text": "18.5 Discussion\nTo wrap up this week, let’s kick off a discussion with a couple of key questions.\n\nAs we learn new tools for scalable and reproducible computing, what can we as software creators do to improve robustness and ease maintenance of our packages?\nGiven the fragility of software ecosystems, are you worried about investing a lot of time in learning and building code for proprietary cloud systems? Can we compel vendors to keep their systems open?\nWhat relative fraction of the research budget should funders invest in software infrastructure, data infrastructure, and research outcomes?",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility and Containers</span>"
    ]
  }
]