[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "",
    "text": "Seminar Series Overview\nRemote Sessions November 2023 though February 2024\nHosted by the National Center for Ecological Anlysis and Synthesis (NCEAS) and in partnership with UCSB’s Office of Research, this seminar series is a training program where participants engage in synthesis research in a small cohort. It will equip participants with data science and team science tools, and provide them with a platform to conduct collaborative synthesis research. The overall aim is to grow cross-departmental relationships and interdisciplinary research outputs."
  },
  {
    "objectID": "index.html#nceas-expertise",
    "href": "index.html#nceas-expertise",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "NCEAS Expertise",
    "text": "NCEAS Expertise\nNCEAS, a research affiliate of UCSB, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science. NCEAS has over 25 years of success with this model among working groups and environmental professionals."
  },
  {
    "objectID": "index.html#full-schedule",
    "href": "index.html#full-schedule",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "Full Schedule",
    "text": "Full Schedule\n\n\n\n\n\n\n\n\n\nSession\nDates\nLocation\nTime\n\n\n\n\nSession One\nSeptember 11-15, 2023\nNCEAS\n9:00AM-5:00PM\n\n\nSession Two\nRemote session every second Tuesday throughout October-February\nOctober 10, 2023\n\nNovember 14, 2023\n\nDecember 12, 2023\n\nJanuary 9, 2024\n\nFebruary 13, 2024\nTeach Me How To Google\nFunctions and Packages in R\nWorking with Spatial Data in R\nHow to Manage Dependencies in Projects (e.g. Containers)\nUC Love Data Week + Office Hours with Learning Hub\n3:00PM-5:00PM\n\n\nSession Three\nMarch 25-28, 2024\nNCEAS\n9:00AM-5:00PM"
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills",
    "section": "About this book",
    "text": "About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Halina Do-Linh and Camila Vargas Poulsen (2023). UCSB Faculty Seminar Series: Grow Your Data & Team Science Skills. NCEAS Learning Hub. URL learning.nceas.ucsb.edu/2023-11-ucsb-faculty.\nAdditional contributors: Ben Bolker, Amber E. Budden, Julien Brun, Samantha Csik, Natasha Haycock-Chavez, S. Jeanette Clark, Julie Lowndes, Stephanie Hampton, Matthew B. Jones, Samanta Katz, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "session_01.html#create-a-new-repository-on-github",
    "href": "session_01.html#create-a-new-repository-on-github",
    "title": "1  Setup",
    "section": "1.1 Create a new Repository on GitHub",
    "text": "1.1 Create a new Repository on GitHub\nTo start fresh and review the Git & GitHub workflow, we are going to create a new repository just for this session.\n\n\n\n\n\n\nStep 1: Create a remote repository on GitHub\n\n\n\n\nLogin to GitHub\nClick the New repository button\nName it {LASTNAME}-functions-and-packages\nAdd a short description\nCheck the box to add a README.md file\nAdd a .gitignore file using the R template\nSet the LICENSE to Apache 2.0\n\n\n\nYou’ve now successfully created a GitHub repository! It has a couple of files that GitHub created for you: README.md, LICENSE, and .gitignore.\n\n\n\n\n\n\nREADME.md files are used to share important information about your repository\n\n\n\nYou should always add a README.md to the root directory of your repository – it is a markdown file that is rendered as HTML and displayed on the landing page of your repository. This is a common place to include any pertinent information about what your repository contains, how to use it, etc.\n\n\nCurrently, our repository just exists on GitHub as a remote repository. Next, we’ll bring a copy of this remote repository down to our local computer (aka clone this repository) so that we can work comfortably in RStudio.\n\n\n\n\n\n\nAn important distinction\n\n\n\nWe refer to the remote copy of the repository that is on GitHub as the origin repository (the one that we cloned from), and the copy on our local computer as the local repository.\n\n\nRStudio makes working with Git and version controlled files easy – to do so, you’ll need to be working within an R project folder. The following steps will look similar to those you followed when first creating an R Project. Let’s follow the instructions below and remind ourselves about this process.\n\n\n\n\n\n\nStep 2: clone your repository and use Git locally in RStudio\n\n\n\n\nStart by clicking the green Code button (top right of your file listing) and copying the URL to your clipboard (this URL represents the repository location).\nGo to RStudio\nClick File &gt; New Project\nSelect Version Control and paste the remote repository URL (which should be copied to your clipboard) in the Repository ULR field\nPress Tab, which will auto-fill the Project directory name field with the same name as that of your remote repo – while you can name the local copy of the repository anything, it’s typical (and highly recommended) to use the same name as the GitHub repository to maintain the correspondence\n\n\n\n\n\n\n\n\n\nOnce you click Create Project, a new RStudio window will open with all of the files from the remote repository copied locally. Depending on how your version of RStudio is configured, the location and size of the panes may differ, but they should all be present – you should see a Git tab, as well as the Files tab, where you can view all of the files copied from the remote repo to this local repo.\nFinally, we will share our project with the instructors. This step is only necessary and possible when working on included-crab. We do this step in case any of you need help debugging your code. Sharing you project allows the instructors to see your code in their screen.\n\n\n\n\n\n\nStep 3: Share your project with the instructors\n\n\n\n\nlocate the “project switcher” dropdown menu in the upper right of your RStudio window. This dropdown has the name of your project (eg: vargas-functions-and-packages), and a dropdown arrow.\nClick the dropdown menu, then “Share Project.”\nWhen the dialog box pops up, add the following usernames to your project:\n\n\nnjlyon\nvargas-pouslen\n\n\nOnce those names show up in the list, click “OK.”"
  },
  {
    "objectID": "session_02.html#learning-objectives",
    "href": "session_02.html#learning-objectives",
    "title": "2  Writing Functions and Packages",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nExplain the importance of using and developing functions\nCreate custom functions using R code\nDocument functions to improve understanding and code communication"
  },
  {
    "objectID": "session_02.html#r-functions",
    "href": "session_02.html#r-functions",
    "title": "2  Writing Functions and Packages",
    "section": "2.1 R Functions",
    "text": "2.1 R Functions\n\n\n\n\n\n\nQuick reminder: What is a function?\n\n\n\n\n\nA set of statements or expressions of code that are organized together to perform a specific task.\nThe statements or expressions of code within the function accept user input(s), does something with it, and returns a useful output.\nSyntax: result_value &lt;- function_name(argument1 = value1, argument2 = value2, ...)\n\n\n\nMany people write R code as a single, continuous stream of commands, often drawn from the R Console itself and simply pasted into a script. While any script brings benefits over non-scripted solutions, there are advantages to breaking code into small, reusable modules. This is the role of a function in R. In this lesson, we will review the advantages of coding with functions, practice by creating some functions and show how to call them, and then do some exercises to build other simple functions.\n\n2.1.1 Why Functions?\n\n\n\n\n\n\nDRY: Don’t Repeat Yourself\n\n\n\n\n“You should consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code).”\nChapter 19 Functions in R for Data Science (Grolemund & Wickham)\n\nBy creating small functions that only complete one logical task and do it well, we quickly gain:\n\nImproved understanding\nReuse via decomposing tasks into bite-sized chunks\nImproved error testing\n\n\n\n\n\n\n\nNaming Functions\n\n\n\nThe name of a function is important. Ideally, function names should be short, but still clearly captures what the function does.\nBest Practices from Chapter 19 Functions in R for Data Science:\n\nFunction names should be verbs and arguments should be nouns (there are exceptions).\nUse the snake_case naming convention for functions that are multiple words.\nFor a “family” of functions, use a common prefix to indicate that they are connected.\n\n\n\n\n\n2.1.2 Exercise: Temperature Conversion\nImagine you have a bunch of data measured in Fahrenheit and you want to convert that for analytical purposes to Celsius. You might have an R script that does this for you.\n\nairtemps &lt;- c(212, 30.3, 78, 32)\ncelsius1 &lt;- (airtemps[1] - 32) * 5/9\ncelsius2 &lt;- (airtemps[2] - 32) * 5/9\ncelsius3 &lt;- (airtemps[3] - 32) * 5/9\n\nNote the duplicated code, where the same formula is repeated three times. This code would be both more compact and more reliable if we didn’t repeat ourselves.\n\nCreate a Function that Converts Fahrenheit to Celsius\nFunctions in R are a mechanism to process some input and return a value. Similarly to other variables, functions can be assigned to a variable so that they can be used throughout code by reference. To create a function in R, you use the function function (so meta!) and assign its result to a variable. Let’s create a function that calculates Celsius temperature outputs from Fahrenheit temperature inputs.\n\nfahr_to_celsius &lt;- function(fahr) {\n  celsius &lt;- (fahr - 32) * 5/9\n  return(celsius)\n}\n\nBy running this code, we have created a function and stored it in R’s global environment. The fahr argument to the function function indicates that the function we are creating takes a single parameter (the temperature in Fahrenheit), and the return statement indicates that the function should return the value in the celsius variable that was calculated inside the function. Let’s use it, and check if we got the same value as before:\n\ncelsius4 &lt;- fahr_to_celsius(airtemps[1])\ncelsius4\n\n[1] 100\n\ncelsius1 == celsius4\n\n[1] TRUE\n\n\nExcellent. So now we have a conversion function we can use. Note that, because most operations in R can take multiple types as inputs, we can also pass the original vector of airtemps, and calculate all of the results at once:\n\ncelsius &lt;- fahr_to_celsius(airtemps)\ncelsius\n\n[1] 100.0000000  -0.9444444  25.5555556   0.0000000\n\n\nThis takes a vector of temperatures in Fahrenheit, and returns a vector of temperatures in Celsius.\n\n\nYour Turn: Create a Function that Converts Celsius to Fahrenheit\n\n\n\n\n\n\nExercise\n\n\n\nCreate a function named celsius_to_fahr that does the reverse, it takes temperature data in Celsius as input, and returns the data converted to Fahrenheit.\nCreate the function celsius_to_fahr in a new R Script file.\nThen use that formula to convert the celsius vector back into a vector of Fahrenheit values, and compare it to the original airtemps vector to ensure that your answers are correct.\nHint: the formula for Celsius to Fahrenheit conversions is celsius * 9/5 + 32.\n\n\nDid you encounter any issues with rounding or precision?\n\n\n\n\n\n\nSolution, but don’t peek!\n\n\n\n\n\nDon’t peek until you write your own…\n\ncelsius_to_fahr &lt;- function(celsius) {\n    fahr &lt;- celsius * 9/5 + 32\n    return(fahr)\n}\n\nresult &lt;- celsius_to_fahr(celsius)\nairtemps == result\n\n[1] TRUE TRUE TRUE TRUE\n\n\n\n\n\n\n\n\n2.1.3 Documenting R Functions\nFunctions need documentation so that we can communicate what they do, and why. The roxygen2 R package provides a simple means to document your functions so that you can explain what the function does, the assumptions about the input values, a description of the value that is returned, and the rationale for decisions made about implementation.\nDocumentation in roxygen2 is placed immediately before the function definition, and is indicated by a special comment line that always starts with the characters #'. Here’s a documented version of a function:\n\n#' Convert temperature values from Fahrenheit to Celsius\n#'\n#' @param fahr Numeric or numeric vector in degrees Fahrenheit\n#' \n#' @return Numeric or numeric vector in degrees Celsius\n#' @export\n#' \n#' @examples\n#' fahr_to_celsius(32)\n#' fahr_to_celsius(c(32, 212, 72))\n\nfahr_to_celsius &lt;- function(fahr) {\n  celsius &lt;- (fahr - 32) * 5/9\n  return(celsius)\n}\n\nNote the use of the @param keyword to define the expectations of input data, and the @return keyword for defining the value that is returned from the function. The @examples function is useful as a reminder as to how to use the function. Finally, the @export keyword indicates that, if this function were added to a package, then the function should be available to other code and packages to utilize.\n\n\n\n\n\n\nCheck it out: Function Documentation Section from R Packages (2e)\n\n\n\nFor more best practices on function documentation, review Hadley Wickham and Jennifer Bryan’s online book R Packages (2e) - Chapter 10, Section 16: Function Documentation.\n\n\n\n\n2.1.4 Exercise: Minimizing Work with Functions\nFunctions can of course be as simple or complex as needed. They can be very effective in repeatedly performing calculations, or for bundling a group of commands that are used on many different input data sources. For example, we might create a simple function that takes Fahrenheit temperatures as input, and calculates both Celsius and Kelvin temperatures. All three values are then returned in a list, making it very easy to create a comparison table among the three scales.\n\nconvert_temps &lt;- function(fahr) {\n  celsius &lt;- (fahr - 32) * 5/9\n  kelvin &lt;- celsius + 273.15\n  return(list(fahr = fahr, celsius = celsius, kelvin = kelvin))\n}\n\ntemps_df &lt;- data.frame(convert_temps(seq(-100,100,10)))\n\n\n\n\n\n\n\n\nOnce we have a dataset like that, we might want to plot it. One thing that we do repeatedly is set a consistent set of display elements for creating graphs and plots. By using a function to create a custom ggplot theme, we can enable to keep key parts of the formatting flexible. For example, in the custom_theme function, we provide a base_size argument that defaults to using a font size of 9 points. Because it has a default set, it can safely be omitted. But if it is provided, then that value is used to set the base font size for the plot.\n\ncustom_theme &lt;- function(base_size = 9) {\n    ggplot2::theme(\n      text             = ggplot2::element_text(family = 'Helvetica', \n                                               color = 'gray30', \n                                               size = base_size),\n      plot.title       = ggplot2::element_text(size = ggplot2::rel(1.25), \n                                               hjust = 0.5, \n                                               face = 'bold'),\n      panel.background = ggplot2::element_blank(),\n      panel.border     = ggplot2::element_blank(),\n      panel.grid.minor = ggplot2::element_blank(),\n      panel.grid.major = ggplot2::element_line(colour = 'grey90', \n                                               linewidth = 0.25),\n      legend.position  = 'right',\n      legend.key       = ggplot2::element_rect(colour = NA, \n                                               fill = NA),\n      axis.ticks       = ggplot2::element_blank(),\n      axis.line        = ggplot2::element_blank()\n      )\n}\n\nlibrary(ggplot2)\n\nggplot(temps_df, mapping = aes(x = fahr, y = celsius, color = kelvin)) +\n    geom_point() +\n    custom_theme(10)\n\n\n\n\nIn this case, we set the font size to 10, and plotted the air temperatures. The custom_theme function can be used anywhere that one needs to consistently format a plot.\nBut we can go further. One can wrap the entire call to ggplot in a function, enabling one to create many plots of the same type with a consistent structure. For example, we can create a scatterplot function that takes a data frame as input, along with a point_size for the points on the plot, and a font_size for the text.\n\nscatterplot &lt;- function(df, point_size = 2, font_size = 9) {\n  ggplot(df, mapping = aes(x = fahr, y = celsius, color = kelvin)) +\n    geom_point(size = point_size) +\n    custom_theme(font_size)\n}\n\nCalling that let’s us, in a single line of code, create a highly customized plot but maintain flexibility via the arguments passed in to the function. Let’s set the point size to 3 and font to 16 to make the plot more legible.\n\nscatterplot(temps_df, point_size = 3, font_size = 16)\n\n\n\n\nOnce these functions are set up, all of the plots built with them can be reformatted by changing the settings in just the functions, whether they were used to create 1, 10, or 100 plots.\n\n\n2.1.5 Summary\n\nFunctions are useful to reduce redundancy, reuse code, and reduce errors\nBuild functions with function()\nDocument functions with roxygen2 comments\n\n\n\n\n\n\n\nWorkflow for Creating Functions\n\n\n\n\nHave a clear goal (sometimes it helps to create a visual).\nOutline the plan and then add more detailed steps or tasks.\nBuild it up bit-by-bit and start with a minimum viable example. As your function becomes more complex, it can harder to track all the bits.\nAlways check intermediates!"
  },
  {
    "objectID": "session_02.html#r-packages",
    "href": "session_02.html#r-packages",
    "title": "2  Writing Functions and Packages",
    "section": "2.2 R Packages",
    "text": "2.2 R Packages\nMost R users are familiar with loading and utilizing packages in their work. And they know how rich CRAN is in providing for many conceivable needs. Most people have never created a package for their own work, and most think the process is too complicated. Really it’s pretty straighforward and super useful in your personal work. Creating packages serves two main use cases:\n\nMechanism to redistribute reusable code (even if just for yourself)\nMechanism to reproducibly document analysis and models and their results\n\nEven if you don’t plan on writing a package with such broad appeal such as, say, ggplot2 or dplyr, you still might consider creating a package to contain:\n\nUseful utility functions you write (i.e. a Personal Package). Having a place to put these functions makes it much easier to find and use them later.\nA set of shared routines for your lab or research group, making it easier to remain consistent within your team and also to save time.\nThe analysis accompanying a thesis or manuscript, making it all that much easier for others to reproduce your results.\n\n\n\n\n\n\n\nPackages for Creating and Maintaining Packages\n\n\n\nThe usethis, devtools and roxygen2 packages make creating and maintining a package to be a straightforward experience.\n\n\n\n2.2.1 Create a Basic Package\nTo create a package we’re going to use the following packages:\n\ndevtools: Provides R functions that make package development easier by expediting common development tasks.\nusethis: Commonly referred to as a “workflow package” and provides functions that automate common tasks in project setup and development for both R packages and non-package projects.\nroxygen2: Provides a structure for describing your functions in the scripts you’re creating them in. It will additionally process the source code and the documentation within it to automatically create the necessary files for the documentation to appear in your R Package.\n\nThanks to the great usethis package, it only takes one function call to create the skeleton of an R package using create_package(). Which eliminates pretty much all reasons for procrastination. To create a package called mytools, all you do is:\n\nusethis::create_package(\"~/mytools\")\n\n✔ Creating '/home/dolinh/mytools/'\n✔ Setting active project to '/home/dolinh/mytools'\n✔ Creating 'R/'\n✔ Writing 'DESCRIPTION'\nPackage: mytools\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R (parsed):\n    * First Last &lt;first.last@example.com&gt; [aut, cre] (YOUR-ORCID-ID)\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to\n    pick a license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n✔ Writing 'NAMESPACE'\n✔ Writing 'mytools.Rproj'\n✔ Adding '^mytools\\\\.Rproj$' to '.Rbuildignore'\n✔ Adding '.Rproj.user' to '.gitignore'\n✔ Adding '^\\\\.Rproj\\\\.user$' to '.Rbuildignore'\n✔ Opening '/home/dolinh/mytools/' in new RStudio session\n✔ Setting active project to '&lt;no active project&gt;'\n\n\n\n\n\n\nWhat did the create_package() function do?\n\n\n\n\nOpen a new project called mytools (the name of the package) in a new RStduio session.\nCreate a top-level directory structure, including a number of critical files under the standard R package structure:\n\nDESCRIPTIONfile: The most important file, which provides metadata about your package. Edit this file to provide reasonable values for each of the fields, including your contact information.\nNAMESPACE file declares the functions your package exports for external use and the external functions your package imports from other packages.\nR/ directory is where you save all your function scripts and other .R files.\n.Rbuildignore lists files that we need to have around but that should not be included when building the R package from source.\n.Rproj.user is a directory used internally by RStudio.\n\nAdd the Build Tab to the Environment Pane.\n\n\n\n\n\n2.2.2 Add a License\nInformation about choosing a LICENSE is provided in the R Package (2e) book Chapter 12: Licensing.\nThe DESCRIPTION file expects the license to be chose from a predefined list, but you can use its various utility methods for setting a specific license file, such as the MIT license or the Apache 2 license:\n\nusethis::use_apache_license()\n\n✔ Setting License field in DESCRIPTION to 'Apache License (&gt;= 2.0)'\n✔ Writing 'LICENSE.md'\n✔ Adding '^LICENSE\\\\.md$' to '.Rbuildignore'\nOnce your license has been chosen, and you’ve edited your DESCRIPTION file with your contact information, a title, and a description, it will look like this:\n\n\n\n\n\n\nPackage: mytools\nTitle: Halina Do-Linh's Utility R Functions\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"Halina\", \"Do-Linh\", email = \"dolinh@nceas.ucsb.edu\", role = c(\"aut\", \"cre\"),\n           comment = c(ORCID = \"YOUR-ORCID-ID\"))\nDescription: A collection of useful R functions that I use for general utilities.\nLicense: Apache License (&gt;= 2)\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n\n\n\n\n\n2.2.3 Add Code\nThe skeleton package created contains a directory R which should contain your source files. Add your functions and classes in files to this directory, attempting to choose names that don’t conflict with existing packages. For example, you might add a file custom_theme that contains a function custom_theme() that you might want to reuse. The usethis::use_r() function will help set up you files in the right places. For example, running:\n\nusethis::use_r(\"custom_theme\")\n\n✔ Setting active project to '/home/dolinh/mytools'\n• Modify 'R/custom_theme.R'\n• Call `use_test()` to create a matching test file\ncreates the file R/custom_theme and stores it in the R directory, which you can then modify as needed:\n\ncustom_theme &lt;- function(base_size = 9) {\n    ggplot2::theme(\n      axis.ticks       = ggplot2::element_blank(),\n      text             = ggplot2::element_text(family = 'Helvetica', \n                                               color = 'gray30', \n                                               size = base_size),\n      plot.title       = ggplot2::element_text(size = ggplot2::rel(1.25), \n                                               hjust = 0.5, \n                                               face = 'bold'),\n      panel.background = ggplot2::element_blank(),\n      legend.position  = 'right',\n      panel.border     = ggplot2::element_blank(),\n      panel.grid.minor = ggplot2::element_blank(),\n      panel.grid.major = ggplot2::element_line(colour = 'grey90', \n                                               linewidth = .25),\n      legend.key       = ggplot2::element_rect(colour = NA, \n                                               fill = NA),\n      axis.line        = ggplot2::element_blank()\n      )\n}\n\n\n\n\n\n\n\nPower of Packages\n\n\n\nRemember when we created custom_theme() from the Functions Lesson Section 15.1.4? Now that we’ve added it to our mytools package, we don’t have to worry about coyping the code from another file, sourcing the file from another directory, or copying the script from an R Project.\nInstead we can leverage the portable functionality of a package to easily access our custom functions and maintain the code in one location.\n\n\n\n\n2.2.4 Add Dependencies\nIf your R code depends on functions from another package, you must declare it. In the Imports section in the DESCRIPTION file, list all the packages your functions depend upon.\nIn our custom_theme() function, we depend on the ggplot2 package, and so we need to list it as a dependency.\nOnce again, usethis provides a handy helper method:\n\nusethis::use_package(\"ggplot2\")\n\n✔ Adding 'ggplot2' to Imports field in DESCRIPTION\n• Refer to functions with `ggplot2::fun()`\nTake a look at the DESCRIPTION file again, and you’ll see the Imports section has been added, with ggplot2 underneath.\n\n\n\n\n\n\nPackage: mytools\nTitle: Halina Do-Linh's Utility R Functions\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"Halina\", \"Do-Linh\", email = \"dolinh@nceas.ucsb.edu\", role = c(\"aut\", \"cre\"),\n           comment = c(ORCID = \"YOUR-ORCID-ID\"))\nDescription: A collection of useful R functions that I use for general utilities.\nLicense: Apache License (&gt;= 2)\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\nImports: \n    ggplot2\n\n\n\n\n\n2.2.5 Add Documentation\nDocumentation is crucial to add to each of your functions. In the Functions Lesson, we did this using the roxygen2 package and that same package and approach can be used for packages.\nThe roxygen2 approach allows us to add comments in the source code, where are then converted into Help pages that we can access by typing ?function_name in the Console.\nLet’s add documentation for the custom_theme() function.\n\n#' My custom ggplot theme\n#'\n#' @param base_size Numeric value of font size of all text elements in plot\n#'\n#' @return A theme used for ggplot point or line plots\n#' @export\n#'\n#' @examples\n#' library(ggplot2)\n#' \n#'   ggplot(data = mtcars, aes(x = mpg, y = disp)) +\n#'     geom_point() +\n#'     custom_theme(base_size = 30)\ncustom_theme &lt;- function(base_size = 9) {\n  ggplot2::theme(\n    axis.ticks       = ggplot2::element_blank(),\n    text             = ggplot2::element_text(family = 'Helvetica',\n                                             color = 'gray30',\n                                             size = base_size),\n    plot.title       = ggplot2::element_text(size = ggplot2::rel(1.25),\n                                             hjust = 0.5,\n                                             face = 'bold'),\n    panel.background = ggplot2::element_blank(),\n    legend.position  = 'right',\n    panel.border     = ggplot2::element_blank(),\n    panel.grid.minor = ggplot2::element_blank(),\n    panel.grid.major = ggplot2::element_line(colour = 'grey90',\n                                             linewidth = .25),\n    legend.key       = ggplot2::element_rect(colour = NA,\n                                             fill = NA),\n    axis.line        = ggplot2::element_blank()\n  )\n}\n\nOnce your files are documented, you can then process the documentation using devtools::document() to generate the appropriate .Rd files that your package needs. The .Rd files will appear in the man/ directory, which is automatically created by devtools::document().\n\ndevtools::document()\n\nℹ Updating mytools documentation\nℹ Loading mytools\nWriting custom_theme.Rd\nWe now have a package that we can check() and install() and release(). These functions come from the devtools package, but first let’s do some testing.\n\n\n2.2.6 Testing\nYou can test your code using the testthat package’s testing framework. The ussethis::use_testthat() function will set up your package for testing, and then you can use the use_test() function to setup individual test files. For example, in the Functions Lesson we created some tests for our fahr_to_celsius functions but ran them line by line in the console.\nFirst, lets add that function to our package. Run the use_r function in the console:\n\nusethis::use_r(\"fahr_to_celsius\")\n\nThen copy the function and documentation into the R script that opens and save the file.\n\n#' Convert temperature values from Fahrenheit to Celsius\n#'\n#' @param fahr Numeric or numeric vector in degrees Fahrenheit\n#' \n#' @return Numeric or numeric vector in degrees Celsius\n#' @export\n#' \n#' @examples\n#' fahr_to_celsius(32)\n#' fahr_to_celsius(c(32, 212, 72))\n\nfahr_to_celsius &lt;- function(fahr) {\n  celsius &lt;- (fahr-32)*5/9\n  return(celsius)\n}\n\nNow, set up your package for testing:\n\nusethis::use_testthat()\n\n✔ Setting active project to '/home/dolinh/mytools'\n✔ Adding 'testthat' to Suggests field in DESCRIPTION\n✔ Setting Config/testthat/edition field in DESCRIPTION to '3'\n✔ Creating 'tests/testthat/'\n✔ Writing 'tests/testthat.R'\n• Call `use_test()` to initialize a basic test file and open it for editing.\nThen write a test for fahr_to_celsius:\n\nusethis::use_test(\"fahr_to_celsius\")\n\n✔ Writing 'tests/testthat/test-fahr_to_celsius.R'\n• Modify 'tests/testthat/test-fahr_to_celsius.R'\nYou can now add tests to the test-fahr_to_celsius.R, and you can run all of the tests using devtools::test(). For example, if you add a test to the test-fahr_to_celsius.R file:\n\ntest_that(\"fahr_to_celsius works\", {\n  expect_equal(fahr_to_celsius(32), 0)\n  expect_equal(fahr_to_celsius(212), 100)\n})\n\nThen you can run the tests to be sure all of your functions are working using devtools::test():\n\ndevtools::test()\n\nℹ Testing mytools\n✔ | F W S  OK | Context\n✔ |         2 | fahr_to_celsius [0.2s]                                                                                             \n\n══ Results ════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\nDuration: 0.4 s\n\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]\nYay, all tests passed!\n\n\n2.2.7 Checking and Installing\nNow that you’ve completed testing your package, you can check it for consistency and completeness using devtools::check().\n\ndevtools::check()\n\n══ Documenting ════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\nℹ Updating mytools documentation\nℹ Loading mytools\n\n══ Building ═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\nSetting env vars:\n• CFLAGS    : -Wall -pedantic -fdiagnostics-color=always\n• CXXFLAGS  : -Wall -pedantic -fdiagnostics-color=always\n• CXX11FLAGS: -Wall -pedantic -fdiagnostics-color=always\n• CXX14FLAGS: -Wall -pedantic -fdiagnostics-color=always\n• CXX17FLAGS: -Wall -pedantic -fdiagnostics-color=always\n• CXX20FLAGS: -Wall -pedantic -fdiagnostics-color=always\n── R CMD build ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n✔  checking for file ‘/home/dolinh/mytools/DESCRIPTION’ (610ms)\n─  preparing ‘mytools’:\n✔  checking DESCRIPTION meta-information (338ms)\n─  checking for LF line-endings in source and make files and shell scripts\n─  checking for empty or unneeded directories\n─  building ‘mytools_0.0.0.9000.tar.gz’\n   \n══ Checking ═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\nSetting env vars:\n• _R_CHECK_CRAN_INCOMING_REMOTE_               : FALSE\n• _R_CHECK_CRAN_INCOMING_                      : FALSE\n• _R_CHECK_FORCE_SUGGESTS_                     : FALSE\n• _R_CHECK_PACKAGES_USED_IGNORE_UNUSED_IMPORTS_: FALSE\n• NOT_CRAN                                     : true\n── R CMD check ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n─  using log directory ‘/tmp/Rtmp1UgqFD/file6d79323df6fae/mytools.Rcheck’ (649ms)\n─  using R version 4.2.2 (2022-10-31)\n─  using platform: x86_64-pc-linux-gnu (64-bit)\n─  using session charset: UTF-8\n─  using options ‘--no-manual --as-cran’\n✔  checking for file ‘mytools/DESCRIPTION’\n─  this is package ‘mytools’ version ‘0.0.0.9000’\n─  package encoding: UTF-8\n✔  checking package namespace information\n✔  checking package dependencies (2.1s)\n✔  checking if this is a source package\n✔  checking if there is a namespace\n✔  checking for executable files\n✔  checking for hidden files and directories\n✔  checking for portable file names\n✔  checking for sufficient/correct file permissions\n✔  checking serialization versions\n✔  checking whether package ‘mytools’ can be installed (3.2s)\n✔  checking installed package size\n✔  checking package directory\n✔  checking for future file timestamps (412ms)\n✔  checking DESCRIPTION meta-information (584ms)\n✔  checking top-level files ...\n✔  checking for left-over files\n✔  checking index information\n✔  checking package subdirectories ...\n✔  checking R files for non-ASCII characters ...\n✔  checking R files for syntax errors ...\n✔  checking whether the package can be loaded (481ms)\n✔  checking whether the package can be loaded with stated dependencies ...\n✔  checking whether the package can be unloaded cleanly ...\n✔  checking whether the namespace can be loaded with stated dependencies ...\n✔  checking whether the namespace can be unloaded cleanly (450ms)\n✔  checking loading without being on the library search path (522ms)\n✔  checking dependencies in R code (1.2s)\n✔  checking S3 generic/method consistency (1s)\n✔  checking replacement functions ...\n✔  checking foreign function calls ...\n✔  checking R code for possible problems (5.2s)\n✔  checking Rd files (449ms)\n✔  checking Rd metadata ...\n✔  checking Rd line widths ...\n✔  checking Rd cross-references ...\n✔  checking for missing documentation entries ...\n✔  checking for code/documentation mismatches (885ms)\n✔  checking Rd \\usage sections (1.3s)\n✔  checking Rd contents ...\n✔  checking for unstated dependencies in examples ...\n✔  checking examples (2.7s)\n✔  checking for unstated dependencies in ‘tests’ ...\n─  checking tests (418ms)\n✔  Running ‘testthat.R’ (1.4s)\n✔  checking for non-standard things in the check directory\n✔  checking for detritus in the temp directory\n   \n   \n── R CMD check results ──────────────────────────────────────────────────────────────────────────────────── mytools 0.0.0.9000 ────\nDuration: 27.3s\n\n0 errors ✔ | 0 warnings ✔ | 0 notes ✔\nThen you can install it locally using devtools::install(), which needs to be run from the parent directory of your module\n\ndevtools::install()\n\n── R CMD build ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n✔  checking for file ‘/home/dolinh/mytools/DESCRIPTION’ (541ms)\n─  preparing ‘mytools’:\n✔  checking DESCRIPTION meta-information ...\n─  checking for LF line-endings in source and make files and shell scripts\n─  checking for empty or unneeded directories\n─  building ‘mytools_0.0.0.9000.tar.gz’\n   \nRunning /opt/R/4.2.2/lib/R/bin/R CMD INSTALL /tmp/Rtmp1UgqFD/mytools_0.0.0.9000.tar.gz --install-tests \n* installing to library ‘/home/dolinh/R/x86_64-pc-linux-gnu-library/4.2’\n* installing *source* package ‘mytools’ ...\n** using staged installation\n** R\n** tests\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (mytools)\nAfter installing, your package is now available for use in your local environment, yay!\n\n\n\n\n\n\nCheck out the Build Tab\n\n\n\nRemember when we ran usethis::create_package() and after we ran it we saw the Build Tab added to the Environment pane?\nIn the Build Tab, each of the buttons correspond with one of the devtools functions we ran, meaning:\n\nTest button is equivalent to running devtools::test() in the Console\nCheck button is equivalent to running devtools::check() in the Console\nInstall button is equivalent to running devtools::install() in the Console\n\n\n\n\n\n2.2.8 Sharing and Releasing\n\nGitHub: The simplest way to share your package with others is to upload it to a GitHub repository, which allows others to install your package using the install_github('mytools','github_username') function from devtools.\nCRAN: If your package might be broadly useful, also consider releasing it to CRAN, using the release() method from devtools(). Releasing a package to CRAN requires a significant amount of work to ensure it follows the standards set by the R community, but it is entirely tractable and a valuable contribution to the science community. If you are considering releasing a package more broadly, you may find that the supportive community at ROpenSci provides incredible help and valuable feeback through their onboarding process.\nR-Universe: A newer approach is to link your package release to R-Universe, which is an effective way to make it easy to test and maintain packages so that many people can install them using the familiar install.pacakges() function in R. In R-Universe, people and organizations can create their own universe of packages, which represent a collection of packages that appear as a CRAN-compatible repository in R. For example, for DataONE we maintain the DataONE R-Universe, which lists the packages we actively maintain as an organization. So, any R-user that wants to install these packages can do so by adding our universe to their list of repositories, and then installing packages as normal. For example, to install the codyn package, one could use:\n\n\ninstall.packages('codyn', repos = c('https://dataoneorg.r-universe.dev', 'https://cloud.r-project.org'))\n\n\n\n2.2.9 Exercise: Add More Functions\nAdd additional temperature conversion functions to the mytools package and:\n\nAdd full documentation for each function\nWrite tests to ensure the functions work properly\nRebuild the package using document(), check(), and install()\n\n\n\n\n\n\n\nDon’t forget to update the version number before you install!\n\n\n\nVersion information is located in the DESCRIPTION file and when you first create a package the version is 0.0.0.9000.\nThis version number follows the format major.minor.patch.dev. The different parts of the version represent different things:\n\nMajor: A significant change to the package that would be expected to break users code. This is updated very rarely when the package has been redesigned in some way.\nMinor: A minor version update means that new functionality has been added to the package. It might be new functions to improvements to existing functions that are compatible with most existing code.\nPatch: Patch updates are bug fixes. They solve existing issues but don’t do anything new.\nDev: Dev versions are used during development and this part is missing from release versions. For example you might use a dev version when you give someone a beta version to test. A package with a dev version can be expected to change rapidly or have undiscovered issues.\n\nAfter you’ve made some changes to a package, but before you install run the code:\n\nusethis::use_version()\n\nCurrent version is 0.0.0.9000.\nWhat should the new version be? (0 to exit) \n\n1: major --&gt; 1.0.0\n2: minor --&gt; 0.1.0\n3: patch --&gt; 0.0.1\n4:   dev --&gt; 0.0.0.9001\nSince we’re adding new functions, we can consider this a minor change and can select option 2.\nSelection: 2\n✔ Setting Version field in DESCRIPTION to '0.1.0'\nSource: COMBINE’s R package workshop, Ch 9: Versioning\n\n\n\n\n2.2.10 Additional Resources\n\nHadley Wickham and Jenny Bryan’s awesome book: R Packages\nROpenSci Blog Post: How to create your personal CRAN-like repository on R-universe\nKarl Broman’s: R package primer: a minimal tutorial on writing R packages\nThomas Westlake’s Short Tutorial: Writing an R package from scratch (his post is an updated version of Hilary Parker’s blog post)"
  },
  {
    "objectID": "session_03.html#learning-objectives",
    "href": "session_03.html#learning-objectives",
    "title": "3  Working with Spatial Data in R",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nHow to use the sf package to wrangle spatial data\nStatic mapping with ggplot\nAdding basemaps to static maps\nInteractive mapping with leaflet"
  },
  {
    "objectID": "session_03.html#brief-introduction-to-sf",
    "href": "session_03.html#brief-introduction-to-sf",
    "title": "3  Working with Spatial Data in R",
    "section": "3.1 Brief introduction to sf",
    "text": "3.1 Brief introduction to sf\nFrom the sf vignette:\n\nSimple features or simple feature access refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects. It also describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them.\n\nThe sf package is an R implementation of Simple Features. This package incorporates:\n\na new spatial data class system in R\n\nfunctions for reading and writing spatial data\n\ntools for spatial operations on vectors\n\nMost of the functions in this package starts with prefix st_ which stands for spatial and temporal.\nIn this lesson, our goal is to use a shapefile of Alaska regions and rivers, and data on population in Alaska by community to create a map that looks like this:"
  },
  {
    "objectID": "session_03.html#about-the-data",
    "href": "session_03.html#about-the-data",
    "title": "3  Working with Spatial Data in R",
    "section": "3.2 About the data",
    "text": "3.2 About the data\nAll of the data used in this tutorial are simplified versions of real datasets available on the KNB Data Repository. We are using simplified datasets to ease the processing burden on all our computers since the original geospatial datasets are high-resolution. These simplified versions of the datasets may contain topological errors.\nThe spatial data we will be using to create the map are:\n\n\n\nData\nOriginal datasets\n\n\n\n\nAlaska regional boundaries\nJared Kibele and Jeanette Clark. 2018. State of Alaska’s Salmon and People Regional Boundaries. Knowledge Network for Biocomplexity. doi:10.5063/F1125QWP.\n\n\nCommunity locations and population\nJeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018. Languages used in Alaskan households, 1990-2015. Knowledge Network for Biocomplexity. doi:10.5063/F11G0JHX.\n\n\nAlaska rivers\nThe rivers shapefile is a simplified version of Jared Kibele and Jeanette Clark. Rivers of Alaska grouped by SASAP region, 2018. Knowledge Network for Biocomplexity. doi:10.5063/F1SJ1HVW.\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\nNavigate to this dataset on KNB’s test site and download the zip folder.\nUpload the zip folder to the data folder in the training_{USERNAME} project. You don’t need to unzip the folder ahead of time, uploading will automatically unzip the folder.\n\nAlternatively, programatically download and extract the demo data with:\n\n\n\nknb_url &lt;- 'https://dev.nceas.ucsb.edu/knb/d1/mn/v2/object/urn%3Auuid%3Aaceaecb2-1ce0-4d41-a839-d3607d32bb58'\ndownload.file(url = knb_url, destfile = 'demo_data.zip')\nunzip('demo_data.zip', exdir = 'data')\nfile.remove('demo_data.zip')\n\n\nCreate a new Quarto file.\n\nTitle it “Intro to sf package for Spatial Data and Making Maps”\nSave the file and name it “intro-sf-spatial-data-maps”.\n\nLoad the following libraries at the top of your Quarto file.\n\n\nlibrary(readr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(leaflet)\nlibrary(scales)\nlibrary(ggmap)\nlibrary(dplyr)"
  },
  {
    "objectID": "session_03.html#exploring-the-data-using-plot-and-st_crs",
    "href": "session_03.html#exploring-the-data-using-plot-and-st_crs",
    "title": "3  Working with Spatial Data in R",
    "section": "3.3 Exploring the data using plot() and st_crs()",
    "text": "3.3 Exploring the data using plot() and st_crs()\nFirst let’s read in the shapefile of regional boundaries in Alaska using read_sf() and then create a basic plot of the data plot().\n\n# read in shapefile using read_sf()\nak_regions &lt;- read_sf(\"data/ak_regions_simp.shp\")\n\n\n# quick plot\nplot(ak_regions)\n\n\n\n\nWe can also examine it’s class using class().\n\nclass(ak_regions)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nsf objects usually have two types of classes: sf and data.frame.\nUnlike a typical data.frame, an sf object has spatial metadata (geometry type, dimension, bbox, epsg (SRID), proj4string) and an additional column typically named geometry that contains the spatial data.\nSince our shapefile object has the data.frame class, viewing the contents of the object using the head() function or other exploratory functions shows similar results as if we read in data using read.csv() or read_csv().\n\nhead(ak_regions)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.2296 ymin: 51.15702 xmax: 179.8567 ymax: 71.43957\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  region_id region           mgmt_area                                  geometry\n      &lt;int&gt; &lt;chr&gt;                &lt;dbl&gt;                        &lt;MULTIPOLYGON [°]&gt;\n1         1 Aleutian Islands         3 (((-171.1345 52.44974, -171.1686 52.4174…\n2         2 Arctic                   4 (((-139.9552 68.70597, -139.9893 68.7051…\n3         3 Bristol Bay              3 (((-159.8745 58.62778, -159.8654 58.6137…\n4         4 Chignik                  3 (((-155.8282 55.84638, -155.8049 55.8655…\n5         5 Copper River             2 (((-143.8874 59.93931, -143.9165 59.9403…\n6         6 Kodiak                   3 (((-151.9997 58.83077, -152.0358 58.8271…\n\nglimpse(ak_regions)\n\nRows: 13\nColumns: 4\n$ region_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n$ region    &lt;chr&gt; \"Aleutian Islands\", \"Arctic\", \"Bristol Bay\", \"Chignik\", \"Cop…\n$ mgmt_area &lt;dbl&gt; 3, 4, 3, 3, 2, 3, 4, 4, 2, 4, 2, 1, 4\n$ geometry  &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-171.1345 5..., MULTIPOLYGON (((-139.9552 6.…\n\n\n\n3.3.1 Coordinate Reference System (CRS)\n\n\n\nSource: ESRI\n\n\nEvery sf object needs a coordinate reference system (or crs) defined in order to work with it correctly. A coordinate reference system contains both a datum and a projection. The datum is how you georeference your points (in 3 dimensions!) onto a spheroid. The projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. All coordinate reference systems require a datum. However, some coordinate reference systems are “unprojected” (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984.\nESRI has a blog post that explains these concepts in more detail with very helpful diagrams and examples.\nYou can view what crs is set by using the function st_crs().\n\nst_crs(ak_regions)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nThis is pretty confusing looking. Without getting into the details, that long string says that this data has a geographic coordinate system (WGS84) with no projection. A convenient way to reference crs quickly is by using the EPSG code, a number that represents a standard projection and datum. You can check out a list of (lots!) of EPSG codes here.\nWe will use multiple EPSG codes in this lesson. Here they are, along with their more readable names:\n\n3338: Alaska Albers (projected CRS)\n4326: WGS84 (World Geodetic System 1984), used in GPS (unprojected CRS)\n3857: Pseudo-Mercator, used in Google Maps, OpenStreetMap, Bing, ArcGIS, ESRI (projected CRS)\n\nYou will often need to transform your geospatial data from one coordinate system to another. The st_transform() function does this quickly for us. You may have noticed the maps above looked wonky because of the dateline. We might want to set a different projection for this data so it plots nicer. A good one for Alaska is called the Alaska Albers projection, with an EPSG code of 3338.\n\nak_regions_3338 &lt;- ak_regions %&gt;%\n    st_transform(crs = 3338)\n\nst_crs(ak_regions_3338)\n\nCoordinate Reference System:\n  User input: EPSG:3338 \n  wkt:\nPROJCRS[\"NAD83 / Alaska Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Alaska Albers (meters)\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-154,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",55,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",65,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Topographic mapping (small scale).\"],\n        AREA[\"United States (USA) - Alaska.\"],\n        BBOX[51.3,172.42,71.4,-129.99]],\n    ID[\"EPSG\",3338]]\n\n\n\nplot(ak_regions_3338)\n\n\n\n\nMuch better!"
  },
  {
    "objectID": "session_03.html#sf-the-tidyverse",
    "href": "session_03.html#sf-the-tidyverse",
    "title": "3  Working with Spatial Data in R",
    "section": "3.4 sf & the Tidyverse",
    "text": "3.4 sf & the Tidyverse\nsf objects can be used as a regular data.frame object in many operations. We already saw the results of plot() and head().\nSince sf objects are data.frames, they play nicely with packages in the tidyverse. Here are a couple of simple examples:\n\n3.4.1 select()\n\n# returns the names of all the columns in dataset\ncolnames(ak_regions_3338)\n\n[1] \"region_id\" \"region\"    \"mgmt_area\" \"geometry\" \n\n\n\nak_regions_3338 %&gt;%\n    select(region)\n\nSimple feature collection with 13 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -2175328 ymin: 405653 xmax: 1579226 ymax: 2383770\nProjected CRS: NAD83 / Alaska Albers\n# A tibble: 13 × 2\n   region                                                               geometry\n   &lt;chr&gt;                                                      &lt;MULTIPOLYGON [m]&gt;\n 1 Aleutian Islands     (((-1156666 420855.1, -1159837 417990.3, -1161898 41694…\n 2 Arctic               (((571289.9 2143072, 569941.5 2142691, 569158.2 2142146…\n 3 Bristol Bay          (((-339688.6 973904.9, -339302 972297.3, -339229.2 9710…\n 4 Chignik              (((-114381.9 649966.8, -112866.8 652065.8, -108836.8 65…\n 5 Copper River         (((561012 1148301, 559393.7 1148169, 557797.7 1148492, …\n 6 Kodiak               (((115112.5 983293, 113051.3 982825.9, 110801.3 983211.…\n 7 Kotzebue             (((-678815.3 1819519, -677555.2 1820698, -675557.8 1821…\n 8 Kuskokwim            (((-1030125 1281198, -1029858 1282333, -1028980 1284032…\n 9 Cook Inlet           (((35214.98 1002457, 36660.3 1002038, 36953.11 1001186,…\n10 Norton Sound         (((-848357 1636692, -846510 1635203, -840513.7 1632225,…\n11 Prince William Sound (((426007.1 1087250, 426562.5 1088591, 427711.6 1089991…\n12 Southeast            (((1287777 744574.1, 1290183 745970.8, 1292940 746262.7…\n13 Yukon                (((-375318 1473998, -373723.9 1473487, -373064.8 147393…\n\n\nNote the sticky geometry column! The geometry column will stay with your sf object even if it is not called explicitly.\n\n\n3.4.2 filter()\n\nunique(ak_regions_3338$region)\n\n [1] \"Aleutian Islands\"     \"Arctic\"               \"Bristol Bay\"         \n [4] \"Chignik\"              \"Copper River\"         \"Kodiak\"              \n [7] \"Kotzebue\"             \"Kuskokwim\"            \"Cook Inlet\"          \n[10] \"Norton Sound\"         \"Prince William Sound\" \"Southeast\"           \n[13] \"Yukon\"               \n\n\n\nak_regions_3338 %&gt;%\n    filter(region == \"Southeast\")\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 559475.7 ymin: 722450 xmax: 1579226 ymax: 1410576\nProjected CRS: NAD83 / Alaska Albers\n# A tibble: 1 × 4\n  region_id region    mgmt_area                                         geometry\n*     &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;                               &lt;MULTIPOLYGON [m]&gt;\n1        12 Southeast         1 (((1287777 744574.1, 1290183 745970.8, 1292940 …"
  },
  {
    "objectID": "session_03.html#spatial-joins",
    "href": "session_03.html#spatial-joins",
    "title": "3  Working with Spatial Data in R",
    "section": "3.5 Spatial Joins",
    "text": "3.5 Spatial Joins\nYou can also use the sf package to create spatial joins, useful for when you want to utilize two datasets together.\n\n\n\n\n\n\nExercise: How many people live in each of these Alaska regions?\n\n\n\nWe have some population data, but it gives the population by city, not by region. To determine the population per region we will need to:\n\nRead in the population data from a csv and turn it into an sf object\nUse a spatial join (st_join()) to assign each city to a region\nUse group_by() and summarize() to calculate the total population by region\nSave the spatial object you created using write_sf()\n\n\n\n1. Read in alaska_population.csv using read.csv()\n\n# read in population data\npop &lt;- read_csv(\"data/alaska_population.csv\")\n\nTurn pop into a spatial object\nThe st_join() function is a spatial left join. The arguments for both the left and right tables are objects of class sf which means we will first need to turn our population data.frame with latitude and longitude coordinates into an sf object.\nWe can do this easily using the st_as_sf() function, which takes as arguments the coordinates and the crs. The remove = F specification here ensures that when we create our geometry column, we retain our original lat lng columns, which we will need later for plotting. Although it isn’t said anywhere explicitly in the file, let’s assume that the coordinate system used to reference the latitude longitude coordinates is WGS84, which has a crs number of 4326.\n\npop_4326 &lt;- st_as_sf(pop,\n                     coords = c('lng', 'lat'),\n                     crs = 4326,\n                     remove = F)\n\nhead(pop_4326)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -176.6581 ymin: 51.88 xmax: -154.1703 ymax: 62.68889\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 6\n   year city       lat   lng population             geometry\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;          &lt;POINT [°]&gt;\n1  2015 Adak      51.9 -177.        122    (-176.6581 51.88)\n2  2015 Akhiok    56.9 -154.         84 (-154.1703 56.94556)\n3  2015 Akiachak  60.9 -161.        562 (-161.4314 60.90944)\n4  2015 Akiak     60.9 -161.        399 (-161.2139 60.91222)\n5  2015 Akutan    54.1 -166.        899 (-165.7731 54.13556)\n6  2015 Alakanuk  62.7 -165.        777 (-164.6153 62.68889)\n\n\n2. Join population data with Alaska regions data using st_join()\nNow we can do our spatial join! You can specify what geometry function the join uses (st_intersects, st_within, st_crosses, st_is_within_distance…) in the join argument. The geometry function you use will depend on what kind of operation you want to do, and the geometries of your shapefiles.\nIn this case, we want to find what region each city falls within, so we will use st_within.\n\npop_joined &lt;- st_join(pop_4326, ak_regions_3338, join = st_within)\n\nThis gives an error!\nError: st_crs(x) == st_crs(y) is not TRUE\nTurns out, this won’t work right now because our coordinate reference systems are not the same. Luckily, this is easily resolved using st_transform(), and projecting our population object into Alaska Albers.\n\npop_3338 &lt;- st_transform(pop_4326, crs = 3338)\n\n\npop_joined &lt;- st_join(pop_3338, ak_regions_3338, join = st_within)\n\nhead(pop_joined)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -1537925 ymin: 472626.9 xmax: -10340.71 ymax: 1456223\nProjected CRS: NAD83 / Alaska Albers\n# A tibble: 6 × 9\n   year city     lat   lng population             geometry region_id region     \n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;          &lt;POINT [m]&gt;     &lt;int&gt; &lt;chr&gt;      \n1  2015 Adak    51.9 -177.        122  (-1537925 472626.9)         1 Aleutian I…\n2  2015 Akhiok  56.9 -154.         84 (-10340.71 770998.4)         6 Kodiak     \n3  2015 Akiac…  60.9 -161.        562  (-400885.5 1236460)         8 Kuskokwim  \n4  2015 Akiak   60.9 -161.        399  (-389165.7 1235475)         8 Kuskokwim  \n5  2015 Akutan  54.1 -166.        899 (-766425.7 526057.8)         1 Aleutian I…\n6  2015 Alaka…  62.7 -165.        777  (-539724.9 1456223)        13 Yukon      \n# ℹ 1 more variable: mgmt_area &lt;dbl&gt;\n\n\n\n\n\n\n\n\nExploring types of joins\n\n\n\nThere are many different types of joins you can do with geospatial data. Examine the help page for these joins (?st_within() will get you there). What other joins types might be appropriate for examining the relationship between points and polygyons? What about two sets of polygons?\n\n\n3. Calculate the total population by region using group_by() and summarize()\nNext we compute the total population for each region. In this case, we want to do a group_by() and summarise() as this were a regular data.frame. Otherwise all of our point geometries would be included in the aggregation, which is not what we want. Our goal is just to get the total population by region. We remove the sticky geometry using as.data.frame(), on the advice of the sf::tidyverse help page.\n\npop_region &lt;- pop_joined %&gt;%\n    as.data.frame() %&gt;%\n    group_by(region) %&gt;%\n    summarise(total_pop = sum(population))\n\nhead(pop_region)\n\n# A tibble: 6 × 2\n  region           total_pop\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Aleutian Islands      8840\n2 Arctic                8419\n3 Bristol Bay           6947\n4 Chignik                311\n5 Cook Inlet          408254\n6 Copper River          2294\n\n\nAnd use a regular left_join() to get the information back to the Alaska region shapefile. Note that we need this step in order to regain our region geometries so that we can make some maps.\n\npop_region_3338 &lt;- left_join(ak_regions_3338, pop_region, by = \"region\")\n\n# plot to check\nplot(pop_region_3338[\"total_pop\"])\n\n\n\n\nSo far, we have learned how to use sf and dplyr to use a spatial join on two datasets and calculate a summary metric from the result of that join.\n\n\n\n\n\n\nsf and tidyverse best practices\n\n\n\nThe group_by() and summarize() functions can also be used on sf objects to summarize within a dataset and combine geometries. Many of the tidyverse functions have methods specific for sf objects, some of which have additional arguments that wouldn’t be relevant to the data.frame methods. You can run ?sf::tidyverse to get documentation on the tidyverse sf methods.\n\n\nSay we want to calculate the population by Alaska management area, as opposed to region.\n\npop_mgmt_338 &lt;- pop_region_3338 %&gt;%\n    group_by(mgmt_area) %&gt;%\n    summarize(total_pop = sum(total_pop))\n\nplot(pop_mgmt_338[\"total_pop\"])\n\n\n\n\nNotice that the region geometries were combined into a single polygon for each management area.\nIf we don’t want to combine geometries, we can specify do_union = F as an argument.\n\npop_mgmt_3338 &lt;- pop_region_3338 %&gt;%\n    group_by(mgmt_area) %&gt;%\n    summarize(total_pop = sum(total_pop), do_union = F)\n\nplot(pop_mgmt_3338[\"total_pop\"])\n\n\n\n\n4. Save the spatial object to a new file using write_sf()\nSave the spatial object to disk using write_sf() and specifying the filename. Writing your file with the extension .shp will assume an ESRI driver driver, but there are many other format options available.\n\nwrite_sf(pop_region_3338, \"data/ak_regions_population.shp\")\n\n\n3.5.1 Visualize with ggplot\nggplot2 now has integrated functionality to plot sf objects using geom_sf().\nWe can plot sf objects just like regular data.frames using geom_sf.\n\nggplot(pop_region_3338) +\n    geom_sf(aes(fill = total_pop)) +\n    labs(fill = \"Total Population\") +\n    scale_fill_continuous(low = \"khaki\",\n                          high =  \"firebrick\",\n                          labels = comma) +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\nDefining x and y axis lables\n\n\n\nOne way to personalize the sequence and breaks in the x and y axis in ggplot is using the functions scale_x_countinuous() or scale_y_continuous(). See example below:\n\nggplot(pop_region_3338)+\n  geom_sf(aes(fill = total_pop))+\n  labs(fill = \"Total Population\")+\n  scale_fill_continuous(low = \"khaki\",\n                        high = \"firebrick\",\n                        labels = comma)+\n  theme_bw()+\n  scale_y_continuous(breaks = seq(40, 66, 2))+\n  scale_x_continuous(breaks = seq(-180, -130, 10))\n\n\n\n\n\n\nWe can also plot multiple shapefiles in the same plot. Say if we want to visualize rivers in Alaska, in addition to the location of communities, since many communities in Alaska are on rivers. We can read in a rivers shapefile, doublecheck the crs to make sure it is what we need, and then plot all three shapefiles - the regional population (polygons), the locations of cities (points), and the rivers (linestrings).\n\n\nCoordinate Reference System:\n  User input: Albers \n  wkt:\nPROJCRS[\"Albers\",\n    BASEGEOGCRS[\"GCS_GRS 1980(IUGG, 1980)\",\n        DATUM[\"D_unknown\",\n            ELLIPSOID[\"GRS80\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1,\n                    ID[\"EPSG\",9001]]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",50,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-154,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",55,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",65,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nrivers_3338 &lt;- read_sf(\"data/ak_rivers_simp.shp\")\nst_crs(rivers_3338)\n\nNote that although no EPSG code is set explicitly, with some sluething we can determine that this is EPSG:3338. This site is helpful for looking up EPSG codes.\n\nggplot() +\n    geom_sf(data = pop_region_3338, aes(fill = total_pop)) +\n    geom_sf(data = pop_3338, size = 0.5) +\n    geom_sf(data = rivers_3338,\n            aes(linewidth = StrOrder)) +\n    scale_linewidth(range = c(0.05, 0.5), guide = \"none\") +\n    labs(title = \"Total Population by Alaska Region\",\n         fill = \"Total Population\") +\n    scale_fill_continuous(low = \"khaki\",\n                          high =  \"firebrick\",\n                          labels = comma) +\n    theme_bw()"
  },
  {
    "objectID": "session_03.html#incorporate-base-maps-into-static-maps-using-ggmap",
    "href": "session_03.html#incorporate-base-maps-into-static-maps-using-ggmap",
    "title": "3  Working with Spatial Data in R",
    "section": "3.6 Incorporate base maps into static maps using ggmap",
    "text": "3.6 Incorporate base maps into static maps using ggmap\nThe ggmap package has some functions that can render base maps (as raster objects) from open tile servers like Google Maps, Stamen, OpenStreetMap, and others.\nWe’ll need to transform our shapefile with population data by community to EPSG:3857 which is the crs used for rendering maps in Google Maps, Stamen, and OpenStreetMap, among others.\n\npop_3857 &lt;- pop_3338 %&gt;%\n    st_transform(crs = 3857)\n\nNext, let’s grab a base map from the Stamen map tile server covering the region of interest. First we include a function that transforms the bounding box (which starts in EPSG:4326) to also be in the EPSG:3857 CRS, which is the projection that the map raster is returned in from Stamen. This is an issue with ggmap described in more detail here\n\n# Define a function to fix the bbox to be in EPSG:3857\n# See https://github.com/dkahle/ggmap/issues/160#issuecomment-397055208\nggmap_bbox_to_3857 &lt;- function(map) {\n    if (!inherits(map, \"ggmap\"))\n        stop(\"map must be a ggmap object\")\n    # Extract the bounding box (in lat/lon) from the ggmap to a numeric vector,\n    # and set the names to what sf::st_bbox expects:\n    map_bbox &lt;- setNames(unlist(attr(map, \"bb\")),\n                         c(\"ymin\", \"xmin\", \"ymax\", \"xmax\"))\n    \n    # Coonvert the bbox to an sf polygon, transform it to 3857,\n    # and convert back to a bbox (convoluted, but it works)\n    bbox_3857 &lt;-\n        st_bbox(st_transform(st_as_sfc(st_bbox(map_bbox, crs = 4326)), 3857))\n    \n    # Overwrite the bbox of the ggmap object with the transformed coordinates\n    attr(map, \"bb\")$ll.lat &lt;- bbox_3857[\"ymin\"]\n    attr(map, \"bb\")$ll.lon &lt;- bbox_3857[\"xmin\"]\n    attr(map, \"bb\")$ur.lat &lt;- bbox_3857[\"ymax\"]\n    attr(map, \"bb\")$ur.lon &lt;- bbox_3857[\"xmax\"]\n    map\n}\n\nNext, we define the bounding box of interest, and use get_stamenmap() to get the basemap. Then we run our function defined above on the result of the get_stamenmap() call.\n\nbbox &lt;- c(-170, 52,-130, 64) # this is roughly southern Alaska\nak_map &lt;- get_stamenmap(bbox, zoom = 4) # get base map\nak_map_3857 &lt;- ggmap_bbox_to_3857(ak_map) # fix the bbox to be in EPSG:3857\n\nFinally, plot both the base raster map with the population data overlayed, which is easy now that everything is in the same projection (3857):\n\nggmap(ak_map_3857) +\n    geom_sf(data = pop_3857,\n            aes(color = population),\n            inherit.aes = F) +\n    scale_color_continuous(low = \"khaki\",\n                           high =  \"firebrick\",\n                           labels = comma)"
  },
  {
    "objectID": "session_03.html#visualize-sf-objects-with-leaflet",
    "href": "session_03.html#visualize-sf-objects-with-leaflet",
    "title": "3  Working with Spatial Data in R",
    "section": "3.7 Visualize sf objects with leaflet",
    "text": "3.7 Visualize sf objects with leaflet\nWe can also make an interactive map from our data above using leaflet.\nleaflet (unlike ggplot) will project data for you. The catch is that you have to give it both a projection (like Alaska Albers), and that your shapefile must use a geographic coordinate system. This means that we need to use our shapefile with the 4326 EPSG code. Remember you can always check what crs you have set using st_crs.\nHere we define a leaflet projection for Alaska Albers, and save it as a variable to use later.\n\nepsg3338 &lt;- leaflet::leafletCRS(\n    crsClass = \"L.Proj.CRS\",\n    code = \"EPSG:3338\",\n    proj4def =  \"+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\",\n    resolutions = 2 ^ (16:7)\n)\n\nYou might notice that this looks familiar! The syntax is a bit different, but most of this information is also contained within the crs of our shapefile:\n\nst_crs(pop_region_3338)\n\nCoordinate Reference System:\n  User input: EPSG:3338 \n  wkt:\nPROJCRS[\"NAD83 / Alaska Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Alaska Albers (meters)\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-154,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",55,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",65,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Topographic mapping (small scale).\"],\n        AREA[\"United States (USA) - Alaska.\"],\n        BBOX[51.3,172.42,71.4,-129.99]],\n    ID[\"EPSG\",3338]]\n\n\nSince leaflet requires that we use an unprojected coordinate system, let’s use st_transform() yet again to get back to WGS84.\n\npop_region_4326 &lt;- pop_region_3338 %&gt;% st_transform(crs = 4326)\n\n\nm &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;%\n    addPolygons(data = pop_region_4326,\n                fillColor = \"gray\",\n                weight = 1)\n\nm\n\n\n\n\n\nWe can add labels, legends, and a color scale.\n\npal &lt;- colorNumeric(palette = \"Reds\", domain = pop_region_4326$total_pop)\n\nm &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;%\n    addPolygons(\n        data = pop_region_4326,\n        fillColor = ~ pal(total_pop),\n        weight = 1,\n        color = \"black\",\n        fillOpacity = 1,\n        label = ~ region\n    ) %&gt;%\n    addLegend(\n        position = \"bottomleft\",\n        pal = pal,\n        values = range(pop_region_4326$total_pop),\n        title = \"Total Population\"\n    )\n\nm\n\n\n\n\n\nWe can also add the individual communities, with popup labels showing their population, on top of that!\n\npal &lt;- colorNumeric(palette = \"Reds\", domain = pop_region_4326$total_pop)\n\nm &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;%\n    addPolygons(\n        data = pop_region_4326,\n        fillColor = ~ pal(total_pop),\n        weight = 1,\n        color = \"black\",\n        fillOpacity = 1\n    ) %&gt;%\n    addCircleMarkers(\n        data = pop_4326,\n        lat = ~ lat,\n        lng = ~ lng,\n        radius = ~ log(population / 500),\n        # arbitrary scaling\n        fillColor = \"gray\",\n        fillOpacity = 1,\n        weight = 0.25,\n        color = \"black\",\n        label = ~ paste0(pop_4326$city, \", population \", comma(pop_4326$population))\n    ) %&gt;%\n    addLegend(\n        position = \"bottomleft\",\n        pal = pal,\n        values = range(pop_region_4326$total_pop),\n        title = \"Total Population\"\n    )\n\nm"
  },
  {
    "objectID": "session_03.html#more-spatial-resources",
    "href": "session_03.html#more-spatial-resources",
    "title": "3  Working with Spatial Data in R",
    "section": "3.8 More Spatial Resources",
    "text": "3.8 More Spatial Resources\nThere is a lot more functionality to sf including the ability to intersect polygons, calculate distance, create a buffer, and more. Here are some more great resources and tutorials for a deeper dive into this great package:\n\nRaster analysis in R\n\nSpatial analysis in R with the sf package\n\nIntro to Spatial Analysis\n\nsf github repo\n\nTidy spatial data in R: using dplyr, tidyr, and ggplot2 with sf\n\nmapping-fall-foliage-with-sf"
  },
  {
    "objectID": "session_04.html#introduction",
    "href": "session_04.html#introduction",
    "title": "Managing Dependencies in R projects",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nEnsuring research reproducibility extends beyond data and code sharing; it encompasses the entire computational setup used in the research. Understanding the causes of issues associated with computing environment dependencies and learning how to overcome them is crucial to achieving open science. Therefore, in this section, we will:\n\nIdentify the root causes of “dependency hell” in computational research.\nDiscuss the significance of enhancing reproducibility in R-based projects.\nOutline practices to improve the reproducibility of R-based projects.\nImplement effective strategies to mitigate dependency issues.\n\n\n4.1.1 What is a dependency?\nA dependency represents a directional connection between two or more elements, indicating a logical or sequential relationship among them. If any of these elements fail, it poses a risk to both the efficiency of the process and the intended outcome.\n\n\n4.1.2 Dependencies in Computational Research\nReusing code for reproducing results and visualizations isn’t always a straightforward ’plug-and-play activity. Complexity arises from the intricate web of dependencies inherent in computational research. These dependencies encompass not only the code itself but also packages, data sources, environment configurations, and nuanced interconnections between these components.\nThink of dependencies as building blocks in a towering structure, akin to a complex and intricate skyscraper. Each block represents a crucial element contributing to the overall stability and functionality of the structure. In this metaphor, the foundation of the skyscraper represents the core components, such as programming languages and operating systems. As you ascend the structure, each floor corresponds to a layer of dependencies, with one layer relying on the stability and support of the layers beneath it. If one block is weak or missing, it can compromise the integrity of the entire building. Therefore, managing dependencies becomes critical for open science.\n\n\n\nDependency Fail. Google Images.\n\n\nDependencies can generally be classified as development or runtime. While the former is more of a concern for software developers and beyond the scope of this section, runtime dependencies directly affect researchers and their ability to perform computations to advance or inspect research findings. Runtime dependencies represent components that applications rely on during execution, essential for proper software functionality.\nIn computational research, dependency refers to any external resource, software, or component necessary for a program, algorithm, or analysis to function correctly. Dependencies encompass libraries/packages, specific software versions, and even hardware configurations needed for executing computational tasks or software applications.\nTypes of dependencies encountered in R-based projects encompass different layers, including:\n\nOperating system: The computer’s operating system.\nSystem configurations: This includes library locations and the search path used to find files and libraries.\nSystem-level libraries: Non-R libraries or R packages might depend on are listed as System Requirements in R package DESCRIPTION files.\nR and R packages: Such as the version of R being used, along with packages like ggplot2 and dplyr.\n\nQuite a list, isn’t it? Each project has its own set of dependencies, making accurate documentation essential! When someone attempts to reproduce your analysis on a different computer, any variation in these elements from your own system during your last analysis could hinder their ability to replicate your work. This discrepancy undermines the principles and motivations behind openly sharing data and code.\n\n\n\nCreated with imgflip.com\n\n\nDon’t panic! We’ll cover some recommended practices to mitigate this issue in a bit.\n\n\n\n\n\n\nTip\n\n\n\nAdding package dependencies\nWhen installing packages and not using the GUI in RStudio/Posit, it’s a good practice to include the following syntax in your command:\ninstall.packages(\"package name\", dependencies = TRUE)\nThis ensures that all required dependencies associated with your installed package will be included.\n\n\n\n\n\n\n\n\nTip\n\n\n\nOther recommendations for installing/updating packages:\nAt any point in the console, R may ask if you want to “update any old packages by asking,” Update all/some/none? [a/s/n]:“. If you see this, type”a” at the prompt and hit Enter to update any old packages. Updating packages can sometimes take a while to run. If you are short on time, you can choose “n” and proceed. Without updates, you run the risk of conflicts between your old packages and the ones from your updated R version later down the road.\nIf you see a message in your console along the lines of “binary version available, but the source version is later”, followed by a question, “Do you want to install from sources the package which needs compilation? y/n”, type n for no, and hit enter.\n\n\n\n\n4.1.3 “Dependency Hell” Exemplified\n“Dependency hell” is a term frequently used to describe a prevalent issue in code reproducibility. It occurs when a researcher tries to execute code that relies on external libraries, packages, or specific software versions. These dependencies can further branch into their own dependencies, forming a complex network of interconnected elements.\nR relies on an extensive library of packages, a major benefit of offering diverse functionalities without users needing to create functions from scratch. Yet, harnessing these packages can present challenges. Starting the process by scripting your analysis is a positive step, but it doesn’t guarantee that you or someone else can accurately replicate your work in the future.\nOver time, these libraries, packages, and software may evolve, introducing new versions and additional dependencies or even becoming outdated and unsupported. This evolution can lead to script failures and irreproducible research outcomes.\nTo illustrate this problem, consider Ruby and Avi’s example. Despite using identical code and data, the software packages installed on their computers have different versions. Ruby’s scripts may break or yield different results in Avi’s computing environment.\n\n\n\nImage CC BY, retrieved from Reproducibility In Cancer Informatics\n\n\nDiverse computing setups aren’t just complex to disentangle; they also influence research outcomes. When someone else attempts to reproduce your analysis on a different computer, any variation in these elements from what existed on your system during your last analysis may prevent them from replicating your work. This issue is more common than perceived and isn’t limited to involving a second person, project, or different computer. Reproducibility issues can also arise in your own projects after system upgrades, as noted below:\n\n\n\nReddit - R/Statistics (2018)\n\n\nVery frustrating, isn’t it? While it’s challenging to draw firm conclusions about the causes for those discrepancies based on the limited information shared, most responses from the community suggest adopting reproducibility practices and verifying package updates as a first step. Before we delve into alternatives to circumvent dependency issues and enhance reproducibility in R-based projects, let’s first recap how the R system operates in handling libraries, projects, and their packages.\n\n\n\n\n\n\nWarning\n\n\n\nCaution about updates\nR packages aren’t guaranteed to be compatible across minor R versions. Therefore, it’s generally a good idea to reinstall all packages from scratch when upgrading from one version to another (e.g., R 3.5 to 3.6). Only patch version upgrades maintain backward compatibility.\n\n\nCheck it out our handout on this topic.\n\n\n\nSource: UCSB Library Research Data Services - Data Literacy Series, 11/2023)\n\n\n\n\n4.1.4 Understanding the R System\nIn R, packages are collections of R functions, data, and compiled code in a well-defined format, created to add specific functionality. The directories in R where the packages are stored are called libraries. The terms “package” and “library” are sometimes used interchangeably and can cause confusion because the library() function is used to load a package. While this terminological issue hasn’t been addressed, let’s understand how libraries work in R and how they impact your projects.\nA repository is a source of packages; install.packages() gets a package from a repository (usually somewhere on the Internet) and puts it in a library (a directory on your computer). The most important repository is The Comprehensive R Archive Network (CRAN), available for installing packages in just about every R session. Other freely available repositories include Bioconductor, the Posit Public Package Manager, and R Universe.\n\n\n\n\n\n\nImportant\n\n\n\nPackages living elsewhere?\nTo download R packages from outside CRAN, you should indicate the repository address as shown below:\ninstall.packages(\"MPAgenomics\", repos = \"http://R-Forge.R-project.org\", dependencies = TRUE)\n\n\nBy default, R utilizes a top-level system library for installing base packages and a user library for the packages installed by individual users. The key difference lies in accessibility: the system-level library is available for all users on your system, whereas the other library is exclusively for your use.\n\n\n\nUser Library\n\n\nCan you identify the potential problem here? It’s very likely that you’ll have multiple projects with unique dependencies but calling packages from the same library. Consequently, when installing a new package version in R, it automatically replaces the previous version in the user library. This library, shared across all projects by default, enforces the latest installed version of a particular package across all projects. Consequently, projects with code relying on different versions of the same package might break.\n\n\n\n\n\n\nTip\n\n\n\nKnow your library path!\nIt’s a good practice to know your library path. You can check the location by running .libPaths() in your console.\nGive it a try! What’s your current library path?"
  },
  {
    "objectID": "session_04.html#strategies-to-mitigate-dependency-issues",
    "href": "session_04.html#strategies-to-mitigate-dependency-issues",
    "title": "Managing Dependencies in R projects",
    "section": "4.2 Strategies to mitigate dependency issues",
    "text": "4.2 Strategies to mitigate dependency issues\nEffectively managing dependencies is crucial to ensure the reproducibility and reliability of research reliant on code. Incorporating packages into our code directly ties the code’s execution to these packages. When sharing our code, others must install the required packages for proper functionality. However, complications arise when code written with an older package version encounters an updated version during installation.\nDifferences in functionality between versions may emerge, posing challenges not just for others using our code but also for those revisiting or transferring code to different environments, such as a year after creation, following R reinstallation, or when changing computing devices.\nPackages undergo continuous maintenance, potentially altering functions or behaving differently from their initial use. Sometimes, code might break due to changes in indirectly linked packages that our used package relies upon, despite not being directly loaded. While keeping software updated is advisable, updating packages can cause unexpected disruptions and considerable frustration.\nTo mitigate these frustrations, adopting dependency management strategies is essential. There are various strategies to address the dependency problem for others and your future self. We’ll describe both minimal and robust approaches to achieve this. Broadly speaking, we recommend:\n\nDocument and share information about your computing environment (i.e., operating system, software, and versions).\nRecord all used packages and their versions.\nEmploy dependency management systems and virtual environments to automate and streamline the aforementioned steps.\nConsider using containers to capture dependencies and offer a consistent runtime environment.\n\nWe’ll delve into practical approaches to adopt these recommendations for R-based projects.\n\n4.2.1 Dependency Management in R\nHaving gained insight into the underlying factors contributing to dependency challenges in computational research, let’s explore methods to implement the aforementioned strategies.\n\n4.2.1.1 Provide a Snapshot of Your Computing Environment with Session Info\nDocumenting your system configuration and all necessary packages with their versions is crucial to assisting others and your future self. Session info offers valuable clues in case results aren’t reproducible. At a minimum, running sessionInfo() and documenting that information in your README within your repository or project folder is recommended. However, this might be less useful for .R scripts (unless you save the function output to a separate file). It also doesn’t provide executable code for others to install the requirements.\n\n\n\n\n\n\nNote\n\n\n\nGet the Session Info for your Project\nOpen your most current project and get your session info if you haven’t yet.\n\n\n\n\n4.2.1.2 Ease the hurdle of dependency installations by including an install.R script\nA slightly improved yet not very robust method of managing R dependencies involves including an executable install.R script containing commands to install all required dependencies. But why might this approach not be ideal?\nAs observed, R’s default library settings can lead to issues. The problem with this approach arises from managing multiple projects at different stages, each with unique dependencies. Every time you start a new project and use the install.packages() function in R, you’ll fetch the latest package version available in CRAN or another repository, reinstall it and potentially replace the package in your environment. This action may break other projects. This is where renv proves advantageous!\n\n\n4.2.1.3 Pack package dependencies with renv\n\nThe renv package (https://rstudio.github.io/renv), short for ‘Reproducible ENVironment,’ aids in setting up R projects and managing dependencies to maintain your project:\n\nIsolated (in a positive sense): Enables adding or updating a package in one project without impacting other projects, achieved by creating separate libraries for each project.\nPortable: Simplifies moving your R projects across computers and platforms, streamlining the installation of necessary packages.\nReproducible: Logs precise package versions you rely on, ensuring consistent installations wherever you work.\n\nrenv establishes a local project library for each project, encapsulating dependencies. This allows easy re-running of results for each project using original package versions. Consequently, you can work on multiple projects without affecting one another and effortlessly share your project and its dependencies for others to reproduce.\n\n\n\nRenv Project Libraries\n\n\nBefore delving into how renv works, it’s important to mention that there are various ways to utilize it—either during the initial project creation or later through RStudio’s GUI or the command line. When creating a new project in RStudio, you can check the box as shown below.\n\n\n\n\n\nIf you happen to overlook that step or collaborate with someone who has not set up renv for the project, you can still do so by selecting Tools &gt; Project Options &gt; Environments:\n\n\n\n\n\nOr check the box to use renv or use the command line:\ninstall.packages(\"renv\")\nlibrary(renv)\nrenv::init()\nBy following any of these approaches, renv establishes your reproducible environment and generates a set of files we will look at in a bit.\n\n\n\n\n\n\nImportant\n\n\n\nYour turn!\nNow that you know the commands, initiate renv for our example project.\n\n\nYou should also see a renv icon showing on the packages tab.\n\n\n\n\n\nMore on RenvFiles\nAfter initiating renv you will notice some new files and directories added to your project.\n\n\n\n\n\n\nA project R profile, .Rprofile. This file is run automatically every time you start R (in that project), and renv uses it to configure your R session to use the project library. It calls the renv/activate.R file. This ensures that once you turn on renv for a project, it stays on until you turn it off.\nrenv/library/* – Folder with many subfolders – contains all packages currently used by your project.\nThe lockfile, renv.lock file in JSON format, which records metadata about every package that can be re-installed on a new machine. This is the key ingredient that makes renv work.\n\n\n\n\n\n\n\nImportant\n\n\n\nYour turn!\nCheck your renv.lock to see how many packages are there.\n\n\nOne key reason to use renv is to facilitate code sharing, ensuring everyone accesses the same package versions as you. Sharing these files allows others to replicate your work. As mentioned earlier, you’ll initiate this process by calling renv::init(). Next, you’ll need to commit renv.lock, .Rprofile, renv/settings.json, and renv/activate.R to version control, ensuring others can replicate your project environment. It’s worth noting that renv also generates a .gitignore file, streamlining the process for Git users and preventing others from importing unnecessary files.\nWhenever someone opens this project on their machine, renv will automatically bootstrap itself, downloading and installing the required renv version. It will also prompt them to download and install all necessary packages.\nWorking with renv\nBesides renv::init(), renv’s workflow include other key functions such as: renv::snapshot(), renv::status() and renv::restore().\n\n\n\nRenv workflow and functions.\n\n\nStep 1: Initiate renv for a new project with renv::init()\nStep 2: Work on your project and add, remove or update packages as needed using the functions below:\n\nrenv::install( ) or install.packages( )\nrenv::remove( ) or remove.packages( )\nrenv::update( ) or update.packages( )\n\n\n\n\n\n\n\nTip\n\n\n\nUpdating Dependencies\nIf you’re making changes to a project that has been collecting some dust, it’s good practice to start with an renv::update() before making any changes to the code.\n\n\nStep 3: Save changes to your project library with renv::snapshot(). By doing so, your renv.lock file will be updated accordingly.\nRestoring an existing project using renv\nEvery time you open a project for which renv has been set up, renv automatically runs and checks that the package versions you have installed on your computer match those of the project. If they match, there is nothing to do. But if there are any mismatches, renv will print a warning resembling the following:\n* Project '~/Desktop/myproject' loaded. [renv 0.16.0]\n* The project library is out of sync with the lockfile.\n* Use `renv::restore()` to install packages recorded in the lockfile.\nIf this happens, simply run renv::restore() from the console pane to download and install the package versions needed to match the project’s requirements. For example, if the project uses tidyverse 1.3.2 and you have an older version tidyverse 1.3.1 installed on your computer, renv will upgrade your RStudio installation to tidyverse 1.3.2. (This works conversely as well: if the project uses an older version of a package you have installed, renv will attempt to download and install the older version for you. Don’t worry about losing the newer version. renv ensures that all versions of all packages remain installed on your computer, available for use by projects as needed).\nYou may also use renv::status() to check for inconsistencies across the project lockfile, library, and dependencies(). In general, you should strive to ensure that status() reports no issues, as this maximizes your chances of successfully restore the project in the future or on another machine.\n\n\n\n\n\n\nImportant\n\n\n\nThere are other packages to help you manage dependencies in R such as Groundhog as described in this lesson. However, not only renv seems to be more widely adopted, but also has the advantage of being integrated with RStudio.\n\n\nDeactivating and Uninstalling renv\nTo deactivate renv in a project, use renv::deactivate(). This removes the renv auto-loader from the project .Rprofile, but doesn’t touch any other renv files used in the project. If you’d like to later re-activate renv, you can do so with renv::activate().\nTo completely remove renv from a project, call renv::deactivate(clean = TRUE). If you later want to use renv for this project, you’ll need to start from scratch with renv::init().\nIf you want to stop using renv for all your projects, you’ll also want to remove renv's global infrastructure with the following R code:\nroot &lt;- renv::paths$root()\nunlink(root, recursive = TRUE)\nYou can then uninstall the renv package with utils::remove.packages(\"renv\").\nLimitations of Renv\n\n\n\nCollaborating in R: renv with different versions of R? Stack Overflow Post\n\n\nRenv is a valuable tool for improving reproducibility in R projects by handling package dependencies. However, it’s important to note that it’s not a universal solution and has its limitations. Renv primarily concentrates on package management, excluding other crucial elements. For instance, while renv tracks R package versions, it doesn’t handle the R version, operating system, or versions of system libraries.\nTake a look at our handout summarizing key points about renv:\n\n\n\nSource: UCSB Library Research Data Services - [Data Literacy Series](https://rcd.ucsb.edu/data-literacy-series), 11/2023)\n\n\n\n\n4.2.1.4 Capture your computational environment with Binder\nSo far, we’ve discussed approaches for managing R package dependencies or documenting and sharing a static snapshot of your computational environment. But what if we aim to streamline the process and ‘ship’ a capsule of our computing environment to mitigate potential dependency issues linked to different operating systems, system libraries, configurations, and R versions?\nOne solution is to use Binder. The mybinder service converts a repository (on GitHub, Dataverse, Zenodo, or Fighsare) into a set of interactive/electronic notebooks or can even initiate RStudio in the cloud as illustrated below:\n\n\n\nCredit: Juliette Taka, Logilab, and the OpenDreamKit project\n\n\nHow to “binderize” your project?\nThe process of providing a runtime computational environment for your R project is simple and a good strategy to mitigate dependency issues.\nBinder supports both R and RStudio. To begin, you need to specify the R version by adding a runtime.txt file at the root directory of your project in your repo:\nr-&lt;version&gt;-&lt;YYYY&gt;-&lt;MM&gt;-&lt;DD&gt;\nE.g., r-4.3.2-2023-03-15\nThen, you should add a install.R script containing the list of R packages that need to be installed in order for your script to run. Here is an example: https://github.com/binder-examples/r\nThen, access https://mybinder.org and enter your repo URL by selecting the desired repo type (we will stick to GitHub. Then click on “launch” and use the link to share with others.\n\n\n\n\n\nCreate a Binder badge for your repo and add it to the README so that it becomes more obvious to potential reusers.\n\nFor example repo: https://github.com/binder-examples/r\n\n\n\n\n\n\nNote\n\n\n\nWant a fancier badge?\nYou can also customize your binder badge here.\n\n\nOther recommendations:\n\nEnsure you have updated all your packages before running your script locally.\nAdd to the README.txt file when your analysis was last performed.\nPatience while your Binder builds! It might take a couple of minutes.\n\nBinder Limitations\n\nNot for heavy computing. The code should take less than 10 minutes to run, and the data should be less than 10 MB. You might need to subset it.\nThe repository should be public.\nThe repository should not require any personal or sensitive information (such as passwords).\n\n\n\n\n\n\n\nNote\n\n\n\nYour turn!\nFollow the steps above and “Binderize” one of your project repos."
  },
  {
    "objectID": "session_04.html#final-considerations",
    "href": "session_04.html#final-considerations",
    "title": "Managing Dependencies in R projects",
    "section": "4.3 Final Considerations",
    "text": "4.3 Final Considerations\nIn summary, we’ve delved into the root causes of “dependency hell” in research using R. We’ve seen how crucial it is to make R-based projects more reproducible, not just for reliability but also to build trust in the scientific community. Throughout, we’ve covered various ways to improve reproducibility, stressing the importance of clear methods and documentation, ranging from more simple to more robust approaches. By learning and applying strategies to tackle dependency issues, researchers pave the way for more reliable, reproducible and accessible science."
  },
  {
    "objectID": "session_05.html#steps-to-check-and-set-your-personal-access-token-pat",
    "href": "session_05.html#steps-to-check-and-set-your-personal-access-token-pat",
    "title": "5  Check and Set your PAT",
    "section": "5.1 Steps to check and set your Personal Access Token (PAT)",
    "text": "5.1 Steps to check and set your Personal Access Token (PAT)\n\nLog in to included-crab\nOpen the Rproj you are working in\nIn the console run: usethis::git_sitrep()\n\n\n\n\n\n\n\nIf your Personal Access Token is , you have to go ahead and reset it following the instructions on how to Set (or reset) your PAT.\n\n\n\n\n\n\n5. Set (or reset) your PAT\n\n\n\n\n\n\nSetting up your PAT\n\n\n\n\nRun usethis::create_github_token() in the Console.\nA new browser window should open up to GitHub, showing all the scopes options. You can review the scopes, but you don’t need to worry about which ones to select this time. This function automatically pre-selects some recommended scopes. Go ahead and scroll to the bottom and click “Generate Token”.\nCopy the generated token.\nBack in RStudio, run gitcreds::gitcreds_set() in the Console.\nPaste your PAT when the prompt asks for it.\nLast thing, run usethis::git_sitrep() in the Console to check your Git configuration and that you’ve successful stored your PAT.\n\n\n\n\nCheck your PAT is .\n\n\nIn the console run usethis::git_sitrep()\nExpect to see this result:"
  }
]